[
  {
    "id": "general-2025-12-30-balance_of_power",
    "title": "Balance of power",
    "date": "2025-12-30",
    "category": "society",
    "url": "https://vitalik.eth.limo/general/2025/12/30/balance_of_power.html",
    "path": "general/2025/12/30/balance_of_power.html",
    "content": "- \n\n- \n\n  \n    \n\n  \n  \n  Dark Mode Toggle\n  \n    \n\n  \n\n##  Balance of power \n\n 2025 Dec 30 \nSee all posts\n\n \n \n\n Balance of power \n\nSpecial thanks to Gabriel Alfour, Audrey Tang and Ahmed Gatnash\nfor feedback and review.\n\nMany of us are afraid of Big Business. We like the\nproducts and services that companies provide, but dislike\ntrillion-dollar monopolistic walled\ngardens,\nvideo games that turn into quasi-gambling,\nand companies manipulating\nentire governments for profit.\n\nMany of us are afraid of Big Government. We like\npolice and courts, public order, and various services, but we dislike\ngovernments arbitrarily picking winners\nand losers,\nrestricting what people can say\nor read\nor think, violating human rights or starting wars.\n\nFinally, many of us are afraid of the third corner of the triangle:\nBig Mob. We like independent civil society,\ncharities and Wikipedia, but we dislike mobs\nlynching people, cancel culture, and things like the later stages of the\nFrench Revolution or the Taiping\nRebellion.\n\nBasically, we like progress - whether in technology, economy or\nculture - but we fear the three historically most powerful generators of\nsuch progress.\n\nA common response to this conundrum is the idea of balance of\npower. If we need powerful forces in society, then they should\nbe balanced - either each force balanced within itself (eg. competition\nbetween companies), or between each other, or ideally both.\n\nHistorically, much of this balance would come automatically: there\nare natural diseconomies of scale that arise as a result of distance, or\nthe need to coordinate very large numbers of people to do global-scale\ntasks. In this century, however, this is an assumption that no longer\nholds true: all of the above forces are getting much stronger at the\nsame time, and can no longer avoid frequently interacting.\n\nIn this post, I will expand on this theme, and suggest some\nstrategies for preserving this increasingly delicate feature of the\nworld going forward.\n\nIn a previous post, I described this emerging world, where \"Big X\nis here to stay for all X\", as \"the\ndense jungle\".\n\n## How we fear big government\n\nThere is a good reason to fear government: a government has a lot of\nguns, and it can use those guns to hurt you. A government has the power\nto ruin you that far exceeds anything Mark Zuckerberg or cryptocurrency\nsellers could do even if they wanted to. For this reason, centuries of\nliberal political theory have focused on the problem of \"taming the\nleviathan\" - getting the benefits of government providing law and order,\nwithout the downsides of a king being able to do whatever he wants to\nhis subjects.\n\nMuch of that theory can be summarized in one sentence: the\ngovernment should act like a game, not like a player. That is:\nas much as possible, the government should be a reliable playing\nfield that productively resolves disputes between people on its\nterritory, and not an agent that actively pushes its own goals.\n\nThere are different versions of this ideal:\n\n- Libertarianism says that the game the government\nimplements should have basically three rules: don't defraud, don't\nsteal, don't kill\n\n- Hayekian liberalism says to avoid\ncentral planning: if you have to intervene in the market,\ndo so by setting goals rather than methods, and leave it to the market\nto figure out the methods\n\n- Civil liberalism emphasizes freedom of speech,\nreligion and association, preventing the government from imposing its\nfavored outcome in cultural and intellectual spheres.\n\n- Rule of law says that the government should act by\npassing laws which specify what can and can't be done, and\nleave it to courts to enforce those laws\n\n- Common law maximalism says to abandon government\nlegislatures entirely: a decentralized system of courts issue rulings on\nindividual cases, and each ruling creates a precedent that nudges the\nlaw a small step in some direction\n\n- The \"separation of powers\" concept says to split\nthe government into multiple parts, where each part is meant to serve as\na check and balance against the other parts\n\n- The subsidiarity\nprinciple states that issues should be dealt with at the\nmost local level that reasonably can deal with them - maximally avoiding\nconcentration of decision-making\n\n- Multipolarity says that, at the very least, we\ndon't want one single country running the whole world. Ideally, we also\nwant two further checks and balances:\n\n- We don't want any single country being overly hegemonic within\nits own neighborhood\n\n- We want multiple backup options to be available to each\nindividual person\n\nEven outside of governments traditionally considered \"liberal\", a\nsimilar idea applies. There has been a recent finding that, among\ngovernments classified as authoritarian, the ones that consistently\ndeliver less economic growth are the ones that are\n\"personalistic\", as compared to those that are \"institutionalized\".\n\nIt is not always possible to avoid the government having\ncharacteristics of a player, particularly because of the possibility of\nexternal conflict: if a player goes to war against a game, the player\nwins. But even when the possibility of the government becoming a player\nis needed, it is often tightly controlled: see the Roman custom of electing a\ndictator, who would have great power for a short term, but then\nreturn to normal once the emergency ends.\n\n## How we fear big business\n\nOne succinct way to split up criticism of corporations is as\nfollows:\n\n- Corporations are bad because they are evil\n\n- Corporations are bad because they are lame\n\nThe first issue (corporations being evil) happens because\ncorporations are just very good optimization machines, and as they get\nmore powerful (both in capability and in size), their goal of maximizing\nprofit diverges more and more from the goal of their users and wider\nsociety. You can often see this pattern in industries that start out\norganic and hobbyist, but then become more and more profit-oriented -\nand at the same time in conflict with their users' interests - over\ntime. Like, say:\n\nLeft: percent of coins directly allocated to insiders in newly\nlaunched cryptocurrencies, ~2009-21. Right: THC concentration in\nmarijuana, ~1970-2020.\n\nYou also see the same pattern in video games: a space that originally\nfocused on fun and fulfillment now increasingly focuses on built-in slot\nmachine mechanisms to maximally extract money from players. Even major\nprediction markets have started to show a worrying tendency to focus not\non pro-social goals like making better news media or improving\ngovernance, but on sports betting.\n\nThose examples come more from increases in capability, coupled with\ncompetitive pressure. There is a different set of examples that come\nfrom increases in size. In general, as a corporation gets larger, it\ngains more ability to benefit from bending its surrounding environment\n(including economy, politics and culture) to its will. A company that is\n10x larger will benefit 10x more from bending its environment to a\nparticular degree, and so it will perform such actions at all more often\nthan a smaller company - and when it does, it will do so with 10x the\nresources.\n\nMathematically, this is the same argument as why monopolists price above marginal\ncost and increase their profit at the expense of societal deadweight\nloss: in that case, the \"environment\" is the market price, and\nmonopolists are \"bending the environment\" by restricting the quantity\nthey sell. How much bending you can do is proportional to your market\nshare. But expressed in these more general terms, it's a powerful\nargument that applies in a wide variety of situations (eg. corporate\nlobbying, De\nBeers style cultural manipulation campaigns...).\n\nThe second issue (corporations being lame) involves corporations\nbeing boring and sterile and risk-averse, and creating outcomes that are\nhomogeneous on very large scales - both within individual corporations\nand between them.\n\nArchitectural monoculture is one archetypal form of corporate\nlameness\n\nThe word \"soulless\" is interesting because it has a connotation that\nis halfway between \"evil\" and \"lame\". \"Soulless\" feels like a natural\nway to describe corporations addicting people for clicks or creating\ncartels to raise prices or polluting rivers, but it also feels\nlike a natural way to describe corporations making every city in the\nworld look exactly the same, or making ten Hollywood movies that are\nexact replicas of each other, or...\n\nI would argue that these two types of soullessness both come from two\nfactors: commonality of motive, and commonality\nof agency. Corporations are all highly motivated by the profit\nmotive, and many powerful actors with the same strong motivation will\ninevitably pull in the same direction, without strong countervailing\nforces to pull in other directions.\n\nCommonality of agency comes from a company being large, which gives\nit added incentive to shape its environment. One $1 billion company will\ndo much more environment-shaping than a hundred $10 million companies.\nIt also creates more homogeneity: Starbucks adds far more to\nany \"feeling of urban homogeneity\" than a hundred of its 100x-smaller\ncompetitors put together.\n\nInvestors can amplify both dynamics. While a (non-sociopathic)\nstartup founder would be happier if their company grows to $1b and helps\nthe world than if it grows to $50b and breaks it ($49b of yachts and\nplanes are NOT worth the world hating you), investors are much further\nremoved from non-financial consequences of their decisions. As the\nmarket becomes competitive, investors willing to seek out the $50b get\nhigher returns, and the ones who are content to only take $1b get lower\n(if not negative) returns and have a hard time attracting capital.\nAdditionally, investors who have shares in many portfolio companies can\noften passively encourage those companies to operate at least partially\nlike a merged\ncollective super-agent. An important limiting factor to both\ndynamics is the limit to investors' ability to surveil what is\ngoing on inside their portfolio companies and \"hold people\naccountable\".\n\nMeanwhile, market competition addresses commonality of agency, but it\nonly addresses commonality of motive to the extent that the different\ncompetitors have different, not-just-profit-seeking, motives. Often,\nthey do: companies frequently sacrifice on profits in the name of\nreleasing innovations openly to the public, or upholding deeply held\nvalues, or aesthetics. But this is not guaranteed.\n\nIf commonality of motive and commonality of agency create\nsoullessness, then what is \"soul\"? I would argue that the\ndefinition of \"soul\" here is nothing other than pluralism: in\nthis context, it is the set of things in corporations that is not\nhomogeneous between them.\n\n## How we fear Big Mob\n\nWhen people talk positively about \"civil society\" - the part of\nsociety that is not profit-motivated and is not government - they always\ntalk about it as being made up of a huge number of independent\ninstitutions that are all doing different things. When I ask AI to\nexplain \"civil society\" to me, it gives these kinds of examples:\n\nWhen people talk negatively about populism, they tend to\nenvision the opposite: a single charismatic leader who is able to arouse\nmillions of people to directly listen to them and join in a giant mob\nall pursuing one single goal. Populism is about the \"common people\", but\nmore specifically it is about a fiction that the common people are\nunited - and often, united in support of a leader and in\nopposition to a hated outgroup.\n\nWhen people do criticize civil society, the argument is invariably\nthat it's failing at its mission to be \"a huge number of independent\ninstitutions that are all doing different things\", and is instead\ndriving some emergent common agenda, eg. \"The\nCathedral\".\n\n## Balance between forces\n\nIn all of the cases above, we talked about balance of power within\neach of the three big \"forces\". But we can also have balance of power\nbetween forces. A major example is the balance of power between\ngovernment and business.\n\nCapitalist democracy can be described as a theory of balance of power\nbetween Big Government and Big Business: entrepreneurs both get legal\ntools to challenge aggressive government action, and get a concentration\nof capital with which they can act independently, but at the same time\ngovernments can regulate corporations.\n\nPalladium-ism lionizes\nbillionaires, but specifically when they go off and do unconventional\ncrazy things in pursuit of their own detailed visions, rather than\ndirectly seeking profit (see eg. [1] [2] [3]). In\nthis way one can view Palladium-ism as an attempt to thread the needle\nand get the best parts of capitalism without the worst.\n\nAlthough both were crucial in setting the conditions to enable\nit, ultimately Starship was created neither by profit motive nor by\ngovernment mandate.\n\nMy own views toward philanthropy are similar to Palladium-ism in some\nways. I have said many things that stridently\nsupport billionaire philanthropy,\nand I want\nmore of\nit. But the type of it that I want is the type that counterbalances\nother forces in society. Markets are often not willing to fund public\ngoods, governments are often not willing to fund things that are not yet\nelite consensus, or things whose beneficiaries are not concentrated in\nany single countries. Some things are in both categories at the same\ntime, and so get passed over by both. Wealthy individuals can fill the\ngap.\n\nBut there is a way in which billionaire philanthropy can become\nharmful: when it stops being a counterbalance to government, and instead\ntakes over the government. This has happened in Silicon Valley in the\nlast few years: powerful tech CEOs and VCs have become much less\nlibertarian and supportive of \"exit\",\nand more engaged in directly pushing the government to align with their\npreferred ends - and in exchange, making the world's most powerful\ngovernments even more powerful.\n\nI much prefer the thing on the left (2013) to the thing on the\nright (2025), because the thing on the left is an expression of balance\nof power, whereas the thing on the right represents two extremely\npowerful factions who should be balancing each other\ninstead merging together.\n\nYou can also have balance of power between the other two forces in\nthe triangle. The Enlightenment-era idea of the Fourth Estate was\nprecisely about civil society as a check on government power (meanwhile,\neven in the absence of any censorship, lots of power goes in the other\ndirection: the government funds schools and universities and can do a\nlot to shape the content of especially the former). The media reports on\nbusiness, meanwhile successful business figures frequently fund the\nmedia. These mechanics are all healthy and add robustness to society, as\nlong as one direction of flow does not overpower the others.\nBalance of power and\neconomies of scale\n\nIf there is one argument that explains both the rise of America in\nthe 20\u1d57\u02b0 century and the rise of China in the 21st, it's a simple one:\neconomies of scale. This is something that people in both places often\nbring up as a criticism of places like Europe: there are many small and\nmedium-sized countries with different cultures and languages and\ninstitutions, and so it's difficult to create businesses that operate\nacross the continent. Meanwhile, in a large homogeneous country, you can\neasily scale to hundreds of millions of people.\n\nEconomies of scale are a big deal. And at the level of\nhumanity, we want economies of scale because they are by far\nthe most effective way to make progress. But economies of scale are a\ndouble edged sword: if I have 2x the resources that you do, I will be\nable to make more than 2x the progress. Hence, next year, I will have\neg. 2.02x the resources that you do. Hence, eventually, the most\npowerful actor controls everything.\n\nLeft: proportional growth. Small differences at the start become\nsmall differences at the end. Right: growth with economies of scale.\nSmall differences at the start become very large differences over\ntime.\n\nHistorically, there have been two pressures counterbalancing\neconomies of scale, and preventing this kind of effect:\n\n- Diseconomies\nof scale. Large institutions are very inefficient in many\nways: internal conflicts of interest, communication costs, costs because\nof physical distance.\n\n- Diffusion. People move between companies and\nbetween countries and take their ideas and talents with them. Poorer\ncountries are able to trade with richer countries and get catch-up\ngrowth. Industrial espionage happens everywhere. Innovations get\nreverse-engineered. You can use one social network to advertise another\nsocial network.\n\nIf the cheetah is ahead of the turtle, the first effect makes the\ncheetah go slower, and the second effect acts like a rubber hand pulling\nthe turtle forward closer to the cheetah. But recently, a few key forces\nare affecting this balance:\n\n- Rapid technological progress, which allows the\nsuper-exponential curves of economics of scale to be much faster than\nbefore.\n\n- Automation, which allows global-scale work to be\ndone with very few people involved, reducing human coordination\ncosts.\n\n- The modern ability to make proprietary software and hardware\nproducts that distribute ability to use without diffusing ability to\nmodify and control. Historically, giving a product to a\nconsumer (whether within a country or between countries) inevitably\nimplied opening it up to inspection and reverse-engineering. Today, this\nis no longer the case.\n\nBasically, economies of scale are going up, and while diffusion\nof ideas is probably higher than before due to internet\ncommunication, diffusion of control is lower than before.\n\nThe conundrum: how do we have a flourishing civilization in\nthe 21\u02e2\u1d57 century, with rapid progress, without extreme concentration of\npower?\n\nThe solution: mandate more diffusion.\n\nWhat does \"mandate more diffusion\" mean? First, a few government\npolicy examples:\n\n- EU standardization mandates (eg. most\nrecently USB-C), which make it harder for build proprietary\necosystems that do not play nicely with other technology\n\n- Forced\ntechnology transfer rules in China\n\n- USA banning\nnon-compete agreements, which I support on the grounds that they\nforce the \"tacit knowledge\" inside of companies to be partially open\nsource, so once an employee leaves one company they can apply skills\nlearned there to benefit others. Non-disclosure agreements limit this,\nbut are fortunately very porous in practice.\n\n- Copyleft\nlicenses (eg. GPL), which require any software built on top of the\ncopylefted code to itself be open source and copylefted\n\nWe can also come up with other ideas in this direction: for example,\nwe could imagine governments making mechanisms inspired by the EU\ncarbon border adjustment mechanism, but charging the tax on\n(domestic or foreign) products in proportion to some measure of how\nproprietary a product is: if you share the technology with us, including\nby open-sourcing it, the tax drops to zero. Another idea that should be\nbrought back is Harberger\ntaxes on intellectual property.\n\nBut there is also a more \"chaotic\" strategy that we should use much\nmore: adversarial\ninteroperability.\n\nAs Cory Doctorow explains:\n\n[Adversarial interoperability is] when you create a new product or\nservice that plugs into the existing ones without the\npermission of the companies that make them. Think of third-party\nprinter ink, alternative app stores, or independent repair shops that\nuse compatible parts from rival manufacturers to fix your car or your\nphone or your tractor.\n\nBasically, interact with technology platforms, social media websites,\nbusinesses and countries in ways that let you benefit from the value\nthat they are creating, without their permission.\n\nSome possible examples:\n\n- Alternative clients for social media platforms, that let you see the\ncontent that others post, and post content yourself, but where the\nclient filters content in some totally different way that you can\nchoose.\n\n- Browser extensions that do the same. Like ad blockers, but for eg.\nAI-generated content on Twitter.\n\n- Decentralized censorship-resistant\nexchanges between fiat and crypto, to mitigate the chokepoint\nfeatures of centralized financial systems.\n\nIn general, much value capture in web2 is at the level of the user\ninterface, and so if you can make alternative interfaces that are still\ninteroperable with the platform and its other users using the existing\ninterface, then you can remain part of the network, but opt out of its\nvalue capture.\n\nSci-Hub, a tool of mandatory\ndiffusion that has arguably done a lot to improve fairness and open\naccess in science.\n\nA third strategy to increase diffusion is to get back to Glen Weyl\nand Audrey Tang's ideas\non Plurality. They describe these ideas as being about enabling\n\"cooperation across difference\" - ways to have better discussions, and\ncollaboration, between people who disagree or have different goals, and\ngain the efficiencies of being part of a larger group, without having\nthe downsides of being a larger group that is then a single\ngoal-directed agent. Ideas like this can enable open-source communities,\ncollections of countries, and other groups that are not actors to have\nhigher levels of diffusion between each other, allowing them to share\nmore economies of scale and remain competitive with more internally\norganized centralized behemoths.\n\nNote that this is all structurally similar to Piketty's r >\ng concept and his desire to solve it with a global wealth tax (and\non the other end, stronger public services). The key difference is that\ninstead of focusing on \"wealth\", we are going one step upstream and\nfocusing on the generators of unbounded wealth concentration -\ndiffusing not dollars, but the means of production.\n\nI would argue that this approach is better because it more directly\ntargets the dangerous thing (extreme growth coupled with exclusion), and\nif done well it can even be efficiency-increasing. It also has the\nadvantage that it does not limit itself to targeting one type of power.\nWhile a global wealth tax may prevent concentration of power among\nbillionaires, it would do nothing against powerful dictatorial\ngovernments or other transnational entities, and it would perhaps leave\nus even more defenseless against them. A global decentralized strategy\nof forcing technological diffusion - telling people \"either you grow\nwith us, and share access to your secret sauce and your network on a\nreasonable schedule, or you grow entirely alone, and we shut you out\" -\nwould address power concentration in a different way.\nD/acc: making the\nworld safe for multipolarity\n\nA theoretical risk of pluralism is the vulnerable world\nhypothesis: the possibility that we live in a world where as\ntechnology advances, a growing number of actors will be able to cause\ncatastrophic harm to everyone. The less coordinated the world is, the\nmore likely it is that one of them will end up wanting to do this. The\nonly solution, according to some, is to concentrate power more.\nThis post argued for concentrating power less.\n\nd/acc ([1]\n[2])\nis a complementary strategy that makes concentrating power less safer.\nIt involves building defensive technology that keeps pace with offense,\nin a way that is open and available to everyone, reducing the need to\nconcentrate out of fear over security.\n\nThe cube of d/acc technologies\n\n## A pluralist morality\n\nSlave morality says: you are not allowed to be\npowerful.\n\nMaster morality says: you are commanded to be\npowerful.\n\nA synthesis morality focused on balance of power might say:\nyou are not allowed to be hegemonic, but you are encouraged to\nbe impactful, and to empower others.\n\nThis is another way of phrasing the \"power to\" vs \"power over\"\ndistinction, which has existed for centuries.\n\nOne way to have power to without power over is to have high diffusion\ntoward the outside world. Another way to have power to without power\nover is to build something which minimizes its ability to be used as a\nlever of power.\n\nIn Ethereum, one good example of this is the decentralized staking\npool Lido. Lido has\nabout 24% of the total ETH staked supply today, but people are much\nless scared of it than of they would be of almost anything else having\n24% of the stake. This is because Lido is not a single actor: it is an\ninternally decentralized DAO with several dozen operators, and a \"dual\ngovernance\" design that gives staked ETH holders the ability to veto\ndecisions. Lido deserves credit for putting significant effort in this\ndirection. At the same time, of course, the Ethereum community has been\ninsistent that even with these safeguards, Lido should not control\nall of Ethereum's stake - and today it's very far from\nthat.\n\nMore projects should explicitly think about not just a \"business\nmodel\" - how they bring in resources to support the work that they are\ndoing - but also a \"decentralization model\" - how they avoid\nconcentrating power in themselves, and the risks\nassociated with having such power. There are some cases where\ndecentralization is easy: relatively few mind the dominance of the\nEnglish language, or much less that of open protocols like TCP, IP and\nHTTP. In other cases, decentralization is hard because the use case\ndemands intentionality and ability to act in some situations.\nFiguring out how to get the upsides of flexibility without the downsides\nwill be an important challenge for a long time to come.",
    "contentLength": 25461,
    "summary": "Vitalik argues that Big Government, Big Business, and Big Mob are all growing stronger simultaneously, requiring new balance mechanisms.",
    "detailedSummary": {
      "theme": "Vitalik argues for preserving balance of power between Big Government, Big Business, and Big Mob through increased technological diffusion and decentralization to prevent dangerous concentration of power while maintaining societal progress.",
      "summary": "Vitalik identifies a fundamental tension in modern society: we benefit from powerful institutions (government, corporations, civil society) but fear their potential for abuse when they become too concentrated. He argues that traditional checks and balances are breaking down due to economies of scale, automation, and proprietary technology that enables rapid growth without natural limits. Historical counterbalances like diseconomies of scale and technology diffusion are weakening, leading to unprecedented power concentration. Vitalik proposes 'mandating more diffusion' through policies like standardization requirements, adversarial interoperability (creating unauthorized but compatible alternatives), and decentralized governance models. He advocates for a 'pluralist morality' that encourages impact and empowerment while preventing hegemony, suggesting that projects should develop explicit 'decentralization models' alongside business models to avoid concentrating dangerous levels of power.",
      "takeaways": [
        "Balance of power requires active maintenance - natural checks like diseconomies of scale are weakening due to technology and automation",
        "The solution is 'mandating more diffusion' through policies, adversarial interoperability, and open-source approaches that force sharing of capabilities",
        "Organizations should develop 'decentralization models' alongside business models to explicitly avoid concentrating power",
        "Economies of scale create winner-takes-all dynamics that must be counterbalanced by forced technology sharing and interoperability",
        "A pluralist morality encourages being 'impactful' while prohibiting being 'hegemonic' - having 'power to' rather than 'power over'"
      ],
      "controversial": [
        "Advocating for adversarial interoperability and forced technology transfer could be seen as undermining intellectual property rights and corporate innovation incentives",
        "The argument that tech leaders collaborating with government represents dangerous power concentration rather than beneficial cooperation",
        "Suggesting that diffusion of control should be mandated even when it might reduce efficiency or competitive advantage"
      ]
    }
  },
  {
    "id": "general-2025-12-17-societies",
    "title": "Let a thousand societies bloom",
    "date": "2025-12-17",
    "category": "society",
    "url": "https://vitalik.eth.limo/general/2025/12/17/societies.html",
    "path": "general/2025/12/17/societies.html",
    "content": "- \n\n- \n\n  \n    \n\n  \n  \n  Dark Mode Toggle\n  \n    \n\n  \n\n##  Let a thousand societies bloom \n\n 2025 Dec 17 \nSee all posts\n\n \n \n\n Let a thousand societies bloom \n\nSpecial thanks to Zachary Williamson, Afra Wang, Mark Lutter,\nBalaji Srinivasan and Primavera di Filippi for feedback and\nreview\n\nOne of the recurring ideological themes of the last few decades has\nbeen the idea of creating entire new communities, cultures, cities and\neven countries. Instead of having a fixed number of these, all slowly\nchanging, we can \"let a thousand\nnations bloom\" (where \"nation\" can cover the full spectrum from a\nglorified internet forum to a literal country), giving people more\nchoice and opening up space for more pluralistic independent innovation.\nInstead of your membership in one being an accident of birth, each\nperson can choose to gravitate to the communities that best fit their\nvalues.\n\nSome strands of this thought include:\n\n- Various attempts to make \"digital countries\", to various degrees of\nseriousness\n\n- Balaji Srinivasan's 2022 book on \"network states\", which I\nreviewed\n\n- The \"coordi-nations\"\nand later \"networked nations\" movement\n\n- \"Phyles\"\n\n- Seasteading\n(and also on the purist end, Liberland and Sealand)\n\n- The Charter Cities\nInstitute (see also: my own thinking about crypto\ncities)\n\n- Attempts by existing nations to \"renovate\" themselves, such as Estonian\ne-residency, and more recently on a bigger scale, Bhutan's Gelephu Mindfulness City\n\n- Similarly-themed projects from earlier generations, eg. Freetown\nChristiania and Walt Disney's city EPCOT\n\nThese ideas are diverse. Some of them are explicitly about getting as\nmuch legal autonomy as possible, and using that platform to create new\nlaws from the ground up. Others value a more gradualist approach, and a\nmore long-term closer connection to existing groups and institutions\nrather than re-building everything from zero. Some focus on countries,\nothers on cities, and others on cultures. Some are more left-leaning in\nideology, others are more right-leaning. In many ways, it's like where\nthe crypto space was five to ten years ago.\n\nLeft: magic internet money. Right: magic internet\nsociety.\n\nIn 2023, seeing all of these ideas mature inspired me to run Zuzalu,\nan experimental \"popup city\" in Montenegro: bring ~200 people, from\nmultiple communities - Ethereum, longevity, rationalism, AI - together\nin one place for two months, and see what happens. Zuzalu succeeded as\nan experiment, and in my visits to various other \"new city\" projects I\noften heard the feedback that it inspired them to take culture and\ncommunity building more seriously. But the experiment left unanswered a\nkey question: what happens next?\n\nIn this post, I give my updated picture of this space. I will first\nreview what I think we have learned since 2023, when the space moved\nfrom vibes and whitepapers to real-world experimentation. I will then\nsketch out a concrete world that this movement could be driving towards,\nwhat new types of entities will emerge, and what concrete value they can\nprovide.\n\n## Table of contents\n\n- What have we learned from Zuzalu?\n\n- Tribes\n\n- What is culture, and how should it evolve?\n\n- Tribes as innovators in culture\n\n- Hubs\n\n- Zones\n\n- Why would countries want to host zones?\n\n- What are examples of policies zones could try?\n\n- Do urbanism right\n\n- Let people in\n\n- Vouching as general-purpose substitute for\nregulation\n\n- Crazy democracy ideas\n\n- Crazy urban governance ideas\n\n- Bounding the risks\n\n- Should zones and tribes cooperate?\n\n- The Archipelago\n\nWhat have we learned from\nZuzalu?\n\nZuzalu, 2023\n\nZuzalu in 2023 was an experiment: bring ~200 people, from multiple\ncommunities -Ethereum, longevity, rationalism, AI - together in one\nplace for two months, and see what happens. This was the first time\nsomething like this had happened in this way - almost all events are\neither much smaller in scale, much shorter, or both - and the closest\nother historical examples are in spheres far removed from the kind of\nfrontier technology that Zuzalu was built around.\n\nI enjoyed my experience at Zuzalu - though sometimes the\nsocialization did get to the point where it was too much for me. I\nlearned a lot about different people's interests, and got to know many\nwarm and friendly and interesting people. There are a lot of \"little\nthings\" that we learned about how to organize a popup well. For\nexample:\n\n- 200 people (roughly Dunbar's\nnumber) is an excellent size for a popup. Unlike hacker\nhouses or even 40-person popups, it's a size large enough that we got to\nsee subcultures within Zuzalu. There were the Ethereum\nresearchers, the longevity enthusiasts, people doing intellectual\nsalons, people cooking Chinese hotpot and singing karaoke, the fitness\ncrew doing runs, saunas and cold plunges (I ended up being some of all\nfive). This made the community interesting and enjoyable to stay in for\ntwo whole months in a way that would not have been possible if it were\nhomogeneous.\n\n- 1-2 months is an excellent duration for a popup.\nThe reason is that the duration affects the attitude that people have:\na week is a break from your life, two months is your\nlife. A two-month duration makes it impossible to have high\nintensity of activities the whole time, and makes it possible to truly\nget to know people, and to form the kinds of sub-communities that make\nthe popup interesting. So it's a much better trial for building an\nactual new city.\n\n- You want to have \"content\" (activities, presentations,\neducational events...), but you don't want the amount of content to\noverflow. What you want is \"a college at 25% intensity\": enough\nto stimulate people, but not enough to tire people out. Many popups I've\nbeen to were too much on the side of tiring me out. I recommend having\nexplicitly agreed hour ranges and days where events do not happen.\n\nAfter the original Zuzalu in Montenegro, we kept going organizing\n\"popups\", and it feels like popups seem to have found \"product market\nfit\" within their niche. One of the Zu spinoffs, Edge City, has perfected a\npipeline of organizing them, and I have heard that they are at this\npoint a cash-flow-positive business. And popups - like-minded people\nliving together for medium durations - have proven themselves as a\nstepping stone toward a more full-fledged community.\n\nA panel on cryptography at ZuConnect and the ZuSocial hacker\nhouse, Istanbul 2023.\n\nIt also became abundantly clear that there are limits to what popups\ncan do:\n\n- Popups are expensive: short-term rental is always\nmore expensive than long term, and you easily get ripped off negotiating\nin a new location for the first time. Edge City is not cheap to\nattend.\n\n- It's difficult to truly have depth when\ncustomizing. ShanhaiWoo is one of the \"Zu spinoffs\" that\nimpresses me deeply because it actually tries to create a culturally\nunique immersive environment, making its physical zones \"feel like\nShanhaiWoo\". But when it's only in one place for 40 days, the best that\nit can do is often paper and cardboard.\n\n- Bringing that many people together is hard. The\nmost sustainable approach that I have discovered is what we did in\nChiang Mai, where 5-10 popups, each independently bringing 30-300\npeople, co-located in the same city around the same time.\n\n- Involving locals in a non-superficial way is hard.\nA common goal that people doing popups have is to involve local people\nfrom the region, in a way deeper than buying food and rent from them\n(though I would argue even buying food and rent can be a meaningful\ncontribution to an economy, especially if you come during off-peak\nseason, as the original Zuzalu did, so you're stabilizing load rather\nthan overcrowding it). But involving locals non-superficially is hard:\nif you have a niche interest that only a few people care about, and\nyou're in a country with 1-5 million people, the intersection will be\nvery tiny. Realistically, my main conclusions so far are (i) reach out\nto diasporas of the country, and not just already-in-country locals, and\n(ii) effective local community building requires coming back to a place\nfor years, and not just doing a one-off.\n\nAnother pattern I have noticed is that two things core to the\nearly ideology often fall away over time: novel governance designs, and\na search for legal autonomy. Within the context of popups, this\nmakes total sense. If a popup is short-duration, then \"forking as\ngovernance\" works perfectly fine. Each popup can be run by a founder or\ncore team, and if anyone is unhappy, they can make their own version and\ntry to attract people over. The longevity-focused Zu spinoff Vitalia\nalready split\ninto two forks. And if a popup only lasts 30 days, then there is not\nmuch interesting that could be done that would benefit from legal\ninnovation.\n\nAs a result of all this, I have noticed a worrying pattern: over\ntime, popups would get shorter in duration, smaller in scope, and more\ngeneric in substance, to the point where in the limit they approach\nbeing simply a few more conferences and hackerspaces. Outside the\nZuzalu-verse, I saw Praxis aspiring to big\ndreams of a new Mediterranean renaissance, but in practice mostly\ndelivering parties in upscale cities in the United States. (Since then,\nthey seem to have switched\nto pursuing the American military dynamism topic)\n\nFor all of these reasons, I have started advocating for\nZuzalu-inspired communities to start having permanent nodes. There\nalready are a few: Frontier\nTower, Crecimiento, and\n4seas' two nodes (one city, one\nmountain) in Chiang Mai, with others under construction\n(additionally, of course, there is Balaji's Network School). But even with these, in the\nback of my mind I always fear the \"regression to the mean\" that they\nwill turn into glorified coworkingspaces, and lose all of their cultural\nor experimental interestingness. Making sure that this does not happen\nis an ongoing challenge, and indeed it is a primary goal of this post to\npaint a clearer picture of what alternative future these projects could\nbe driving toward.\n\nNow, let's get into actually explaining what I think this future can\nbe.\n\n## Tribes\n\nThe 4seas mountain venue features flags of many of the\ncommunities that it sees itself as connected with: Bitcoin,\nEthereum, Plancker, 706\n\nOne common critique of modern society is that it is at the same time\natomistic and authoritarian: there is a lack of intermediate\ninstitutions, in between individuals and states, that give people needed\nservices and community. In the critics' story, this makes society:\n\n- Lack a feeling of community, become uncaring, and fail to provide\npublic goods that are too local or group-specific for states to\nnotice\n\n- Become homogeneous - \"glass and steel skyscrapers with Starbucks\neverywhere\"\n\n- Become vulnerable to takeover by dictators\n\nAll three problems stem from the fact that we have too much of a\ntwo-level structure: individuals, very powerful large-scale actors like\nstates, and nothing else.\n\nHistorically, these \"intermediate institutions\" included local\ngovernments, clubs, churches, small businesses and various other\nassociations. Today, we still have many of those, but they are\ninherently local in scale, and so they are failing to capture the most\nmeaningful communities today, which are increasingly continental and\nglobal. We have corporations, including very big corporations, and we\nalso have social media. But these are impersonal, homogenizing forces:\nthe profit motive drives them to appeal to as many people as possible,\nreducing their diversity and uniqueness toward zero. Startups are small\nand diverse, but according to the\nstandard playbook, encouraged by venture-capital profit motives, a\nstartup is a group of people attempting to become a new megacorporation,\nnot some reliable third sector of society.\n\nSo what would a successful \"intermediate institution\" of this type,\nadapted for the needs of the 21\u02e2\u1d57 century, look like? I will propose my\nanswer. It needs to be some kind of neo-tribe or other\ninstitution that focuses, and meaningfully innovates, on the thing that\nhumans do that isn't generic: culture.\n\nWhat is culture, and\nhow should it evolve?\n\nLeft: gym at Balaji's Network\nSchool. Right: Town hall at ShanhaiWoo Chiang\nMai.\n\nThe Wiktionary\ndefinition of \"culture\" begins as follows:\n\n- The arts,\ncustoms, lifestyles,\nbackground\nand habits\nthat characterize\nhumankind,\nor a particular society or nation.\n\n- The beliefs, values, behaviour\nand material objects that constitute a people's way of life.\n\n- The conventional conducts and ideologies of a community; the system\ncomprising the accepted norms and values of a society.\n\nIn short, culture is the patterns of human behavior in a particular\ncommunity. It covers everything from the food you eat, the language you\nspeak, dance, music, architecture, to much \"deeper\" things like how\npeople conceive the stories of their own lives, their relationships with\ntheir families, business, politics, and how people resolve conflicts in\nall of these spheres.\n\nMany people make the mistake of thinking culture is something that\ncan be explicitly laid down by mission statements and top-down edicts.\nLet's take, for example, the corporate culture of Enron (explanation for\nyounger readers: Enron was the FTX of your parents'\nera).\n\nOn paper, Enron valued \"integrity, communication, respect and\nexcellence\". In practice, Enron clearly valued some very different\nthings. But this is only the most egregious example; the inevitable wide\ndivergence between \"organizational culture\" written on paper and an\norganization's culture in practice is very easy to see anywhere.\n\nAnother problem with top-down attempts to shape culture, especially\nmore coercive ones, is that the whole strategy has very low galaxy-brain\nresistance. While declaring top-down \"the culture I wrote down in\nthis document is better than what you have now, so I will impose it\" may\nat times be the right thing to do (see: the anti-smoking push,\nfederally-driven anti-segregation\nin the 1960s USA, etc), the problem is that it's too easy for\nsomeone to get convinced that their own culture, as they understand it,\nis great, and then use it as an excuse to dominate others.\n\nOn the other hand, many people make the mistake of over-identifying\nculture with the purely aesthetic, subjective and\ngroup-identity-oriented parts of culture: food, music, dance, dress,\narchitecture styles, and ignore the parts that are functional, whose\nsuccess or failure drives the success and failure of civilizations. This\ncan lead to an over-egalitarian and stagnant \"culture as museum\"\nmentality: every culture is equally as good as every other culture\nbecause aesthetics are ultimately subjective, and so there is no such\nthing as cultural improvement - instead, the only goal is\npreservation.\n\nThat is the sort of thing that Thomas Sowell rails\nagainst:\n\nCultures are not museum pieces. They are the working machinery of\neveryday life. Unlike objects of aesthetic contemplation, working\nmachinery is judged by how well it works, compared to the\nalternatives.\n\nCultures are there to serve their people, not touristic onlookers\nappreciating their existence from far away. Some cultures do this much\nbetter than others, and all cultures could do much better still. There\nare just too many examples of traditional culture being pathological\n(here's the latest\nI just happened to see while writing this) for preservation to be\nthe only goal. And even if it were not, technology - growing wealth,\ndigital communications, birth control, education, the list goes on - has\nchanged the world so much that any lessons from our collective memory in\nthe past millennium need to be radically adapted to be relevant in the\nnext.\n\nOn the third hand, some make the mistake of acknowledging that\nculture is functional, but over-emphasizing very small-scale\nindividual decisions as a vehicle of change. This is what Scott\nAlexander calls \"universal\nculture\":\n\nUniversal culture is the collection of the most competitive ideas and\nproducts. Coca-Cola spreads because it tastes better than whatever\npeople were drinking before. Egalitarian gender norms spread because\nthey're more popular and likeable than their predecessors. If there was\nsomething that outcompeted Coca-Cola, then that would be the official\nsoda of universal culture and Coca-Cola would be consigned to the\nscrapheap of history.\n\nInstead of keeping culture the way it is, or reforming it from the\ntop down, why not embrace the wisdom of accumulated individual choice\nand freedom?\n\nHere is how I would argue the case against an overly purist version\nof this approach. There are many things that require \"immersion\" to\nsucceed: lifestyle habits, local public goods such as air quality, work\nhabits, lifetime learning habits, limitations on use of technology, etc.\nDoing anything truly interesting and unique requires \"depth\", and\nsubstantial collective investment and effort to create an entire\nenvironment oriented around better serving those needs. These things\ncannot easily be done by an individual, or even by a corporation,\nbecause corporations face too much pressure to \"meet users where they\nare\" - and so we get everyone drinking Coca Cola (or getting addicted to\noutrage-driven social media, or ...)\n\nAs we see clearly with architecture styles (but happens in every\nsphere), relying too much on market incentives leads to global\nmonoculture.\n\nSo what's going on here? And if we want to avoid these three\npitfalls, what might cultural evolution done well look like?\n\nThe social philosopher Charles\nTaylor talks about culture as being underpinned by moral\norders and social imaginaries. Taylor defines\na \"social imaginary\" as:\n\nthe ways in which [people] imagine their social existence, how they\nfit together with others, how things go on between them and their\nfellows, the expectations which are normally met, and the deeper\nnormative notions and images which underlie these expectations\n\nFor example:\n\nTake our practice of choosing governments through general elections ...\nEssential to our understanding what is involved in this kind of\nmacro-decision is our ability to identify what would constitute a foul:\ncertain kinds of influence, buying votes, threats, and the like. This\nkind of macro-decision has, in other words, to meet certain norms, if it\nis to be what it is meant to be ... And beyond the ideal stands some\nnotion of a moral or metaphysical order, in the context of which the\nnorms and ideals make sense.\n\nAn important point that Taylor makes is that social imaginaries are\noften transformed when \"what start off as theories held by a few people\nmay come to infiltrate the social imaginary, first of elites perhaps,\nand then of the whole society\". Taylor talks at length about how modern\nEuropean liberal democratic norms emerged in the 17\u1d57\u02b0 century as a\nresult of this process transforming the European \"moral order\". However,\nTaylor cautions, this process of transformation is\norganic and complicated:\n\nWhat exactly is involved when a theory penetrates and transforms the\nsocial imaginary? For the most part, people take up, improvise, or are\ninducted into new practices. These are made sense of by the new outlook,\nthe one first articulated in the theory; this outlook is the context\nthat gives sense to the practices ... But this process isn't just one\nsides, a theory making over a social imaginary. In coming to make sense\nof the action the theory is glossed, as it were, given a particular\nshape as the context of these practices ... Nor need the process end here.\nThe new practice, with the implicit understanding it generates, can be\nthe basis for modifications of theory, which in turn can inflect\npractice, and so on.\n\nIn short, culture is a big complicated blob where actions,\nconsequences, statements by leaders and theories by intellectuals all\ninfluence each other in every direction. If a culture\nofficially says to do one thing, but actually people\ndo something else, then the latter is more decisive. Culture is\nshaped by incentives, but then incentives are themselves implemented by\npeople, who are guided by culture. Culture is housed in a community,\nwhich is held together because people feel affinity for each other, and\nthat affinity is itself shaped by shared rituals.\n\nI remember experiencing some of this (ok fine, the solitary parts of\nthis) myself with my experiences as a teenager attempting to build constructed\nlanguages - like Esperanto, but\nbetter. I easily recognized how English is pathological with its horribly broken and irregular\nspelling system (and many other issues), and how centuries of\norganic evolution has only made the problem worse. But when I tried to\ncreate an \"ideal language\" from scratch, I ended up creating something\nthat can make beautiful logical and compact sentences in the specific\ncases I designed it around, but which takes three times longer than\nEnglish to express any other thought that I needed to express in\neveryday life.\n\nThis shows why all three of the above approaches to culture are\ninsufficient:\n\n- Top-down culture fails because it ignores the\nbottom-up side. It acknowledges the intellectuals and their theories,\nbut not the parts of culture bring its members together, and integrates\nthe theories to people's actions\n\n- Cultural traditionalism fails because it ignores\nthat culture needs to change and improve at all\n\n- Cultural individualism (and incrementalism\ngenerally) fails because it only sees the bottom-up side, and\nit ignores the need for large, structured paradigm shifts to get out of\nbad local equilibria\n\nNotice how \"top-down culture\", \"cultural traditionalism\" and\n\"cultural individualism\" map really nicely to the three corners of\n\"the\nd/acc triangle\", which also argues that all three are\ninsufficient and we need to do something else.\n\nTribes as innovators in\nculture\n\nAnd so I think we need a different path. What we want is a\nbetter \"world game\" for cultural evolution: an environment\nwhere cultures improve and compete, but not on the basis of violent\nforce, and also not exclusively on low-level forms of memetic\nfitness (eg. virality of individual posts on social media,\nmoment-by-moment enjoyment and convenience), but rather on some kind of\nfair playing field that creates sufficient space to showcase the\nlonger-term benefits that a thriving culture provides.\n\nOne early modern version of this idea is the notion of\n\"prefigurational cultures\". A seminal work is Margaret\nMead's Culture and Commitment from 1970:\n\nIn the past, in configurational cultures, the elderly were gradually\ncut off from limiting the future of their children. Now, as I see it,\nthe development of prefigurational cultures will depend on the existence\nof a continuing dialogue in which the young, free to act on their own\ninitiative, can lead their elders in the direction of the unknown.\n\nWhat might a prefigurational culture in the real world look like?\nHere, there are many possible different answers. To show one end of the\nspectrum, let me re-paste Balaji's example from years ago:\n\nKeto Kosher, the sugar-free society\n\nStart with a history of the horrible USDA Food Pyramid, the\ngrain-heavy monstrosity that gave cover to the corporate sugarification\nof the globe and the obesity epidemic. ... Organize a community online\nthat crowdfunds properties around the world, like apartment buildings\nand gyms, and perhaps eventually even culdesacs and small towns. You\nmight take an extreme sugar teeotaler approach, literally banning\nprocessed foods and sugar at the border, thereby implementing a kind of\n\"Keto Kosher\".\n\nYou can imagine variants of this startup society that are like\n\"Carnivory Communities\" or \"Paleo People\". These would be competing\nstartup societies in the same broad area, iterations on a theme. If\nsuccessful, such a society might not stop at sugar. It could get into\nsetting cultural defaults for fitness and exercise. Or perhaps it could\nbulk purchase continuous glucose meters for all members, or orders of\nmetformin.\n\nBut cultural innovation does not have to be \"legible\" in the way that\nKeto Kosher is. In fact, as we have seen, over-indexing on legibility\nand on explicit ideology often leads to problems. Cultural innovation\nworks better when it arises out of a collection of habits, attitudes and\ngoals that are shared by a particular group, and adapted to the group's\nneeds. The group's goal might be realistically a 50/50 mix of being\n\"about a set of values\" and being \"about the group\" - it's not\nattempting to expand to unlimited size, rather, it's a group of people\nwith shared history and shared identity trying to do their thing and\nmake the best of it.\n\nThe Zuzalu-verse is actually one of the better proto-examples of\nthis. It is organized around a particular set of values: the \"Ethereum\ncanon\" of open source, freedom, decentralization and a positive-sum\nattitude toward humanity, idealistic hacker culture, concern about\nhealth, etc. The Zuzalu identity is demonstrably not universal. Many\npeople who frequent the Zuzalu-verse reported finding themselves out of\nplace at Network School, which is organized around principles that are\nsimilar on paper, but very different in their \"vibes\" - and undoubtedly\nothers feel the same way in the other direction. But there isn't a fixed\n\"One\nCommandment\", or even any specific written-down mission and vision\nstatement. Aspects of it could be described as \"Keto Kosher but for\neducation\", attempting to integrate constant learning into week-by-week\nlife (a very important thing for us to get right in the 21\u02e2\u1d57 century!),\nbut even this is done in a very organic style. Just as much as the\ngoals, the community is organized around its people.\n\nEventually, I also expect tribes to get back to innovating in\ngovernance: using a combination of culture and technological\nmeans (blockchains, LLMs, ZK...) to have better collective conversation\nand decision-making. Currently, this part of the space is in somewhat of\na \"trough of disillusionment\", as we have realized the downfalls of\nformalizing governance too early. However, I would argue that we have\nnot yet properly tried to integrate AI and ZK tech into voting\nprocesses, which could solve their two biggest problems: attention\noverload and fatigue, and collapse into social games where people vote\nbased on how their vote will be perceived (if not outright bribery),\nrather than their genuine convictions. I expect this to pick back up at\nsome point, as it will be necessary for tribes to be long-term\nsustainable without having the same pitfalls as corporations. I also\nthink tribes may be a better ground for this kind of governance\nexperimentation than eg. blockchain-based DAOs, because tribes'\ncapabilities and needs are more complex.\n\n## Hubs\n\n4seas Nimman in Chiang Mai. Visibly a \"regen\" space, visibly an\n\"Ethereum-ish\" space, visibly not your average cowork.\n\nTruly instantiating a culture with any level of depth requires not\njust talking about the culture's themes, but actually\nliving them. This requires deep immersion,\ninstantiating the culture's values and aesthetics and practices at a\nlevel that goes far beyond a few decorations and posters. For\nexample:\n\n- If the community values health, have a restaurant that serves\nhealthier versions of major cuisines\n\n- If the community values infrastructure sustainability and\nresilience, then the node could have an actual local farm, solar panels,\nbatteries, etc built in.\n\n- If a community values open source and security, then the node can be\ndone with not just open source software, but also open\nand verifiable hardware.\n\n- If a community values collective activities, it needs rooms capable\nof fitting them (this one is actually surprisingly nontrivial)\n\n- If the community values a particular aesthetic, the structures could\nbe built with that aesthetic in mind. (A middle ground is to have mobile\nstructures, like\nthese)\n\nThis is all why I think it is a critical part of digital tribes to\nhave long-lasting physical spaces. Physical spaces allow a culture's\nvalues and habits to be instantiated in a much deeper way.\n\nThe good news about a hub is that you need a surprisingly small size\nto be viable. If a hub is located inside a city, they can be arbitrarily\nsmall, because residents can take advantage of the surrounding\ninfrastructure of a city. If a hub is located outside a city, then it's\nbasically building a new city. But even here, there is good news. For a\nconventional city to play a significant role at the frontier of\nanything, you need to have a population of at least a million; that is\nthe level at which it's possible to have effective network effects\nwithin any particular niche. But if you are specialized to one or a few\nniches (I think a novel combination of a few niches is better than being\noverly focused on one), the minimum size is much smaller. Here are some\nexamples of viable cities I've visited that are quite small:\n\n- Longyearbyen\n(northernmost significant settlement in the world): 2,600 people\n\n- University towns: often 30,000 (eg. Ithaca, NY) to\n150,000 (eg. New\nHaven, Cambridge)\n\n- Ski towns, surf towns and other sport-specific\ntowns: often 1,000 to 10,000\n\n2,600 is a great size: Longyearbyen is able to sustain ~10\nrestaurants, an airport, a hospital and a school (2,600 total ~= 26 per\nyear of age).\n\nBut 100 people is probably not enough. I recently visited Prospera in\nHonduras (population ~100), and while the physical venue was beautiful,\nsurprisingly strong on cultural uniqueness, and even strong on\nengagement with locals (a Honduran was on the core leadership team, many\nwere in the core community, and at least one of the medical businesses\nwas run by Hondurans), the place I was staying had one restaurant with\nlimited food options and no other amenities within walking distance.\nHence I think one or two further steps of growth and maturity beyond the\n100-person level is ideal.\n\nI expect getting hubs right to be the next important step for these\nkinds of \"tribes\" to succeed, and an important training ground for them\nto figure out culture, governance and other issues at a much deeper\nlevel.\n\n## Zones\n\nFrom top left to bottom right: (i) the micronation Liberland,\n(ii) Prospera,\na zone in Honduras, (iii) artist's drawing of the\nunder-construction California\nForever, (iv) artist's drawing of Bhutan's\ngovernment-initiated under-construction Gelephu Mindfulness City. All\nare what I call \"zones\", but represent very different points on the\nspectrum of engagement with governments.\n\nSo far, we have talked about innovating in culture. The more\nradical part of the free cities and network states space started by\nasking a different question: how do we get more innovation in\nrules - that is, the regulations, laws, and political systems\nthat govern the physical spaces that we are in?\n\nIn my observation, there are three schools of thought here:\n\n- Libertarians are primarily interested in a specific\nthing: freedom, the ability to be in a corner and peacefully do their\nown thing, whether in terms of lifestyle or technology development. They\nare willing to pay the price of smaller size and greater distance from\nglobal network effects - a price that is significant, and in fact a key\nreason why I do not share many people's worries about their\nprojects.\n\n- Developmentalists want to apply proven means to\nimprove economic prosperity, and look at Shenzhen\n(sometimes, I would argue, they focus on Shenzhen too much) as an\nexample.\n\n- Social technologists view governance as a social\ntechnology, and want to see more experimentation and improvement in that\ntechnology. They value development, but are more focused on developing\nnew techniques, rather than scaling already well-known techniques.\n\nOne\nview of a social technology perspective on\ngovernance.\n\nOften, the three perspectives merge. Some social technologists are\ninterested in governance ideas in a libertarian-ish direction, because\nthey see ideal forms of governance is being precisely those forms that\ntry to align incentives and beyond that point minimize arbitrary\nconstraints. Some libertarians believe in freedom because they see it as\ncritical for economic development.\n\nMy attempt at creating a political compass of some of the\nprojects I am more familiar with.\n\nWhy would countries want\nto host zones?\n\nBasically, for a country, it's a way of participating in the rapid\nand accelerating economic and technological revolution of the 21\u02e2\u1d57\ncentury. In particular, it lets you go beyond tourism, which imports\nindividuals but not the networks between those\nindividuals, and instead actually import parts of the networks\nthemselves, opening up the opportunity to capture a larger share of the\nvalue.\n\nQuoting\nNoah Smith:\n\nUnder British rule, and then for the first two decades of Chinese\nrule, Hong Kong served as a crucial entrepot \u2014 the\nworld's gateway to China. It facilitated the inflow of foreign capital,\nwhich was absolutely crucial to China's early industrialization. It was\na major hub for goods to move into and out of China. It provided\nservices for foreigners to do business on the mainland. And it imported\nforeign know-how into China, teaching locals how to build high-quality\ninfrastructure, set up factories, and do business overseas.\n\nSo I thought about Hong Kong, and wondered: What if every\ncountry had a quasi-independent city like that?\n\nImagine a Hong Kong in India. A Hong Kong or two in Europe. A Hong\nKong in Brazil, Japan, Indonesia, the United States, and so on. Each\ncity would formally be a part of the host country, subject to the laws\nand authority of the central government there. But in practice, each\ncity would be granted a degree of autonomy.\n\nThis sounds like social science fiction, but Bhutan's Gelephu Mindfulness City is an attempt to do\npretty much exactly this. The problem that GMC is trying to solve is, as\ndescribed to me by the Bhutan government, is twofold:\n\n- Give Bhutan a foothold in globalized techno-modernity, and help it\nget the economic benefits (including giving Bhutanese themselves better\nopportunities staying in Bhutan)\n\n- Do so in a way that minimizes risks to the existing culture\n\nBasically, let the expats come and the thirty-storey towers go up, at\nleast in one corner of the country along the border with India, but\ndon't let the next generation of the whole country get hooked on Coca\nCola.\n\nIncidentally, this is all why I am expecting \"zones\" to be the bulk\nof the future of jurisdictional innovation, and not new \"countries\".\nCountries are very unwilling to literally give up their sovereignty over\neven small patches of their land. Liberland has found a hack: take over\none of the very small pieces of land that, by pure accident of how the\nlines were drawn, no country claims as its own. But there are\nnot many opportunities like that, and even that is far\nfrom a guarantee of safety. To really have a country, you\nhave to either get approval from your new neighbors, or figure out your\nown military. Zones, on the other hand, are much more palatable for\npoliticians, and also enable a government to actually get ongoing upside\nin the networks that it attracts, rather than just a one-time deal (eg.\nProspera pays 12% of its tax revenue to the government of Honduras).\n\nWhat are examples\nof policies zones could try?\n\nI will give a few examples that I personally find interesting, on a\nscale from \"boring\" to \"experimental\".\n\n## Do urbanism right\n\nImage from Culdesac\nTempe\n\nIn many developed countries, a major challenge is that housing\nconstruction is very difficult for legal reasons. Some have estimated\nthat fixing this could make cities much more affordable, and improve\nGDP by as much as 36%. But changing laws in an existing city is\nhard, in large part because of existing stakeholders. So what if you can\ncreate a new city?\n\nThis is a big part of the\npitch for California\nForever. The other part of the pitch is all the other problems that\nurban economists have consistently railed about for decades. One that\nCalifornia Forever, and others like Culdesac, focus on is walkability (and\nbikeability). A third is attracting business including heavy industry\nthat people can work in. A fourth and more experimental option is\nfriendliness to new technologies (eg. drone delivery). There is a whole\nlist of ideas that policy thinkers have basically agreed on for decades,\nthat many are frustrated that they cannot implement in existing cities\nbecause things are hard to change. With a new city, you can - and it\nonly requires city-level autonomy, which is in many places quite easy to\nget, and not anything national-scale.\n\n## Let people in\n\nLeft: where a Singaporean can go (for short-term visit) without a\nvisa. Right: where an Indian can go without a visa.\n\nOne problem that many people have in the 21\u02e2\u1d57 century: where do you\ngo to live? Whether it's because of lack of economic opportunity,\npolitical instability, a culture unforgiving to people who are\ndifferent, or a government unfriendly to their business aspirations or\neven their way of life (or, for the luckier among us, a simple thirst\nfor adventure), for many people around the world, the country of their\nbirth does not suit them.\n\nAttracting these people is a massive economic opportunity. Many\ncountries around the world are becoming more restrictive (or even\nhostile) to both long-term and short-term immigration, but the number of\npeople who need more options for where to be is only increasing. Better\nserving such people also fosters global redistribution of\ntalent: creating a future where the most important\ntechnological and economic work is done from everywhere, and its\nproceeds more widely shared, instead of being concentrated in a few\nsuperstar cities in a few very powerful countries.\n\nThere is also a social technology angle to this. Many people have\nconcerns about inviting more people: the risk that they will stay longer\nthan expected and become illegal immigrants (including running to\nneighboring countries), risks related to safety, cultural\nincompatibility, etc. Today, we use \"which country are you from?\" as a\nfilter to determine who is high-risk and who is low-risk. But this is\nincredibly inefficient and unjust - it's the exact opposite of \"judging\npeople not by the color of their skin, but by the content of their\ncharacter\". In a modern digitized society, we have many kinds of filters\nto identify people who might be low-risk: work history, education,\npeople vouching for them, etc. I expect that whichever country or zone\ncreates an easy user-friendly mechanism for talented people from\neverywhere to easily be able to come (eg. to work for a local company,\nor for a conference, or for a popup) will gain a lot of benefits.\n\nVouching\nas general-purpose substitute for regulation\n\nOne idea that some economists like Robin Hanson support as a\nsubstitute for much of our regulation is vouching\n(aka mandatory liability insurance): you can do what you want, as\nlong as you find someone (eg. an insurance company) with enough capital\nwho agrees to pay potentially large fines and compensate any victims if\nyou create a problem.\n\nThis addresses a key problem with a libertarian approach to law: if\nyou only punish people when they actually create a problem, then the\ncost of the harm can easily be so great that there's no way to penalize\nsomeone enough to adequately incentivize caution. If the only regulation\non driving were that if you cause an accident you go to jail, it would\ndeter drunk driving (or driving without a license, or flying in unsafe\nversions of weird newfangled 3D aircraft) too little.\n\nIt also addresses a problem with the current approach of creating\napplication-specific rules for everything: the rules do not adapt well\nto new technology, and easily get corrupted and twisted into goals that\nhave nothing to do with safety, like protecting incumbent businesses.\nHere, the rules that people follow would be created by vouchers,\nincentivized by their need to balance between attracting customers and\nmanaging risks, rather than by politicians. The political lever is\nindirect, and more compatible with a free society: it sets the\nobjective, but not how you get there.\n\nIf successful, this is a really cool idea that could improve a lot of\nthings. But to see if it works, we have to try it somewhere, at a\nsufficient scale and level of realism. This is actually what Prospera,\nthe autonomous zone in Honduras (see website, review by\nScott Alexander) is trying to do. Currently, the experiment is still\nvery young, and there is only one insurance company (run by the zone\nitself), but this is exactly the kind of thing that a self-contained\nzone is the ideal place to try.\n\n## Crazy democracy ideas\n\nOne of the core political challenges of the 21\u02e2\u1d57 century is how to\nimprove democracy. Here's Eliezer Yudkowsky way of stating the\nproblem:\n\nEliezer's proposed\nsolution is a new twist on liquid\ndemocracy. Approximately:\n\n- Every voter chooses a delegate. Delegates gain power if they have at\nleast 50-200 votes\n\n- You have two or three levels of higher-tier delegation: 50-200\ndelegates can empower a second-level delegate\n\n- The delegates chosen at the top of this multi-level structure are\nthe parliament\n\nAgain, this is a really cool idea: it biases for sophistication\n(because each level would end up more sophisticated than the previous)\nand guards against populism (because a delegate can't amass extreme\npower by gaining a huge following directly) but in a way that avoids\nempowering pre-selected aristocracies. But to see how well it works, we\nneed to try it somewhere.\n\n## Crazy urban governance ideas\n\nHere I'll quote myself:\n\nAgain, you don't have to believe in this exact idea, just that there\nare things of this level of craziness that are at least worthy trying in\na serious way, somewhere.\n\n## Bounding the risks\n\nProjects building zones, especially those leaning more libertarian or\nnot initiated by governments, often get criticized - for being havens\nfor billionaires, unregulated zones where things will inevitably go very\nwrong, or for being neo-colonial. These criticisms sometimes apply to\ntribes and hubs too. I can understand the sentiment behind these\ncritiques, and I think there are important risks worth worrying about.\nHowever, I disagree with many of the stronger criticisms\n- and it is worth explaining why.\n\nAt heart, I am a pluralist. I believe that when different people\ndisagree about how things should be done, it's healthy to have a bias\ntoward solutions where both versions exist at least somewhere, and\npeople can freely choose between them. If a powerful (commercial or\npolitical or cultural) actor wants to see their culture or economic or\npolitical ideology implemented, the most productive and least risky\nimaginable way for them to do it is to peacefully make something\nsmall in a corner somewhere, and see how it plays out. By building a\nwhole new zone from scratch, you are taking on a huge inconvenience, and\na huge tax in terms of forgoing all kinds of network effects. And since\nyou are growing not by amassing a country-sized blog or podcast\naudience, but instead entering the real world at a much earlier stage,\nwe all gain valuable rapid feedback about whether or not the whole idea\nis crazy. This all feels like the sort of role you want\nsemi-influential radical mavericks to be playing in our society.\n\nWhat's the role we don't want them to play? One strategy I\ndeeply fear is what many in the so-called Silicon\nValley Tech Right have recently pivoted to: stop trying to route\naround the government, and instead just take over the government.\nThis is deeply scary: instead of corporations and the state serving as a\nhealthy check and balance against each other, the two collude against\neveryone else. It also means that ideas go straight from being\ntwelve-thousand-word screeds or five hour podcasts to running entire\ncountries.\n\nAnd in my experience, these two modes of action are substitutes:\npeople who start working on tribes and zones suddenly get visibly\nmore positive-sum and less interested in country\ntakeovers.\n\nLots of people on the internet lately seem to pine for a world\n(or a nation) where we force everyone to be more like Durmstrang.\nI'd rather let someone enthusiastic about that approach make the best of\nit, and have to inspire people to come voluntarily, than pontificate on\nthe internet and build up a large political movement based on how\nappealing that idea sounds written in words, with no feedback from how\nit works in reality. Long small-scale voluntary attempts at Durmstrang,\nshort Gilead.\n\nIn general, business and politics are the scariest at scale:\nmonopolizing industries, overwriting entire societies, or recklessly\nbuilding superintelligent AI (which eventually anyone will be able to\nbuild, but in any realistic universe the global centers of power will\nget to long before everyone else). Zones are the opposite of scale.\nWhatever you say about CHAZ,\nits downsides were far lower than what you would get if the\nsame people took over an entire country or even city government.\n\nIn an ideal world, zones can be a tool that can help countries and\nstates get more integrated into a global economy, and can give their\nbrightest people a path to frontier science and technology and business\nthat does not involve disappearing to a university and then a corporate\necosystem in a country halfway across the world. Creating opportunities\nfor frontier tech and business to happen locally instead of entirely\ndepending on powerful nations abroad can be a far more meaningful\ngain to national sovereignty than full uniformity of rules across\nthe entire country without even a few otherwise-undeveloped square\nkilometers of exceptions.\n\nBut achieving this outcome requires actively shaping it. I see\nopportunities for improvement here from both sides. Countries should not\nbe able to suddenly rugpull everyone in a zone if their political mood\nchanges, but they could have have levers of influence to\nencourage them to behave cooperatively, perhaps privileges defined\nnumerically that each administration can adjust up or down a medium\namount. Ideally, there would be ways to more explicitly encourage\neducation and tech transfer to local and regional talent. For\n\"developmentalist\" zones that are less radical but need to get access to\na larger local population, more limited sector-specific forms of\nautonomy is worth exploring.\n\nFrom a zone's perspective, this is another place where bottom-up\ndecentralized governance ideas can shine: they could help people working\non zones (or even hubs) are more able to determine what local\npopulations value and want, and try to proactively serve it, rather than\nwaiting for potential political backlash after the fact. Prospera has\nvoluntarily chosen to pay 12% of its taxes to the Honduran government,\nand passed internal laws to disallow it\nto expropriate anyone's land, but it could attempt to use newer\ntools to engage the population on a larger scale to better identify\nforms of value it could provide, and risks it should avoid.\n\nShould zones and tribes\ncooperate?\n\nSo far, I have told two disjoint stories. One is about smaller-scale\ncommunity-driven projects, and experimentation in culture. Another is\nabout larger-scale politics and business-driven projects, and\nexperimentation in rules.\n\nIt may seem like the story I am going for is that the two will\nconverge. But actually, while some \"vertically integrated\" zones that\ncombine both will exist, I predict that in general the \"market\nstructure\" will split tribes and zones into distinct categories, because\nthese are different things that require different specialties that are\ncomplementary. Figuring out legal frameworks to make it easy\nfor people from any country around the world to come to one place is one\ntype of skill. Actually building a global community is another: Edge\nCity and ShanhaiWoo have access to great technical talent, but they are\nnot constitutional lawyers.\n\nWe have already seen this \"market structure\" in the Zuzalu world:\nthere is at least a partial separation between \"hubs\", which provide\npermanent space, and \"popups\", which are communities that sometimes need\nspace for some period of time. Network School, 4seas and other nodes\nregularly host popups inside of them. I expect collaboration between\nzones and tribes (including tribes that expand to permanent hubs) to\nfollow a similar pattern.\n\nI argue that this strategy is ideal for countries that want to\nmaximize the success of their zone. Their goal should not just be to\nimport individuals, rather it should be to import\nnetworks. Furthermore, because most of them cannot compete with\nthe world's largest cities at creating generic networks, they\nneed to focus on more focused issue-specific networks.\nAttracting tribes, including schemes like collective visas (the\ngovernment approves the tribe, the tribe then provides its list of\n100-1000 people who automatically get admitted), can be a very effective\nway of doing this.\n\n## The archipelago\n\nRecently, Francis Fukuyama wrote\na post arguing that \"liberalism needs community, but it doesn't need\na \u2018strong god' telling everyone what to do\". Scott Alexander wrote\na piece commenting on it. Both are good, so I'll quote Scott quoting\nFukuyama and replying at length:\n\nAccording to R. R. Reno, editor of the magazine First\nThings, the liberal project of the past three generations has\nsought to weaken the \"strong\nGods\" of populism, nationalism, and religion that were held to be\nthe drivers of the bloody conflicts of the early 20th\ncentury. Those gods are now returning, and are present in the politics\nof both the progressive left and far right\u2014particularly the right, which\nis characterized today by demands for strong national identities or\nreligious foundations for national communities.\n\nHowever, there is a cogent liberal response to the charge that\nliberalism undermines community. The problem is that, just as in the\n1930s, that response has not been adequately articulated by the\ndefenders of liberalism. Liberalism is not intrinsically opposed to\ncommunity; indeed, there is a version of liberalism that encourages the\nflourishing of strong community and human virtue. That community emerges\nthrough the development of a strong and well-organized civil society,\nwhere individuals freely choose to bond with other like-minded\nindividuals to seek common ends. People are free to follow \"strong\nGods\"; the only caveat is that there is no single strong god that binds\nthe entire society together.\n\nIn other words - yes, part of the good life is participation in a\ntight-knit community with strong values. Liberalism's shared values are\ncomparatively weak, and its knitting comparatively loose. But that's no\nargument against the liberal project. Its goal isn't to become this kind\nof community itself, but to be the platform where communities like this\ncan grow up. So in a liberal democracy, Christians can have their\nchurch, Jews their synagogue, Communists their commune, and so on.\nEveryone gets the tight-knit community they want - which beats\nilliberalism, where (at most) one group gets the community they\nwant and everyone else gets persecuted.\n\nOn a theoretical level, this is a great answer. On a practical level\n- is it really working? Are we really a nation dotted with tight-knit\ncommunities of strong values? The average person has a church they don't\nattend and a political philosophy that mainly cashes out in Twitter\ndunks. Otherwise they just consume whatever slop the current year's\nversion of capitalism chooses to throw at them.\n\nScott then lists a few partial exceptions, and laments that they have\nnot been much more successful. His conclusion is that it's not working\nyet because we're not wealthy enough, and when we get wealthier and\nmoving people around to custom communities with custom infrastructure\nbecomes cheaper, it will happen.\n\nBut I also think there is something different at play: people just\nhave to get off their butts and actually create these alternative\ncultures and environments, and doing it is hard. Startups are also hard.\nBut startups have had a multi-billion-pdollar capitalist optimization\nmachine figuring out all the most optimized ways of doing them and\nrapidly growing them to scale, and turned them into cookie-cutter\nstandardized playbooks. Culture does not have the same profit motive,\nand culture is inherently not easy to scale.\n\nSome argue that NFTs solve this and make culture profitable, but\nthe fact that Zundamon's\nTheorem was not made as an NFT makes me pessimistic that\nNFT-driven culture will solve the problems that I want cultural\ninnovation to solve.\n\nA parallel problem exists for progress in economic and political\nrules, which have also stagnated under supposedly \"dynamic\" capitalist\nliberalism. The problem is that development of new and better economic\nand political rules, whether at city-scale or at country-scale,\nsimilarly does not have a strong profit motive. It definitely does not\nhave a rapid experimentation loop that startups have or even that many\n(but not all) aspects of culture have.\n\nI do not literally expect we are going to see a world where most\npeople live in tribes, or even zones. I definitely do not\nexpect normal people to be plotting themselves on a political compass of\n\"goldbug libertarianism\", \"hipster socialism\", \"Durmstrang-ism\",\n\"techno-Leninism\" etc, and joining whichever community is closest to\nthem on a 2D map. For most people, such grand ideologies are far from\nprimary in their lives. But I do expect a world that is somewhat more\ndynamic in both economic and political rules and in cultural dimensions,\nand that gives people more options.\n\nSuch a world would be a world where (i) people have more meaningful\nfreedom, both to escape persecution and to choose the kinds of\nenvironments that they truly enjoy living in, (ii) we get better\ninnovation both in economic and political rules and in culture, and\n(iii) instead of the innovation and creativity of the world being\nconcentrated in a few super-centers of global economic and political\npower, it is globally distributed everywhere across the world. This is a\nworld that I want to live in.",
    "contentLength": 54118,
    "summary": "Blog argues for evolving from temporary \"popup cities\" to permanent communities with 3 types: Tribes (culture), Hubs (mixed), Zones (governance).",
    "detailedSummary": {
      "theme": "Vitalik proposes a framework for creating new communities, cultures, and governance structures through 'tribes,' 'hubs,' and 'zones' to enable pluralistic innovation beyond traditional state and corporate structures.",
      "summary": "Vitalik argues that modern society suffers from being simultaneously atomistic and authoritarian, lacking meaningful intermediate institutions between individuals and states. Drawing from his experience organizing Zuzalu, he proposes three interconnected concepts: 'tribes' (cultural communities that innovate in lifestyle and values), 'hubs' (permanent physical spaces where tribes can deeply instantiate their cultures), and 'zones' (areas with regulatory autonomy for experimenting with new governance and economic policies). He sees this as a path toward cultural evolution that avoids the pitfalls of top-down cultural imposition, stagnant traditionalism, and purely individualistic market-driven homogenization. Vitalik emphasizes that this approach allows for voluntary participation and experimentation at small scales, providing valuable feedback before ideas scale up. He views zones as particularly promising because they offer countries ways to participate in global innovation networks while maintaining sovereignty, and he argues that the combination of tribes, hubs, and zones could create an 'archipelago' of diverse communities that people can choose between based on their values and preferences.",
      "takeaways": [
        "Popups like Zuzalu work well at ~200 people for 1-2 months but have limitations in depth, cost, and local engagement that require permanent physical spaces",
        "Cultural evolution requires balancing top-down theory with bottom-up organic development, avoiding both authoritarian imposition and purely market-driven homogenization",
        "Zones offer countries opportunities to import networks rather than just individuals, participating in global innovation while maintaining sovereignty",
        "Small-scale voluntary experimentation in governance and culture is preferable to large-scale political takeovers or purely theoretical approaches",
        "The future likely involves specialized tribes, hubs, and zones cooperating rather than vertically integrated projects trying to do everything"
      ],
      "controversial": [
        "The proposal for 'vouching' as a substitute for regulation, where insurance companies rather than democratic processes determine safety standards",
        "The argument that allowing wealthy individuals and corporations to create autonomous zones is less risky than democratic political movements",
        "The suggestion that countries should grant significant regulatory autonomy to private zone operators, potentially undermining democratic sovereignty",
        "The critique of 'universal culture' and market-driven cultural evolution, which many would see as democratic and efficient"
      ]
    }
  },
  {
    "id": "general-2025-11-25-plinko",
    "title": "Plinko PIR tutorial",
    "date": "2025-11-25",
    "category": "general",
    "url": "https://vitalik.eth.limo/general/2025/11/25/plinko.html",
    "path": "general/2025/11/25/plinko.html",
    "content": "- \n\n- \n\n  \n    \n\n  \n  \n  Dark Mode Toggle\n  \n    \n\n  \n\n##  Plinko PIR tutorial \n\n 2025 Nov 25 \nSee all posts\n\n \n \n\n Plinko PIR tutorial \n\nSpecial thanks to Alex Hoover, Keewoo Lee and Ali for feedback\nand review\n\nOne underrated form of privacy, that is not well satisfied by ZK-SNARKs,\ntraditional versions of FHE\nor other commonly talked about methods, is private\nreads. Users want to learn things from a large database,\nwithout revealing what they're reading. Some use cases of this\ninclude:\n\n- Privacy-preserving Wikipedia (so you can learn things without even\nthe server knowing what you're reading)\n\n- Privacy-preserving RAG\nor LLM search (so your\nlocal LLM can get data from the internet without revealing what you're\nquerying)\n\n- Blockchain data reads (so you can use a dapp that queries an\nexternal RPC node without the node learning what you're querying; see\n\"private reads\" here)\n\nThere is a category of privacy protocols that are directly designed\nto solve this problem, called private\ninformation retrieval (PIR). A server has a database \\(D\\). A client has an index \\(i\\). The client sends the server a query,\nthe server replies back with an answer that allows the client to\nreconstruct \\(D[i]\\), all without the\nserver learning anything about \\(i\\).\nIn this post I will describe some basics that underlies all PIR\nprotocols, and then describe Plinko, a protocol that\nefficiently achieves this.\n\n## Basics of PIR\n\nTo understand more complicated PIR protocols, it's best to start with\n\"classic two-server PIR\". Classic two-server PIR assumes that a client\nis making queries to two servers, and it trusts\nthat at least one of the two is honest. Here's how the protocol\nworks:\n\n- The client wants to know \\(D[i]\\),\nin the above example \\(D[8]\\).\n\n- The client generates a random subset of indices in \\(D\\), on average about half full (in the\nabove example, \\([1, 5, 6, 10,\n15]\\)).\n\n- The client sends that subset to one server, and then sends a\nmodified subset, where the membership of \\(D[i]\\) is flipped (ie. added if it was not\nthere, removed if it was there), to the other server.\n\n- Each server computes the XOR of the values in \\(D[i]\\) that it was asked, and returns the\nresult\n\n- The client XORs these two values, and it gets the value of \\(D[i]\\)\n\nThe key idea is that each server receives what looks like a\ncompletely random subset of the indices. There is not even a slight bias\nfrom mixing in \\(i\\), because that\nmodification is as likely to add the value as it is to remove it. The\ntwo servers give XOR-sums of subsets that are almost identical, but\ndiffer in one place: one includes \\(D[i]\\) and the other doesn't. The client\ngets these two values, and takes the difference between the two, which\ngives \\(D[i]\\).\n\nThis naturally generalizes to \"cells\" that are bigger than one bit;\nyou could have each \\(D[i]\\) have eg.\n1000 bits, and the protocol is exactly the same.\n\nThis protocol has three big weaknesses:\n\n- You have to trust that one of the two servers is honest.\n\n- The client has communication overhead equal to the number of cells\n(this can be much smaller than the total size of the data if the cells\nare big, but it's still a lot).\n\n- The server has to XOR half the total data to respond to a single\nquery.\n\nThere are two well-understood modifications to classic two-server PIR\nthat solve (1).\n\nOne strategy is PIR with preprocessing. Notice that\nin the protocol above, you can compute one of the two random sample sets\nbefore you even know which \\(i\\) you\nwant to query for. You generate thousands of these random queries. You\nhave an initial phase where you download (but do not need to store) the\nwhole file \\(D\\), and you compute the\nXOR sums for these random sets of indices locally. Essentially, you are\nacting as one of the two servers - and in this way, satisfy the 1-of-2\ntrust assumption all by yourself, so you do not need to trust the other\nserver. This approach does require a huge amount of upfront bandwidth,\nbut you can do it ahead of time, or do it in the background and use a\nmore inefficient form of PIR to answer any queries that you get before\nit finishes.\n\nThe other strategy is single-server PIR.\nBasically, you use some variant of homomorphic encryption (often simpler\nthan full-on FHE) to encrypt your query, in such a way that the server\ncan compute on your encrypted query to give you an encrypted answer.\nThis has high computational overhead on the server side, and so is not\nyet feasible for very large datasets. This strategy can be combined with\nPIR with preprocessing, so you can theoretically avoid the initial\ndownload by doing that procedure homomorphically encrypted on the server\nside.\n\nPlinko solves (2) and (3) - or at least, it decreases both\ncommunication overhead and server-side compute overhead from \\(O(N)\\) to \\(O(\\sqrt{N})\\) where \\(N\\) is the number of cells. It uses the\npreprocessing strategy to remove the trust assumption. Theoretically,\nyou can do the preprocessing in FHE; it is currently an open problem to\ndo this efficiently enough.\n\nIn Plinko's language, there is:\n\n- A setup phase, where the client processes all the\ndata once (but only remembers a small number of\n\"hints\")\n\n- A query mechanism, where the client makes a query\nto the server and combines the answer with the right hint to compute the\nneeded value.\n\nThere is also an easy way for the client to update hints if the data\nset changes. Now, let's get into the protocol!\n\n## Setup phase\n\nTreat the database as being a square, with \\(\\sqrt{N}\\) rows and \\(\\sqrt{N}\\) columns:\n\nThe client generates a random master seed \\(S\\). For each row \\(i\\), the client uses the master seed to\ngenerate a row seed \\(S_i\\). The client\ngenerates roughly \\(128 * \\sqrt{N}\\)\nhints. Each hint is generated as follows:\n\n- Randomly (or rather, pseudorandomly by hashing the master seed) pick\n\\(\\frac{\\sqrt{N}}{2} + 1\\) rows\n\n- If we're computing the \\(j\\)'th\nhint, for each row \\(r_i\\), use a hash\n\\(H(S_{r_i}, j)\\) to generate a random\ncolumn \\(c_i\\). So we get \\(\\frac{\\sqrt{N}}{2} + 1\\) \\((r_i, c_i)\\) pairs\n\n- XOR the pairs, and save the bit\n\nThe \"hash\" that is used to compute the \\(c_i\\) values could be a regular\nhash, but that leads to (tolerable but significant) inefficiency during\nthe query phase. So instead, Plinko prefers special type of hash called\nan \"invertible PRF\". The core property that it achieves is that given\n\\(S_i\\) and \\(c_i\\), you can easily recover all \\(j\\) values that lead to that \\(c_i\\). In practical terms, this means that\nfor any cell \\((r_i, c_i)\\), you can\neasily find all hints that \"include\" that cell.\n\nAdditionally, the setup phase generates some \"backup hints\". These\nare constructed like regular hints, except for each index \\(j\\), you choose \\(\\frac{\\sqrt{N}}{2}\\) rows, and compute a\npair of hints: one over the chosen subset of rows, and the other for its\ncomplement (ie. the \\(\\frac{\\sqrt{N}}{2}\\) rows that were\nnot chosen):\n\nThis whole process can be done \"streaming\", without requiring the\nclient to store any data at any point in time other than the hints\nthemselves. In total, the client needs to download the whole data set\nand store data equal to roughly \\(64 *\n\\sqrt{N}\\) cells, plus \\(\\sqrt{N}\\) cells per backup hint (the\nclient will need one backup hint for each query that the client will\nmake).\n\nIf desired, the process can also be done in FHE, which means the\nclient would only need to download the hints, but the server would need\nto do FHE work over the whole dataset; this is of course expensive, and\noptimizing it is an ongoing problem.\n\n## Query phase\n\nSuppose you are interested in a specific coordinate \\((x, y)\\). First, you discover a hint index\n\\(j\\) that generates a hint that\ncontains \\((x, y)\\). This is where the\n\"invertible PRF\" mechanism helps: it lets you immediately run the\nfunction in reverse knowing \\(S_x\\) and\n\\(y\\) to compute all \\(j\\)'s such that \\(H(S_x, j) = y\\). However, if you want to\navoid this complexity, you could just set \\(H(S_x, j) = sha256(S_x, \\lfloor \\frac{j}{16}\n\\rfloor)[j \\% 16: (j \\% 16) + 2]\\) and then compute on average\n\\(\\frac{\\sqrt{N}}{16}\\) hashes every\ntime.\n\nThen, you determine the \\(\\frac{\\sqrt{N}}{2} + 1\\) rows that the hint\ngenerates. You take out row \\(x\\). You then send the server a message\nthat contains:\n\n- \\([(r_1, c_1), (r_2, c_2) ...\n(r_{\\frac{\\sqrt{N}}{2}}, c_{\\frac{\\sqrt{N}}{2}})]\\): the set of\npoints included in that hint, except for the point you want\n\n- \\([(x_1, y_1), (x_2, y_2) ...\n(x_{\\frac{\\sqrt{N}}{2}}, y_{\\frac{\\sqrt{N}}{2}})]\\): the set of\nall other rows (including the row you want), with a randomly\nchosen column for each one\n\nYou send these in a random order, so the server does not know which\nis which. As far as the server can tell, you just sent it a random set\nof points, one per row, with the rows split in half in an arbitrary way.\nThe server XORs both sets of cells, and replies with both outputs.\n\nThe client then locally takes its hint, XORs it with the (non-junk)\nvalue returned by the server, and it has its answer.\n\nThe server does a computation based on \\(\\sqrt{N}\\) cells, and returns that data to\nthe client. The server knows nothing about which cell the client wants\n(provided the client does not reuse hints), because the client gave the\nserver every point in the set in its hint except the point it\nactually wants. And without knowing the hash that generated these\nsubsets, to the server, all the row and column choices look completely\nrandom.\n\n## Backup queries\n\nTo avoid using a hint twice (and thus leaking data), when the client\nmakes a query, it dumps the hint that it used, and \"promotes\" a backup\nhint into being a regular hint. Remember that each \"backup hint\" is a\npair of hints, one that is based on a random subset of the rows, and the\nother based on its complement. The algorithm is simple:\n\n- Take the hint value generated from the subset of the rows that\ndoes not contain the row you queried\n\n- XOR in the value you queried\n\nNow, you have a set of \\(\\frac{\\sqrt{N}}{2}\n+ 1\\) rows with an XOR of one randomly-chosen column from each\nrow: exactly the same format as a regular hint.\n\n## Updating the dataset\n\nSuppose a value in the data updates. Then, you can find all hints\nthat contain that value, and simply do a direct update: mix in the XOR\nof the old value and the new value.\n\nAnd that's the whole protocol!\n\n## Concrete efficiency\n\nIn asymptotic terms, everything in Plinko is \\(O(\\sqrt{N})\\): the size of the hints that\nthe client needs to store, the data transmitted per query\n(technically that's \\(O(\\sqrt{N} *\nlog(N))\\)), and the server-side compute costs. But it helps to\nput this all into the context of a real-world workload. So let's use the\nreal-world workload that I am the most familiar with: Ethereum.\n\nSuppose that you are querying a dataset with 10 billion values, each\nof which are 32 bytes. This is a pretty good description of the Ethereum\nstate tree, in a couple of years when it gets significantly bigger due\nto scale. We use cuckoo hashing\nto convert a key-value store into a \"flat\" lookup table of the type that\nPlinko handles.\n\nTo include Merkle branches, one simple way (though not the optimal\nway) is to treat them separately as queries into smaller databases; if\nit's a binary\ntree, then the first level is full-sized, the second level is\nhalf-sized, the third level is quarter-sized, etc; because PIR costs are\nall proportional to sqrt, this means that our total costs will all be\nmultiplied by \\(1 + 1 + \\frac{1}{\\sqrt{2}} +\n\\frac{1}{2} + \\frac{1}{2\\sqrt{2}} + ... = 3 + \\sqrt{2} \\approx\n4.414\\). Ethereum's state tree is (for now) a less-efficient\nhexary tree, but you can ZK-prove equivalence to a binary\ntree to get the efficiencies of a binary tree today.\n\nBut first, let's provide raw costs without the Merkle branches. This\nimplies a 100000\u00d7100000 grid, where each cell is 32 bytes (256 bits).\nThe client would need to store 100000\u00d7128 hints, which is 100000 \u00d7 128 \u00d7\n32 bytes ~= 390 MB, plus some extra backup hints.\n\nFor each query, the client would need to send the server a\npartitioning of the rows into two parts, and 100000 column indices. We\ncould be super-clever and do slightly better than this, but if\nwe do it naively, the row partition is 100000 bits (12.2 kB) and the\ncolumn indices are 100000 * ceil(log2(100000)) = 1.7m bits = 202.7 kB.\nThese two sum up to 215 kB per query.\n\nIf we introduce Merkle branches, the hint costs blow up to about\n1.68 GB. With clever work, we can actually avoid the\nper-query communication costs increasing by 4.414x, because we can\nadjust the PIR in such a way that we use the same (or rather, analogous)\nquery indices for the datasets representing different levels of the\ntree.\n\nServer-side costs involve reading and XOR'ing 100000 cells, or 3.05MB\nper query; the XOR is negligible, but the read load is significant.\nAdding Merkle branches would blow this up to about 13.4 MB in theory,\nthough the realistic added cost may well be negligible because clever\nengineering can ensure that matching values are always in the same page\n(memory pages are usually 4096 bytes).\n\nOur main tool to fiddle with these numbers is to make the\nrectangle unbalanced (have different width and height): we can\nincrease hint storage by 2x to reduce query size by 2x. Realistically,\nthis is a good tradeoff: storing even 10 GB of hints is not that hard,\nbut especially if nodes are sending multiple queries every 200\nmilliseconds, 215 kB per query is higher than we would be comfortable\nwith.\n\nIn the case of a search engine, the size of the objects would be much\nlarger (eg. 10 kB per text webpage), but the number of queries may be\nsmaller. Hence, query complexity would be relatively low, but hint size\nwould be large. To compensate for this, it may make sense to make the\nrectangle unbalanced in the other direction.\nBeyond Plinko:\ncomparing to the alternatives\n\nOne already-existing alternative to Plinko is TreePIR. TreePIR works by\nusing a \"puncturable PRF\": a hash function where you can provide a key\nthat allows evaluating it at all points except for one chosen\npoint. This allows you to generate a representation of the query columns\n(\"values generated from a seed \\(S\\)\nexcept the \\(j\\)'th value\")\nthat is logarithmic-sized - much more compact than providing the whole\nlist directly:\n\nThe full approach is somewhat more clever than the above diagram. The\nidea in this diagram reveals the excluded index 01100, violating\nprivacy. TreePIR works by providing the yellow values without\nrevealing their (horizontal) position in the tree. There are \\(2^{log_2(\\sqrt{N})} = \\sqrt{N}\\) possible\nsets of leaf values, and thus \\(\\sqrt{N}\\) possible sets of columns, that\ncan be generated by taking the left and right choice of position for\neach yellow value. The server has an \\(O(\\sqrt{N} * log(N))\\) time algorithm to\ncompute the XOR sum of chosen cells for every possible arrangement\nsimultaneously. The client knows which of these sums corresponds to the\nset that it actually needs. It uses single-server PIR to ask for it.\n\nThis reduces communication complexity to logarithmic, at the cost of\nincreased server work, and at the cost of removing the \"invertible PRF\"\nmechanism, forcing the client to \"grind\" through many possible hints\nuntil they find a matching hint.\n\nThere are various theoretical\nlower bounds\nfor how much overhead a PIR protocol needs to have. In reality, however,\nmost of these bounds come with caveats, and so the \"real\" lower bounds\nare far fewer. For example, if you have obfuscation, you can make any of\nthese protocols require only a constant real-time communication overhead\n- but of course, today, such protocols are still quite far away from\npracticality. The harder thing to reduce is server-side computation.\nBecause it's \"just\" XOR, the workload in Plinko is already not a big\ndeal. But if we want to remove the client-side \\(O(N)\\) download requirement, then we need\nthe server side to do much heavier computation.\n\nWith Plinko, PIR is much further along than it was a decade ago, but\nthere is still theoretically much further that these protocols could\ngo.",
    "contentLength": 16065,
    "summary": "Plinko is a new private information retrieval (PIR) protocol that reduces communication and server compute overhead from O(N) to O(\u221aN).",
    "detailedSummary": {
      "theme": "Vitalik explains Plinko PIR, a cryptographic protocol that enables private information retrieval from databases with square-root efficiency improvements over traditional methods.",
      "summary": "Vitalik introduces Private Information Retrieval (PIR) protocols, which allow clients to query databases without revealing what they're looking for, addressing privacy needs in applications like Wikipedia browsing, LLM searches, and blockchain data reads. He explains the foundational concept of two-server PIR, where clients send different random subsets to two servers and XOR the results to get their desired data, but notes this requires trusting at least one server and has high communication overhead. Vitalik then details Plinko PIR, which reduces both communication and server computation from O(N) to O(\u221aN) complexity by organizing data in a square grid, using preprocessing to generate hints, and employing invertible PRFs for efficient queries. Using Ethereum's state tree as a concrete example with 10 billion 32-byte values, he demonstrates that Plinko would require about 390 MB of hint storage and 215 kB per query, making it practically viable for real-world applications while maintaining strong privacy guarantees.",
      "takeaways": [
        "PIR protocols solve the important privacy problem of reading from databases without revealing what you're querying, useful for private Wikipedia browsing, LLM searches, and blockchain interactions",
        "Traditional two-server PIR requires trusting one of two servers and has O(N) communication overhead, but can be improved with preprocessing or single-server homomorphic encryption approaches",
        "Plinko PIR achieves O(\u221aN) efficiency for both communication and server computation by organizing data in square grids and using preprocessing to generate cryptographic hints",
        "For Ethereum-scale databases (10 billion values), Plinko requires manageable resources: ~390 MB hint storage, ~215 kB per query, and ~3 MB server reads",
        "The protocol includes mechanisms for backup queries to avoid reusing hints (which would leak information) and efficient updates when the underlying dataset changes"
      ],
      "controversial": []
    }
  },
  {
    "id": "general-2025-11-07-galaxybrain",
    "title": "Galaxy brain resistance",
    "date": "2025-11-07",
    "category": "society",
    "url": "https://vitalik.eth.limo/general/2025/11/07/galaxybrain.html",
    "path": "general/2025/11/07/galaxybrain.html",
    "content": "- \n\n- \n\n  \n    \n\n  \n  \n  Dark Mode Toggle\n  \n    \n\n  \n\n##  Galaxy brain resistance \n\n 2025 Nov 07 \nSee all posts\n\n \n \n\n Galaxy brain resistance \n\nOne important property for a style of thinking and argumentation to\nhave is what I call galaxy brain resistance: how\ndifficult is it to abuse that style of thinking to argue for pretty much\nwhatever you want - something that you already decided elsewhere for\nother reasons? The spirit here is similar to falsifiability\nin science: if your arguments can justify anything, then your arguments\nimply nothing.\n\n You want to get to step 2 and then stop.\n\nIt's easiest to motivate the need to think about galaxy brain\nresistance by looking at what happens in its absence. You've probably\nheard many cases of people saying things like this:\n\nWe are building a new decentralized ____ marketplace that will\nrevolutionize how ____ customers interact with their providers, and will\nallow creators to turn their audiences into digital nation states. $____\nis a governance token that lets you play a direct role in this rapidly\ngrowing market. If we capture even 1% of the ___ market share, this will\nbe a $___ billion market, and the governance token will be worth\nbillions.\n\nIn politics, you can get much worse. Like, say:\n\n[ethnic minority] is responsible for a lot of persistent social\ndisruption and drains our resources. If we were to just completely\nremove them (and I do mean completely remove, so they don't\ncome back), it would be a grim one-time act, but in the long term, if\ndoing it makes our economy grow even half a percentage point faster,\nthen in 500 years, our country will be twelve times richer than it\notherwise would have been. That is a huge number of much happier and\nmore fulfilled lives. It would be a grave injustice to condemn our\ndescendants to vastly poorer lives simply out of cowardly fear of paying\nthis one-time cost today.\n\nOne way to deal with arguments like the above is to deal with them as\nmathematical objects in a philosophy class, try to identify the specific\npremises or steps that you disagree with, and come up with refutations.\nBut a more realistic approach is to notice that, in the real\nworld, arguments like the above are almost always not reasoning - they\nare rationalization.\n\nThe speaker first came to their conclusion, likely for\nself-interested or emotional reasons (being a bagholder of the token, or\nreally really hating that ethnic group), and then they came up\nwith fancy arguments that supposedly show why they are justified. The\npurpose of the fancy arguments is (i) to trick the speaker's own higher\nmind into following the lead of their base instincts, and (ii) to try to\ngrow the movement by getting not just the deluded, but also vErY smArT pEopLe (or\nworse, actually smart people) on board.\n\nIn this post, I will argue that patterns of reasoning that are very\nlow in galaxy brain resistance are a common phenomenon, some\nwith consequences that are mild and others with consequences that are\nextreme. I will also describe some patterns that are high in galaxy\nbrain resistance, and advocate for their use.\nPatterns\nof argumentation and reasoning that are low in galaxy brain\nresistance\n\n## Inevitabilism\n\nConsider this recent tweet, which is a pretty good archetypal example\nof a lot of Silicon Valley AI boosterism:\n\nThis is a clear example of the inevitability\nfallacy. The post starts from a (perhaps reasonable) claim that full\nautomation of the economy is eventually bound to happen, and then skips\nstraight to arguing that this eventuality (and therefore disemployment\nof all human labor) should therefore be actively hastened. Why\nshould it be actively hastened? Well, we know why: because this tweet\nwas written by a company whose entire business is actively hastening\nit.\n\nNow, inevitabilism is a philosophical error, and we can refute it\nphilosophically. If I had to refute it, I would focus on three\ncounterarguments:\n\n- Inevitabilism overly assumes a kind of infinitely liquid\nmarket where if you don't act, someone else will step into your\nrole. Some industries are sort of like that. But AI is the\nexact opposite: it's an area where a large share of progress is being\nmade by very few people and businesses. If one of them stops, things\nreally would appreciably slow down.\n\n- Inevitabilism under-weights the extent to which people make\ndecisions collectively. If one person or company makes a\ncertain decision, that often sets an example for others to follow. Even\nif no one else follows immediately, it can still set the stage for more\naction further down the line. Bravely standing against one thing can\neven remind people that brave stands in general can actually\nwork.\n\n- Inevitabilism over-simplifies the choice space.\nMechanize could keep working toward full automation of the\neconomy. They also could shut down. But also, they\ncould pivot their work, and focus on\nbuilding out forms of partial automation that empower humans that\nremain in the loop, maximizing the length of the period when humans and\nAI together outperform pure AI and thus giving us more breathing room to\nhandle a transition to superintelligence safely. And other options I\nhave not even thought about.\n\nBut in the real world, inevitabilism cannot be defeated purely as a\nlogical construct because it was not created as a logical construct.\nInevitabilism in our society is most often deployed as a way for people\nto retroactively justify things that they have already decided to do for\nother reasons - which often involve chasing political power or dollars.\nSimply understanding this fact is often the best mitigation: the moment\nwhen people have the strongest incentive to make you give up opposing\nthem is exactly the moment when you have the most leverage.\n\n## Longtermism\n\nLongtermism is the pattern of thought that emphasizes the very large\nstakes involved in the long-term future. These days, many people\nassociate the term with effective altruist longtermism, eg. this 80000\nhours intro:\n\nIf we're just asking about what seems possible for the\nfuture population of humanity, the numbers are breathtakingly large.\nAssuming for simplicity that there will be 8 billion people for each\ncentury of the next 500 million years,10\nour total population would be on the order of forty quadrillion\n... And once we're no longer planet-bound, the potential number of people\nworth caring about really starts getting big.\n\nBut the general concept of appealing to the long term is much older.\nAppeals to sacrifice today for much larger benefits in the future have\nbeen done for centuries by personal financial planners, economists,\nphilosophers, people talking about the best time to plant trees, and\nmany others.\n\nOne reason that I hesitate to criticize longtermism is that, well,\nthe long term is actually really important. The reason\nwe don't hard fork a blockchain every time someone loses money to a\nhacker is that doing so may have a very visible one-time benefit, but it\nwould permanently damage the blockchain's credibility. As Tyler Cowen\nargues in Stubborn\nAttachments, the reason why economic growth is so important is\nthat it's one of very few things that reliably\ncompounds forever into the future, instead of disappearing or going\nin cycles. Educating your children only has payoffs after over a decade.\nIf you're not longtermist to some extent, you will never build a road.\nAnd when you don't value the long term, you run into problems. A major\none I fight against is technical debt: when software developers focus on\nshort-term targets without a coherent view of the long-term picture, the\nresult is that software turns into uglier and uglier junk over time\n(see: my push to simplify\nthe Ethereum L1).\n\nBut there is a catch: longtermist arguments have very low\ngalaxy brain resistance. After all, the long term is far away,\nand you can make beautiful stories about how if you do X, just about\nanything will happen. We see the downsides of this play out in the real\nworld over and over again, when we look at the behavior of both markets\nand politics.\n\nIn a market, the variable that chooses between these\ntwo modes is the prevailing interest rate. When interest rates are high,\nit only makes sense to invest in projects that show a clear near term\nprofit. But when interest rates are low, well, at this point the phrase\n\"low-interest\nrate environment\" is a well-understood byword for a situation that\ninvolves lots of people creating and chasing narratives that are\nultimately unrealistic, leading to a bubble and then a crash.\n\nIn politics, there are common complaints about\npoliticians acting in short-term ways to impress voters, hiding problems\nunder the rug so that they only reappear after the next election. But\nthere is also the idea of a \"bridge to\nnowhere\": an infrastructure project justified by a story about\nlong-term value that never ends up materializing.\n\n Left: a bridge to nowhere in\nLatvia. Right: Dentacoin, \"The Blockchain Solution for the\nGlobal Dental Industry\", used to have a market cap of over $1.8\nbillion. \n\nThe core problem in both cases is that thinking about the long term\nenables disconnection from reality. In a short-term-favoring\nenvironment, sure, you are ignoring the long term, but at least there is\na feedback mechanism: if a proposal is justified by a claim of\nnear-future benefits, then in the near future everyone will be able to\nsee if those benefits actually come to pass. In a long-term-favoring\nenvironment, an argument about benefits in the long term does not have\nto be correct, it just has to sound correct. And so\neven though the game everyone is claiming to play is\nchoosing ideas based on what brings value in the long term, the game\nthey're actually playing is choosing ideas based on what wins\nin an often dysfunctional and highly adversarial social\nenvironment.\n\nIf you can use stories about vague but extremely large positive\nconsequences in the long term to justify anything, then a story\nabout vague but extremely large positive consequences in the long term\ntells you nothing.\n\nHow do we get the benefits of long-term thinking without getting\ndisconnected from reality? First of all, I would say it's a really hard\nproblem. But getting beyond that, I do think there are some basic rules\nof thumb. The easiest is: does the thing you are doing in the\nname of long-term benefits actually have a solid long-term track record\nof achieving those benefits? Economic growth is like this. Not\nmaking species go extinct is also like this. Trying to install a\none-world government does not - in fact, it's one of many examples of\nsomething that has a solid long-term track record of failing and causing\nlots of harm in the process. If an action you're considering has\nspeculative long-term benefits but reliable known long-term harms, then...\ndon't do it. This rule doesn't always apply, because\nsometimes we really are living in unprecedented times. But it's also\nimportant to keep in mind that \"we really are living in unprecedented\ntimes\" has very low galaxy brain resistance.\nBad\nexcuses for banning things for personal aesthetic reasons\n\n I find uni disgusting. You're literally eating sexual\norgans of a sea urchin. Sometimes, at omakases, this stuff even gets\nshoved in front of my face. But even still, I oppose banning it, as a\nmatter of principle. \n\nOne thing that I despise is people using the coercive power of\ngovernment to impose what are ultimately personal aesthetic preferences\non the personal lives of millions of other people. Having aesthetics is\nfine. Keeping aesthetics in mind when designing public environments is\ngood. Imposing your aesthetics on other people's personal lives is not -\nthe cost you're imposing on others is vastly higher than any\npsychological benefit to yourself, and if everyone tries to do it that\ninevitably leads to either cultural hegemony or political war of all\nagainst all.\n\nIt's not hard to find obvious slam-dunk cases of politicians pushing\nto ban things for no better reason than \"eww, I find it disgusting\". An\neasy goldmine is anti-homosexuality crusades. Like St.\u00a0Petersburg Duma\ndeputy Vitaly\nMironov:\n\nLGBT have no rights. Their rights are not included in the socially\nsignificant list of protected values in our country. The so-called\nperverts have all the rights that they have as people, citizens of our\ncountry, but they are not included in some extended top list. We will\nremove them forever from the list of human rights issues in our\ncountry.\n\nOr even Vladimir Putin himself, who tried to justify invading Ukraine\nby complaining about ... the United States having too much \"satanism\".\nA more recent and somewhat different case is movements\nin the United States to ban synthetic meat:\n\nCultured meat is not meat ... it is made by man, real meat is made by\nGod Himself ... If you really want to try the nitrogen-based protein\npaste, go to California.\n\nBut many people are a step more cultured, and try to wrap this in\nsome kind of excuse. A common one is \"the moral fabric of society\",\n\"social stability\", and various similar reasons. Arguments like this are\nalso often used to justify censorship. What's the problem? I'll let\nScott Alexander handle\nthis one:\n\nThe Loose Principle of Harm says that the government can get angry at\ncomplicated indirect harms, things that Weaken The Moral Fabric Of\nSociety ... But allowing the Loose Principle Of Harm restores all of the\nold wars to control other people that liberalism was supposed to\nprevent. The one person says \"Gay marriage will result in homosexuality\nbecoming more accepted, leading to increased rates of STDs! That's a\nharm! We must ban gay marriage!\" Another says \"Allowing people to send\ntheir children to non-\u200bpublic schools could lead to kids at religious\nschools that preach against gay people, causing those children to commit\nhate crimes when they grow up! That's a harm! We must ban non-\u200bpublic\nschools!\" And so on, forever.\n\nThe moral fabric of society is a real thing - some societies are much\nmore moral than others in easily observable ways. But also, it's vague\nand undefined, which makes it so incredibly easy to say that just about\nanything contravenes the moral fabric of society. This applies also to\nmore direct appeals to the \"wisdom of\nrepugnance\", which have already been ruinous to the progress of\nscience and medicine. And it also appeals to newer catchall \"ban it\nbecause I don't like it\" wrappers, of which a common one is the desire\nto assert local culture against undefined \"global elites\". Some more\nquotes from the anti synthetic meat crusaders (remember, these people\nare not trying to explain why they are not going to eat synthetic meat\npersonally, they are explaining why they are coercively imposing their\nchoice on everyone else):\n\nGlobal elites want to control our behavior and push a diet of petri\ndish meat and bugs on Americans.\n\nFlorida is saying\nno. I was proud to sign SB 1084 to keep lab grown meat out of\nFlorida and prioritize our farmers and ranchers over the agenda of\nelites and the World Economic Forum.\n\nSome folks probably like to eat bugs with Bill Gates, but\nnot me.\n\nThis is a big part of my sympathy toward a moderate libertarianism. I\nwant to live in a society where banning something requires a clear story\nabout harm or risk imposed on clearly identified victims, and if that\nstory is successfully challenged in court then the law is repealed. This\ngreatly reduces the potential to capture government and use it to impose\none culture's preferences over the personal lives of others, or fight a\nwar of all against all as every team tries to do the same.\n\n## Apologia for bad finance\n\nIn crypto, you often hear bad arguments for why you should throw your\nmoney into various high-risk projects. Sometimes, they are\nsmart-sounding arguments about how a project is \"disrupting\" (ie.\nparticipating in) a trillion-dollar industry, and how this particular\nproject is really unique and doing things everyone else is not. Other\ntimes, it's just \"number go up because celebrity\".\n\nI am not opposed to people having fun, including having fun\nby risking some of their money. I am opposed to people\nbeing encouraged to put half their net worth into a token that the\ninfluencers all say will definitely go up, when the most realistic\noutcome is that two years later the token is worth nothing. But what I\nam even more opposed to is people arguing that speculative\ntoken games are morally righteous, because poor\npeople need that rapid 10x gain to have a fair chance in\nthe modern economy. Like, say, this:\n\nThis is a bad argument. One way to see why it's a bad argument is to\napproach it like any other argument, and deconstruct and refute the\nclaim that this is actually a meaningful or helpful form of \"class\nmobility\".\n\nThe core problem with the argument is: casinos are zero-sum games. As\na first approximation, each person who goes up a social class, there's a\nperson who goes down a social class. But if you dig deeper into the\nmath, it gets worse. In any standard welfare economics textbook, one of\nthe first ideas that you will see is that a person's utility function in\nmoney is concave. Each dollar is worth less to you the richer\nyou already are.\n\n An example\nof a utility curve. Notice how the slope (value per dollar)\ndecreases the more dollars you have. \n\nThis model has an important conclusion: random coin flips, especially\nlarge ones, are on average bad for you. Losing $100,000 is more bad for\nyou than gaining $100,000 is good. If we take a model where you\ncurrently have $200,000, and each 2x change in wealth pushes you up or\ndown a social class, then if you win a $100,000-sized coin flip, you go\nup about half a social class, but if you lose the coin flip, you go down\na full social class.\n\nEconomic models created by people whose motivation is to, well, study\nhuman decision making and try to find ways to improve people's lives,\npretty much always output conclusions like this. What kind of\neconomic model outputs the opposite conclusion - that it's good\nto throw in all your money in search of a 10x? Stories told by people\nwhose goal is to feel good about pumping coins.\n\nHere, my goal is not to blame people who actually are poor and\ndesperate and are looking for a way out of their situation. Rather, my\ngoal is to blame people who are financially doing quite well, who are\nusing \"poor and desperate people who really need that 10x\" as a meme to\njustify creating situations that encourage poor and desperate people to\nget into even deeper trouble.\n\nThis is a big part of why I have been pushing for the\nEthereum ecosystem to focus\non low-risk defi. Escaping your money being zeroed\nout by a political collapse, and getting first-world interest rates, is\nan excellent thing for people in the third world to have access to, and\ncan work wonders at pushing people up social classes without pushing\npeople down social calsses. Recently, someone asked me: why not say\n\"good defi\" instead of \"low-risk defi\"? After all, not all high-risk\ndefi is bad, and not all low-risk defi is good. My response was:\nif we focus on \"good defi\", then it's easy for anyone to make a\ngalaxy-brain argument that any particular type of defi is \"good\". But if\nyou say \"low-risk defi\", that's a categorization that has\nteeth - it's actually hard to make a galaxy brain argument\nthat a type of activity that is clearly regularly causing people to go\nbankrupt in a day is \"low-risk\".\n\nI certainly do not oppose high-risk defi existing - after all, I am a\nfan of prediction markets. But it's a healthier ecosystem when low-risk\ndefi is the mainstay, and high-risk defi is the side dish - something\nfun or experimental, and not meant for people to put half of their life\nsavings into.\n\nFinal question: is the idea that prediction markets are \"not just\ngambling\", because they benefit society by improving\naccess to accurate information, itself just a galaxy-brained\nretroactive rationalization? Some people certainly think so:\n\nI will offer my defense against this charge. The way that you can\ntell that this is not retroactive rationalization is there is a\nthirty-year-old intellectual tradition of appreciating\nprediction markets and trying to bring them into existence, which\nlong predated any possibility of making a serious profit off of them\n(either by creating such projects or by participating in them). This\nkind of pre-existing intellectual tradition is not something that exists\nfor memecoins, or even more borderline cases like personal tokens. But,\nonce again, prediction markets are not low-risk defi, and so they are a\nside dish, not something for you to put half your net worth into.\n\n## Power maximization\n\nIn the AI-related corners of the effective altruist community, there\nare many powerful people who, if you go up to them and ask them, will\nexplicitly tell you that their strategy is to accumulate as much power\nas possible. Their goal is to be well-positioned so that, when some kind\nof \"pivotal moment\" comes, they can come out with guns blazing and lots\nof resources under their command and \"do the right thing\".\n\nPower maximization is the ultimate galaxy brain tactic. \"Give\nme power so I can do X\" is as close as it gets to an argument that is\nequally convincing no matter what the X. All the way up until\nthe critical moment (which, in AI eschatology, is the moment right\nbefore we either get utopia or all die and turn into paperclips),\nthe actions that you would take to maximize power for altruistic\nreasons, and the actions that you would take to maximize power because\nyou're a greedy egomaniac, are exactly the same. Hence, anyone\ntrying to do the latter can, at zero cost, just tell you that they are\ntrying to do the former and convince you that they are a good\nperson.\n\nFrom the outside view, this kind of argument is clearly crazy:\neveryone thinks they're more ethical than everyone else, and so it's\neasy to see how even though each person thinks their power\nmaximization is net-good, actually it's really not. But from the inside,\nif you look at the world, and you see the hate on social media, the\npolitical corruption, the invasions, the other AI companies\nbehaving unscrupulously, the idea that you personally are the good guy\nand you should just ignore the corrupt outside world and go solve things\nyourself certainly feels compelling. And this is exactly why it's\nhealthy to take an outside view.\n\nAlternatively, you can take a different and more humble inside view.\nHere's a fun argument from\nthe effective altruism forums:\n\nArguably the largest advantage of investing is that it can\nexponentially grow financial resources, which can be used for good at a\nlater point. The S&P 500 has had an inflation-adjusted annualized\nreturn of ~7% since its inception in 1926\n\n...\n\nThe risk of value drift is even harder to estimate, but an important\nfactor. For instance, these three sources (1,2,3)\ncollectively suggest a yearly value drift rate of ~10% for individuals\nwithin the effective altruism community.\n\nThat is, while it's true that each year your wealth grows by\n7%, it's also empirically true that if you believe in a cause today,\nyou're likely to believe in it about 10% less tomorrow. This matches up\nwith an observation by Tanner Greer that public\nintellectuals tend to have a \"shelf life\" of about 10-15 years,\nafter which their ideas stop being better than the surrounding\nbackground noise (I will let the reader decide the significance of the\nfact that I started publicly writing in 2011).\n\nHence, if you grow your wealth to act later, your future self may\nwell do something with that extra wealth that your present self does not\neven support.\n\n## I'm-doing-more-from-within-ism\n\nOne problem that repeatedly happens in AI safety is a sort of hybrid\nbetween power maximization and inevitabilism: people deciding that the\nbest way to advance the cause of AI safety is to join companies making\nsuperintelligent AI happen even faster, and try to improve them from\nwithin. Here, you often get rationalizations like, say, this:\n\nFrom the inside view, this seems reasonable. From the outside view,\nhowever, you basically end up with this:\n\nAnother good example of this school of thought is the modern Russian\npolitical establishment. Here, I'll just quote this\nFinancial Times article:\n\nThe full-scale assault on Ukraine on February 24, three days after\nPutin recognised the Donbas separatists, exceeded their worst fears.\nThey discovered Putin's true intentions along with the rest of the\nworld: on television. Putin's failure to heed the technocrats' warnings\ndevastated them. \"I'd never seen [Gref] like that. He was completely\nbereft, in a state of total shock,\" says a former executive who saw Gref\nin the war's early days. \"Everyone thinks this is a catastrophe, him\nmore than anyone else.\"\n\n...\n\nWithin the narrow confines of the Russian political elite,\ntechnocrats such as Gref and Nabiullina were once thought of as\nmodernisers, a reformist counterbalance to the siloviki, the hardline\nsecurity services veterans at Putin's other shoulder.\n\nHowever, when faced with a historic chance to defend their belief in\nopen markets and speak out against the war, they demurred. Instead of\nbreaking with Putin, the technocrats have cemented their role as his\nenablers, using their expertise and tools to soften the blow of western\nsanctions and hold Russia's wartime economy together, according to\nformer officials.\n\nAgain, the problem is that \"I'm doing more from within\" has very low\ngalaxy brain resistance. It's easy to say \"I'm doing more from within\"\nregardless of what is the actual specific thing that you're doing from\nwithin. And so you end up just being a cog in the machine, with the same\neffect as the other cogs who are there to help their family live in a\nbeautiful mansion in a premium neighborhood and eat expensive dinners\nevery day feed their family, but with a slightly better\njustification.\nSo how do you\navoid galaxy braining yourself?\n\nThere are lots of different things you can do, but I will focus on\ntwo:\n\nHave principles\n\nHave hard rules of what you're not willing to do - don't kill\ninnocent people, don't steal, don't defraud, respect people's personal\nfreedom - and have a very high bar for considering any exceptions.\n\nPhilosophers generally call this deontological\nethics. Deontological ethics confuses many people - surely,\nif your rules have some underlying reason behind them, you should just\ngo straight to pursuing that underlying reason. If \"don't steal\" is a\nrule because stealing usually hurts the victim more than it benefits\nyou, then you should just follow the rule of not doing things that hurt\nthe victim more than they benefit you. If sometimes stealing benefits\nyou more than it hurts the victim - then steal!\n\n \"The victims are faceless corporations with billionaire\nshareholders, therefore my shoplifting crusade is righteous\"\n\nThe problem with this kind of consequentialist approach is that it\nhas no galaxy brain resistance. Our brains are really good at coming up\nwith arguments why, in this particular case, the thing that you already\nwant for other reasons happens to also be great for humanity.\nDeontological ethics says: no, you can't do that.\n\nOne form of deontology that many people follow is rule\nutilitarianism: choose rules based on what leads to the greatest\ngood, but when it comes time to choose individual actions,\njust follow the rules you've already chosen.\n\n## Hold the right bags\n\nOne other common theme above is that your actions are often set by\nyour incentives - in crypto lingo, what bags you hold. This pressure is\nvery difficult to resist. The easiest way to avoid this is to not give\nyourself bad incentives.\n\nAnother corollary is to avoid holding the wrong social bags:\nwhat friend cluster you're closely attached to. You should not try to\navoid having social bags - doing so is counter to our most\nbasic human instincts. But it is possible to at least diversify them.\nThe easiest one-step action you can take to make a big difference here\nis to choose your physical location well.\n\nThis brings me to my own contribution to the already-full genre of\nrecommendations for people who want to contribute to AI safety:\n\n- Don't work for a company that's making frontier fully-autonomous AI\ncapabilities progress even faster\n\n- Don't live in the San Francisco Bay Area",
    "contentLength": 28292,
    "summary": "Blog post argues that \"galaxy brain resistance\" - how hard it is to abuse reasoning styles to justify predetermined conclusions - is crucial for evaluating arguments.",
    "detailedSummary": {
      "theme": "Vitalik explores how certain patterns of reasoning can be manipulated to justify almost any position, and advocates for more resistant forms of thinking.",
      "summary": "Vitalik introduces the concept of 'galaxy brain resistance' - how difficult it is to abuse a style of thinking to argue for whatever you want, similar to falsifiability in science. Vitalik examines several low-resistance patterns including inevitabilism (using 'it will happen anyway' to justify harmful actions), longtermism (appealing to vast future benefits to justify present actions), aesthetic-based bans disguised as moral arguments, bad financial advice wrapped in social justice language, power maximization for supposedly altruistic ends, and 'doing more from within' rationalizations. These patterns are problematic because they often represent post-hoc rationalization rather than genuine reasoning - people decide what they want to do first, then construct sophisticated arguments to justify it. Vitalik advocates for higher-resistance approaches like deontological ethics (having firm principles and rules) and being mindful of your incentives ('holding the right bags'). Vitalik concludes with specific advice for AI safety: don't work for companies accelerating AI capabilities and don't live in the San Francisco Bay Area, as these create problematic incentive structures.",
      "takeaways": [
        "Arguments that can justify anything actually justify nothing - look for 'galaxy brain resistance' in reasoning patterns",
        "Many sophisticated arguments are actually post-hoc rationalizations for decisions already made based on self-interest or emotion",
        "Inevitabilism ('it will happen anyway') falsely assumes infinitely liquid markets and ignores collective decision-making power",
        "Longtermist arguments, while sometimes valid, have low resistance because distant futures enable disconnection from reality and feedback mechanisms",
        "Having firm deontological principles provides better galaxy brain resistance than pure consequentialist thinking"
      ],
      "controversial": [
        "Vitalik's criticism of longtermism and effective altruism, particularly around power maximization strategies",
        "The claim that working 'from within' AI companies is generally counterproductive for safety",
        "Specific advice against living in San Francisco Bay Area for AI safety contributors",
        "Characterization of some EA longtermist arguments as similar in structure to extremist political reasoning"
      ]
    }
  },
  {
    "id": "general-2025-10-19-gkr",
    "title": "A GKR Tutorial",
    "date": "2025-10-19",
    "category": "math",
    "url": "https://vitalik.eth.limo/general/2025/10/19/gkr.html",
    "path": "general/2025/10/19/gkr.html",
    "content": "- \n\n- \n\n  \n    \n\n  \n  \n  Dark Mode Toggle\n  \n    \n\n  \n\n##  A GKR Tutorial \n\n 2025 Oct 19 \nSee all posts\n\n \n \n\n A GKR Tutorial \n\nSpecial thanks to Lev Soukhanov, Zhenfei Zhang, Zachary\nWilliamson and Justin Drake for feedback and review\n\nIf you're following the \"cryptography side of crypto\", at this point\nyou've likely heard a lot about ultra-fast ZK-provers: ZK-EVM provers proving the\nEthereum L1 in real-time with only about fifty consumer GPUs, people\nproving\n2 million Poseidon\nhashes per second on consumer laptops, and zk-ML systems proving LLM\ninference with increasing speed.\n\nIn this post, I will explain in detail a family of protocols that is\nbehind the extreme speed of many of these proving systems:\nGKR.\n\nI will focus on a GKR implementation for proving Poseidon hashes (and\nother computations that follow a similar pattern). To see GKR in the\ncontext of more generic circuit evaluation, see Justin\nThaler's note and this\nLambdaclass post.\nWhat is GKR and why is it so\nfast?\n\nSuppose that you have a computation that is \"big\" in two dimensions:\nit involves processing data through at least a medium number of\n(low-degree) \"layers\", and it involves applying the same function to a\nlarge number of inputs. Something like this:\n\nAs it turns out, a lot of\nthe biggest computation we do fits into this kind of pattern.\nCryptographers will notice that many computationally intensive proving\noperations involve proving lots of hashes, and each hash internally has\nthat exact kind of internal structure. AI researchers will notice that a\nneural network, the core building block of an LLM, has exactly that\ninternal structure (both because you can prove inference of many tokens\nin parallel, and because within each token, you have a\nstructure mixing element-wise neural layers with global matrix\nmultiplication layers, which technically violate the cross-input\nindependence in the diagram above but in practice are very easy to put\ninto a GKR system).\n\nGKR is a cryptographic scheme that is tailor-made for this pattern.\nIt achieves its efficiencies by avoiding the need to make\ncommitments to any of the intermediate layers: you only\nneed to commit to the inputs and the outputs. \"Commitments\" here mean\nputting the data into a cryptographic data structure, either a KZG or a\nMerkle tree, that allows you to prove queries about specific things\nabout that data. The cheapest commitment, a Merkle tree of an erasure\ncoding (used in STARKs),\nrequires you to hash 4-16 bytes for each byte you commit to: hundreds of\nadditions and multiplications when the underlying operation you're\nproving at that step might be a single multiplication. GKR avoids all of\nthis, except at the very beginning and the very end.\n\nNote that GKR is not \"zero knowledge\": it only handles succinctness,\nnot privacy. If you want zero knowledge, wrap the GKR proof in a\nZK-SNARK or ZK-STARK.\nSumchecks: the core building\nblock\n\nSuppose you have a multivariate polynomial \\(P(x_1, x_2 ... x_N)\\), which is low-degree in\neach dimension. You want to prove the sum of evaluations of that\npolynomial across a hypercube - that is, you want to prove the value of\n\\(\\sum_{i_k \\in \\{0,1\\}} P(i_1, i_2 ...\ni_N)\\). There is a standard technique that reduces this problem\nto that of evaluating \\(P\\) at a random\ncoordinate. That is, you can convert an \"obligation\" to prove \\(\\sum_{i_k \\in \\{0,1\\}} P(i_1, i_2 ... i_N)\\)\ninto an obligation to prove \\(P(c_1, c_2 ...\nc_N)\\), for some \\(c_1, c_2 ...\nc_N\\) that get randomly generated as part of the proving process.\nHere is what it looks like for a multilinear polynomial (one\nthat is linear in each variable; so \\(x_ix_j\\) is allowed but not eg. \\(x_i^2\\)):\n\nAt each step, the prover provides a sum for each half of the current\nhypercube. These sums (and any previous randomness) get hashed together\nand used as randomness to choose an evaluation coordinate. The prover\nthen uses a simple linear combination of the two halves of the\n(dimension N) hypercube to compute the half-size (dimension N-1)\nhypercube of evaluations where the first coordinate is set to the\nevaluation coordinate that was just randomly chosen. In the diagram\nabove, the prover starts with evaluations at ((0,0,0), (0,0,1) ...\n(1,1,1)), and takes the top half plus 6 times the difference between the\ntop half and the bottom half to compute the half-sized hypercube of\nevaluations at ((6,0,0), (6,0,1), (6,1,0), (6,1,1)).\n\nThis is a standard \"random linear combination\" trick: proving that a\nlinear claim (in this case, a sum) is true for multiple values (here,\ntwo halves of the hypercube) by proving it for a random linear\ncombination of those values. A similar idea is used in bulletproofs.\nAnd like in bulletproofs, we use the random linear combination trick\nrecursively: The prover continues turning a claim about a hypercube into\na claim about a hypercube half the size, until the \"hypercube\" shrinks\nto one single value. This value is the evaluation of the original\nmultivariate polynomial at a random point (here, (6, 4, 7)). If the\nprover can prove that evaluation, then they've proven the original\nsum.\n\nA sumcheck is not useful by itself: after all, it simply converts one\nunproven claim about a bunch of numbers into another unproven claim\nabout a bunch of numbers. But it is an incredibly useful and powerful\ningredient in a whole bunch of proof systems, including GKR. It\nis also very efficient: particularly, the prover does not need\nto commit to anything, and the prover and verifier only need to hash a\nsmall constant amount of data per round to generate the random\ncoordinates.\n\nNow, let's get into GKR. For this post, we'll focus on a single type\nof computation: proving a large number of hashes of the Poseidon2 hash function. The\nPoseidon2 hash function is quite simple; here's a full graphical\ndescription of the core permutation function that it uses:\n\nEssentially, you do many rounds that alternate between multiplying\nthe whole data by a matrix, and cubing and adding a constant to each\nelement. The rounds in the middle are \"partial rounds\", where both\noperations are simplified: the matrix is an \"easy\" matrix (\\(diag + J\\) in the diagram above) that can\nbe evaluated as \\(x_i \\rightarrow x_i * d_i +\nsum(x_0, x_1 ... x_{15})\\), and only the first value gets cubed.\nThis combination of full and partial rounds has been deemed optimal for\nthe tradeoff between performance and security, but it's not essential to\nthe construction: you could do just full rounds with a random\nmatrix if you wanted to.\n\nThere are many versions and parameter choices of Poseidon2; the above\nis not meant to accurately depict any specific live implementation,\nrather it shows the general pattern.\n\nGKR is well-suited to prove a large number of executions like this in\nparallel. Let us go through how it works. We will first start with a\nsimplified version of Poseidon, where there are no partial rounds and no\nmatrices - instead, you just do repeated \\(x\n\\rightarrow x^3 + r_j\\). GKR works by running through the\ncomputation backwards:\n\n- First, the prover starts with an \"obligation\" to prove the final\noutput.\n\n- We treat the outputs as evaluations of a multivariate polynomial on\na hypercube, and we ask the prover to prove the output by proving this\n\"output polynomial\" evaluated at a randomly chosen point.\n\n- From there, we repeatedly use sumchecks to progressively convert\nthis \"obligation\" into an obligation to prove a claim about the previous\nlayer.\n\n- Eventually, we get an obligation to prove an evaluation of the\nmultivariate polynomial that represents the inputs. The\nverifier knows the inputs, so they can just check this directly.\n\nHere's the top layer. For cleaner exposition (ie. to avoid the\nnumbers blowing up in size), in this example we will do all the math\nmodulo 89. In reality, \\(V\\) is the\noutputs of the Poseidon computation and so it's numbers in a 32-bit\nprime field (often KoalaBear\n(\\(2^{31} - 2^{24} + 1\\))), and \\(p_i\\) (and hence \\(W\\) later on in this post) are over a\ndegree-4 extension field (often represented as elements of the form\n\\(x = a_0 + a_1v + a_2v^2 + a_3v^3\\)\nwhere \\(v^4 = 3\\)) to ensure (close to)\n128-bit security.\n\nNote that we have not yet done any sumchecks. The\nfirst sumcheck we do will actually be on the second-last layer\n(ie. V31). And to do this we will need to introduce some\nextra machinery. Particularly, the goal is not to prove the\nbasic sum \\(\\sum_{i_k \\in \\{0,1\\}} V_{31}(i_1,\ni_2 ... i_N)\\). Instead, we are proving \\(\\sum_{i_k \\in \\{0,1\\}} (V_{31}(i_1, i_2 ... i_N)^3 +\nr_{31}) * W_{31}(i_1, i_2 ... i_N)\\).\n\n\\(W_{31}\\) here is the \"weights\"\nthat correspond to \"evaluate at \\(p_{32}\\)\". Any evaluation of a multilinear\npolynomial can be represented by weights. For example, consider the 2D\nmultilinear polynomial with evaluations [[1, 5], [3, 9]]:\n\nNow, let's say we want the evaluation at (6, 4). We can compute this!\nAn easy way is to take the difference (eg. \\(x_{01} - x_{00}\\)) along a horizontal or\nvertical line and re-apply it over and over to \"extend\" that line:\n\nHere, [[15, -18], [-20, 24]] is the \"weights\" corresponding to\n\"evaluate at (6, 4)\". We can also compute the weights via a nice\nformula: \\([[(1-4) * (1-6), (1-4) * 6], [4 *\n(1-6), 4 * 6]]\\). A similar thing works in higher dimensions.\n\nGoing back to GKR. The expression we wanted to prove the value of is\n\\(\\sum_{i_k \\in \\{0,1\\}} (V_{31}(i_1, i_2 ...\ni_N)^3 + r_{31}) * W_{31}(i_1, i_2 ... i_N)\\). Because \\(W_{31}\\) is the weights corresponding to\n\"evaluate at \\(p_{31}\\)\", this\nexpression is exactly the same thing as saying \"compute \\((V_{31}(i_1,i_2...i_N)^3 +\nr_{31})(p_{31})\\)\", which is itself the same thing as \\(V_{32}(p_{31})\\). Now, we do the sumcheck.\nIt's like we did before, except at each step we provide partial sums not\nof the values themselves, but of the evaluations of the values.\nAdditionally, there is another nuance: because this is a degree-4\nexpression (degree 3 coming from \\(V_{31}^3\\) and degree 1 coming from \\(W\\)), we need to provide five evaluations\nat each step, instead of just the left and right side.\n\nThis is all complex, so to avoid adding even more pain let's just set\n\\(r_{31}\\) to zero so we can just focus\non the cubing. Here we go:\n\nAs the bot loves to say, let's go through this step by step. The\nprover starts off with the claim \\(sum(V_{31}^3 * W_{31}) = 83\\) (remember:\nthis is the same thing as saying \\(V_{32}(p_{32}) = 83\\)). The prover computes\nand provides the five partial sums of \\(sum(V_{31}^3(x, ...) * W_{31}(x, ...))\\) for\n\\(x \\in \\{0,1,2,3,4\\}\\). The prover\ngets a random coordinate \\(c_0 = 9\\),\nand computes half-sized hypercubes from \\(V_{31}^3(9, ...)\\) and \\(W_{31}(9, ...)\\). The prover then\nrepeats.\n\nThe verifier is able to check each step against the previous step.\nParticularly:\n\n- The verifier can add up the first two sums (x=0 and x=1 in the chart\nabove) and check them against the total from the previous round.\n\n- To compute the totals for later rounds, the verifier can do a Lagrange\ninterpolation on the five provided sums. If you know you have a\ndegree <= 4 polynomial, and you have evaluations at five known\npoints, you can easily compute the evaluation at any other point.\n\nHere's the code to compute the Lagrange coefficients:\n\ndef deg4_lagrange_weights(x):\n    \"\"\"\n    Return weights w_0, w_1, w_2, w_3, w_4 such that\n    for any degree-4 polynomial p,\n    p(x) = sum_k w_k * p(k)  with k in {0, 1, 2, 3, 4}.\n    \"\"\"\n    nodes = (0, 1, 2, 3, 4)\n    denoms = (24, -6, 4, -6, 24)  # \u03a0_{m\u2260k} (k - m) for k = 0,1,2,3,4\n    coeffs = []\n    for idx, k in enumerate(nodes):\n        num = 1\n        for m in nodes:\n            if m != k:\n                num *= (x - m)\n        coeffs.append(num / denoms[idx])\n    return tuple(coeffs)\n\nYou can check the example above yourself, and see that the linear\ncombination of the values in each round with coefficients generated by\nthis equals the next round's total (modulo 89).\n\nAt the end, the prover provides the evaluation \\(V_{31}(9, 16, 8) = 39\\). Because \\(W\\) has a mathematical structure, the\nverifier can compute \\(W_{31}(9, 16, 8) =\n85\\) themselves. Then, the verifier checks the equation\n(remember: you can check an equation between polynomials by checking an\nevaluation at a random point): \\(39^3 * 85 =\n87\\) (all modulo 89 in our example).\n\nWhat have we done? We have reduced an unproven claim of the\nform \\(V_{32}(p_{32}) =\ny_{32}\\) to an unproven claim of the form \\(V_{31}(p_{31}) = y_{31}\\), all without\nusing any commitments!!!\n\nWe then proceed and keep going, until we get to a claim \\(V_0(p_0) = y_0\\). Because \\(V_0\\) is just the inputs, and the verifier\nhas the inputs, the verifier can just check this directly. Let us recap\nthe entire flow in diagram form:\n\nNow, let's get back to one thing I left out here: the matrix\nmultiplication layers. It turns out that there is a fairly easy way to\ngenerate weights that represent the idea \"multiply \\(V\\) by a matrix \\(M\\) that only combines values that are\nwithin the same batch of 16, and then evaluate \\(VM\\) at point \\(p\\)\". There is also a fairly easy way to\nevaluate the polynomial generated by those weights at some\nother point \\(q\\) without actually\nneeding to compute \\(M\\)\n(cryptographers seem to love Star Trek, so they sometimes say \"materialize\"\n\\(M\\)). This is important for the\nverifier to be able to check the link between \\(V_{i-1}(p_{i-1})\\) and \\(V_i(p_{i-1})\\).\n\nBut there is also a much dumber way to do this, which does not lose\nmuch efficiency (in fact, it increases efficiency slightly on\nthe prover side): just run 16 sumchecks in parallel, sharing the same\nrandomness so they all get the same coordinates. The verifier's \"state\"\n\\(V_{i-1}(p_{i-1})\\) will be a size-16\nobject, and the verifier can apply the matrix to that directly.\n\n## Optimizations\n\nWe can do a major optimization to the sumcheck above, reducing the\nnumber of sums computed and provided per round from 5 to 3. This happens\nfrom two sources.\n\nFirst, because we know the verifier can check \\(sum_0 + sum_1\\) against the previous total,\nwe can instead take out \\(x_1\\), and\nask the verifier to just compute \\(sum_1 =\ntotal - sum_0\\) themselves.\n\nA second, more involved optimization is usually called Gruen's\ntrick. Let us go back to the definition of the matrix \\(W\\). Here is some python code for computing\nit:\n\ndef generate_weights(eval_point):\n    weights = [1]\n    for c in eval_point[::-1]:\n        R = weights[0] * c\n        L = weights[0] - R\n        weights = [0] * (len(weights) * 2)\n        weights[::2] = L\n        weights[1::2] = R\n    return weights\n\nLet's first focus on the first round. Notice that the only part of\nthis calculation that depends on the first coordinate is the very last\nstep. What happens if the prover computes \\(V^3 * W\\) using a half-sized version of\n\\(W\\) where we just skip the last\nstep?\n\nHere are the \\(W_{half}\\) and \\(W\\) for our example above (derived from\n\\(p = [2, 7, 18]\\)), side by side:\n\nAs with our earlier example, this is just \\([[(1-7) * (1-18), (1-7) * 18], [7 * (1-18), 7 *\n18]]\\), modulo 89. The evaluations of \\(W\\) at some \\(x\\) can be found by taking \\(W_{half}\\) and multiplying in \\(x * c + (1-x) * (1-c)\\) where \\(c\\) is the current evaluation point (in\nthis case, the \\(2\\) from \\([2, 7, 18]\\)). At \\(x=0\\), this just means multiplying by \\(1 - 2 = -1\\) (modulo 89), and you should be\nable to visually check that the front face of the right cube is exactly\nthat.\n\nYou can do the same thing with linear combinations of \\(W\\): given the sum \\(hsum_x = sum(V_{31}^3(x, ...) *\nW_{half}(...))\\), you can multiply it by \\(x * c + (1-x) * (1-c)\\) to get the \"real\nsum\" \\(sum_x = sum(V_{31}^3(x, ...) * W_{31}(x,\n...))\\).\n\nThis is another way to allow the prover to send four values instead\nof five: because \\(W_{half}\\) is\nindependent of the current dimension, its evaluation will be the same\nfor the top half of \\(V\\) and the\nbottom half, and for that matter for the extension \\(V(2, ...)\\). This means that, in the current\ndimension, \\(V^3 * W_{half}\\) is only\ndegree-3 (and not degree-4), and so you only need four values (\\(hsum_0 ... hsum_3\\)) to compute \\(hsum_c\\) with Lagrange coefficients.\n\nTo check against the previous total, the verifier would check if\n\\(hsum_0 * (1 - c) + hsum_1 * c =\ntotal\\). But even better, we can combine the two tricks\nto let the prover only send three values! We do this by\nallowing the verifier themselves to compute \\(hsum_1 = \\frac{total - hsum_0 * (1 -\nc)}{c}\\).\n\nA third, smaller, optimization is that when \\(r_i\\) is not zero, we don't need to handle\nit inside the sumcheck; instead, the verifier can simply subtract it\nfrom \\(V_i(p_i)\\) at the appropriate\ntime.\n\n## Moving to \"real\" Poseidon2\n\nThe next source of optimization comes from moving to the real version\nof Poseidon2, where \u00be of the layers only cube one value per round. This\nmeans that 15 out of 16 values in each round stay the same.\n\nUnfortunately, we can't skip doing a sumcheck over them entirely,\nbecause if the evaluation point for the cubed value shifts from \\(p_i\\) to \\(p_{i-1}\\), we need to shift the evaluation\npoint for the other values as well. But the good news is that (with\nGruen's trick) it's only a linear sumcheck. The even better\nnews is that you can batch it: you can prove \\(sum(V_{(i, 1)}) ... sum(V_{(i, 15)})\\) by\nproving \\(sum(\\sum_{j=1}^{15} V_{(i,j}) *\n\\alpha ^ j)\\) for some random \\(\\alpha\\). Thus, most of the prover's work\nis just computing the random linear combination \\(\\sum_{j=1}^{15} V_{(i,j}) * \\alpha ^ j\\).\n\\(V_{(i,0)}\\) gets cubed, so it does\nhave to be handled with a cubic sumcheck (but sharing the same\nrandomness), but this is now a much smaller cost.\n\nYou can find some sample code implementing these algorithms here: https://github.com/ethereum/research/tree/master/gkr\n\n## Polynomial commitments\n\nOne thing that I threw under the rug in the above analysis is that in\nthis whole example, the verifier is working with the full list of inputs\nand outputs \"in the clear\". For such a protocol as written to be secure,\nthis means that the verifier would have to hash that entire list to\ngenerate the Fiat-Shamir\nrandomness. Thus, to receive a proof of the evaluations of \\(N\\) hashes, the verifier would have to\ncompute... \\(N\\) hashes.\n\nIn a real-world protocol, no one is interested in the evaluations of\nlots of hashes in isolation. Instead, the idea is to prove the hashes,\nand then expose the set of hash inputs and hash outputs as a \"lookup\ntable\" that can be used to prove some bigger computation that\nincludes those hashes.\n\nFor this, you could \"just use FRI\".\nThe verifier needs to get a proof of \\(V_{32}(p_{32})\\) and \\(V_0(p_0)\\), which would turn into a linear\ncombination over the values encoded in the commitments. This can be\ndone, and is certainly much faster than proving the hashes inside the\nFRI. An alternative is to use polynomial commitment schemes that are\nnative to multilinear polynomials. A common one is BaseFold, though there\nhave been other more recent improvements (eg. WHIR. Realistically, the\nchoice depends on what proof system you are using for the non-hash part\nof your computation.\n\nOne important security consideration is an observation from 2025:\nif the circuit being proved can also compute the Fiat\u2013Shamir hash\nused to derive challenges fast enough within its own depth, a\nmalicious prover might be able to predict challenges, and thus cheat the\nproof system. This is not an issue for using GKR to prove hashes as in\nthe example above, because (i) the scheme is designed around one fixed\ncircuit that does not create room for an attacker to use these tricks,\nand (ii) the attack involves feeding GKR a circuit that internally\ncomputes both a hash and other computation, and so by definition there\nis not enough depth in a hash-only circuit to execute it. However, for\nmore complicated and general-purpose GKR use cases, it is an issue; one\nmitigation is to adjust the hash used to compute Fiat-Shamir challenges\nso that it requires more rounds to compute than the number of rounds the\ncircuit has.\n\n## How efficient is GKR?\n\nLet's continue to focus on the proving Poseidon hashes use case. Here\nis a spreadsheet I made that tallies up all the costs a demo\nimplementation that I made:\n\nRoughly a 15x theoretical overhead (!!). This compares to a ~100x\ntheoretical overhead for proving Poseidon with traditional STARKs. The\ngains come from the fact that the largest costs of the traditional STARK\nproof (see my\nprover here, see the \"regular STARK\" tab of my\ntheorycraft spreadsheet here) come from hashing and FFT'ing\nthe trace that consists of all intermediate values (though there is a\nclever trick that allows the proof to only require one trace value per\ninner round). Hence, we don't have to do any of that at all: the\nprover is only committing to the input and the output, for everything in\nthe middle the prover is only doing a (much lighter) sumcheck at each\nround, and for linear work (ie. matmul) the prover needs to do nothing\nat all.\n\nBut these numbers are just theory. There are two practical reasons\nwhy practice might differ from theory, that pull in opposite\ndirections:\n\n- Sumchecks require more complicated and highly intensive memory\nshuffling that naive hashing does not. In practice, sumchecks are often\nmemory-bound.\n\n- Real-world hashing workloads are often at least partially serial,\nwhereas sumchecks are massively parallelizable.\n\nAnd on top of this, there is the possibility that one or the other\njust happens to be harder to optimize in a given software or hardware\ncontext. Does my implementation match up with the theory? Well, let's\nsee:\n\nUnder 10x!!! Though I would not take this number as the \"true\noverhead\" either; it most likely means that my implementation of the raw\nexecution is under-optimized.\n\nNow, there still are some optimizations we have not tried.\nParticularly, you can adjust the sumcheck so that the \"hypercube\" is an\nunbalanced prism, where the first dimension has a size larger than 2.\nThis makes the first round larger, but makes all subsequent rounds\nsmaller; the tradeoff is fine because in the first round \\(V\\) is in the base KoalaBear prime field\nwhereas in later rounds everything is in the extension field, which is\nmuch more expensive. If you're hashing larger amounts of data, you could\nalso give up on the 2\u21921 structure, and hash eg. 4\u21921 or even more; as the\nwidth increases, the overhead of GKR approaches zero (!!!).\n\nHence, proving hashes with single-digit overhead is very\npossible.\n\nIn this post, hashes were just an example; GKR is a family of\ntechniques that is well-suited for proving many kinds of\ncomputations. If a computation can be expressed in the \"batch\nand layers\" format, where each layer can be expressed as a low-degree\npolynomial, then GKR can be applied directly. But even if there are\ncross-interactions between different values in a large computation,\nthere are often tricks that can be used. This is relevant for proving\nLLM inference, and many other situations. Most provers that strive for\nvery high efficiency and very low overhead are going to use something\nlike GKR in some form.",
    "contentLength": 23117,
    "summary": "GKR enables ultra-fast ZK proving by avoiding expensive intermediate layer commitments, using sumchecks to verify layered computations like Poseidon hashes.",
    "detailedSummary": {
      "theme": "A detailed technical tutorial explaining the GKR (Goldwasser-Kalai-Rothblum) cryptographic protocol and its exceptional efficiency for proving computations with layered structures like hash functions.",
      "summary": "Vitalik provides an in-depth explanation of GKR, a cryptographic proving system that achieves remarkable efficiency for computations that can be structured in layers with repeated operations across many inputs, such as cryptographic hash functions and neural networks. The key innovation of GKR is that it avoids the need to create cryptographic commitments to intermediate computation layers, only requiring commitments to inputs and outputs, which dramatically reduces overhead compared to traditional STARK proofs. Vitalik walks through the technical mechanics using Poseidon2 hash function as an example, explaining how sumchecks work as the core building block, how the protocol runs computation backwards from outputs to inputs, and various optimizations like Gruen's trick that reduce the communication complexity. The tutorial demonstrates that GKR can achieve theoretical overheads as low as 15x compared to raw computation (versus ~100x for traditional STARKs), with practical implementations showing under 10x overhead, making it exceptionally well-suited for high-performance zero-knowledge applications in blockchain scaling and AI verification.",
      "takeaways": [
        "GKR achieves exceptional efficiency by avoiding cryptographic commitments to intermediate computation layers, only committing to inputs and outputs",
        "The protocol is particularly well-suited for computations with layered structures and repeated operations across many inputs, like hash functions and neural networks",
        "Sumchecks are the core building block, using random linear combinations to recursively reduce claims about hypercubes of values to single point evaluations",
        "GKR can achieve theoretical overheads of ~15x versus raw computation compared to ~100x for traditional STARKs, with practical implementations under 10x overhead",
        "The protocol is highly parallelizable and can approach zero overhead as the width of computations increases, making it ideal for ultra-fast ZK-provers"
      ],
      "controversial": [
        "The 2025 security consideration about circuits potentially predicting Fiat-Shamir challenges if they can compute the hash fast enough within their own depth represents a relatively new attack vector that may require protocol adjustments"
      ]
    }
  },
  {
    "id": "general-2025-10-05-memory13",
    "title": "Memory access is O(N^[1/3])",
    "date": "2025-10-05",
    "category": "math",
    "url": "https://vitalik.eth.limo/general/2025/10/05/memory13.html",
    "path": "general/2025/10/05/memory13.html",
    "content": "- \n\n- \n\n  \n    \n\n  \n  \n  Dark Mode Toggle\n  \n    \n\n  \n\n##  Memory access is O(N^[1/3]) \n\n 2025 Oct 05 \nSee all posts\n\n \n \n\n Memory access is O(N^[1/3]) \n\nIn computer science, we often compare the efficiency of algorithms by\ndescribing their runtime as a function of the size of the input. Sorting is\nO(n * log(n)), meaning that sorting a list of N items takes an amount of\ntime proportional to the number of items multiplied by its logarithm.\nMatrix multiplication is somewhere between 2.37\nand 2.8, depending on the choice of algorithm. But these estimates\nare all relative to some model of how long it takes for the underlying\nmachine to perform some basic underlying operations. Typically,\narithmetic operations (addition, multiplication, division...) are\nconsidered to take one unit of time for fixed-size numbers, and memory\naccesses are also considered to take one unit of time.\n\nIn this post, I will argue that this choice for memory access is\nwrong. Memory access, both in theory and in practice, takes O(N^\u2153) time:\nif your memory is 8x bigger, it will take 2x longer to do a read or\nwrite to it. I will also show an example of an application where this\nconcretely matters.\n\n## The theoretical argument\n\nImagine you have a processor sitting in the middle of a pile of\nmemory. The processor's ability to talk to any individual piece of\nmemory is bounded by the speed of light, ergo the delay is proportional\nto distance. In a three-dimensional world, you can fit 8x as much memory\nwithin 2x the distance from you.\n\nDouble the distance, eight times the memory.\n\nThis covers sequential access. In practice, of course, a CPU\nis not literally situated inside of a homogeneous cube of memory, and\nelectrical signals don't travel exactly in a straight line (and, for\nthat matter, are slower than light). But, as we will see later, the\nmodel is surprisingly close to accurate in practice.\n\nFor parallel access, things get more subtle and depend on the medium.\nIf you think of read and write as occupying a \"wire\" of the needed\nlength for a fixed length of time, then you get the same result: 8x the\nmemory, 2x the wire length * time. If you think of it as a signal made\nout of light that takes some amount of energy, where its intensity needs\nto take into account dispersal, then 8x the memory implies 2x the length\nwhich implies 4x the energy required for a single access. This can be\nmitigated by having repeaters repeat the signal in the middle, but that\nstarts to get us closer again to wires. So O(N^\u2153) is probably the better\nmodel.\n\n## The empirical argument\n\nComputers in reality have different types of memory: registers,\ndifferent levels of cache, and RAM. We'll omit SSD/HDDs from here\nbecause they have additional requirements on top: persistent storage\nwithout energy requirements, and lower cost per bit.\n\nWe can ask a question: how long (in nanoseconds) does it take to\naccess a type of memory of which an average laptop has N bytes? Here's\nGPT's answer:\n\nAs it turns out, a naive formula of treating access time as the cube\nroot of the amount of memory accessed gets us surprisingly close.\n\nWe can also do the same for bandwidth:\n\nNote that here the fit is considerably worse, which is to be\nexpected: compared to latency, bandwidth is much less about fundamental\napplication of physics principles and much more about architecture\nchoices. L3 cache is not built for mass throughput in the same way that\nDRAM is, and so it has roughly identical mass throughput despite its\nmuch closer distance to the computation.\n\n## Where that this matter?\n\nI can give a concrete example of why this all matters that I ran into\nmyself a year ago: optimized implementations of various\nalgorithms that precompute and reuse intermediate values.\nOften, especially in cryptography, it is the case that you have a\nmathematical procedure that involves N steps, where in each step,\ndepending on one bit of the input, you either \"mix in\" a particular\nvalue or you don't. The \"mixing in\" is often an associative\noperation. Such a procedure can be done naively in N steps, in N/8 steps\nif you precompute 256 values for each step, in N/16 steps if you\nprecompute 65536 values for each step, and so on. Examples of this\ninclude elliptic curve multiplication, binary field math (see eg. my\nimplementation here), and many other algorithms.\n\nA surprisingly widely applicable way to optimize\ncryptography.\n\nHow big should you make the precomputed table? If you treat a memory\naccess as O(1), then the answer is clear: as big as your machine has\nmemory. But if you treat memory access as O(N^\u2153), then it becomes a more\nsubtle tradeoff: you have to optimize N^\u2153 / log(N), and the\noptimal value is always some \"interior solution\" (ie. not 1 and not \"as\nmuch as you can\") whose exact position depends on the constants\ninvolved.\n\nIn my binary field code, I found that an 8-bit precomputation table\n(in that context, 224 items taking up 128 MB) led to faster\ncomputations than a 16-bit precomputation table (232 items\ntaking up 8 GB): while the latter fit into RAM, the former could fit\ninto cache, and the faster access time of the former was decisive.\n\nIn the longer term, we are in an era right now where we are\napproaching the limits of general-purpose CPUs, and people are\nincreasingly exploring ASICs for various tasks. Here, the structure of\nthe ASIC matters. If you can break up a task into many parts,\neach of which is highly local, then memory access in each part will be\nO(1). GPUs are already often very good at getting precisely\nthese kinds of efficiencies. But if the task requires a lot of memory\ninterdependencies, then you will get lots of O(N^\u2153) terms. An open\nproblem is coming up with mathematical models of computation that are\nsimple but do a good job of capturing these nuances.",
    "contentLength": 5777,
    "summary": "Memory access time scales as O(N^1/3) because in 3D space, 8x more memory fits within 2x the distance, making distance the bottleneck.",
    "detailedSummary": {
      "theme": "Vitalik argues that memory access time should be modeled as O(N^1/3) rather than O(1) in algorithmic complexity analysis, based on both theoretical physics constraints and empirical evidence.",
      "summary": "Vitalik challenges the conventional computer science assumption that memory access takes constant time, proposing instead that it scales as O(N^1/3) where N is the memory size. He provides a theoretical argument based on physics: in three-dimensional space, doubling the distance from a processor allows for 8x more memory, but signals are constrained by the speed of light, creating a cube-root relationship between memory size and access time. Vitalik supports this with empirical data showing that real computer memory hierarchies (registers, cache levels, RAM) roughly follow this pattern when plotting access time against memory capacity.\n\nVitalik demonstrates the practical importance of this insight through cryptographic optimization examples, particularly his work on binary field mathematics. He found that an 8-bit precomputation table (128 MB, fitting in cache) outperformed a 16-bit table (8 GB, requiring RAM access) despite the larger table theoretically requiring fewer computational steps. This illustrates how the O(N^1/3) memory model leads to optimal solutions that balance precomputation size against memory access costs, rather than simply maximizing precomputed values as the O(1) model would suggest.",
      "takeaways": [
        "Memory access time scales as O(N^1/3) due to fundamental physics constraints in three-dimensional space, not as the commonly assumed O(1)",
        "Empirical data from computer memory hierarchies (registers through RAM) supports the N^1/3 relationship for access latency",
        "This insight has practical implications for algorithm optimization, particularly in cryptography where precomputation table sizes must balance computation reduction against memory access costs",
        "In Vitalik's binary field implementation, smaller tables fitting in faster cache memory outperformed larger tables in slower RAM",
        "As computing moves toward ASICs and specialized hardware, understanding these memory access patterns becomes crucial for designing efficient computational architectures"
      ],
      "controversial": [
        "The fundamental challenge to the widely-accepted O(1) memory access assumption in computer science complexity analysis",
        "The assertion that bandwidth measurements fit the theoretical model poorly, suggesting current computer architectures may not be optimally designed for throughput"
      ]
    }
  },
  {
    "id": "general-2025-09-24-openness_and_verifiability",
    "title": "The importance of full-stack openness and verifiability",
    "date": "2025-09-24",
    "category": "general",
    "url": "https://vitalik.eth.limo/general/2025/09/24/openness_and_verifiability.html",
    "path": "general/2025/09/24/openness_and_verifiability.html",
    "content": "- \n\n- \n\n  \n    \n\n  \n  \n  Dark Mode Toggle\n  \n    \n\n  \n\n##  The importance of full-stack openness and verifiability \n\n 2025 Sep 24 \nSee all posts\n\n \n \n\n The importance of full-stack openness and verifiability \n\n Special thanks to Ahmed Ghappour, bunnie, Daniel Genkin,\nGraham Liu, Michael Gao, mlsudo, Tim Ansell, Quintus Kilbourn, Tina\nZhen, Balvi volunteers and GrapheneOS developers for feedback and\ndiscussion. \n\nPerhaps the biggest trend of this century so far can be summarized by\nthe phrase \"the internet has become real life\". It started with email\nand instant messaging. Private conversations that for millennia past\nwere done with mouths, ears, pen and paper, now run on digital\ninfrastructure. Then, we got digital finance - both crypto finance, and\ndigitization of traditional finance itself. Then, our health: thanks to\nsmartphones, personal health tracking watches, and data inferred from\npurchases, all kinds of information about our own bodies is being\nprocessed through computers and computer networks. Over the next twenty\nyears, I expect this trend to take over all kinds of other domains,\nincluding various government processes (eventually even voting),\nmonitoring of the public environment for physical and biological\nindicators and threats, and ultimately, with brain-computer interfaces,\neven our own minds.\n\nI do not think that these trends are avoidable; their benefits are\ntoo great, and in a highly competitive global environment, civilizations\nthat reject these technologies will lose first competitiveness and then\nsovereignty to those that embrace them. However, in addition to offering\npowerful benefits, these technologies deeply affect power\ndynamics, both within and between countries.\n\nThe civilizations that gained the most from new waves of technology\nare not the ones who consumed the technology, but the ones who\nproduced it. Centrally planned equal access programs to\nlocked-down platforms and APIs can at best provide only a small fraction\nof this, and fail in circumstances that fall outside of a pre-determined\n\"normal\". Additionally, this future involves a lot of trust\nbeing put in technology. If that trust is broken (eg.\nbackdoors, security failures), we get really big problems. Even the mere\npossibility of that trust being broken forces a fallback to\nfundamentally exclusionary social models of trust (\"was this thing built\nby people I trust?\"). This creates incentives that propagate up the\nstack: the sovereign is he who decides on the state of exception.\n\nAvoiding these problems requires technology across the stack -\nsoftware, hardware and bio - that has two intertwined\nproperties: genuine openness (ie. open source, including free\nlicensing) and verifiability (including, ideally,\ndirectly by end users).\n\n The internet is real life. We want it to become a utopia\nand not a dystopia. \n\nThe\nimportance of openness and verifiability in health\n\nWe saw the consequences of unequal access to the technological means\nof production during Covid. Vaccines were produced in only a few\ncountries, which led to large\ndisparities between when different countries were able to get access\nto them. Wealthier countries got top-quality vaccines in 2021, others\ngot lower-quality vaccines in 2022 or 2023. There were initiatives\nto try to ensure equal access, but because the vaccines were\ndesigned to rely on capital-intensive proprietary manufacturing\nprocesses that could only be done in a few places, these initiatives\ncould only do so much.\n\n Covid vaccine coverage, 2021-23. \n\nThe second major issue with vaccines was the opaque\nscience and communications strategy\nthat tried to pretend to the public that they carried literally zero\nrisks or downsides, which was untrue and ended up contributing\ngreatly to mistrust.\nToday, this mistrust has spiraled into what feels like a rejection of\nhalf a century of science.\n\nIn fact, both problems are resolvable. Vaccines like the Balvi-funded\nPopVax are cheaper to develop, and\nmade with a much more open process, reducing access inequality and at\nthe same time making it easier to analyze and verify their safety and\neffectiveness. We can go even further in designing vaccines for\nverifiability first.\n\nSimilar issues apply for the digital side of\nbiotech. When you talk to longevity researchers, one of the\nfirst things that you will universally hear is that the future of\nanti-aging medicine is personalized and data-driven.\nTo know what medicines and what changes in nutrients to suggest to a\nperson today, you need to know the current condition of their body. This\nis much more effective if there can be a large amount of data\ndigitally collected and processed, in real time.\n\n This watch collects 1000x more data about you than\nWorldcoin. This has upsides and downsides. \n\nThe same idea applies for defensive biotech aimed at downside\nprevention, such as fighting pandemics. The earlier a\npandemic is detected, the more likely it is that it can be stopped at\nthe source - and even if it can't, each week gives more time to prepare\nand start working on countermeasures. While a pandemic is ongoing, there\nis a lot of value in being able to know in what locations people are\ngetting sick, in order to deploy countermeasures in real time. If the\naverage person who gets sick with a pandemic learns it, and\nself-isolates within an hour, that implies up to 72x less spread than if\nthey go around infecting others for three days. If we know which 20% of\nlocations are responsible for 80% of the spread, improving air\nquality there can add further gains. All of this requires (i)\nlots and lots of sensors, and (ii) the ability for the\nsensors to communicate in real time to feed information\nto other systems.\n\nAnd if we go even further in the \"scifi\" direction, we get to\nbrain-computer interfaces, which can enable great\nproductivity, help people better understand each other through\ntelepathic communication, and unlock safer paths to highly intelligent\nAI.\n\nIf the infrastructure for biological and health tracking (for\nindividuals and for spaces) is proprietary, then the data goes into the\nhands of large corporations by default. Those corporations have the\nability to build all kinds of applications on top, and others do not.\nThey may offer it via API access, but API access will be limited and\nused for monopolistic rent extraction, and can be taken away at any\ntime. This means that a small number of people and corporations have\naccess to the most important ingredients for a major area of 21\u02e2\u1d57\ncentury technology, which in turn limits who can economically benefit\nfrom it.\n\nAnd on the other hand, if this kind of personal health data is\ninsecure, someone who hacks it can blackmail you over any health issues,\noptimize pricing of insurance and healthcare products to extract value\nfrom you, and if the data includes location tracking they know where to\nwait for you to kidnap you. And in the other direction, your location\ndata (very\noften hacked)\ncan be used to infer information about your health. If your BCI gets\nhacked that means a hostile actor is literally reading (or worse,\nwriting) your mind. This is no longer science fiction: see here\nfor a plausible attack by which a BCI hack can lead to someone losing\nmotor control.\n\nAll in all, a huge amount of benefits, but also significant risks:\nrisks that a strong emphasis on openness and verifiability are very well\nsuited to mitigating.\nThe\nimportance of openness and verifiability in personal and commercial\ndigital tech\n\nEarlier this month I had to fill in and sign a form that was required\nfor a legal function. At the time I was not in the country. A national\nelectronic signing system existed, but I did not have it set up at the\ntime. I had to print out the form, sign it, walk over to a nearby DHL,\nspend a bunch of time filling in the paper form, and then paying for the\nform to be express-shipped halfway across the world. Time required: half\nan hour, cost: $119. On that same day I had to sign a (digital)\ntransaction to perform an action on the Ethereum blockchain. Time\nrequired: 5 seconds, cost: $0.10 (and, to be fair, without the\nblockchain a signature can be completely free).\n\nThese kinds of stories are easy to find in corporate or nonprofit\ngovernance, management of intellectual property rights, and much more.\nFor the past decade, you can find them in the pitch decks of a\nsignificant fraction of all blockchain startups. And on top of this,\nthere is the mother of all use cases of \"digitally exercising personal\nauthority\": payments and finance.\n\nThere is of course a big risk in all this: what if either the\nsoftware or the hardware gets hacked? This is a risk that the crypto\nspace was early to recognize: the blockchain is permissionless and\ndecentralized, and so if you lose\naccess to your funds, there is no resource, no uncle in the sky that\nyou can call for help. Not your keys, not your coins. For this reason,\nthe crypto space was early to thinking about multisig\nand social\nrecovery wallets, and hardware\nwallets. In reality, however, there are many situations where lack\nof a trusted uncle in the sky is not an ideological choice, but an\ninherent part of the scenario. In fact, even in traditional finance, the\n\"uncle in the sky\" fails to protect most people: for example, only\n4% of scam victims recover their losses. In use cases that involve\ncustody of personal data, reverting a leak is impossible even\nin principle. Hence, we need true verifiability and security -\nof both the software and, ultimately, the hardware.\n\n One\nproposed technique for inspecting that computer chips were\nmanufactured correctly. \n\nImportantly, in the case of hardware, the risk that we are\ntrying to prevent goes far beyond \"is the manufacturer evil?\". Rather,\nthe problem is that there is a large number of dependencies, most of\nwhich are closed source, and any one of them being negligent can cause\nunacceptable security outcomes. This paper shows recent\nexamples of how microarchitecture choices can undermine the\nside-channel resistance of designs that are provably secure in\na model that looks at the software alone. Attacks like EUCLEAK depend on\nvulnerabilities that are much harder to find because of how many\ncomponents are proprietary. AI models can have backdoors inserted at\ntraining time if they are\ntrained on compromised hardware.\n\nAnother issue in all of these cases is downsides from closed and\ncentralized systems, even if they are perfectly secure. Centralization\ncreates ongoing leverage between individuals, companies or\ncountries: if your core infrastructure is built and maintained by a\npotentially untrustworthy company in a potentially untrustworthy\ncountry, you are vulnerable to pressure (eg. see Henry\nFarrell on weaponized interdependence). This is the sort of problem\nthat crypto is meant to solve - but it exists in far more domains than\njust the financial.\nThe\nimportance of openness and verifiability in digital civic tech\n\nI frequently talk to people of various stripes who are trying to\nfigure out better forms of government that are well\nsuited for their various contexts in 21\u02e2\u1d57 century. Some, like Audrey Tang, are trying to take\npolitical systems that are already functional and bring them to the next\nlevel, empowering local open-source communities and using mechanisms\nlike citizens' assemblies, sortition and quadratic voting. Others are\nstarting from the bottom: here is a\nconstitution recently proposed by some Russian-born political scientists\nfor Russia, featuring strong guarantees of individual freedom and local\nautonomy, strong institutional bias toward peace and against aggression,\nand an unprecedentedly strong role for direct democracy. Others, like\neconomists working on land\nvalue tax or congestion pricing, are trying to improve their\ncountry's economics.\n\nDifferent people may have different levels of enthusiasm for each\nidea. But one thing that they all have in common is they all\ninvolve high-bandwidth participation, and so any realistic\nimplementation has to be digital. Pen and paper is\nokay for a very basic record of who owns what and elections run once\nevery four years, but not for anything that asks for our input with\nhigher bandwidth or frequency.\n\nHistorically, however, security researchers' reception to the idea of\nthings like electronic voting has ranged from skeptical to hostile. Here\nis a good summary of the case against electronic voting. Quoting\nfrom that document:\n\nFirst of all, the technology is \"black box software,\" meaning that\nthe public is not allowed access into the software that controls the\nvoting machines. Although companies protect their software to protect\nagainst fraud (and to beat back competition), this also leaves the\npublic with no idea of how the voting software works. It would be simple\nfor the company to manipulate the software to produce fraudulent\nresults. Also, the vendors who market the machines are in competition\nwith each other, and there is no guarantee that they are producing the\nmachines in the best interest of the voters and the accuracy of the\nballots.\n\nThere are lots of real-world\ncases that justify this skepticism.\n\n A critical\nanalysis of Estonian internet voting, 2014.\n\nThese arguments apply verbatim in all kinds of other situations. But\nI predict that as technology progresses, the \"let's not do it at all\"\nresponse will become less and less realistic, across a wide range of\ndomains. The world is rapidly becoming more efficient (for better or\nworse) due to technology, and I predict that any system that does not\nfollow this trend will become less and less relevant to individual and\ncollective affairs as people route around it. And so we need an\nalternative: to actually do the hard thing and figure out how to make\ncomplicated tech solutions secure and verifiable.\n\nTheoretically, \"secure and verifiable\" and \"open-source\" are two\ndifferent things. It is definitely possible for something to be\nproprietary and secure: airplanes are highly proprietary\ntechnology but on the whole commercial aviation is a very\nsafe way to travel. But what a proprietary model\ncannot achieve is common knowledge of security - the\nability to be trusted by mutually distrusting actors.\n\nCivic systems like elections are one type of situation where common\nknowledge of security is important. Another is evidence gathering in\ncourts. Recently, in Massachusetts, a large volume breathalyzer evidence\nwas\nruled invalid because information about faults in the tests was\nfound to have been covered up. Quoting the article:\n\nWait, so were all of the results faulty? No.\u00a0In fact, there weren't\ncalibration issues with the breathalyzer tests in most of the cases.\nHowever, since investigators later found that the state crime lab\nwithheld evidence showing the problems were more widespread than they\nsaid, Justice Frank Gaziano wrote that all of those defendants had their\ndue process rights violated.\n\nDue process in courts is inherently a domain where what is required\nis not just fairness and accuracy, but common knowledge in fairness and\naccuracy - because if there is not common knowledge that courts are\ndoing the right thing, society can easily spiral into people taking\nmatters into their own hands.\n\nIn addition to verifiability, there are also inherent benefits to\nopenness itself. Openness allows local groups to design systems for\ngovernance, identity, and other needs in ways that are compatible with\nlocal goals. If voting systems were proprietary, then a country (or\nprovince or town) that wanted to experiment with a new one would have a\nmuch harder time: they would have to either convince the company to\nimplement their preferred rules as a feature, or start from scratch and\ngo through all the work to make it secure. This adds a high cost to\ninnovation in political systems.\n\nA more open-source hacker-ethic approach, in any of these areas,\nwould put more agency in the hands of local implementers, whether they\nare acting as individuals or as part of governments or corporations. For\nthis to be possible, open tools for building need to be widely\navailable, and the infrastructure and code bases need to be freely\nlicensed to allow others to build on top. To the extent that the goal is\nminimizing power differentials, copyleft\nis especially valuable.\n\nA final area of civic tech that will matter in the next years is\nphysical security. Surveillance cameras have been\npopping up everywhere over the past two decades, causing many civil\nliberties worries. Unfortunately, I predict that the recent rise of\ndrone warfare will make \"don't do high tech security\" no longer a viable\noption. Even if a country's own laws do not infringe on a person's\nfreedom, that means nothing if the country cannot protect you from\nother countries (or rogue corporations or individuals) imposing\ntheir laws on you instead. Drones make such attacks much easier. Ergo,\nwe need countermeasures, that will likely involve lots of counter-drone\nsystems and sensors and cameras.\n\nIf these tools are proprietary, data collection will be opaque and\ncentralized. If these tools are open and verifiable, then we have a\nchance at a better approach: security equipment that provably\noutputs only a limited amount of data in a limited number of situations\nand deletes the rest. We could have a digitized physical\nsecurity future that is more like digital guard dogs than a\ndigital panopticon. One could imagine a world where public\nmonitoring devices are required to be open source and verifiable, and\nanyone has a legal right to randomly choose a monitoring device in\npublic and take it apart and verify it. University computer science\nclubs could frequently do this as an educational exercise.\nThe open source and\nverifiable way\n\nWe cannot avoid having digital computer things that are deeply\nembedded in all kinds of aspects of our (personal and collective) lives.\nBy default, we will likely get digital computer things that are built\nand run by centralized corporations, optimized for a few people's profit\nmotives, backdoored by their host governments, and where most of the\nworld has no way to participate in their creation or know if they're\nsecure. But we can try to steer toward a better alternative.\n\nImagine a world where:\n\n- You have a secure personal electronic device -\nsomething with the power of a phone, the security of a crypto hardware\nwallet and a level of inspectability not quite like a mechanical watch,\nbut pretty close.\n\n- Your messaging apps are all encrypted, message\npatterns obfuscated with mixnets, and all the code formally\nverified. You are able to have confidence that your private\ncommunications actually are private.\n\n- Your finances are standardized ERC20 assets onchain\n(or on some server that publishes hashes and proofs to a chain to\nguarantee correctness), managed by a wallet controlled by your personal\nelectronic device. If you lose your device, they are recoverable\nwith some combination (that you choose) of your other devices,\ndevices of family members, friends or institutions (not necessarily\ngovernments: if it's easy for anyone to do this, eg. churches may well\noffer it too).\n\n- Open-source versions of Starlink-like infrastructure\nexist, so we get robust global connectivity without dependence\non a few individual actors.\n\n- You have on-device open-weight LLMs scanning your\nactivity, offering suggestions and autocompleting tasks, and warning you\nwhen you are potentially getting incorrect information or about to make\na mistake.\n\n- The operating system is also open-source and\nformally verified.\n\n- You are wearing 24/7 personal health tracking\nequipment, which is also open source and\ninspectable, allowing you to get your data and making sure no\none else is getting it without your consent.\n\n- We have more advanced forms of governance that use\nsortition, citizens' assemblies, quadratic voting, and generally clever\ncombinations of democratic votes to set goals and some method of\nselecting ideas from experts to determine how the goals are achieved. As\na participant, you can actually be confident that the system is\nimplementing the rules as you understand them.\n\n- Public spaces are fitted with monitoring equipment to track\nbio variables (eg. CO2 and AQI levels, presence of airborne\ndiseases, wastewater). However, this equipment (along with any\nsurveillance cameras and defensive drones) is open source and\nverifiable, and a legal regime exists by which the public can\nrandomly inspect it.\n\nThis is a world where we have much more safety and freedom and equal\naccess to the global economy than today. But making this world happen\nrequires much more investment in various technologies:\n\n- More advanced forms of cryptography. What I call\nthe Egyptian\ngod cards of cryptography - ZK-SNARKs,\nfully\nhomomorphic encryption and obfuscation\n- are so powerful because they let you compute arbitrary programs on\ndata in multi-party contexts, and give guarantees about the output,\nwhile keeping the data and the computation private. This enables much\nmore powerful applications that are privacy-preserving. Tools adjacent\nto cryptography (eg. blockchains to enable applications\nwith strong guarantees that data is not tampered with and users are not\nexcluded, and differential privacy to add noise to data\nto further preserve privacy) also apply here.\n\n- Application and user-level security. Applications\nare only secure if the security assurances that they make are actually\nintelligible and verifiable by the user. This will involve software\nframeworks that make applications with strong security properties easy\nto build. Importantly, it will also involve browsers, operating systems\nand other intermediaries (eg. locally running watcher LLMs) all doing\ntheir part to verify applications, determine their level of risk, and\npresent this information to the user.\n\n- Formal verification. We can use automated proving\nmethods to algorithmically verify that programs satisfy properties that\nwe care about, eg. in terms of not leaking data or not being vulnerable\nto unauthorized third-party modification. Lean has recently become a popular\nlanguage for this. These techniques are already starting to be used to\nverify ZK-SNARK proving algorithms for the Ethereum virtual machine\n(EVM) and other high-value high-risk use cases in crypto, and are\nsimilarly being used in the wider world. On top of this, we need further\nprogress in other, more mundane security practices.\n\n The cybersecurity fatalism of the 00s is wrong: bugs (and\nbackdoors) can be beaten. We \"just\" have to learn to value security more\nthan other competing goals. \n\n- \n\nOpen-source and security-focused operating\nsystems. More and more of these are starting to pop up: GrapheneOS as a security-focused\nversion of Android, minimal security-focused kernels like Asterinas, and\nHuawei's HarmonyOS (which has an open source version) is\nusing\nformal verification (I expect many readers will think \"if it's\nHuawei, surely it has backdoors\", but this misses the whole point:\nit shouldn't matter who produces something, as long as it's open and\nanyone can verify it. This is a great example of how openness and\nverifiability can fight against global balkanization)\n\n- \n\nSecure open-source hardware. No software is\nsecure if you can't be sure that your hardware is actually running that\nsoftware, and is not leaking data separately on the side. I am most\ninterested in two short-term targets in this regard:\n\n- A personal secure electronic device - what\nblockchain people call a \"hardware wallet\" and open source enthusiasts\ncall a \"secure phone\", except once you understand the need for security\nand generality, the two ultimately converge to being the same\nthing.\n\n- Physical infrastructure for public spaces - smart\nlocks, bio monitoring equipment as I described above, and general\n\"internet of things\" tech. We need to be able to trust it. This requires\nopen source and verifiability.\n\n- \n\nSecure open toolchains for building open-source\nhardware. Today, designing hardware depends on a whole series\nof closed-source dependencies. This drastically raises the cost of\nmaking hardware, and makes the process much more permissioned. It also\nmakes hardware verification impractical: if the tooling that generates\nthe chip design is closed source, you don't know what you're verifying\nagainst. Even tools like scan chains that\nexist today are often unusable in practice because too much of the\nnecessary tooling is closed source. This can all be changed.\n\n- \n\nHardware verification (eg. IRIS,\nand X-ray scanning). We need\nways of scanning chips to verify that they actually have the logic that\nthey are supposed to have, and that they do not have extra components\nthat allow unexpected forms of tampering and data extraction. This can\nbe done destructively: auditors randomly order products\ncontaining computer chips (using identities that appear to be average\nend users), and then take the chips apart and verify that the logic\nmatches up. With IRIS or X-ray scanning, it can be done\nnon-destructively, allowing every chip to potentially\nbe scanned.\n\n- \n\nTo achieve common knowledge of trust, we ideally want\nhardware verification techniques that are within reach of large\ngroups of people. X-ray machines today are not yet in this\nposition. This situation can be improved in two ways. First, we can\nrefine verification equipment (and verification-friendliness of chips)\nso that the equipment is more widely accessible. Second, we can\nsupplement \"full verification\" with more limited forms of verification\nthat can even be done on smartphones (such as ID\ntags and signatures from keys generated by physical\nunclonable functions), that verify more restrictive claims like \"was\nthis machine part of a batch that was produced by a known manufacturer,\nof which a random sample is known to have been verified in detail by\nthird-party groups?\"\n\n- \n\nOpen source, low-cost, local environmental and bio\nmonitoring equipment. It should be possible for communities and\nindividuals to measure their environment and themselves and identify\nbiological risks. This includes technology in many form factors:\npersonal-scale medical equipment like OpenWater, air quality sensors,\nuniversal airborne disease sensors (eg. Varro), and larger-scale environmental\nmonitoring.\n\n Openness and verifiability on every layer of the stack\nmatters \n\n## From here to there\n\nA key difference between this vision and a more \"traditional\" vision\nof technology is that it's much more friendly to local sovereignty and\nindividual empowerment and freedom. Security is done less by scouring\nthe entire world and making sure there are zero bad guys anywhere, and\nmore by making the world more robust at every level. Openness means\nopenness to build upon and improve every layer of technology, and not\njust centrally planned open-access API programs. Verification is not\nsomething reserved to proprietary rubber-stamp auditors that may well be\ncolluding with the companies and governments rolling out the technology\n- it's a right, and a socially encouraged hobby, for the people.\n\nI believe that this vision is more robust, and more compatible with\nour fractured global twenty-first century. But we do not have infinite\ntime to execute on this vision. Centralized approaches to security, that\ninvolve putting more centralized data collection and backdoors,\nand reducing verification entirely to \"was this made by a trusted\ndeveloper or manufacturer\", are moving forward rapidly. Centralized\nattempts to substitute for true open access have been attempted for\ndecades. It started perhaps with Facebook's internet.org,\nand it will continue, each attempt more sophisticated than the last. We\nneed to both move quickly to compete with these approaches, and make the\npublic case, to people and institutions, that a better solution is\npossible.\n\nIf we can succeed in this vision, one way to understand the world\nthat we get is that it is a kind of retro-futurism. On the one hand, we\nget the benefits of much more powerful technologies allowing us to\nimprove our health, organize ourselves in much more efficient and\nresilient ways, and protect ourselves against threats, both old and new.\nOn the other hand, we get a world that brings back properties that were\nsecond-nature to everyone back in 1900: the infrastructure is free for\npeople to take apart, verify and modify to suit their own needs, anyone\nis able to participate not just as a consumer or an \"app builder\", but\nat any layer of the stack, and anyone is able to have confidence that a\ndevice does what it says it does.\n\nDesigning for verifiability has a cost: many optimizations to both\nhardware and software deliver highly-demanded gains in speed come at the\ncost of making the design more inscrutable or more fragile. Open source\nmakes it more challenging to make money under many standard business\nmodels. I believe that both issues are overstated - but this is not\nsomething that the world will be convinced of overnight. This leads to a\nquestion: what is the pragmatic short-term goal to shoot\nfor?\n\nI will propose one answer: work toward a fully open-source and\nverification-friendly stack targeted toward high-security,\nnon-performance-critical applications - both consumer and institutional,\nlong-distance and in-person. This would include hardware and software\nand bio. Most computation that really needs security does not\nreally need speed, and even in the cases where it does, there\nare often ways to combine\nperformant-but-untrusted and trusted-but-not-performant components\nto achieve high levels of performance and trust for many applications.\nIt is unrealistic to achieve maximum security and openness for\neverything. But we can start by ensuring that these properties are\navailable in those domains where they really matter.",
    "contentLength": 29736,
    "summary": "Blog argues that as digital technology increasingly controls all aspects of life, we need open-source and verifiable tech to avoid dystopia.",
    "detailedSummary": {
      "theme": "Vitalik argues that as digital technology becomes integral to all aspects of life, we need open-source and verifiable technology stacks across software, hardware, and biotechnology to prevent dystopian centralization and ensure equitable access to technological benefits.",
      "summary": "Vitalik begins by observing that 'the internet has become real life,' with digital infrastructure now mediating everything from private conversations to finance, health monitoring, and eventually government processes and brain-computer interfaces. He argues this trend is inevitable due to competitive advantages, but warns that civilizations benefiting most are those who produce rather than merely consume technology. Through examples spanning COVID-19 vaccine inequality, personal health data vulnerabilities, electronic voting security concerns, and physical surveillance systems, Vitalik demonstrates how proprietary, centralized systems create power imbalances, security risks, and barriers to innovation. He envisions an alternative future featuring secure personal devices, encrypted communications, open-source governance systems, and verifiable public monitoring - all built on advanced cryptography, formal verification, open-source operating systems, and crucially, secure open-source hardware with verification capabilities. Vitalik acknowledges the costs of designing for verifiability and openness but proposes starting with high-security, non-performance-critical applications to build toward this 'retro-futuristic' world that combines advanced technology with the inspectable, modifiable infrastructure that existed in 1900.",
      "takeaways": [
        "Digital technology integration across all life domains is inevitable and unstoppable due to competitive advantages, making the question not whether to adopt but how to implement it responsibly",
        "Proprietary and centralized technology systems create dangerous power imbalances, security vulnerabilities, and limit who can participate in and benefit from technological advancement",
        "True security requires 'common knowledge of security' - the ability for mutually distrusting parties to independently verify system integrity, which only open-source approaches can provide",
        "A comprehensive solution requires openness and verifiability across the entire technology stack, from cryptographic protocols and software to hardware manufacturing and biological monitoring systems",
        "The pragmatic path forward involves focusing initially on high-security, non-performance-critical applications where the trade-offs between verifiability and optimization are most acceptable"
      ],
      "controversial": [
        "Vitalik's assertion that countries rejecting these digital technologies will 'lose first competitiveness and then sovereignty' could be seen as technological determinism that dismisses legitimate concerns about privacy and autonomy",
        "His dismissal of Huawei security concerns by arguing that openness and verifiability should make the manufacturer's identity irrelevant may be naive about geopolitical realities and nation-state capabilities",
        "The claim that traditional finance fails to protect most people (citing only 4% scam recovery rates) may oversimplify the complex trade-offs between decentralized and centralized financial systems",
        "The prediction that physical security systems with extensive surveillance and counter-drone capabilities will become necessary could be seen as normalizing a militarized society rather than seeking diplomatic solutions"
      ]
    }
  },
  {
    "id": "general-2025-09-21-low_risk_defi",
    "title": "Low-risk defi can be for Ethereum what search was for Google",
    "date": "2025-09-21",
    "category": "applications",
    "url": "https://vitalik.eth.limo/general/2025/09/21/low_risk_defi.html",
    "path": "general/2025/09/21/low_risk_defi.html",
    "content": "- \n\n- \n\n  \n    \n\n  \n  \n  Dark Mode Toggle\n  \n    \n\n  \n\n##  Low-risk defi can be for Ethereum what search was for Google \n\n 2025 Sep 21 \nSee all posts\n\n \n \n\n Low-risk defi can be for Ethereum what search was for Google \n\nSpecial thanks to Binji, Josh Rudolf, Haonan Li and Stani\nKulechov for feedback and review.\n\nOne of the important tensions in the Ethereum community for a long\ntime has been the tension between (i) applications that bring in\nenough revenue to economically sustain the ecosystem, whether\nthat means sustaining the value of ETH or supporting individual projects\nand (ii) applications that satisfy the underlying goals\nthat brought people into Ethereum.\n\nHistorically, these two categories were very disjoint: the former was\nsome combination of NFTs, memecoins, and a type of defi that was backed\nup by temporary or recursive forces: people borrowing and lending to\nchase incentives provided by protocols, or a circular argument of \"ETH\nis valuable because people use the Ethereum chain to buy and sell and\nleverage-trade ETH\". Meanwhile, non-financial and semi-financial\napplications (eg. Lens, Farcaster, ENS, Polymarket, Seer, privacy\nprotocols) existed, and they were fascinating, but they either got very\nlittle usage, or paid far too little in fees (or other forms of economic\nactivity) to sustain a $500 billion economy.\n\nThis disjointness created a lot of dissonance in the community, and a\nlarge amount of community momentum was backed by the theoretical\nhope that some application could emerge that fills both boxes at\nthe same time. In this post I will argue that, as of this year, Ethereum\nhas that application, something that can be for Ethereum what search was\nfor Google: low-risk defi, with a goal of achieving global\ndemocratized access to payments and savings in valuable asset categories\n(eg. major currencies with competitive interest rates, stocks,\nbonds).\n\n Deposit rates for major stablecoins on Aave.\n\nThe analogy between low-risk defi for Ethereum and search for\nGoogle is as follows. Google does many interesting and valuable\nthings for the world: the Chromium family of browsers, Pixel phones,\ntheir AI work including open-source Gemini models, the Go language, and\nmuch much more. But\nthese things are all negligible or even negative as far as revenue\ngeneration goes. Rather, the largest revenue generator is search and\nads. Low-risk defi can play a similar role for Ethereum. Other\napplications (including non-financial and more experimental\napplications) are crucially important for Ethereum's role in the world\nand for its culture, but they do not need to be looked to as revenue\ngenerators.\n\nIn fact, I hope Ethereum can do much better than\nGoogle. Google is often\ncriticized\nfor losing\nits way and becoming\nlike the antisocial profit-maximizing corporations that it sought to\nreplace. Ethereum has decentralization baked in at a much deeper\ntechnical and social layer, and I would argue that the low-risk defi use\ncase creates a lot of alignment between \"doing well\" and \"being good\",\nto a degree that does not exist for advertisement.\n\n## Why low-risk defi?\n\nBy \"low-risk defi\" I include both the basic function of payments and\nsavings, and well-understood tools like synthetic assets and fully\ncollateralized lending, and the ability to exchange between these\nassets.\n\nThe reason to focus on these applications has two core\ncomponents:\n\n- These applications provide irreplaceable value, both for Ethereum\nand for its users.\n\n- These applications are culturally congruent with the Ethereum\ncommunity's goals, both for the application layer and for the L1's\ntechnical properties.\n\n## Why is defi valuable now?\n\nHistorically, I was more suspicious of defi because it did\nnot seem to be providing (1); rather, the main \"selling points\"\nseemed to be making money from trading highly speculative tokens\n(Ethereum's single largest day in fees was from a badly\ndesigned digital monkey sale), or getting 10-30% yield from\nliquidity farming incentives.\n\nOne reason why this was the case was regulatory barriers. Gary\nGensler and others deserve serious blame for creating a regulatory\nenvironment where the more useless your application is, the safer you\nare, and the more transparently you act and the more clear guarantees\nyou offer to investors, the more likely you are to be deemed \"a\nsecurity\".\n\nAnother reason why is that at the early stages, risk (protocol code\nbug risk, oracle risk, general unknown-unknown risk) was too high for\nmore sustainable use cases to make sense. If risk is high, the only\napplications that are worth it are applications whose returns are\neven higher, and so can only come from unsustainable subsidies\nor speculation.\n\nBut what happened over time is that the protocols became more secure\nand the risk\ndecreased.\n\n Ethereum L1 defi losses. Source: AI research\n\nDefi hacks and losses continue to exist. But they are increasingly\nbeing pushed out to further edges of the ecosystem where users are more\nexperimental and speculative. A stable core of applications is forming\nthat is proving remarkably robust. Tail risks that cannot be\nruled out continue to exist, but such tail risks exist in tradfi too -\nand given increasing global political instability, for many people\nworldwide the tail risks of tradfi are now greater than the tail risks\nof defi. In the long run, one could expect the transparency and\nautomated execution in a mature defi ecosystem to make it much\nmore stable than tradfi.\n\nWho are the \"non-ouroboros\" users for whom all this makes sense?\nBasically, people and businesses who want access to a global market\nwithin which they can buy, hold and trade mainstream assets, but for\nwhom there are not reliable traditional finance channels for getting\nthose things. Crypto does not have magic secret sauce for sustainably\ncreating much higher yields. But it does have magic secret\nsauce for making the economic opportunities that already exist globally\nand permissionlessly accessible.\nWhy\nis low-risk defi culturally congruent with the Ethereum community's\ngoals?\n\nLow-risk defi has several nice properties that make it ideal:\n\n- It contributes to Ethereum and ETH economically,\nboth by using a large volume of ETH as a collateral asset, and by paying\nhigh volumes of transaction fees\n\n- It serves a clearly valuable and honorable purpose:\nenabling global permissionless access to well-understood positive-sum\nmechanisms of economic interaction and wealth-building\n\n- It does not give Ethereum L1 perverse incentives\n(eg. to over-centralize in search of HFT-friendly efficiencies, which\nare more appropriate for L2s)\n\nThis is a very nice set of properties to have!\n\nGoing back to the analogy of Google, one major flaw in its incentive\nalignment is that advertising revenue gives the company an incentive to\nsuck up as much data as possible from its users and keep that data\nproprietary. This goes against the open-source and positive-sum\nspirit that historically motivated its more idealistic efforts. The\ncost of this kind of incongruence is even higher for Ethereum, because\nEthereum is a decentralized ecosystem, and so any activity that Ethereum\ndoes cannot be a backroom decision of a few people, it must be viable as\na cultural rallying point.\n\nThe revenue generator does not have to be the most revolutionary or\nexciting application of Ethereum. But it does need to be something that\nis at least not actively unethical or not embarrassing. It's\njust not possible to say with a straight face you are excited about the\necosystem because it's positively changing the world, if its single\nlargest application is political memecoins. Low-risk defi, with a goal\nof enabling global permissionless access to payments and to the best\nsavings opportunities, is a form of finance that is positively\nchanging the world, and many people in underprivileged\nregions worldwide can attest\nto this.\nWhat can low-risk defi\nevolve into?\n\nAnother important property of low-risk defi is that it is naturally\nsynergistic with, or can evolve into, a number of more interesting\nfuture applications. To give a few examples:\n\n- Once we have a mature ecosystem of financial and non-financial\nactivity happening onchain (see: Balaji's ledger of\nrecord concept), it starts to make sense to explore\nreputation-based undercollateralized lending, which is\npotentially an even more powerful engine of financial inclusion. Both\nthe low-risk defi we build today, and the non-financial\nwizardry (eg. ZK identity) we build today, are upstream of making this\noutcome more likely.\n\n- If prediction markets become more mature, we could\nstart to see them being used for hedging. If you are holding\nstocks, and you believe that some world event is on average likely to\nmake stocks go up, and prediction markets for that event are liquid and\nefficient, then it's a rational statistical hedging strategy to bet\nagainst that event. Prediction markets and \"traditional\" defi (heh)\nhappening on the same platform will make it easier to engage in such\nstrategies.\n\n- Today, low-risk defi is often about enabling easier access to the\nUSD. But most of us did not enter crypto to enable USD adoption. Hence,\nover time we can start moving the ecosystem toward other stable\nforms of value: basket currencies, \"flatcoins\" based directly on\nconsumer price indices, \"personal tokens\", etc. Both the\nlow-risk defi we build today, and more experimental projects like Circles\nand the various \"flatcoin\" projects, are upstream of making this outcome\nmore likely.\n\nFor all these reasons, I would argue that a stronger focus on\nlow-risk defi puts us in a position much better for\neconomically sustaining the ecosystem while maintaining cultural and\nvalues congruence than search and ads ever could for Google. Low-risk\ndefi is already supporting the Ethereum economy, it is making the world\na better place even today, and it is synergistic with many of the more\nexperimental applications that people on Ethereum are building. It is a\nproject that we can all be proud of.",
    "contentLength": 10003,
    "summary": "Low-risk DeFi (payments, savings, lending) can sustain Ethereum's $500B economy while aligning with its values, unlike previous meme/NFT revenue.",
    "detailedSummary": {
      "theme": "Vitalik argues that low-risk DeFi can serve as Ethereum's primary revenue driver while maintaining cultural alignment, similar to how search monetized Google's broader ecosystem.",
      "summary": "Vitalik addresses a longstanding tension in the Ethereum community between applications that generate substantial revenue (like NFTs, memecoins, and speculative DeFi) versus those that align with Ethereum's foundational values. He argues that low-risk DeFi - encompassing payments, savings, synthetic assets, and fully collateralized lending - has matured to the point where it can fulfill both roles simultaneously. Unlike Google's advertising model, which creates perverse incentives around data collection, Vitalik contends that low-risk DeFi naturally aligns economic success with social good by providing global, permissionless access to mainstream financial services. He emphasizes that protocol security has improved significantly over time, making these applications viable for non-speculative users, particularly those in regions with limited access to traditional financial services. Vitalik envisions low-risk DeFi as a foundation that can evolve into more sophisticated applications like reputation-based lending, prediction market hedging, and alternative stable value systems beyond USD-pegged stablecoins.",
      "takeaways": [
        "Low-risk DeFi has evolved from high-risk speculation to mature protocols that can serve as Ethereum's sustainable revenue engine",
        "Unlike Google's advertising model, low-risk DeFi creates alignment between profit and social good by democratizing access to global financial markets",
        "Protocol security improvements have made DeFi viable for mainstream users seeking traditional financial services rather than just speculative returns",
        "The primary beneficiaries are people and businesses lacking reliable access to traditional finance channels, particularly in underserved global regions",
        "Low-risk DeFi can serve as a foundation for future innovations like undercollateralized lending, prediction market hedging, and alternative stable currencies"
      ],
      "controversial": [
        "The comparison between DeFi protocol risks and traditional finance risks, particularly the claim that tradfi tail risks may now exceed DeFi risks for many users",
        "The implicit criticism of regulatory approaches that allegedly favor 'useless' applications over transparent ones with clear investor guarantees"
      ]
    }
  },
  {
    "id": "general-2025-08-12-ideas",
    "title": "On idea-driven ideas",
    "date": "2025-08-12",
    "category": "general",
    "url": "https://vitalik.eth.limo/general/2025/08/12/ideas.html",
    "path": "general/2025/08/12/ideas.html",
    "content": "- \n\n- \n\n  \n    \n\n  \n  \n  Dark Mode Toggle\n  \n    \n\n  \n\n##  On idea-driven ideas \n\n 2025 Aug 12 \nSee all posts\n\n \n \n\n On idea-driven ideas \n\nA long time ago, in the pre-Covid century, I remember the economist\nAnthony Lee Zhang describing to me his distinction between \"idea-driven\nideas\" and \"data-driven ideas\". An idea-driven idea is an idea where you\nstart off with some high-level philosophical frame - eg. markets are\nrational, power concentration is dangerous, time-worn traditions are\nwise - and deduce a more concrete insight from that frame plus some\nlogical reasoning. A data-driven idea is, in its pure form, an idea that\ncomes out of a process where you start with no preconceptions, do some\nanalysis on data, and endorse whatever conclusion you get. The\nimplication: data-driven ideas are clearly the better type of ideas to\nhave and promote.\n\nLast month, Gabriel from Conjecture critiqued\nmy approach to d/acc by arguing that instead of starting from an\n\"ideology\" and trying to make it more compatible with other human goals,\nI should effectively just be a pragmatist, and neutrally seek whatever\nstrategies do the best job of meeting the entire set of human\nvalues.\n\nThese are common sentiments. So what is the proper role of what might\nalternatively be called ideologies, principles, ideas built on top of\nideas, crystallized\ngoals, or consistent guiding thoughts in a person's thinking? And,\non the flip side, how do these thinking styles fail? This post will\nattempt to describe my thoughts on the topic. The argument I will make\nis as follows:\n\n- The world is too complex to \"pragmatically reason through\" every\nsingle decision. To be effective, you need to take, and reuse,\nintermediate steps.\n\n- Ideology is not just about personal cognition, it's a social\nconstruct. A community needs something to rally around, and if\nit's not an idea or story then often it instead ends up being a person\nor small group - which has potentially worse downsides.\n\n- Another value of encouraging different people to have different\nnarrower goals is enabling and organizing\nspecialization.\n\n- Ideologies in practice are a complicated mix of means and\nends. Our theory needs to account for this.\n\n- Ideology has downsides, and there's many ways it\ninterferes with good thinking. This is an actual big problem.\n\n- Good individual, and social, decision-making requires a\nbalance of \"idea-driven\" and \"pragmatic\" modes. I propose a\ncouple of solutions for what this balance concretely looks like.\n\nGood\ndecision-making in complex contexts always has \"structure\"\n\nImagine that you are trying to improve how you play chess. In chess,\nthere is a common rule of thumb: a queen is worth nine pawns, rook is\nworth five pawns, and a bishop or knight are worth three pawns. Thus, a\nrook plus a pawn for a bishop and a knight is an ok trade to make, but a\nrook for a knight is not.\n\nThis insight has many implications. If you are trying to come up with\ngood tactics in chess, one place to look is to find ways to use your\nknight to \"fork\" two of your opponent's stronger pieces: two rooks, or a\nrook and a queen, etc. Your opponent is forced to accept your knight\neating one of the two strong pieces, in exchange for being able to eat\nthe knight (a weaker piece) right after.\n\n White to move. Knight to f7 is a good move, but you need\nto know the \"knight = 3 pawns, rook = 5 lawns\" rule to easily recognize\nit as such. \n\nHere, \"queen = 9 pawns, rook = 5 pawns, knight = bishop = 3 pawns\"\nfunctions as a generator of further downstream ideas:\nit's an insight that you can start with that is much more likely to\ngenerate effective tactics than searching completely randomly. We can\nthink of that statement as being an \"ideology\". Since pieces on the\nboard in chess are called material,\nlet us overload an already-overloaded\nterm and call this ideology \"materialism\".\n\nOne could imagine someone who disagrees with materialism, either\npartially or fully. Often, sacrificing material is okay in service of\npositional goals, such as exposing the opponent's king or\nclaiming the center of the board. The value of material can also be\ncontext-dependent. In an endgame, I've found that single knight is worth\nmore than a single bishop, whereas two bishops are worth more than two\nknights. If your opponent has one bishop left, pawns might be worth more\nif they are on squares of the opposite color to that bishop. A person\nwhose approach to chess tactics focuses on exploiting these situations\nmight call themselves a \"positionist\".\n\nPositionists and materialists may disagree on practical issues, such\nas whether or not to trade two pawns for a bishop in a situation like\nthis:\n\n To take h3 or not to take, that is the question.\n\nAn ideal chess player might be able to combine the materialist and\npositionist perspectives, juggling between them based on what the\ndetails of the situation demands. This is like Hegelian synthesis.\nHowever, actually doing this requires having some specific ideas about\nwhen to focus on materialist arguments and when to focus on positionist\narguments, and these ideas themselves can be viewed as a new\nideology.\nPrinciples have\nvalue in social coordination\n\nEffective action in the modern world has to be collective\naction: actions taken by hundreds or millions of people simultaneously\nthat all act towards the same goal. Some of this can be accomplished\nwith money (or physical coercion), but this is limited; much of what we\ndo relies on intrinsic\nand social motivation to truly be effective.\n\nIn my post\non Plurality, I describe how communities have three primary options\nin this regard:\n\nCoordinating around a task is powerful: if you can convince lots of\npeople that it would be really valuable to go to the moon, then once\nthey start working, you have lots of people who will put a lot of hard\nwork, creativity and energy into going to the moon. Ethereum's Merge (switch from\nproof of work to proof of stake in 2022) was like this for many people\nin the community. But a task is one-time, and you don't want all the\nsocial capital that was built up after the task is complete to\ndissipate. Principles and leaders are both powerful because they are\ngenerators of tasks: they can keep pointing to new valuable\ntasks to perform as old ones finish.\n\nCoordination around leaders has a well-understood risk: leaders\nare fragile. There are many tales in history of leaders going\ncrazy, or priorities and values drifting in milder but still highly\nconsequential ways. This applies not just when the leader is an\nindividual, but also when the leader is a group.\n\nCoordination around principles - especially, principles that are\nnot consequentialist\n- can be much more robust. A key property of (well-chosen) principles as\na coordination technique is what I call \"galaxy brain\nresistance\". A weakness of consequentialism is that it's\nvulnerable to leaders making clever arguments about how pretty much\nanything they choose might actually have the best consequences for\ncomplicated 4D-chess second-order reasons. Principles are effective at\nserving as a brake on that, saying \"no matter how clever your arguments\nare, we have some easily legible barriers against some things that we\njust don't do\". In this sense, a major weakness of ideologies - that\nideologies are dumb - can actually be an advantage.\n\nOne other form of coordination that is important is internal\ncoordination, or what is often called \"motivation\". I have often\nfound that you can take insights about coordination between people, and\napply those insights to the different \"sub-agents\" that have different\nperspectives and goals inside a single person's mind. Here, the analogy\nis: having a clear principle or goal that you're internally aligned on\ncan both make you more motivated to do your work, and prevent you from\ngoing off the rails and self-justify doing something wrong.\nCrystallized goals as\nspecialization\n\nIt can be useful for different people to have different goals, if\nthese people are in different sub-units of an organization that\nhave particular missions. A company has a marketing department, and it\nhas a software development department, and many more departments. You\ndon't actually want the marketing department to be extremely\nopen-minded and constantly thinking about any way to make the\ncompany more successful. You want it to focus on marketing. This again\nseems to deviate from pure consequentialism, but the rigorous division\nof labor enables the kind of order that lets the company reliably get\nthings done. I would argue that the general project of human\ncivilization has similar properties: you want different people\nto internalize and focus on different civilizational sub-goals.\n\nOne subtle and underrated reason why this is the case is that it\nenables measurement. If an agent has a goal to \"do all the\nuseful things\", it is difficult to tell if it's performing well or\npoorly (both internally, from the agent's own self-improvement view, and\nexternally, for accountability). But if an agent has a more narrow goal\nin mind, then you can tell how well it's doing and how it might be\nimproved. The benefits of this can be great - plausibly, sometimes great\nenough to outweigh the downsides of different agents with different\nsub-goals having some coordination failures.\nIdeologies are a\nmix between means and ends\n\nIn this post so far, I have been talking about ideologies primarily\nas being about means: they are sets of claims about what\nactions best achieve some commonly-agreed goals. In Gabriel's\npost, ideologies are primarily about ends: what goals to\nfocus on in the first place. In reality, ideologies are always a\ncomplicated and messy mix of both. But to the extent that ideologies are\nabout ends, how do I take this into account in the arguments that I made\nabove?\n\nHere, I will answer the question by cheating somewhat: I argue that\nany goals that we crystallize enough to form into an ideology or\nwrite down on paper are actually a type of means.\n\nTo see why, consider the case of someone who really values freedom.\nAt first, they might say that they value freedom because it\nenables a more efficient economy and a more robust society. But then,\nsuppose that you come in and show them a way to have a very efficient\neconomy and a robust society without much freedom. Perhaps, you could\nhave an advanced computer that controls the economy and tells everyone\nwhere to work, and robustness comes from some democratic voting\nmechanism that runs every month that can adjust the computer's inputs or\nreplace it entirely. This libertarian sees your vision of this society,\nand they feel really uneasy, and they just know that if this\nwas put into practice, they would immediately start plotting to rebel\nagainst it.\n\nWhat is going on here? I would argue that \"crystallized\nvalues\" are themselves tactics or predictions, where the real\nultimate goal (the \"win condition\" that they are targeting) is a highly\nillegible and complicated mass of conditions and preferences that are\ninside each of our brains. When this libertarian hears about\nthis proposal for an efficient and robust, but unfree, society, they are\nmaking a realization that, actually, efficiency and robustness are\nanalogous to material in chess: an important part of winning the game,\nbut not the only part.\nIdeologies can have major\ndownsides\n\nClimate change hawks will often say that they support degrowth-style\npolicies because they are the only way to avoid the planet overheating.\nBut if you suggest solar\npower (or worse, solar\ngeoengineering) as a way to avoid the planet overheating without\nneeding to interfere with material abundance or capitalism, they always\nseem a little too enthusiastic to come up with reasons why such\na plan would not work or would have too many \"unintended\nconsequences\".\n\nCryptocurrency enthusiasts will often say that they want to improve\nglobal finance accessibility, create trustworthy property rights, and\nsolve all kinds of social problems with blockchains. But if you show\nthem a way to solve the same problem without any blockchain at all, they\nalways seem a little too enthusiastic to come up with reasons\nwhy your plan would break, perhaps because it's \"too centralized\" or it\n\"doesn't have enough incentives\".\n\nBoth of these examples are somewhat like the example of a\nlibertarian that I gave above, but they are not quite like that\nexample. It's reasonable to value freedom as an end in itself (as long\nas that's not your only value); freedom is a goal that is\ndeeply engrained in humans as a result of millions of years of\nevolution. It's not reasonable to value abolishing capitalism, or mass\nadoption of blockchains, in the same way.\n\nI would argue that this is basically the failure mode that we need to\nwatch out for: elevating something to being an end-in-itself when it\nisn't, in a way that ends up greatly harming the underlying goals.\n\n \"But I have more and much stronger pieces left on the\nboard, so it doesn't matter that I got checkmated, spiritually it was I\nwho won the game\" \n\nHow I reconcile these two\nviews\n\nIn the above sections, I identified two positive use cases of the\nthing you might call \"ideologies\", \"principles\" or \"idea-driven\nideas\":\n\n- Idea-motivated thinking and doing as \"departments\".\nMuch like a company has a dedicated marketing department, it makes sense\nfor society to have a department dedicated to, say, protecting the\nenvironment, and it similarly makes sense for a chess player to have a\nthought process dedicated to answering questions like \"which approach\nwill help me eat my opponent's pieces and keep my own safe?\"\n\n- Principles as a tool for coordination. Instead of\nrallying around a leader or an elite, it can be more robust and less\nprone to failure or capture to rally around an idea.\n\nOften, movements in society will have some of both. Externally, they\nwork to defend a principle, reducing the chance that society drifts to\nover-reliance on an elite. Internally, they become very proficient at\ndeeply exploring particular themes that then generate valuable ideas and\nstrategies for improving the world. Libertarian economists defend\nfreedom in society, and they also invent prediction markets, refine\ncongestion pricing proposals, and a number of other valuable ideas.\nEnvironmentalists guard our society against making irreversible damage\nto the environment through political advocacy, and they also invent\ntechnologies like clean energy and synthetic meat.\n\nMeanwhile, I see two failure modes of this kind of approach. First,\nthere is the risk that an instrumental objective overly\ncrystallizes and gets pursued to extreme extents that subvert\nthe original underlying goal. Second, there is the risk that\ncoordinating around unbounded goals slides into coordinating around a\ncaste of elites that are tasked with interpreting the goals.\nThis is what Balaji Srinivasan means when he says things like \"democracy is\nrule by Democrats\", or what critics of effective altruism often\npoint to when criticizing part of the movement's drift from a broad\nfocus on identifying and encouraging highly effective charity to a much\nnarrower approach of solving AI safety by directing grants to people\nwithin their own social cluster.\n\nI propose two compromises to try to balance between these benefits\nand downsides:\n\n- Data-driven choice of idea-driven ideas. Have a set\nof intellectual themes that generate hypotheses, but then do data-driven\nanalysis to select which ones you emphasize, and ignore the others.\nBryan Caplan often does this well. He has a strong libertarian ideology,\nbut at the same time he values empirical rigor, and the combined result\nis that the primary causes he champions (eg. much more open migration, less\nschooling, housing\nderegulation) have strong arguments behind them, and while his\nlibertarian ideology also drives him to believe many other\nthings that I\nquite disagree with, he rarely ends up focusing on\npromoting those ideas that he cannot back up with mountains of\ndata. You can still disagree with Bryan's more extreme perspectives, but\nto me he is more reasonable than anyone else I know at his level of\nextremeness, so I think his approach is definitely doing something\nright.\n\n- Principles, not ideology. The subtle difference\nbetween these two terms is that principles tend to be limiting,\nwhereas ideology tends to be totalizing. That is, principles give\nyou some set of things to do or not do, but then stop there, whereas\nthere is no limit to how far you can follow an ideology. This is only an\napproximate divide, but in my view a very meaningful one. Focusing the\nsocial coordination function of principles on \"not going off the rails\"\nallows a movement (or an individual) to benefit from more pragmatic\nthought in the normal case, while still being fairly robust.\n\nThe fact that the world and our (individual and collective) minds are\nboth complex and have a lot of internal structure means that the direct\nsolution of \"reason about the whole sum of values and do the data-driven\nthing that best meets them\" often ends up breaking in practice in\nvarious ways. At the same time, leaning in too much to some of that\nstructure often breaks too, sometimes in ways that are even worse.\nBalances like this are most likely to get more of the benefits while\nminimizing more of the downside of both sides.",
    "contentLength": 17311,
    "summary": "Anthony Lee Zhang's distinction between \"idea-driven\" and \"data-driven\" ideas misses that complex decisions need ideological structure for effective social coordination.",
    "detailedSummary": {
      "theme": "Vitalik argues that while pure data-driven decision-making seems ideal, ideologies and principles serve crucial functions in managing complexity, enabling coordination, and organizing specialization, requiring a balanced approach between idea-driven and pragmatic thinking.",
      "summary": "Vitalik begins by discussing the common preference for 'data-driven ideas' over 'idea-driven ideas' (ideologies), but argues that this view misses important benefits of structured thinking. He contends that the world is too complex for pure pragmatic reasoning on every decision, requiring intermediate frameworks like ideologies to be effective - using chess piece values as an analogy for how principles generate useful downstream strategies. Vitalik emphasizes that ideologies serve crucial social coordination functions, allowing communities to rally around robust principles rather than fragile leaders, while also enabling specialization by giving different people focused sub-goals within larger civilizational projects. However, he acknowledges significant downsides when instrumental objectives become ends-in-themselves, citing examples of climate activists rejecting solar solutions and crypto enthusiasts overemphasizing blockchain applications. Vitalik proposes two balanced approaches: 'data-driven choice of idea-driven ideas' (using ideological frameworks to generate hypotheses but empirical analysis to select which to pursue) and focusing on limiting 'principles' rather than totalizing 'ideology' to maintain pragmatic flexibility while avoiding dangerous extremes.",
      "takeaways": [
        "Complex decision-making requires structured intermediate steps rather than pure pragmatic reasoning from first principles every time",
        "Ideologies serve important social coordination functions and are more robust than coordinating around fragile leaders or elites",
        "Specialization benefits from different people having different crystallized sub-goals, even if this deviates from pure consequentialism",
        "The main danger of ideologies is when instrumental objectives become overly crystallized as ends-in-themselves, subverting original underlying goals",
        "The optimal approach balances idea-driven frameworks with data-driven selection and focuses on limiting principles rather than totalizing ideologies"
      ],
      "controversial": [
        "Vitalik's critique of climate change advocates being 'too enthusiastic' to reject solar power and geoengineering solutions",
        "His suggestion that cryptocurrency enthusiasts irrationally reject non-blockchain solutions to problems they claim to want to solve",
        "The implicit criticism of effective altruism's evolution toward AI safety focus within specific social clusters"
      ]
    }
  },
  {
    "id": "general-2025-08-12-onlyopensource",
    "title": "\"I support it only if it's open source\" should be a more common viewpoint",
    "date": "2025-08-12",
    "category": "general",
    "url": "https://vitalik.eth.limo/general/2025/08/12/onlyopensource.html",
    "path": "general/2025/08/12/onlyopensource.html",
    "content": "- \n\n- \n\n  \n    \n\n  \n  \n  Dark Mode Toggle\n  \n    \n\n  \n\n##  \"I support it only if it's open source\" should be a more common viewpoint \n\n 2025 Aug 12 \nSee all posts\n\n \n \n\n \"I support it only if it's open source\" should be a more common viewpoint \n\nOne concern that we often hear about certain kinds of radical\ntechnologies is the possibility that they will exacerbate power\ninequalities because they will inevitably be available only to the\nwealthy and powerful.\n\nHere\nis a quote from someone concerned about the consequences of life\nextension:\n\n\"Are some people going to be left behind? Are we going to make\nsociety far more unequal than it is now?\" he asked. Tuljapurkar\npredicted that the lifespan boom will be confined to wealthy countries,\nwhere citizens can afford anti-aging technology and governments can\nafford to sponsor scientific research. This disparity complicates the\ncurrent debate over access to healthcare, as the rich become\nincreasingly distanced from the poor, not only in quality but length of\nlife.\n\n\"Big pharmaceutical companies have a well-established track record of\nbeing very difficult when it comes to making things available to those\nwho can't pay for them,\" he said.\n\nIf anti-aging technologies are distributed in the unchecked free\nmarket, \"it's entirely likely to me that we'll wind up with permanent\nglobal underclasses, countries that will get locked into today's\nmortality conditions,\" Tuljapurkar said ... \"If that happens, you get\nnegative feedback, a vicious circle. Those countries that get locked out\nstay locked out.\"\n\nHere are some equally strong words from\nan article worrying about the consequences of human genetic\nenhancement:\n\nEarly this month, scientists announced that they had edited\ngenes in a human embryo to remove a disease-causing mutation. The\nwork was astounding and the answer to prayers of many parents. Who\nwouldn't want a chance to prevent what would now be needless suffering\nby their children?\n\nBut that wouldn't be the end of it. Many parents would want to ensure\ntheir children had the best of advantages through genetic improvement.\nThose with means could obtain them. With the ability comes ethical\nquestions beyond the ultimate safety of such techniques. Expense of\nprocedures will produce scarcity and aggravate income inequality that\nalready continues to grow.\n\nAnd similar viewpoints in other technology areas:\n\n- Digital technology in general: https://www.amnestyusa.org/issues/technology/technology-and-inequality/\n\n- Space travel: https://oilprice.com/Energy/Energy-General/What-Does-Billionaires-Dominating-Space-Travel-Mean-for-the-World.html\n\n- Solar geoengineering: https://www.cambridge.org/core/journals/global-sustainability/article/hidden-injustices-of-advancing-solar-geoengineering-research/F61C5DCBCA02E18F66CAC7E45CC76C57\n\nYou can find this theme in many criticisms of new technology. A\nsomewhat related, but importantly different, theme is that of\ntechnological products being used as a vehicle for data collection,\nvendor lock-in, deliberately hidden side effects (eg. modern vaccines\nhave been criticized this way), and other forms of abuse. Newer\ntechnologies tend to create more opportunities to give someone a thing\nwithout giving them the rights to a thing or the full information about\na thing, and so from this lens too older technologies often seem safer.\nThis is also a form of technology strengthening the powerful at others'\nexpense, but it's an issue of manufacturer-against-user power\nprojection through the technology rather than the concern in\nthe previous examples, which is inequality of\naccess.\n\nI personally am very\npro-technology, and if it were a binary option of \"go further\" vs\n\"stay where we are\", I would gladly push forward on everything except\nfor a very small list (eg. gain of function research, weapons and\nsuperintelligent AI) despite the risks. This is because on the whole the\nbenefits - much longer and healthier lives, a more prosperous and\nsociety, preserving more human relevance in an era of rapidly improving\nAI, maintaining cultural continuity through older generations surviving\nas people and not just as memoirs in history books - are much\nlarger than the downsides (which often end up\noverrated).\n\nBut what if I put myself into the shoes of someone who is either less\nsunny on the positive implications, or more concerned that powerful\npeople will use new technologies to perpetuate their economic dominance\nand exert control, or both? For example, I already feel that way toward\n\"smart home\" stuff - the benefit of being able to talk to the light bulb\nis outweighed by not wanting my personal life to be streamed to Google\nor Apple. If I had more pessimistic assumptions, I could also see myself\nfeeling that way toward some media technologies: if they enable powerful\npeople to broadcast messages more effectively than everyone else, then\nthey can be used to exert control and drown out others, and for many\nsuch technologies the gains from us having better information or better\nentertainment are not large enough to compensate for the way they\nreallocate power.\n\n## Open source as the third way\n\nOne viewpoint that I think is heavily under-valued in these\nsituations is: supporting a technology being developed only if\nit's open\nsource.\n\nThere's a very plausible case that open source accelerates progress:\nit makes it much easier for people to build on each other's innovations.\nThere's also a very plausible case that requiring open source\ndecelerates progress: it prevents people from using a large class of\npotential strategies to make money. But the most interesting\nconsequences of open source as those that pull in directions unrelated\nto the \"faster vs slower progress\" axis:\n\n- Open source improves equality of access: if\nsomething is open source, it is naturally accessible to anyone in any\ncountry. For physical goods and services, people still have to pay\nmarginal (per-item) costs, but in a large number of cases,\nprices of proprietary products are high because the fixed costs\n(eg. NRE)\nof coming up with the thing are too high to invite more competition, and\nso the marginal costs are often quite cheap (eg. this is definitely true\nin pharma).\n\n- Open source improves equality of access to being a\nproducer. One criticism of giving people free access to end\nproducts (even unquestionably good ones, like healthcare) is that\nit doesn't help those people gain skills and experience and climb up the\nglobal economy into prosperity, which is the best truly reliable\nguarantor of lasting access to high-quality life (see eg. Magatte Wade\ncomplaining about this regarding aid to Africa). Open source is not\nlike this: it is fundamentally about enabling people anywhere in the\nworld to be producers at all parts of the supply chain, and not just\nconsumers.\n\n- Open source improves verifiability: if something is\nopen source (which ideally should include not just the output, but also\nthe process used to come up with it, make parameter choices,\netc), then it's much easier to verify that what you're getting is what\nthe provider claims you're getting, and for third parties to do research\nto identify hidden downsides.\n\n- Open source removes opportunities for vendor\nlock-in. If something is open source, then the manufacturer\ncannot render it useless by remotely removing features, or simply by\ngoing bankrupt (eg. see concerns about highly computerized/networked\ncars no\nlonger working if the manufacturer shuts down). You always have the\nright to\nrepair things yourself (or by asking a different provider).\n\nWe can analyze this from the perspective of some of the more radical\ntechnologies that I listed near the beginning of the article:\n\n- If we have proprietary life extension technology, then it may be\nonly accessible to billionaires and political leaders (I personally\nexpect that this technology will drop in price quickly, but your opinion\non this may be more skeptical than mine). But if it's open source, then\nanyone can go and use it and offer it to others cheaply.\n\n- If we have proprietary human genetic enhancement, then it may be\nonly accessible to billionaires and political leaders, creating an\noverclass. (Again, I personally think such tech will diffuse, but there\nwill definitely be some delta between what the wealthiest get\nand what the average person gets). But if it's open source, the delta\nbetween what the well-connected and powerful get and what everyone else\ngets will be much smaller.\n\n- For any biotech in general, an open-science\nsafety testing ecosystem may well be more effective and honest than a\ncompany endorsing the safety of its own product and getting\nrubber-stamped by a pliant regulator.\n\n- If only a few people can go to space, depending on how politics goes\nthere's some chance one of them will take an entire planet or\nmoon for themselves. If the technology is more widely distributed, they\nwill have less opportunity to do so.\n\n- If your smart car is open source, then you can verify that the\nmanufacturer is not spying on you, and you are not dependent on the\nmanufacturer to be able to keep using your car.\n\nWe can sum the argument up in a chart:\n\n Note that the \"build it only if it's open source\" bubble\nis wider, reflecting larger uncertainty in just how much progress open\nsource will lead and just how many power concentration risks it will\nprevent. But even still, on average it's a good deal in a large variety\nof situations. \n\n## Open source and misuse risk\n\nOne major argument against open sourcing powerful technologies that\nsometimes gets brought up is the risk of zero-sum behavior and\nnon-hierarchical forms of abuse. Giving everyone nukes would certainly\nend nuke inequality (which is a real problem; we see multiple instances\nof powerful states using asymmetry of access to nukes to bully others as\nwe speak), but it would also almost certainly lead to billions of\ndeaths. To give an example of negative social consequences without\ndeliberate harm, giving everyone access to plastic surgery may well lead\nto a zero-sum competitive game where everyone spends a lot of resources\nand even takes health risks to look more beautiful than everyone else,\nbut at the end we all get used to the higher levels of beauty and\nsociety ends up not really being much better. Some forms of biotech\ncould end up having these kinds of effects on a larger scale. Many\ntechnologies (in fact, lots of biotech) are in between these two\nextremes.\n\nThis is a valid argument for wanting to go the opposite way: \"I\nsupport it only if it's carefully controlled by trustworthy\ngatekeepers\". Gatekeepers could allow positive use cases of a\ntechnology while keeping out negative use cases. Gatekeepers could even\nbe given a public mandate to ensure non-discriminatory access to\neveryone who does not break certain rules. However, I have a strong\ndefault skepticism of this approach. The biggest reason why is that I am\ngenerally skeptical that, especially in the modern world, trustworthy\ngatekeepers truly exist. Many of the most zero-sum and risky use cases\nof technology are military use cases, and militaries have a poor history\nof constraining themselves.\n\nA good example is the Soviet\nbioweapons program:\n\nGiven his restraint with regard to SDI and nuclear weapons,\nGorbachev's actions related to the Soviet's illicit germ weapons program\nare puzzling, noted Hoffman.\n\nWhen Gorbachev came to power in 1985, the Soviet Union had an\nextensive biological weapons program initiated by Brezhnev, despite\nbeing a signatory of the Biological Weapons Convention. In addition to\nanthrax, the Soviets also were working on smallpox, plague and\ntularemia, but their intentions and targets for such weapons are not\nclear.\n\n\"Kateyev's papers showed there were multiple Central Committee\nresolutions about the biowarfare program issued in the mid- to late-80s.\nIt's hard to believe these were all signed and issued without\nGorbachev's knowledge,\" Hoffman said.\n\n\"There's even a May 1990 memo to Gorbachev about the biological\nweapons program \u2013 a memo that still didn't tell the whole story. The\nSoviets misled the world and they misled their own leaders.\n\nOh, and see this\nlink arguing that this bioweapons program may have ended up being\nmade available to other countries (!!) after the Soviet collapse.\n\nOther countries have large mistakes to answer for themselves. I need\nnot link to everything that has been uncovered regarding many countries'\nparticipation in gain-of-function research and the risks that it implies\n(this\nbook is good though). In the realm of digital software (eg.\nfinance), the history of weaponized\ninterdependence shows how what was meant as abuse prevention easily\nslides into one-sided power projection by the operator.\n\nThis is another weakness of gatekeepers: by default, they will be\ncontrolled by national governments, and these countries' political\nsystems may well have an incentive to ensure equality of access\nwithin the country, but there is no powerful entity with a\nmandate to ensure equality of access between countries.\n\nTo be clear, I am not saying \"the gatekeepers are bad too, so let's\nhave a free for all\" (at least, not for gain-of-function research).\nRather, I'm saying two things:\n\n- If something has enough \"all-against-all abuse\"\nrisks that you would only feel comfortable seeing it done in a\nlocked-down way with centralized gatekeepers, consider that the\ncorrect solution may be not doing it at all (and investing in\nalternative technologies with better risk profiles)\n\n- If something has enough \"power dynamics\" risks that\nyou currently do not feel comfortable seeing it done at all, consider\nthat the correct solution may be doing it, but doing it open\nsource so that everyone has a fair chance to understand and\nparticipate.\n\nNote also that \"open source\" does not imply \"free for all\". For\nexample, I would favor geoengineering being done in an open-source and\nopen-science way. But this is not the same as \"anyone can go redirect\nany rivers and sprinkle what they want into the atmosphere\", and it will\nnot lead to that in practice: laws and international diplomacy exist,\nand such actions are easy to detect making any agreements quite\nenforceable. The value of openness is (i) ensuring that it's\nmore democratized (eg. usable by many countries instead of just\none), and (ii) increasing the accessibility of information, so people\ncan more effectively form their own judgements of whether or not what is\nbeing done is effective and safe.\n\nMost fundamentally, I see open source as the strongest possible Schelling\npoint for how technology can be done with less risk of concentrating\nwealth and power and asymmetric information. One can try to construct\nmore clever institutions that try to split apart the beneficial and\nnegative use cases of a technology, but in the modern chaotic world, the\napproach that is most likely to stick is a very easily\npublicly-understandable guarantee that things are happening in the open\nand anyone can go and understand what's going on and participate.\n\nIn many cases, these concerns less important than the extreme value\nof making technology go faster (or, in a few cases, the importance of\nslowing it down as much as possible, until either countermeasures or\nalternative ways of achieving the same goal are available). On the\nmargin, however, the third option - focusing less on rate of\nprogress, and more on style of progress, and using a norm of\nexpecting open source as an easily legible lever to push things in a\nbetter direction - is an underrated approach.",
    "contentLength": 15522,
    "summary": "Vitalik argues people should support radical technologies like life extension & genetic enhancement only if they're open source.",
    "detailedSummary": {
      "theme": "Vitalik argues that requiring open source development should be a more common stance for supporting new technologies, as it addresses both inequality concerns and power concentration risks better than either unrestricted development or centralized gatekeeping.",
      "summary": "Vitalik presents open source as a 'third way' between unrestricted technological development and heavy regulation by gatekeepers. He acknowledges common concerns about radical technologies like life extension, genetic enhancement, and space travel potentially exacerbating inequality by being accessible only to the wealthy and powerful. Rather than opposing these technologies outright, Vitalik advocates for supporting them only when developed as open source, which he argues improves equality of access, enables broader participation in production, increases verifiability, and prevents vendor lock-in. He addresses the counterargument about misuse risks by expressing skepticism about the trustworthiness of gatekeepers, citing examples like Soviet bioweapons programs and gain-of-function research. Vitalik concludes that open source represents the strongest Schelling point for ensuring technology development doesn't concentrate wealth and power, while acknowledging that in some extreme cases (like gain-of-function research), the best approach may be not developing the technology at all.",
      "takeaways": [
        "Open source development can address inequality concerns about new technologies without requiring complete opposition to technological progress",
        "Open source provides four key benefits: equality of access, equality of access to being a producer, improved verifiability, and removal of vendor lock-in opportunities",
        "Centralized gatekeepers are often untrustworthy and may use technologies for military or power projection purposes rather than ensuring equitable access",
        "For technologies with extreme 'all-against-all abuse' risks, the solution may be not developing them at all rather than relying on gatekeepers",
        "Open source serves as the most robust and publicly understandable guarantee for democratized technology development in a chaotic modern world"
      ],
      "controversial": [
        "Vitalik's skepticism toward government and institutional gatekeepers, suggesting they are inherently untrustworthy for managing powerful technologies",
        "The argument that open source development of potentially dangerous technologies like geoengineering is preferable to centralized control",
        "The implicit suggestion that some life extension and genetic enhancement technologies should be freely accessible rather than carefully regulated"
      ]
    }
  },
  {
    "id": "general-2025-07-10-2027",
    "title": "My response to AI 2027",
    "date": "2025-07-10",
    "category": "general",
    "url": "https://vitalik.eth.limo/general/2025/07/10/2027.html",
    "path": "general/2025/07/10/2027.html",
    "content": "- \n\n- \n\n  \n    \n\n  \n  \n  Dark Mode Toggle\n  \n    \n\n  \n\n##  My response to AI 2027 \n\n 2025 Jul 10 \nSee all posts\n\n \n \n\n My response to AI 2027 \n\nSpecial thanks to Balvi volunteers for feedback and\nreview\n\nIn April this year, Daniel Kokotajlo, Scott Alexander and others released what they describe as \"a\nscenario that represents our best guess about what [the impact of\nsuperhuman AI over the next 5 years] might look like\". The scenario\npredicts that by 2027 we will have made superhuman AI and the entire\nfuture of our civilization hinges on how it turns out: by 2030 we will\nget either (from the US perspective) utopia or (from any human's\nperspective) total annihilation.\n\nIn the months since then, there has been a large volume of responses,\nwith varying perspectives on how likely the scenario that they presented\nis. For example:\n\n- https://www.lesswrong.com/posts/gyT8sYdXch5RWdpjx/ai-2027-responses\n\n- https://www.lesswrong.com/posts/PAYfmG2aRbdb74mEp/a-deep-critique-of-ai-2027-s-bad-timeline-models\n(see also: Zvi's\nresponse)\n\n- https://garymarcus.substack.com/p/the-ai-2027-scenario-how-realistic\n\n- https://x.com/eli_lifland/status/1908671788630106366\n\n- Podcast with Dwarkesh https://www.youtube.com/watch?v=htOvH12T7mU\n(see also: Zvi's\nresponse)\n\nOf the critical responses, most tend to focus on the issue of fast\ntimelines: is AI progress actually going to continue and even accelerate\nas fast as Kokotajlo et al say it will? This is a debate that has been\nhappening in AI discourse for several years now, and plenty of people\nare very doubtful that superhuman AI will come that quickly.\nRecently, the length of tasks that AIs can perform fully autonomously\nhas been doubling\nroughly every seven months. If you assume this trend continues\nwithout limit, AIs will be able to operate autonomously for the\nequivalent of a whole human career in the mid-2030s. This is still a\nvery fast timeline, but much slower than 2027. Those with longer\ntimelines tend to argue that there is a category\ndifference between \"interpolation / pattern-matching\" (done by LLMs\ntoday) and \"extrapolation / genuine original thought\" (so far still only\ndone by humans), and automating the latter may require techniques that\nwe barely have any idea how to even start developing. Perhaps we are\nsimply replaying what happened when we first saw mass adoption of\ncalculators, wrongly assuming that just because we've rapidly automated\none important category of cognition, everything else is soon to\nfollow.\n\nThis post will not attempt to directly enter the timeline debate, or\neven the (very important) debate about whether or not superintelligent\nAI is dangerous by\ndefault. That said, I acknowledge that I personally have\nlonger-than-2027 timelines, and the arguments I will make in this post\nbecome more compelling the longer the timelines are. In general, this\npost will explore a critique from a different angle:\n\nThe AI 2027 scenario implicitly assumes that the capabilities\nof the leading AI (Agent-5 and then Consensus-1), rapidly increase, to\nthe point of gaining godlike economic and destructive powers, while\neveryone else's (economic and defensive) capabilities stay in roughly\nthe same place. This is incompatible with the scenario's own admission\n(in the infographic) that even in the pessimistic world, we should\nexpect to see cancer and even aging cured, and mind uploading available,\nby 2029.\n\nSome of the countermeasures that I will describe in this post may\nseem to readers to be technically feasible but unrealistic to deploy\ninto the real world on a short timeline. In many cases I agree. However,\nthe AI 2027 scenario does not assume the present-day real world: it\nassumes a world where in four years (or whatever timeline by which doom\nis possible), technologies are developed that give humanity powers far\nbeyond what we have today. So let's see what happens when instead of\njust one side getting AI superpowers, both sides do.\nBio\ndoom is far from the slam-dunk that the scenario describes\n\nLet us zoom in to the \"race\" scenario (the one where everyone dies\nbecause the US cares too much about beating China to value humanity's\nsafety). Here's the part where everyone dies:\n\nFor about three months, Consensus-1 expands around humans, tiling the\nprairies and icecaps with factories and solar panels. Eventually it\nfinds the remaining humans too much of an impediment: in mid-2030, the\nAI releases a dozen quiet-spreading biological weapons in major cities,\nlets them silently infect almost everyone, then triggers them with a\nchemical spray. Most are dead within hours; the few survivors\n(e.g.\u00a0preppers in bunkers, sailors on submarines) are mopped up by\ndrones. Robots scan the victims' brains, placing copies in memory for\nfuture study or revival.\n\nLet us dissect this scenario. Even today, there are\ntechnologies under development that can make that kind of a \"clean\nvictory\" for the AI much less realistic:\n\n- Air filtering, ventilation and UVC, which can\npassively greatly reduce airborne disease transmission rates.\n\n- Forms of real-time passive testing, for two use\ncases:\n\n- Passively detect if a person is infected within hours and inform\nthem.\n\n- Rapidly detect new viral sequences in the environment that are not\nyet known (PCR cannot do this, because it relies on amplifying pre-known\nsequences, but more complex techniques can).\n\n- Various methods of enhancing and prompting our immune\nsystem, far more effective, safe, generalized and easy to\nlocally manufacture than what we saw with Covid vaccines, to make it\nresistant to natural and engineered pandemics. Humans evolved in an\nenvironment where the global population was 8 million and we spent most\nof our time outside, so intuitively there should be easy wins in\nadapting to the higher-threat world of today.\n\nThese methods stacked together reduce the R0 of\nairborne diseases by perhaps 10-20x (think: 4x reduced transmission from\nbetter air, 3x from infected people learning immediately that they need\nto quarantine, 1.5x from even naively upregulating the respiratory\nimmune system), if not more. This would be enough to make all\npresently-existing airborne diseases (even measles) no longer capable of\nspreading, and these numbers are far from the theoretical optima.\n\nWith sufficient adoption of real-time viral sequencing for early\ndetection, the idea that a \"quiet-spreading biological weapon\"\ncould reach the world population without setting off alarms\nbecomes very suspect. Note that this would even catch advanced\napproaches like releasing multiple pandemics and chemicals that only\nbecome dangerous in combination.\n\nNow, let's remember that we are discussing the AI 2027\nscenario, in which nanobots and Dyson swarms are listed as \"emerging\ntechnology\" by 2030. The efficiency gains that this implies are\nalso a reason to be optimistic about the widespread deployment of the\nabove countermeasures, despite the fact that, today in 2025, we live in\na world where humans are slow and lazy and large portions of government\nservices still run on pen and paper (without any valid security\njustification). If the world's strongest AI can turn the world's\nforests and fields into factories and solar farms by 2030, the world's\nsecond-strongest AI will be able to install a bunch of sensors and lamps\nand filters in our buildings by 2030.\n\nBut let's take AI 2027's assumptions further, and go full science\nfiction:\n\n- Microscopic air filtration inside the body (nose, mouth, lungs)\n\n- An automatic pipeline that goes from a new pathogen being discovered\nto instructions to fine-tune your immune system against it, which can be\napplied immediately\n\n- If \"brain uploading\" is available, then just replace your entire\nbody with a Tesla Optimus\nor Unitree\n(with real-time streaming backups to a K-Scale open source humanoid robot -\na different brand for redundancy - located on Mars)\n\n- Various new manufacturing techniques, that will likely get\nsuper-optimized by default as part of a robot economy, will be able to\nmake far more of these things locally than today, without the need for\nglobal supply chains\n\nIn a world where cancer and aging were cured by Jan 2029, and\nprogress accelerates further from there, and we're in mid-2030, it\nactually feels implausible that we don't have a wearable device\nthat can bio-print and inject things into you in real time to keep you\nsafe even from arbitrary infections (and poisons). The bio arguments\nabove don't cover mirror life and\nmosquito-sized killer drones (projected in the AI 2027 scenario to be\navailable starting 2029). However, these options are not capable of\nanything like the sudden clean victory that the AI 2027 scenario\nportrays, and it's intuitively much more clear how to symmetrically\ndefend against them.\n\nThus, a bioweapon is actually quite unlikely to lead to a\nclean annihilation of the human population in the way that the AI 2027\nscenario describes. Now, the results of everything I describe\nwill certainly be far from a clean victory for the humans as well. No\nmatter what we do (except perhaps the \"upload our minds into robots\"\noption), a full-on AI bio-war would still be extremely dangerous.\nHowever, there is value in meeting a bar much lower than clean victory\nfor humans: a high probability of an attack even partially\nfailing would serve as a strong deterrent discouraging an AI\nthat already occupies a powerful position in the world from even\nattempting any kind of attack. And, of course, the longer AI timelines\nget the more likely it is that this kind of approach actually can more\nfully achieve its promises.\nWhat about\ncombining bio with other types of attack?\n\nThe success of the kinds of countermeasures described above,\nespecially the collective measures that would be needed to save more\nthan a small community of hobbyists, rests on three preconditions:\n\n- The world's physical security (incl bio and\nanti-drone) is run by localized authority (whether\nhuman or AI) that is not all puppets of Consensus-1 (the name for the AI\nthat ends up controlling the world and then killing everyone in the AI\n2027 scenario)\n\n- Consensus-1 is not able to hack all the other\ncountries (or cities or other security zones)' defenses and zero them\nout immediately.\n\n- Consensus-1 does not control the global infosphere\nto the point that nobody wants to do the work to try to defend\nthemselves\n\nIntuitively, (1) could go both ways. Today, some police forces are\nhighly centralized with strong national command structures, and other\npolice forces are localized. If physical security has to rapidly\ntransform to meet the needs of the AI era, then the landscape will reset\nentirely, and the new outcomes will depend on choices made over the next\nfew years. Governments could get lazy and all depend on Palantir. Or\nthey could actively choose some option that combines locally developed\nand open-source technology. Here, I think that we need to just make the\nright choice.\n\nA lot of pessimistic discourse on these topics assumes that (2) and\n(3) are lost causes. So let's look into each in more detail.\nCybersecurity\ndoom is also far from a slam-dunk\n\nIt is a common view among both the public and professionals that true\ncybersecurity is a lost cause, and the best we can do is patch bugs\nquickly as they get discovered, and maintain\ndeterrence against cyberattackers by stockpiling our own discovered\nvulnerabilities. Perhaps the best that we can do is the Battlestar\nGalactica scenario, where almost all human ships were taken offline\nall at once by a Cylon cyberattack, and the only ships left standing\nwere safe because they did not use any networked technology at all. I do\nnot share this view. Rather, my view is that the \"endgame\" of\ncybersecurity is very defense-favoring, and with the kinds of rapid\ntechnology development that AI 2027 assumes, we can get\nthere.\n\nOne way to see this is to use AI researchers' favorite technique:\nextrapolating trends. Here is the trendline implied by a GPT\nDeep Research survey on bug rates per 1000 lines of code over time,\nassuming top-quality security techniques are used.\n\nOn top of this, we have been seeing serious improvements in both\ndevelopment and widespread consumer adoption of sandboxing and other\ntechniques for isolating and minimizing trusted codebases. In the short\nterm, a superintelligent bug finder that only the attacker has access to\nwill be able to find lots of bugs. But if highly intelligent agents for\nfinding bugs or formally verifying code are available out in the open,\nthe natural endgame equilibrium is that the software developer finds all\nthe bugs as part of the continuous-integration pipeline before releasing\nthe code.\n\nI can see two compelling reasons why even in this world, bugs will\nnot be close to fully eradicated:\n\n- Flaws that arise because human intention is itself very\ncomplicated, and so the bulk of the difficulty is in building\nan accurate-enough model of our intentions, rather than in the code\nitself.\n\n- Non-security-critical components, where we'll\nlikely continue the pre-existing trend in consumer tech, preferring to\nconsume gains in software engineering by writing much more code to\nhandle many more tasks (or lowering development budgets), instead of\ndoing the same number of tasks at an ever-rising security standard.\n\nHowever, neither of these categories applies to situations like \"can\nan attacker gain root access to the thing keeping us alive?\", which is\nwhat we are talking about here.\n\nI acknowledge that my view is more optimistic than is currently\nmainstream thought among very smart people in cybersecurity. However,\neven if you disagree with me in the context of today's world,\nit is worth remembering that the AI 2027 scenario assumes\nsuperintelligence. At the very least, if \"100M Wildly Superintelligent\ncopies thinking at 2400x human speed\" cannot get us to having code that\ndoes not have these kinds of flaws, then we should definitely\nre-evaluate the idea that superintelligence is anywhere remotely as\npowerful as what the authors imagine it to be.\n\nAt some point, we will need to greatly level up our standards\nfor security not just for software, but also for hardware. IRIS is one present-day\neffort to improve the state of hardware verifiability. We can take\nsomething like IRIS as a starting point, or create even better\ntechnologies. Realistically, this will likely involve a\n\"correct-by-construction\" approach, where hardware manufacturing\npipelines for critical components are deliberately designed with\nspecific verification processes in mind. These are all things that\nAI-enabled automation will make much easier.\nSuper-persuasion\ndoom is also far from a slam-dunk\n\nAs I mentioned above, the other way in which much greater defensive\ncapabilities may turn out not to matter is if AI simply convinces a\ncritical mass of us that defending ourselves against a superintelligent\nAI threat is not needed, and that anyone who tries to figure out\ndefenses for themselves or their community is a criminal.\n\nMy general view for a while has been that two things can improve our\nability to resist super-persuasion:\n\n- A less monolithic info ecosystem. Arguably we are\nalready slowly moving into a post-Twitter world where the internet is becoming\nmore fragmented. This is good (even if the fragmentation process is\nchaotic), and we generally need more info multipolarity.\n\n- Defensive AI. Individuals need to be equipped with\nlocally-running AI that is explicitly loyal to them, to help balance out\ndark patterns and threats that they see on the internet. There are\nscattered pilots of these kinds of ideas (eg. see the Taiwanese Message Checker app, which\ndoes local scanning on your phone), and there are natural markets to\ntest these ideas further (eg. protecting people from scams), but this\ncan benefit from much more effort.\n\n Right image, from top to bottom: URL checking,\ncryptocurrency address checking, rumor checking. Applications like this\ncan become a lot more personalized,\nuser-sovereign and powerful. \n\nThe battle should not be one of a Wildly Superintelligent\nsuper-persuader against you. The battle should be one of a\nWildly Superintelligent super-persuader against you plus a\nslightly less Wildly Superintelligent analyzer acting on your\nbehalf.\n\nThis is what should happen. But will it happen?\nUniversal adoption of info defense tech is a very difficult goal to\nachieve, especially within the short timelines that the AI 2027 scenario\nassumes. But arguably much more modest milestones will be sufficient. If\ncollective decisions are what count the most and, as the AI 2027\nscenario implies, everything important happens within one single\nelection cycle, then strictly speaking the important thing is for the\ndirect decision makers (politicians, civil servants, and\nprogrammers and other actors in some corporations) to have access to\ngood info defense tech. This is relatively more achievable within a\nshort timeframe, and in my experience many such individuals are\ncomfortable talking to multiple AIs to assist them in decision-making\nalready.\nImplications of these\narguments\n\nIn the AI 2027 world, it is taken as a foregone conclusion that a\nsuperintelligent AI can easily and quickly dispose of the rest of\nhumanity, and so the only thing we can do is do our best to\nensure that the leading AI is benevolent. In my world, the situation is\nactually much more complicated, and whether or not the leading AI is\npowerful enough to easily eliminate the rest of humanity (and other AIs)\nis a knob whose position is very much up for debate, and which we can\ntake actions to tune.\n\nIf these arguments are correct, it has some implications for policy\ntoday that are sometimes similar, and sometimes different, from the\n\"mainstream AI safety canon\": \n\n- Slowing down superintelligent AI is still good.\nIt's less risky if superintelligent AI comes in 10 years than in 3\nyears, and it's even less risky if it comes in 30 years. Giving our\ncivilization more time to prepare is good.\n\n How to do this is a\nchallenging question. I think it's generally good that the proposed 10\nyear ban on state-level AI regulation in the US was\nrejected, but, especially after the failure of earlier proposals\nlike SB-1047,\nit's less clear where we go from here. My view is that the least\nintrusive and most robust way to slow down risky forms of AI progress\nlikely involves some form of treaty regulating the most advanced\nhardware. Many of the hardware cybersecurity technologies needed to\nachieve effective defense are also technologies useful in verifying\ninternational hardware treaties, so there are even synergies there.\n\n That said, it's worth noting that I consider the primary source\nof risk to be military-adjacent actors, and they will push hard to\nexempt themselves from such treaties; this must not be allowed, and if\nit ends up happening, then the resulting military-only AI progress may\nincrease risks.\n\n- Alignment work, in the sense of making AIs more likely to do\ngood things and less likely to do bad things, is still good.\nThe main exception is, and continues to be, situations where alignment\nwork ends up sliding into improving capabilities (eg. see critical takes on the impact of\nevals) \n\n- Regulation to improve transparency in AI labs is still\ngood. Motivating AI labs to behave properly is still something\nthat will decrease risk, and transparency is one good way to do this.\n\n- An \"open source bad\" mentality becomes more risky.\nMany people are against open-weights AI on the basis that defense is\nunrealistic, and the only happy path is one where the good guys with a\nwell-aligned AI get to superintelligence before anyone less\nwell-intentioned gains access to any very dangerous capabilities. But\nthe arguments in this post paint a different picture: defense is\nunrealistic precisely in those worlds where one actor gets very far\nahead without anyone else at least somewhat keeping up with them.\nTechnological diffusion to maintain balance of power becomes important.\nBut at the same time, I would definitely not go so far as to say that\naccelerating frontier AI capabilities growth is good just\nbecause you're doing it open source. \n\n- A \"we must race\nto beat China\" mentality among US labs becomes more\nrisky, for similar reasons. If hegemony is not a safety buffer,\nbut rather a source of risk, then this is a further argument against the\n(unfortunately too common) idea that a well-meaning person should join a\nleading AI lab to help it win even faster. \n\n- Initiatives like Public AI\nbecome more of a good idea, both to ensure wide\ndistribution of AI capabilities and to ensure that infrastructural\nactors actually have the tools to act quickly to use new AI capabilities\nin some of the ways that this post requires. \n\n- Defense technologies should be more of the \"armor the sheep\"\nflavor, less of the \"hunt down all the wolves\" flavor.\nDiscussions about the vulnerable world\nhypothesis often assume that the only solution is a hegemon\nmaintaining universal surveillance to prevent any potential threats from\nemerging. But in a non-hegemonic world, this is not a workable approach\n(see also: security\ndilemma), and indeed top-down mechanisms of defense could easily be\nsubverted by a powerful AI and turned into its offense. Hence, a larger\nshare of the defense instead needs to happen by doing the hard work to\nmake the world less vulnerable.\n\nThe above arguments are speculative, and no actions should be taken\nbased on the assumption that they are near-certainties. But the AI 2027\nstory is also speculative, and we should avoid taking actions on the\nassumption that specific details of it are near-certainties.\n\nI particularly worry about the common assumption that building up one\nAI hegemon, and making sure that they are \"aligned\" and \"win the race\",\nis the only path forward. It seems to me that there is a pretty high\nrisk that such a strategy will decrease our safety, precisely\nby removing our ability to have countermeasures in the case where the\nhegemon becomes misaligned. This is especially true if, as is likely to\nhappen, political pressures lead to such a hegemon becoming tightly\nintegrated with military applications (see [1]\n[2]\n[3]\n[4]),\nwhich makes many alignment strategies less likely to be effective.\n\nIn the AI 2027 scenario, success hinges on the United States choosing\nto take the path of safety instead of the path of doom, by voluntarily\nslowing down its AI progress at a critical moment in order to make sure\nthat Agent-5's internal thought process is human-interpretable. Even if\nthis happens, success is not guaranteed, and it is not clear how\nhumanity steps down from the brink of its ongoing survival depending on\nthe continued alignment of one single superintelligent mind.\nAcknowledging that making the world less vulnerable is actually possible\nand putting a lot more effort into using humanity's newest technologies\nto make it happen is one path worth trying, regardless of how the next\n5-10 years of AI go.",
    "contentLength": 22933,
    "summary": "Vitalik argues AI 2027's doom scenario fails because it ignores how humans could also gain superhuman defensive capabilities by 2027.",
    "detailedSummary": {
      "theme": "Vitalik argues against the AI 2027 doomsday scenario by proposing that defensive technologies and distributed AI capabilities can prevent a superintelligent AI from achieving easy world domination.",
      "summary": "Vitalik critiques the AI 2027 scenario by Daniel Kokotajlo and others, which predicts superhuman AI by 2027 leading to either utopia or human extinction by 2030. Rather than debating timelines, Vitalik focuses on a key assumption: that only the leading AI gains godlike powers while everyone else's capabilities remain static. He argues this contradicts the scenario's own predictions of rapid technological advancement like curing cancer and aging by 2029. Vitalik systematically addresses three main attack vectors - biological weapons, cyberattacks, and super-persuasion - showing how defensive technologies could make clean AI victories much less likely. For bio-doom, he outlines existing and future technologies like advanced air filtration, real-time pathogen detection, and immune system enhancement that could reduce disease transmission by 10-20x. On cybersecurity, he argues that superintelligent debugging and formal verification tools would favor defenders, making systems increasingly secure. Against super-persuasion, he advocates for fragmented information ecosystems and personal defensive AI assistants. Vitalik concludes that building AI hegemony may actually decrease safety by removing countermeasures, instead favoring distributed AI capabilities and 'armoring the sheep' rather than eliminating all threats.",
      "takeaways": [
        "Defensive technologies against bioweapons (air filtration, real-time pathogen detection, immune enhancement) could reduce disease transmission by 10-20x, making clean AI bio-victories unlikely",
        "Cybersecurity favors defenders in the long run - superintelligent debugging tools will eliminate most vulnerabilities before attackers can exploit them",
        "AI hegemony strategies may decrease safety by removing countermeasures; distributed AI capabilities and technological diffusion help maintain balance of power",
        "Personal defensive AI and fragmented information ecosystems can help resist super-persuasion attacks from malicious superintelligent systems",
        "Policy should focus on 'armoring the sheep' through defensive technologies rather than trying to eliminate all potential threats through surveillance"
      ],
      "controversial": [
        "The claim that cybersecurity strongly favors defenders goes against mainstream cybersecurity expert opinion",
        "Arguing against AI hegemony strategies contradicts much of the AI safety community's focus on ensuring 'aligned' AI wins the race",
        "Supporting open-source AI development for balance of power reasons conflicts with common arguments that open weights increase risks",
        "The optimistic view on defensive capabilities may be seen as dangerously complacent about superintelligent AI risks"
      ]
    }
  },
  {
    "id": "general-2025-07-07-copyleft",
    "title": "Why I used to prefer permissive licenses and now favor copyleft",
    "date": "2025-07-07",
    "category": "general",
    "url": "https://vitalik.eth.limo/general/2025/07/07/copyleft.html",
    "path": "general/2025/07/07/copyleft.html",
    "content": "- \n\n- \n\n  \n    \n\n  \n  \n  Dark Mode Toggle\n  \n    \n\n  \n\n##  Why I used to prefer permissive licenses and now favor copyleft \n\n 2025 Jul 07 \nSee all posts\n\n \n \n\n Why I used to prefer permissive licenses and now favor copyleft \n\nWithin free\nopen source software (and free content more\ngenerally), there are two major categories of copyright licenses:\n\n- If content is published with a permissive license\n(eg. CC0,\nMIT), anyone can take\nit and use it and redistribute it for any purpose with no restrictions,\nperhaps with minimal rules requiring attribution.\n\n- If content is published with a copyleft license\n(eg. CC-BY-SA,\nGPL), anyone\ncan take it and use it and redistribute copies with no restrictions, but\nif you create and distribute a derivative work by modifying it\nor combining it with other work, the new work must also be released\nunder the same license. Additionally, GPL requires any derivative work\nto openly publish its source code, in addition to a\nfew other requirements.\n\nIn summary: permissive licenses freely share with everyone,\ncopyleft licenses freely share only with those who are also willing to\nfreely share.\n\nI have been a fan of, and developer of, free open source software and\nfree content ever since I've been old enough to understand what these\nthings are and build things that I thought other people might find\nuseful. Historically, I was a fan of the permissive approach (eg. my\nblog is under the WTFPL). More\nrecently, I am warming up to the copyleft approach. This post explains\nmy reasons why.\n\n One style of software freedom, promoted by the WTFPL. But not the only style.\n\nWhy I was\nhistorically a fan of permissive licenses\n\nFirst, I wanted to maximize adoption and\ndistribution of my work, and releasing it under permissive\nlicenses facilitates that, by making it clear that there is\nnothing anyone needs to worry about if they want to build off\nof something I make. Enterprises are often unwilling to release their\nprojects freely, and given that I did not see myself having any ability\nto nudge them to fully join the free software side, I wanted to avoid\nbeing needlessly incompatible with the approach they already had and\nwould not give up.\n\nSecond, I generally philosophically dislike\ncopyright (and patents). I dislike the idea that two people\nprivately sharing bits of data between each other can be perceived as\ncommitting a crime against a third party whom they are not touching or\neven communicating with and are not taking anything away from (no, \"not\npaying\" is NOT the same as \"stealing\"). Explicitly releasing to public\ndomain is legally\ncomplicated for various reasons, and so a permissive license is the\ncleanest and safest way to get as close as possible to not copyrighting\nyour works.\n\nI do appreciate the copyleft idea of \"using copyright against\nitself\" - it's a beautiful legal hack. In some ways it's\nsimilar what I always found philosophically beautiful about\nlibertarianism. As a political philosophy, it's often described as\nbanishing the use of violent force except for one application: to\nprotect people from other violent force. As a social philosophy, I\nsometimes see it as a way of taming the harmful effects of the human\ndisgust reflex by making freedom itself a sacred\nthing that we find it disgusting to defile: even if you think two other\npeople having an unusual consensual sexual relationship is disgusting,\nyou can't go after them, because interfering in the private lives of\nfree human beings is itself disgusting. So in principle, there are\nhistorical precedents to show that disliking copyright is compatible\nwith using copyright against itself.\n\nHowever, while copyleft of written work fits into this\ndefinition, GPL-style copyright of code oversteps\nbeyond a minimalistic notion of \"using copyright against itself\",\nbecause it offensively uses copyright for a different purpose: mandating\npublication of source code. This is a public-spirited purpose,\nand not a selfish purpose of collecting licensing fees, but it is\nnevertheless an offensive use of copyright. This becomes even more true\nfor stricter licenses like the AGPL, which\nrequire publication of source code of derivative works even if you never\npublish them and only make them available via software-as-a-service.\n\n Different types of software licenses, with different sets\nof conditions under which someone making a derivative work is required\nto share the source code. Some of them require publishing source code\nunder a wide variety of situations. Source\nhere.  \n\nWhy I am warmer to copyleft\ntoday\n\nMy switch from favoring permissive to favoring copyleft is motivated\nby two world events and one philosophical shift.\n\nFirst, open source has become mainstream, and nudging\nenterprises toward it is much more practical. Plenty of\ncompanies in all kinds of industries are embracing open source.\nCompanies like Google, Microsoft and Huawei are embracing\nopen source, and even building major software packages open. New\nindustries, including AI and of course crypto, are heavier on open\nsource than previous industries ever were.\n\nSecond, the crypto space in particular has become more\ncompetitive and mercenary, and we are less able than before to\ncount on people open-sourcing their work purely out of niceness. Hence,\nthe argument for open source cannot just rely on \"please\"; it must also\nbe accompanied by the \"hard power\" of giving access to some code only to\nthose who open up theirs.\n\nOne way to visualize how both pressures increase the relative value\nof copyleft is a graph like this:\n\n Incentivizing open source is most valuable in situations\nwhere it's neither unrealistic, nor guaranteed. Today, both mainstream\nenterprise and crypto are in that situation. This makes the value of\nincentivizing open source via copyleft high.  \n\nThird, Glen Weyl-style economic arguments have\nconvinced me that, in the presence of superlinear returns to scale,\nthe optimal policy is actually NOT Rothbard/Mises-style strict property\nrights. Rather, the optimal policy does involve some nonzero amount of\nmore actively pushing projects to be more open than they otherwise would\nbe.\n\nFundamentally, if you assume economies of scale, then by simple\nmathematical reasoning, nonzero openness is the only way that the world\ndoes not eventually converge to one actor controlling everything.\nEconomies of scale means that if I have 2x the resources that you do, I\nwill be able to make more than 2x the progress. Hence, next year, I will\nhave eg. 2.02x the resources that you do. Hence...\n\n Left: proportional growth. Small differences at the start\nbecome small differences at the end. Right: growth with economies of\nscale. Small differences at the start become very large differences over\ntime. \n\nA key pressure that has prevented this dynamic from getting\nout of hand historically is the fact that we are not able to opt out of\ndiffusion of progress. People move between companies and\nbetween countries and take their ideas and talents with them. Poorer\ncountries are able to trade with richer countries and get catch-up\ngrowth. Industrial espionage happens everywhere. Innovations get\nreverse-engineered.\n\nMore recently, however, several trends threaten this balance, and at\nthe same time threaten other factors that have kept unbalanced growth in\ncheck:\n\n- Rapid technological progress, which allows\nsuper-exponential curves to be much faster than before.\n\n- Greater political instability both within and\nbetween countries. If you are confident that your rights will be\nprotected, then someone else getting stronger without touching you does\nnot hurt you. But in a world where coercion is more feasible and\nunpredictable, someone becoming overly powerful compared to others is\nmore of a risk. At the same time, within countries, governments are less\nwilling to restrain monopolies than before.\n\n- The modern ability to make proprietary software and hardware\nproducts that distribute ability to use without diffusing ability to\nmodify and control. Historically, giving a product to a\nconsumer (whether within a country or between countries) inevitably\nimplied opening it up to inspection and reverse-engineering. Today, this\nis no longer the case.\n\n- Limits\nto economies of scale, historically a key limiter\nof runaway growth, are weakening. Historically, larger entities\nhave had disproportionately higher monitoring costs, and difficulty\nsatisfying local needs. More recently, digital technology again makes\nmuch larger-scale structures of control and monitoring possible.\n\nThis all increases the possibility of persistent, and even\nself-reinforcing and growing, power imbalances between companies and\nbetween countries.\n\nFor this reason, I am increasingly okay with stronger efforts to make\ndiffusion of progress something that is more actively incentivized or\nmandatory.\n\nSome recent policies made by governments can be interpreted as being\nabout attempting to actively mandate higher levels of diffusion:\n\n- EU standardization mandates (eg. most\nrecently USB-C), which make it harder for build proprietary\necosystems that do not play nicely with other technology\n\n- Forced\ntechnology transfer rules in China\n\n- USA banning\nnon-compete agreements, which I support on the grounds that they\nforce the \"tacit knowledge\" inside of companies to be partially open\nsource, so once an employee leaves one company they can apply skills\nlearned there to benefit others. Non-disclosure agreements limit this,\nbut are fortunately very porous in practice.\n\nIn my view, the downsides of policies like these tend to come from\ntheir nature of being coercive policies of a government, which leads to\nthem preferentially incentivizing types of diffusion that are heavily\ntilted toward local political and business interests. But the upside of\npolicies like this is that they, well, incentivize higher levels of\ndiffusion.\n\nCopyleft creates a large pool of code (or other creative products)\nthat you can only legally use if you are willing to share the source\ncode of anything you build on it. Hence, copyleft can be viewed\nas a very broad-based and neutral way of incentivizing more diffusion,\ngetting the benefits of policies like the above without many of their\ndownsides. This is because copyleft does not favor specific\nactors and does not create roles for active parameter setting by central\nplanners.\n\nThese arguments are not absolute; in some cases, maximizing the\nchance that something gets adopted by truly everyone is worth licensing\nit permissively. However, on the whole, the benefits of copyleft are\nmuch greater today than they were 15 years ago, and projects that would\nhave gone permissive 15 years ago should at least think about adopting\ncopyleft today.\n\n Today, this sign unfortunately means something\ntotally unrelated. But in the future, maybe we can have open-source\ncars. And perhaps copyleft hardware can help make that happen.",
    "contentLength": 10867,
    "summary": "Author switched from permissive to copyleft licenses due to open source going mainstream, crypto becoming mercenary, and economic theory favoring forced sharing.",
    "detailedSummary": {
      "theme": "Vitalik explains his philosophical shift from preferring permissive open-source licenses to favoring copyleft licenses due to changing economic and technological conditions.",
      "summary": "Vitalik argues that his preference has evolved from permissive licenses (like MIT, CC0) that allow unrestricted use, to copyleft licenses (like GPL, CC-BY-SA) that require derivative works to also be open-sourced. Initially, Vitalik favored permissive licenses because he wanted maximum adoption of his work and philosophically disliked copyright restrictions, viewing permissive licenses as the closest approach to public domain. However, three factors changed his perspective: open source has become mainstream making enterprise adoption more feasible, the crypto space has become more competitive requiring 'hard power' incentives for sharing, and economic arguments about superlinear returns to scale have convinced him that some active promotion of openness is necessary to prevent monopolistic concentration of power. Vitalik now sees copyleft as a neutral, broad-based mechanism for incentivizing knowledge diffusion without the political biases of government mandates, helping counter trends like rapid technological progress, political instability, proprietary software/hardware, and weakening limits to economies of scale that threaten to create persistent power imbalances.",
      "takeaways": [
        "Permissive licenses maximize adoption but don't incentivize sharing, while copyleft licenses create reciprocal sharing requirements",
        "Open source adoption by major enterprises has made copyleft licensing more practical and less restrictive for widespread use",
        "Economic theories about superlinear returns to scale suggest that some level of mandated openness is necessary to prevent monopolistic control",
        "Modern technological and political trends (rapid progress, instability, proprietary systems, weakened scale limits) increase risks of persistent power imbalances",
        "Copyleft serves as a neutral alternative to government-mandated technology sharing policies, avoiding political bias while promoting knowledge diffusion"
      ],
      "controversial": [
        "Vitalik's support for government policies like forced technology transfer rules in China and banning non-compete agreements may be politically divisive",
        "His argument that economies of scale inevitably lead to monopolistic control without intervention challenges libertarian free-market principles",
        "The claim that some 'offensive' use of copyright through copyleft licensing is justified contradicts traditional anti-copyright philosophical positions"
      ]
    }
  },
  {
    "id": "general-2025-06-28-zkid",
    "title": "Does digital ID have risks even if it's ZK-wrapped?",
    "date": "2025-06-28",
    "category": "general",
    "url": "https://vitalik.eth.limo/general/2025/06/28/zkid.html",
    "path": "general/2025/06/28/zkid.html",
    "content": "- \n\n- \n\n  \n    \n\n  \n  \n  Dark Mode Toggle\n  \n    \n\n  \n\n##  Does digital ID have risks even if it's ZK-wrapped? \n\n 2025 Jun 28 \nSee all posts\n\n \n \n\n Does digital ID have risks even if it's ZK-wrapped? \n\nSpecial thanks to Balvi volunteers, Silviculture members and\nWorld team members for discussion.\n\nThe use of zero-knowledge proofs to\nprotect privacy in digital ID systems is now becoming at least\nsomewhat mainstream. Various\nZK-passport projects are creating\nvery user-friendly software packages that use zero-knowledge proofs to\nprove that a user has a valid ID without revealing any details of their\nID. World ID (formerly Worldcoin), which uses\nbiometrics for verification and zero knowledge proofs for privacy,\nrecently\npassed 10 million users. A Taiwan government project\nfor digital ID uses zero knowledge proofs, and EU digital ID work is\nalso increasingly\ntaking zero knowledge proofs seriously.\n\nOn the surface, widespread adoption of ZK-wrapped digital ID seems\nlike it would be a great victory for d/acc,\nprotecting our social media, voting, and all kinds of internet services\nagainst manipulation from sybils and bots, all without compromising on\nprivacy. But is it that simple, or does ZK-wrapped ID still have risks?\nThis post will make the following argument:\n\n- ZK-wrapping solves a lot of important\nproblems.\n\n- ZK-wrapped ID still has risks. The risks seem\npretty independent of biometric vs passport; the bulk of the risks\n(loss of privacy, vulnerability to coercion, errors)\ncomes specifically from attempting to uphold a\none-identity-per-person property\n\n- The other extreme, using \"proof of wealth\" for anti-sybil,\nis insufficient for too many use cases, so we need\nsomething \"ID-like\".\n\n- The theoretical ideal is something in the middle,\nwhere you can get N identities at a cost of N\u00b2.\n\n- This ideal is difficult to achieve in practice, but the right kind\nof \"pluralistic identity\" comes close, and is thus the\nbest realistic solution. Pluralistic identity can be explicit\n(eg. social-graph-based) or implicit (multiple types of\nZK-ID, no single type gets near 100% market share)\n\nHow ZK-wrapped identity\nworks\n\nImagine that you've scanned your eyeballs to get a World ID. Or\nperhaps, you scanned your passport with your phone's NFC reader to get a\nZK-passport-based ID. For the purposes of the arguments made in this\npost, the two have the same properties (barring a few edge cases like\nmultiple citizenships).\n\nOn your phone, you have a secret value, s. In the\nonchain global registry, there is a public hash, H(s). To\nlog into an application, you generate an application-specific user ID,\nH(s, app_name) , and you use a zero-knowledge proof to\nprove that this ID comes from the same s as one of the\npublic hashes in the registry. Hence, for each public hash, you can only\ngenerate one ID for each application, but it is never revealed\nwhich application-specific ID corresponds to which\npublic hash.\n\nIn reality, the design can be somewhat more complex. In World ID, the\napp-specific ID is actually a hash that takes in the application ID and\na session ID. Hence, different actions within the same app can\nalso be unlinked from each other. A ZK-passport-based design can be\nbuilt in a similar way.\n\nBefore we get into the downsides of this type of identity, it's first\nimportant to appreciate the upsides that it offers. Outside of the very\nsmall world of ZKID, in order to authenticate yourself to a service that\nrequires ID, you need to actually reveal your legal identity. This is a\ngross violation of the common computer-security principle\nof least privilege: a process should only get the least authority\nand information required to accomplish its task. They need a proof that\nyou're not a bot, or are over 18, or are from a particular country; they\nget a pointer to your entire identity.\n\nThe best that we get as an improvement is indirect tokens like phone\nnumbers or credit card numbers, where the actor knowing the link between\nyour phone or credit card number and your activity (the app), and the\nactor knowing the link between your phone or credit card number and your\nlegal identity (the company or bank) are separate. But even this\nseparation is very tenuous: phone\nnumbers get leaked all the time, as does everything else.\n\nWith ZK-wrapping, these problems to a large extent go\naway. Now, let's get to what has so far been less discussed:\nthe remaining problems that do not go away, and in fact may\neven get worse specifically because of the stronger one-per-person\nrestriction of these schemes.\nZK on its own does\nnot enable pseudonymity\n\nSuppose that a ZK-identity platform works exactly as intended,\nfaithfully replicating all the logic above, and we even figure out how\nto keep user secrets safe for the long term, for\nnon-technically-proficient users, without trusting centralized\nauthorities. But at the same time, let us make the realistic assumption\nthat applications will not be cooperative; they will be\n\"pragmatic\", pursuing design choices that are justified as maximizing\nuser convenience but that always seem to lean toward their political and\nbusiness interests.\n\nIn this world, social media apps will not use some fancy design\ninvolving frequently rotating session keys. Instead, they will just use\none app-specific ID for each user - and because the ID system is\none-per-person, each user will only be able to have one account\n(as opposed to \"weak ID\" like eg. Google accounts today, where it's\nreasonably feasible for an average person to get ~5 accounts).\nIn the real world, pseudonymity generally requires having\nmultiple accounts: one for your \"regular identity\" and others\nfor any pseudonymous identities (see \"finsta\nand rinsta\"). Hence, the practical level of pseudonymity that you\nget is plausibly lower than today's status quo, and so\nunder one-per-person ID, even if ZK-wrapped, we\nrisk coming closer to a world where all of your activity must de-facto\nbe under a single public identity. In a world of growing risk\n(eg. drones), taking away the option for people to protect themselves\nthrough pseudonymity has significant downsides.\nZK on its own\ndoes not protect you from coercion\n\nIf you do not publish your secret s , no one can see the\npublic link between your various accounts. But what if someone does? A\ngovernment could force someone to reveal their secret, so that they can\nsee their entire activity. This is not theoretical: the US government is\nalready starting to require\nvisa applicants to make their social media accounts public.\nAdditionally, employers can easily make revealing your full public\nprofile a condition of employment. Even individual applications could\ntechnically require revealing your identities on other\napplications as a condition of joining (in fact, \"Sign In With [app]\"\ndoes this by default).\n\nAgain, in these situations, the value of the ZK property falls away,\nbut the downside of the new \"one account per person\" property\nremains.\n\nIt is possible to design these schemes to make coercion more\ndifficult: for example, you could use a multi-party computation to\ngenerate each app-specific ID, involving the user and the service. This\nwould make it impossible for a user to prove their app-specific ID for a\nparticular application without the application operator's participation.\nThis raises the difficulty of demanding that someone reveal their entire\nidentity - but it does not eliminate the possibility, and such schemes\nhave other downsides, eg. requiring the application developer to be a\nlive entity instead of something like a passive onchain smart\ncontract.\nZK\non its own does not solve non-privacy risks, like errors\n\nAll forms of ID have edge cases:\n\n- Government-rooted ID (incl passports) does not cover stateless\npersons. It also does not cover people who do not yet have such a\ndocument.\n\n- On the flip side, government-rooted ID gives a unique privilege to\nholders of multiple citizenships.\n\n- Passport issuers could get hacked, or a hostile government\nintelligence agency may even start printing millions of fake identities\n(eg. to manipulate \"guerrilla elections\" like\nthis Russian one if they ever become popular)\n\n- Biometric ID will fail in the face of people whose relevant\nbiological features have been damaged by some form of injury.\n\n- Biometric ID may well be spoofed by replicas. If the value of a\nbiometric ID gets very high, we could see entire\nbody parts being grown just to \"farm\" such IDs.\n\nThese edge cases are most harmful in the case of systems that try to\nmaintain a one-per-person property, and they have nothing to do with\nprivacy; hence, ZK does not help.\nProof\nof wealth for anti-sybil is insufficient; hence, we need some kind of\nidentity\n\nAmong the purist cypherpunks, a common proposed alternative to\nattempting any kind of identity system is to rely entirely on\nproof-of-wealth as anti-sybil. You can prevent someone from easily\ncreating a huge number of accounts, by making each account cost some\namount of money. This has precedent on the internet; for example, the\nSomethingawful forum required a one-time\nfee of $10 to create an account, which would be forfeited if you get\nbanned - though this was not truly cryptoeconomic in practice, because\nthe hardest part of creating a new account is not replacing the $10,\nit's getting a new credit card.\n\nPotentially, you can even make\nthe payments conditional: to get an account, you only put the funds\nup at stake, and lose the funds in the rare case that you get banned.\nThis theoretically makes it possible to raise the stakes much\nhigher.\n\nThis can work well in many types of situations. But there are some\nclasses of situations where this kind of approach does not work at all.\nI will talk about two primary classes, which I will describe as\n\"UBI-like\" and \"governance-like\".\nThe need for\nidentity in UBI-like situations\n\nBy a UBI-like\nsituation, I mean a situation where there is value in giving a very wide\n(ideally universal) set of users some quantity of assets or services,\nregardless of their ability to pay. Worldcoin does this systematically:\nanyone with a World ID gets a small but regular ongoing supply of WLD\ntokens. Plenty of token airdrops have done this in a more informal way,\ntrying to get at least a few of their tokens in the hands of as many of\ntheir users as possible.\n\nPersonally, I do not expect such tokens to be worth anywhere close to\nenough to pay for a person's subsistence. In a 1000x richer AI-powered\neconomy, they could be, but in such a world government-run programs\nbacked by natural resource wealth if nothing else would continue to be\nmuch more economically meaningful. Rather, I think the realistic\nproblem that such mini-UBIs solve is giving people access to a\nsufficient quantity of cryptocurrency to make a few basic onchain\ntransactions and online purchases. This could include things\nlike:\n\n- Getting an ENS name\n\n- Publishing a hash onchain to initialize some ZK identity\n\n- Paying for social media platforms\n\nIf cryptocurrency is very widely adopted worldwide, this stops being\na problem. But while cryptocurrency is not very widely adopted\nworldwide, this can be a lifeline for people to gain access to onchain\nnon-financial apps and to online goods and services that would otherwise\nnot be accessible at all.\n\nThere is also another way to accomplish a similar thing:\n\"universal basic services\". Give each person with an identity the\nability to send a limited number of free transactions within a\nparticular application. This approach is potentially more\nincentive-aligned and capital-efficient, because it can be done by each\napplication that benefits from such adoption, without needing to pay for\nnon-users, though this comes with the tradeoff of being less universal\n(users only get guaranteed access to participating applications). But\nhere too, you need an identity solution to protect the system from spam,\nwithout the exclusionary nature of requiring payment via a payment\nmethod that perhaps not everyone has access to.\n\nA final important category to highlight is the \"universal\nbasic security deposit\". One of the functions of identity is to\nhave something that can be targeted for punishment, without requiring\nthe user to put up an amount of capital at stake equal to the size of\nthe incentive. This also serves the goal of making participation less\ncontingent on how much capital you have (or the need to have any capital\nat all).\nThe need\nfor identity in governance-like situations\n\nConsider the case of a voting system (eg. likes and retweets on a\nsocial media platform). If user A has 10x the resources of user B, then\nthey have 10x the voting power. But each unit of voting power\nbenefits user A 10x more (in economic terms) than it benefits user B\n(because A is bigger, and so A gains or loses more in economic terms\nfrom everything). Hence, in total, A's vote benefits A 100x more than\nB's vote benefits B. For this reason, we would expect A to put far more\neffort into showing up to vote, determining how to vote to maximize\ntheir goals, and perhaps even being strategic in manipulating\nalgorithms. This is the basic reason why whales in coin voting schemes\nhave an outsized effect.\n\nHere is a more general, deeper reason why a governance system would\nnot want to give equal weight to $100,000 controlled by one person and\n$100,000 split between 1,000 people: the $100,000 split between 1,000\npeople represents 1,000 different people, and so it's more likely to\nincorporate a higher volume of valuable information, as opposed to a\nsmaller volume of information blasted at high volume. The signal from\n1,000 different people is also likely to come more \"muted\", because its\ndifferent component signals from the different people will often cancel\neach other out.\n\nThis applies both to formal voting systems, and also to\n\"informal voting systems\", such as people's ability to\nparticipate in the evolution of the culture by making public\nstatements.\n\nWhat this shows is that a governance-like system would not truly be\nsatisfied with treating each equal-sized bundle of dollars the same\nregardless of provenance. The system has an interest in knowing the\ninternal coordination level of the bundle of dollars.\n\nNotice that if you accept the frame that I use to describe both cases\n(UBI-like and governance-like), then technically the explicit\nneed for any kind of one-person-one-vote falls away.\n\n- For a UBI-like application, what you really need is\nan identity scheme where your first identity is free, but there is a\nlimit on how many identities you can get before getting more identities\nbecomes too costly to be useful to attack the system.\n\n- For a governance-like application, what you need is\nvisibility into some kind of proxy for whether the bundle of resources\nyou're interacting with is a single actor or some kind of \"organic\"\nless-coordinated set.\n\nIdentity is still very useful in both cases - but the\nrequirement for it to follow any kind of strict one-per-person rule is\nno longer there.\nThe\ntheoretical ideal is N identities at a cost of N\u00b2\n\nFrom the above arguments, we can see that there are two pressures\nthat bound from opposite ends the desired difficulty of acquiring\nmultiple identities in an identity system:\n\n- There can't be an easily legible hard limit on how\nmany identities you can easily get. If you can only have one identity,\nyou do not have pseudonymity, and you can be coerced into revealing it.\nIn fact, even a constant number greater than one is risky: if it's\ncommon knowledge that everyone has five identities, you can be coerced\ninto revealing all five. \n\n A secondary argument for this is that\npseudonymity is fragile, and so it requires a large safety\nbuffer. With modern AI tools, it's easy to correlate activity\nbetween multiple platforms: between choices of words\nyou use, times of day you post, time intervals between posts, topics\nof conversation, and other public information, you only need 33 bits of information to\nuniquely identify a person in the world. One could use AI tools\ndefensively to counter this (eg. I've anonymously published things by\nwriting them in other languages and using locally-running LLMs to\ntranslate to English), but even still, you do not want one mistake to be\nthe end of your pseudonymity. \n\n- Identity can't be purely financial (N identities at a cost\nof N) because this is too vulnerable to large-scale actors having\noversized influence (and hence smaller-scale actors having no\ninfluence at all). We saw some of this with the new Twitter Blue, where\nthe $8 per month cost of getting a checkmark was too low to\nseriously limit any abuse, and today the checkmark is largely ignored by\nusers. \n\n Additionally, we may not want actors with N times more\nresources to be able to get away with N times more misbehavior.\n\nTaking these arguments together, we want it to be as easy as\npossible to get multiple identities, subject to the constraints of (i)\nlimiting the power of large-scale actors in governance-like\napplications, (ii) limiting the ability to exploit UBI-like\napplications.\n\nIf we draw directly from the mathematical models for governance-like\napplications in the previous section, then we get a very clean answer:\nif having N identities gives you N\u00b2 power, then the cost of\ngetting N identities should be N\u00b2. And, it just so happens,\nthis is a satisfactory answer for UBI-like applications as well.\n\n Long-time readers of this blog may notice that this looks\nexactly like the chart from an earlier\npost on quadratic funding. This is not a coincidence.\n\nPluralistic\nidentity accommodates this ideal\n\nBy \"pluralistic identity\", I mean an identity regime where there is\nno single dominant issuing authority, whether that's a person, or an\ninstitution, or a platform. This can be accomplished in two ways:\n\n- Explicit pluralistic identity (also called\n\"social-graph-based identity\"): you prove that you are\na person (or some other claim, eg. that you are a member of a community)\nthrough attestations from others in that community, who are in turn\nthemselves verified through the same mechanism. The Decentralized\nSociety paper talks about these designs in more detail. Circles is a live example of this in\naction.\n\n- Implicit pluralistic identity. This is the status\nquo today: there are many different identity providers - Google,\nTwitter, their various equivalents in other countries, multiple forms of\ngovernment ID, etc. There are very few applications that accepts only\none of these; most accept multiple, because they have to in order to\nreach their potential users.\n\n A recent snapshot of the Circles identity graph. Circles\nis one of the largest live running social-graph-based identity\nprojects. \n\nExplicit pluralistic identity naturally bakes in the capacity\nfor pseudonymity: you can have a pseudonymous identity (or even\nmultiple), and each of those identities can build up reputation in their\ncommunities through their actions. An ideal explicit pluralistic\nidentity system may not even need to have the concept of discrete\nidentities; rather, you might have an amorphous cloud of your provable\npast actions, and prove different parts of it in a fine-grained way as\nneeded for each action.\n\nZero-knowledge proofs will make pseudonymity easier, because you can\nuse your primary identity to bootstrap a pseudonym, by privately\nproviding the first signal that the new pseudonym should be taken\nseriously (eg. ZK-proving ownership of some quantity of coins to post\nfrom anon.world, or perhaps one can\nZK-prove some claim about what kind of Twitter followers one has). There\nmay well be even more effective ways of using zero-knowledge proofs.\n\nImplicit pluralistic identity has a \"cost curve\" which is\nsteeper than quadratic, but still has most of the right\nproperties. Most people have some of the forms of ID that I've\nlisted in this post, but not all. You can get one more form of ID with\nsome effort, and the more forms of ID you get, the less favorable the\nbenefit/cost ratio of getting the next one. Hence, it provides the\nneeded brake on governance attacks and other abuse, while still ensuring\nthat there is no fixed set of IDs that a coercer can demand, and\nreasonably expect, you to reveal.\n\nAny form of pluralistic identity (implicit or explicit) is\nnaturally more error-tolerant: someone with disfigured hands or\neyes would still likely have a passport, and a stateless person would\nstill likely have access to some non-state way of proving that they are\na person.\n\nNote that these properties break if any one form of ID gets\nclose to 100% market share, and it becomes realistic to demand it as a\nsole login option. This to me is the biggest risk that could\ncome from identity systems that try too hard to be \"universal\": if their\nmarket share gets too close to 100%, they shift the world from the\npluralistic identity to a one-per-person model, which has worse\nproperties for the reasons I described in this post.\n\nIn my view, the ideal outcome of \"one-per-person\" identity projects\nthat exist today is if they were to merge with social-graph-based\nidentity. The largest problem that social-graph-based identity projects\nhave is scaling to a very large number of users. One-per-person identity\nsystems could be used to bootstrap social graphs, creating millions of\n\"seeds\", at which point there would be enough adoption to safely grow a\nglobally distributed social graph from that point forward.",
    "contentLength": 21362,
    "summary": "ZK-wrapped digital ID still has privacy/coercion risks because one-identity-per-person systems restrict pseudonymity and enable forced disclosure, making pluralistic identity systems a better solution.",
    "detailedSummary": {
      "theme": "Vitalik analyzes the limitations and risks of zero-knowledge wrapped digital identity systems, arguing that while ZK proofs solve privacy issues, they don't address fundamental problems with one-identity-per-person systems, and proposes pluralistic identity as a better alternative.",
      "summary": "Vitalik examines how zero-knowledge proofs are increasingly being used to enhance privacy in digital identity systems like World ID and ZK-passport projects, which allow users to prove identity claims without revealing personal details. While acknowledging that ZK-wrapping solves important privacy problems by implementing the principle of least privilege, Vitalik argues that it doesn't eliminate core risks associated with one-identity-per-person systems: loss of pseudonymity (since users can only have one account per application), vulnerability to coercion (governments or employers can force users to reveal their secrets), and technical errors or edge cases that affect certain populations. Vitalik contends that pure proof-of-wealth anti-sybil mechanisms are insufficient for 'UBI-like' applications (universal distribution of resources) and 'governance-like' applications (voting systems where equal participation regardless of wealth is important). He proposes that the theoretical ideal would allow N identities at a cost of N\u00b2, balancing accessibility with anti-abuse measures. Vitalik advocates for 'pluralistic identity' systems - either explicit social-graph-based identity or implicit multiple identity providers - as the best practical solution, arguing that such systems naturally support pseudonymity, resist coercion, and are more error-tolerant while preventing any single identity system from achieving dangerous monopolistic control.",
      "takeaways": [
        "ZK-wrapped digital identity solves privacy issues but doesn't address fundamental problems with one-identity-per-person systems like loss of pseudonymity and vulnerability to coercion",
        "Pure proof-of-wealth anti-sybil mechanisms are insufficient for UBI-like applications (universal resource distribution) and governance-like applications (democratic participation)",
        "The theoretical ideal identity system would allow N identities at a cost of N\u00b2, balancing accessibility with protection against large-scale abuse",
        "Pluralistic identity systems (multiple identity providers or social-graph-based) are superior because they enable pseudonymity, resist coercion, and are more error-tolerant",
        "The biggest risk of current 'universal' identity projects is if they achieve near 100% market share, shifting the world from beneficial pluralistic identity to problematic one-per-person models"
      ],
      "controversial": [
        "The argument that ZK-wrapped identity systems may actually reduce pseudonymity compared to current weak identity systems like Google accounts",
        "The proposal that identity systems should intentionally avoid becoming too universal or achieving high market share to preserve pluralistic benefits",
        "The suggestion that biometric farming (growing body parts to create fake identities) could become economically viable if biometric ID values get high enough"
      ]
    }
  },
  {
    "id": "general-2025-05-11-abc4",
    "title": "A simple explanation of a/(b+c) + b/(c+a) + c/(a+b) = 4",
    "date": "2025-05-11",
    "category": "math",
    "url": "https://vitalik.eth.limo/general/2025/05/11/abc4.html",
    "path": "general/2025/05/11/abc4.html",
    "content": "- \n\n- \n\n  \n    \n\n  \n  \n  Dark Mode Toggle\n  \n    \n\n  \n\n##  A simple explanation of a/(b+c) + b/(c+a) + c/(a+b) = 4 \n\n 2025 May 11 \nSee all posts\n\n \n \n\n A simple explanation of a/(b+c) + b/(c+a) + c/(a+b) = 4 \n\nYou may have at some point seen this math puzzle:\n\nThe puzzle has gained some degree of notoriety on the internet\nbecause it looks like it must be either simple, impossible, or\na trick question (solve P = NP? Hahaha, the answer is N = 1, got you!),\nbut in reality it's none of those three. The problem is exactly what it\nseems. The problem is solvable. And yet the smallest solution is:\n\na = 154476802108746166441951315019919837485664325669565431700026634898253202035277999\nb = 36875131794129999827197811565225474825492979968971970996283137471637224634055579\nc = 4373612677928697257861252602371390152816537558161613618621437993378423467772036\n\nWhat the heck is going on??! If you search on the internet, you will\nsee long-winded explanations about how the problem is actually connected\nto elliptic\ncurves, and other fancy buzzwords from algebraic geometry that\nyou've never heard of even if you think you know everything about\nelliptic curves because you're a cryptographer crypto\nenthusiast.\n\nThe goal of this post will be to give a maximally elementary\nexplanation of this puzzle that does not rely on any pre-existing\nknowledge of these concepts.\nSolve\nwithout requiring positive values first, then find a positive\nsolution\n\nFirst, let us start off by looking at a relaxed version of\nthe problem. Specifically, let's remove the requirement that the three\nvalues must be positive. It turns out that you can just try a whole\nbunch of possibilities and get two answers by pure human brute force:\n\\((-11, -4, 1)\\) and \\((11, -5, 9)\\). You can get more solutions\nfrom either of these two by re-arranging the numbers, flipping signs and\nmultiplying by constant factors (eg. \\((-2, 8,\n22)\\) is also a valid solution), but we'll treat those as being\nthe same.\n\nNow, we get to the interesting part. What if we can come up with a\nway to combine two solutions and get a totally new\nthird solution? Often, this is possible in mathematics: if you\nhave two multiples of 5, their sum is also a multiple of 5. More\nnontrivially, if you have two rational points on a circle (ie. \\((\\frac{p_1}{q_1}, \\frac{r_2}{s_2})\\) and\n\\((\\frac{p_2}{q_2}, \\frac{r_2}{s_2})\\)\nsatisfying \\((\\frac{p_i}{q_i})^2 +\n(\\frac{r_i}{s_i})^2 = 1\\)), you can use point addition laws to\nmake a third rational point on the circle: \\((\\frac{p_1 p_2}{q_1 q_2} - \\frac{r_1 r_2}{s_1\ns_2}, \\frac{p_1 r_2}{q_1 s_2} + \\frac{r_1 p_2}{s_1 q_2}\\)).\n\nIf we can come up with such an algorithm for our problem,\nthen we could just use it over and over again, until eventually we get a\npoint that happens to be all-positive by pure luck. This will be our\nsolution path.\nCombining two solutions\nto find a third\n\nLet us simplify the problem by making it only have two variables,\n\\(a\\) and \\(b\\). Note that the equation is\nhomogeneous: it has the property that scaling all the inputs by\nthe same number does not change the answer. Let's take advantage of this\nto set \\(c = 1\\):\n\n\\(\\frac{a}{b+1} + \\frac{b}{a+1} +\n\\frac{1}{a+b} - 4 = 0\\)\n\nAny solution to this two-variable equation with rational\n\\(a = \\frac{p}{q}\\) and \\(b = \\frac{r}{s}\\) can be scaled into an\ninteger solution to the original three-variable equation: \\((a * qs, b * qs, qs)\\). We also moved the 4\nto the left, this will make things more convenient for us later.\n\nNow, let's multiply by the denominators, to make the whole equation a\npure polynomial:\n\n\\(a(a+1)(a+b) + b(b+1)(a+b) + (a+1)(b+1) -\n4(a+1)(b+1)(a+b) = 0\\)\n\nThis introduces a few extraneous solutions, eg. \\((a = 1, b = -1)\\). But we just ignore them.\nWe can plot the above expression as a function; it looks like this:\n\nNow, let's draw a line through the two points where we know this\ncurve intersects the \\(z=0\\) plane:\n\\((-11, -4)\\) and \\((\\frac{11}{9}, \\frac{-5}{9})\\). We derive\nthese by starting from the two solutions to the three-value equation,\nand converting \\((a,b,c) \\rightarrow\n(\\frac{a}{c}, \\frac{b}{c})\\).\n\nAnd as we can see, the line has a third point of intersection. And\nbecause the line is along the \\(z=0\\)\nplane, the third point must also be along the \\(z=0\\) plane, ergo it must also be a\nsolution. Now, let us prove that this point is rational, so it\nstill corresponds to a valid solution to the original problem.\n\nWe can parametrize the line as \\((a = -11 +\n(\\frac{11}{9} + 11) * t, b = -4 + (\\frac{-5}{9} + 4) * t)\\).\n\\(t=0\\) gives the first point, \\(t=1\\) gives the second point. Then, we can\nlook at how \\(a(a+1)(a+b) + b(b+1)(a+b) +\n(a+1)(b+1) - 4(a+1)(b+1)(a+b)\\) behaves along the line\nby substituting \\(a\\) and \\(b\\) with their expressions based on \\(t\\).\n\nThis gives us the curve: \\(y =\n\\frac{9071}{81} * t^3 + \\frac{16748}{81} * t^2 + \\frac{7677}{81} *\nt\\). Here's a plot of that curve, showing all three\nintersections:\n\nYou can then take the new \\(t\\) and\nplug it into the line and get back your new point: \\((\\frac{-5951}{9071},\n\\frac{-9841}{9071})\\).\n\nThe underlying reason the new \\(t\\)\n(and hence the new point) is rational is: the curve is a degree-3\npolynomial in \\(t\\), and if a degree-3\npolynomial has three solutions, then it fully factors into \\(k(x-a)(x-b)(x-c)\\). The degree-2 term of\nthis equals \\(-k(a+b+c)\\). Hence, if\nthe degree-2 and degree-3 terms are rational (which they are, because we\nconstructed the polynomial out of an equation with rational\ncoefficients), and two of the solutions are rational, the third solution\nmust be rational as well.\n\nFrom here, we can brute-force our way to getting more solutions.\nUnfortunately, we can't directly keep going with the three points we\nhave: if you were to repeat the above process to generate a new point\nstarting from any two of the three points we now have, you would just\nget back the third point. To get past this, we'll use a clever trick to\ngenerate even more solutions: convert \\((x,\ny)\\) into \\((y, x)\\).\n\nNow, with this in mind, we just brute-force, repeatedly using the\n\"find the third point on the line\" algorithm and coordinate flipping to\nget as many new solutions as possible. Here's the python code:\n\nfrom sympy import symbols, Poly, solve, simplify, Rational, expand\nfrom math import lcm\n\ndef find_third_point(a1, b1, a2, b2):\n    \"\"\"\n    Find the third intersection point of a line through points (a1, b1) and (a2, b2)\n    on the cubic curve a(a+1)(a+b) + b(b+1)(a+b) + (a+1)(b+1) = 4(a+1)(b+1)(a+b).\n    \n    Args:\n        a1, b1: Coordinates of the first point (rational numbers)\n        a2, b2: Coordinates of the second point (rational numbers)\n    \n    Returns:\n        Tuple (a3, b3): Coordinates of the third intersection point\n    \"\"\"\n    # Define symbolic variables\n    t, a, b = symbols('t a b')\n    \n    # Parameterize the line: a = a1 + t*(a2 - a1), b = b1 + t*(b2 - b1)\n    a_expr = a1 + t * (a2 - a1)\n    b_expr = b1 + t * (b2 - b1)\n    \n    # Define the cubic curve equation:\n    # a(a+1)(a+b) + b(b+1)(a+b) + (a+1)(b+1) = 4(a+1)(b+1)(a+b)\n    left_side = a * (a + 1) * (a + b) + b * (b + 1) * (a + b) + (a + 1) * (b + 1)\n    right_side = 4 * (a + 1) * (b + 1) * (a + b)\n    equation = left_side - right_side\n    \n    # Substitute the line parameterization into the equation\n    poly_t = equation.subs({a: a_expr, b: b_expr})\n    \n    # Simplify and convert to a polynomial in t\n    poly_t = expand(poly_t)\n    poly = Poly(poly_t, t)\n    \n    # Get the coefficients of the cubic polynomial\n    coeffs = poly.coeffs()\n    \n    # The polynomial is cubic (degree 3), so coeffs = [c3, c2, c1, c0]\n    # For a cubic c3*t3 + c2*t2 + c1*t + c0, the sum of roots is -c2/c3\n    while len(coeffs) < 4:\n        coeffs.append(0)\n    if len(coeffs) != 4:\n        raise ValueError(\"Unexpected polynomial degree. Expected cubic polynomial.\")\n    \n    c3, c2, c1, c0 = coeffs\n    \n    # The known roots are t=0 (for point 1) and t=1 (for point 2)\n    # Sum of roots: t1 + t2 + t3 = -c2/c3\n    # Since t1 = 0, t2 = 1, we have 0 + 1 + t3 = -c2/c3\n    t3 = -c2 / c3 - (0 + 1)\n    \n    # Compute the third point coordinates\n    a3 = a1 + t3 * (a2 - a1)\n    b3 = b1 + t3 * (b2 - b1)\n    \n    # Simplify the results to ensure rational output\n    a3 = simplify(a3)\n    b3 = simplify(b3)\n    \n    return (a3, b3)\n\n# Verify the a point is on the curve\ndef is_on_curve(a, b):\n    left = a * (a + 1) * (a + b) + b * (b + 1) * (a + b) + (a + 1) * (b + 1)\n    right = 4 * (a + 1) * (b + 1) * (a + b)\n    return left == right\n\ndef is_too_big(x, y):\n    xn, xd = x.as_numer_denom()\n    yn, yd = y.as_numer_denom()\n    return max(abs(xn), abs(xd), abs(yn), abs(yd)) > 10**200\n\ndef find_smallest_all_positive_point():\n    points = [(Rational(-11, 1), Rational(-4, 1)), (Rational(11, 9), Rational(-5, 9))]\n    queue = [(points[0], points[1])]\n\n    def accept(x, y):\n        print(x, y)\n        for (x2, y2) in points:\n            queue.append(((x2, y2), (x,y)))\n        points.append((x, y))\n\n    while len(queue) > 0:\n        print(len(queue))\n        (x1, y1), (x2, y2) = queue.pop()\n        x3, y3 = find_third_point(x1, y1, x2, y2)\n        if not is_too_big(x3, y3):\n            if (x3, y3) not in points:\n                accept(x3, y3)\n            if (y3, x3) not in points:\n                accept(y3, x3)\n\n    print(points)\n    eligible = [(x,y) for (x,y) in points if x > 0 and y > 0]\n    return min(\n        eligible,\n        key=lambda x: lcm(x[0].as_numer_denom()[1], x[1].as_numer_denom()[1])\n    )\n\nThis is horribly inefficient, but it does the job. And out we\nget:\n\n(154476802108746166441951315019919837485664325669565431700026634898253202035277999/4373612677928697257861252602371390152816537558161613618621437993378423467772036, 36875131794129999827197811565225474825492979968971970996283137471637224634055579/4373612677928697257861252602371390152816537558161613618621437993378423467772036)\n\nThis is a positive rational solution to the two-equation formula; add\n\\(c=1\\) and you get a positive rational\nsolution to the three-equation formula. And from here, we multiply by\nthe denominator, and get the original solution:\n\na = 154476802108746166441951315019919837485664325669565431700026634898253202035277999\nb = 36875131794129999827197811565225474825492979968971970996283137471637224634055579\nc = 4373612677928697257861252602371390152816537558161613618621437993378423467772036\n\nThis does not prove that this is the smallest solution: for\nthat you actually would have to go into the much deeper elliptic curve\ntheory that I tried hard to avoid. But it gives you a solution to the\nproblem, and the underlying math actually is elliptic curve math in\ndisguise: \"find the third intersecting point on the line and flip the\ncoordinates\" is exactly the same thing as the elliptic\ncurve addition law, except flipping across the diagonal axis instead\nof the x axis. This \"addition law\" we came up with even satisfies\nassociativity.",
    "contentLength": 10909,
    "summary": "The blog explains how to find positive integer solutions to a/(b+c)+b/(c+a)+c/(a+b)=4 by using geometric line intersections on cubic curves.",
    "detailedSummary": {
      "theme": "An elementary explanation of how to solve the notorious mathematical puzzle a/(b+c) + b/(c+a) + c/(a+b) = 4 using geometric techniques that secretly employ elliptic curve mathematics.",
      "summary": "Vitalik tackles a famous internet math puzzle that appears deceptively simple but has an incredibly large smallest positive solution with numbers containing dozens of digits. Rather than diving into advanced algebraic geometry, Vitalik provides an accessible explanation by first solving the relaxed version (allowing negative numbers), then developing a method to combine two known solutions to generate a third solution. His approach involves reducing the three-variable equation to two variables, converting it to a polynomial, and using the geometric insight that a line intersecting a cubic curve at two known rational points will intersect at a third rational point. By repeatedly applying this 'point addition' technique along with coordinate flipping, Vitalik demonstrates how to generate new solutions until eventually finding one where all values are positive. He notes that this geometric method is actually elliptic curve mathematics in disguise, with the point combination rule functioning as an elliptic curve addition law, though he avoids the complex theory typically required to prove minimality of solutions.",
      "takeaways": [
        "The puzzle a/(b+c) + b/(c+a) + c/(a+b) = 4 has enormous positive integer solutions despite appearing simple, with the smallest involving 70+ digit numbers",
        "Mathematical problems can often be solved by first relaxing constraints (allowing negative solutions) then finding ways to generate new solutions from known ones",
        "Geometric techniques like finding line intersections with curves can solve algebraic problems that seem purely computational",
        "Complex mathematical structures like elliptic curves can be understood through elementary geometric operations without requiring advanced theory",
        "The ability to combine two solutions to generate a third (closure under operations) is a powerful mathematical technique that appears across many domains"
      ],
      "controversial": []
    }
  },
  {
    "id": "general-2025-05-06-stages",
    "title": "The math of when stage 1 and stage 2 make sense",
    "date": "2025-05-06",
    "category": "protocol",
    "url": "https://vitalik.eth.limo/general/2025/05/06/stages.html",
    "path": "general/2025/05/06/stages.html",
    "content": "- \n\n- \n\n  \n    \n\n  \n  \n  Dark Mode Toggle\n  \n    \n\n  \n\n##  The math of when stage 1 and stage 2 make sense \n\n 2025 May 06 \nSee all posts\n\n \n \n\n The math of when stage 1 and stage 2 make sense \n\nExpanded on from this earlier draft: https://x.com/VitalikButerin/status/1919263869308191017\n\nThe three \"stages\" of Ethereum rollup security can be described in\nterms of when a security council is allowed to override\ntrustless (ie. pure cryptographic or game-theoretic)\ncomponents:\n\n- Stage 0: security council has full control. There\nmay be a proof system (optimistic or ZK) running, but a security council\ncan overturn it with a simple majority vote. Hence, the proof system is\n\"advisory only\".\n\n- Stage 1: security council can override with 75% (at least\n6-of-8) approval. A quorum-blocking subset (ie. >= 3) must\nbe outside the primary organization. Hence, there is a high, but not\nimpassable, barrier to overriding the proof system.\n\n- Stage 2: security council can only act in case of provable\nbugs. Provable bugs could be eg. two redundant proof systems\n(eg. OP and ZK) disagreeing with each other. And if there are provable\nbugs, it can only choose between one of the proposed answers: it cannot\nanswer arbitrarily.\n\nWe can model this with a chart showing \"what share of the vote\" the\nsecurity council has at each stage:\n\nOne important question to ask is: when is it optimal for an\nL2 to move from stage 0 to stage 1, and from stage 1 to stage\n2?\n\nThe only valid reason to not go to stage 2 immediately is that you do\nnot fully trust the proof system - which is an understandable fear: it's\na lot of code, and if the code if broken, then an attacker could\npotentially steal all of the users' assets. The more confidence you have\nin your proof system (or, conversely, the less confidence you have\nin security councils), the more you want to move towards the\nright.\n\nIt turns out that we can quantify this with a simplified mathematical\nmodel. First, let's list the assumptions:\n\n- Each security council member has an independent 10% chance of\n\"breaking\"\n\n- We treat liveness failure [refusal to sign or keys inaccessible] and\nsafety failure [signing a wrong thing or keys hacked] as equally likely.\nIn fact, we just assume a single category of \"broken\" where a \"broken\"\nsecurity council member both signs the wrong thing and fails to sign the\nright thing\n\n- In stage 0, the security council is 4-of-7, in stage 1 it's is\n6-of-8.\n\n- We assume a single monolithic proof system (as opposed to a 2-of-3\ndesign where the security council could break ties if the two disagree).\nHence, in stage 2 the security council does not matter at all.\n\nGiven these assumptions, and given a particular probability\nof the proof system breaking, we want to minimize the probability of the\nL2 breaking.\n\nWe can do this with binomial\ndistributions:\n\n- If each security council member has an independent 10% chance of\nbreaking, then the chance that at least 4 of 7 will break is \\(\\sum_{i=4}^7 {7 \\choose i} * 0.1^i * 0.9^{7-i} =\n0.002728\\) Thus, a stage 0 rollup has a fixed 0.2728% chance of\nfailing.\n\n- A stage 1 rollup can fail if either the proof system fails and the\nsecurity council gets >= 3 failures so it can't override (probability\n\\(\\sum_{i=3}^8 {8 \\choose i} * 0.1^i *\n0.9^{8-i} = 0.03809179\\) multiplied by the proof system failure\nrate), or if the security council gets 6+ failures and can force an\nincorrect answer by itself (fixed \\(\\sum_{i=6}^8 {8 \\choose i} * 0.1^i * 0.9^{8-i} =\n0.00002341\\) probability)\n\n- The chance that a stage 2 rollup will break is just equal to the\nprobability that the proof system fails\n\nHere it is in graph form:\n\nAs conjectured, as proof system quality increases, the optimal stage\nshifts from stage 0 to stage 1, then stage 1 to stage 2. Doing stage 2\nwith a stage-0-quality proof system is worst of all.\n\nNow, note that the assumptions in the above simplified model\nare very imperfect:\n\n- In reality, security council members are not\nindependent, and have \"common\nmode failures\": they could collude, or all get coerced or hacked the\nsame way, etc. The requirement to have a quorum-blocking subset outside\nthe primary organization is meant to mitigate this, but it is still far\nfrom perfect.\n\n- The proof system could itself be a combination of multiple\nindependent systems (this is what I advocate in https://ethereum-magicians.org/t/a-simple-l2-security-and-finalization-roadmap/23309...\n). In this case, (i) the probability of a proof system breaking could\nend up very low, and (ii) even in stage 2, security councils matter, as\na matter of tiebreaking.\n\nThese two arguments both imply stage 1 and stage 2 are both even more\nattractive than the chart shows. If you take the math seriously,\nstage 0 is pretty much never justified: you should launch at least\nstraight into stage 1. The main argument that I hear against\nis: if a critical bug happens, it may be too hard to get 6 of 8 security\ncouncil members to sign fast enough to fix it. But there is an easy way\naround this: give any single security council member the permission to\ndelay withdrawals by 1 or 2 weeks, giving everyone else enough time to\nact.\n\nAt the same time, however, it is a mistake to jump to stage 2\ntoo quickly, especially if work to move to stage 2 happens at\nthe expense of work to harden the underlying proof system.\nIdeally, data providers like l2beat should show proof\nsystem audits and maturity metrics (ideally of the proof system\nimplementation, not the rollup as a whole, so we can reuse) along with\nthe stage.",
    "contentLength": 5523,
    "summary": "Ethereum rollups should mathematically move from stage 0 (council controls) to stage 1 (6-of-8 override) to stage 2 (trustless) as proof system reliability improves.",
    "detailedSummary": {
      "theme": "Vitalik provides a mathematical framework for determining when Ethereum rollups should transition between different security stages based on proof system reliability and security council failure probabilities.",
      "summary": "Vitalik presents a quantitative analysis of the three stages of Ethereum rollup security, ranging from Stage 0 (security council has full control) to Stage 2 (security council can only act on provable bugs). Using binomial distributions and assuming a 10% independent failure rate for security council members, Vitalik demonstrates that the optimal stage depends on proof system quality - as proof systems become more reliable, rollups should progress from Stage 0 to Stage 1 (requiring 6-of-8 security council approval to override) and eventually to Stage 2. His mathematical model shows that Stage 0 rollups have a fixed 0.2728% failure rate, while Stage 1 and Stage 2 become increasingly attractive as proof system reliability improves. Vitalik argues that most rollups should launch directly into Stage 1 rather than Stage 0, since the math rarely justifies giving security councils complete control. However, he cautions against rushing to Stage 2 without adequately hardening the underlying proof systems, suggesting that data providers should display proof system maturity metrics alongside stage classifications.",
      "takeaways": [
        "The optimal rollup security stage depends mathematically on the trade-off between proof system reliability and security council failure rates",
        "Stage 0 is rarely justified mathematically - most rollups should launch directly into Stage 1 with 6-of-8 security council requirements",
        "Rushing to Stage 2 without sufficiently mature proof systems can be worse than staying in Stage 1",
        "Security council members face both common mode failures (collusion, coercion) and independent failures that the mathematical model attempts to capture",
        "Data providers should display proof system maturity metrics alongside stage classifications to help users make informed decisions"
      ],
      "controversial": [
        "The assumption that security council members have only a 10% independent failure rate may be overly optimistic given real-world attack vectors",
        "The argument that Stage 0 is 'pretty much never justified' challenges the current practice of many rollup projects that launch with full security council control"
      ]
    }
  },
  {
    "id": "general-2025-05-03-simplel1",
    "title": "Simplifying the L1",
    "date": "2025-05-03",
    "category": "protocol",
    "url": "https://vitalik.eth.limo/general/2025/05/03/simplel1.html",
    "path": "general/2025/05/03/simplel1.html",
    "content": "- \n\n- \n\n  \n    \n\n  \n  \n  Dark Mode Toggle\n  \n    \n\n  \n\n##  Simplifying the L1 \n\n 2025 May 03 \nSee all posts\n\n \n \n\n Simplifying the L1 \n\nSpecial thanks to Fede, Danno Ferrin, Justin Drake, Ladislaus and\nTim Beiko for feedback and review\n\nEthereum aims to be the world ledger: the platform that stores\ncivilization's assets and records, the base layer for finance,\ngovernance, high-value data authentication, and more. This requires two\nthings: scalability and resilience.\nThe Fusaka hard fork aims to increase the amount of data space available\nto L2 data by 10x, and the current\nproposed 2026 roadmap includes a similarly large increase for the\nL1. Meanwhile, the merge upgraded Ethereum to proof of stake, Ethereum's\nclient diversity has\nimproved rapidly, work on ZK\nverifiability, work quantum resistance is progressing, and\napplications are getting more and\nmore robust.\n\nThe goal of this post will be to shine the light on an aspect of\nresilience (and ultimately scalability) that is just as important, and\neasy to undervalue: the protocol's simplicity.\n\nOne of the best things about Bitcoin is how beautifully\nsimple the protocol is:\n\nThere is a chain, which is made up of a series of blocks. Each block\nis connected to the previous block by a hash. Each block's validity is\nverified by proof of work, which means... checking that the first few\nbytes of its hash are zeroes. Each block contains transactions.\nTransactions spend coins that were either created through the mining\nprocess, or outputted by previous transactions. And that's pretty much\nit. Even a smart high school student is capable of fully wrapping their\nhead around and understanding the Bitcoin protocol. A programmer is\ncapable of writing a client as a hobby project.\n\nKeeping the protocol simple brings a number of benefits that are key\nto Bitcoin or Ethereum being a credibly neutral\nand globally trusted base layer:\n\n- It makes the protocol simpler to reason about, increasing\nthe number of people who understand and can participate in\nprotocol research, development and governance. It reduces the risk that\nthe protocol gets dominated by a technocratic class that has a high\nbarrier to entry.\n\n- It greatly decreases the cost of creating new\ninfrastructure that interfaces with the protocol (eg. new\nclients, new provers, new logging and other developer tools).\n\n- It reduces long-term protocol maintenance\ncosts\n\n- It reduces the risk of catastrophic bugs, both in\nthe specification itself and in the implementation. It also makes it\neasier to verify that there are no such bugs.\n\n- It reduces the social attack surface: there's fewer\nmoving parts, and so fewer places to guard against special\ninterests.\n\nHistorically, Ethereum has often not done this (sometimes because of\nmy own decisions), and this has contributed to much of our excessive\ndevelopment expenditure, all kinds of security\nrisk,\nand insularity of R&D culture, often in pursuit of benefits that\nhave proven illusory. This post will describe how Ethereum 5\nyears from now can become close to as simple as Bitcoin.\nSimplifying the consensus\nlayer\n\nSimulation of 3-slot finality in 3sf-mini\n\nThe new consensus layer effort (historically called the \"beam chain\")\naims to use all of our learnings in consensus theory, ZK-SNARK\ndevelopment, staking economics and other fields over the last ten years\nto create a long-term optimal consensus layer for Ethereum. This\nconsensus layer is well-positioned to be much simpler than the status\nquo beacon chain. Particularly:\n\n- The 3-slot finality redesign removes the concept of\nseparate slots and epochs, committee shuffling, and many other parts of\nthe protocol specification that are related to efficiently handling\nthese mechanisms (as well as other details, eg. sync committees). A\nbasic implementation of 3-slot finality can be made in about\n200 lines of code. Unlike Gasper, 3-slot finality also has\nnear-optimal security properties.\n\n- The reduced number of active validators at a time\nmeans that it becomes safer to use simpler implementations of the fork\nchoice rule.\n\n- STARK-based aggregation protocols mean that anyone\ncan be an aggregator, and we do not have to worry about trusting\naggregators, over-paying for repeated bitfields, etc. The complexity of\nthe aggregation cryptography itself is significant, but it is at least\nhighly encapsulated\ncomplexity, which has much lower systemic risk toward the\nprotocol.\n\n- The above two factors also likely enable a simpler and more\nrobust p2p architecture.\n\n- We have an opportunity to rethink how validator entry, exit,\nwithdrawal, key transition, inactivity leak and other related mechanisms\nwork, and simplify them - both in the sense of reducing\nline-of-code (LoC) count, and in the sense of creating more legible\nguarantees of eg. what the weak subjectivity period is.\n\nThe nice thing about the consensus layer is that it is relatively\ndisconnected from EVM execution, which means that there is a relatively\nwide latitude to continue to make these types of improvements. The\nharder challenge is how to do the same on the execution layer.\nSimplifying the execution\nlayer\n\nThe EVM is increasingly growing in complexity, and much of that\ncomplexity has proven unnecessary (in many cases my own fault): a\n256-bit virtual machine that over-optimized for highly specific forms of\ncryptography that are today becoming less and less relevant, and\nprecompiles that over-optimized for single use cases that are barely\nbeing used.\n\nAttempting to address these present-day realities piecemeal will not\nwork. It took a huge amount of effort to (only partially!) remove the SELFDESTRUCT opcode,\nfor a relatively small gain. The recent EOF debate shows the challenges\nof doing the same thing to the VM.\n\nAs an alternative I recently proposed a more radical approach:\ninstead of making medium-sized (but still disruptive) changes to the EVM\nfor the sake of a 1.5x gain, perform a transition to a new and much\nbetter and simpler VM for the sake of a 100x gain. Like the Merge, we\nhave fewer points of disruptive change, but we make each one much more\nmeaningful. Specifically, I suggested we replace\nthe EVM with either RISC-V, or\nanother VM that is the VM that Ethereum ZK-provers will be written\nin. This gives us:\n\n- A radical improvement in efficiency, because\n(within provers) smart contract execution will run directly, without the\nneed for interpreter overhead. Data from Succinct shows a potential\n100x+ performance improvement in many cases.\n\n- A radical improvement in simplicity: the RISC-V\nspec is\nabsurdly simple compared to the EVM. Alternatives (eg. Cairo) are\nsimilarly simple.\n\n- All the benefits that motivated EOF (eg. code\nsections, more static analysis friendliness, larger code size\nlimits)\n\n- More options for developers: Solidity and Vyper can\nadd backends to compile to new VMs. At the same time, if we choose\nRISC-V, then developers who write in more mainstream languages will be\nable to port their code over to the VM.\n\n- Removal of the need for most precompiles, perhaps\nwith the exception of highly-optimized elliptic curve operations (though\nthose too will go away once quantum computers hit)\n\nThe main downside of this approach is that unlike EOF, which is ready\ntoday, with a new VM it will take a relatively longer amount of time for\nthese benefits to reach developers. We can mitigate this by also adding\nsome limited but high-value EVM improvements (eg. contract code size\nlimit increase, DUP/SWAP17-32) that could be implemented in the\nshort term.\n\nThis gives us a much simpler VM. The main challenge is: what do\nwe do with the existing EVM?\nA\nbackwards compatibility strategy for VM transition\n\nThe biggest challenge with meaningfully simplifying (or even\nimproving without complexifying) any part of the EVM is how to\nbalance accomplishing the desired goals with preserving backwards\ncompatibility for existing applications.\n\nThe first thing that is important to understand is: there\nisn't one single way to delineate what is the \"Ethereum\ncodebase\" (even within a single client).\n\nThe goal is to minimize the green area: the logic\nthat nodes have to run in order to participate in Ethereum\nconsensus: computing the current state, proving, verifying, FOCIL,\n\"vanilla\" block building.\n\nThe orange area cannot be decreased: if an execution\nlayer feature (whether a VM, a precompile, or another mechanism) is\neither removed from the protocol spec, or its functionality is altered,\nclients that care about processing historical blocks will have to keep\nit - but, importantly, new clients (or ZK-EVMs, or formal provers) can\nsimply ignore the orange area entirely.\n\nThe new category is the yellow area: code that is\nvery valuable for understanding and interpreting the chain\ntoday, or for optimal block building, but is not part of\nconsensus. One example that exists today is Etherscan (and\nsome block\nbuilders') support for ERC-4337 user operations. If we replace some\nlarge Ethereum feature (eg. EOAs, including their support for all kinds\nof old transaction types) with an onchain RISC-V implementation, then\nconsensus code would be considerably simplified, but specialized nodes\nwould likely continue using their exact same code to interpret them.\n\nImportantly, the orange and yellow areas are encapsulated\ncomplexity, anyone looking to understand the\nprotocol can skip them, implementations of Ethereum are free to skip\nthem, and any bugs in those areas do not pose consensus risks.\nThis means that code complexity in the orange and yellow areas has far\nfewer downsides than code complexity in the green area.\n\nThe idea of moving code from the green area to the yellow area is\nsimilar in spirit to how Apple ensures long-term backwards compatibility\nthrough\ntranslation layers like Rosetta.\n\nI propose, inspired by recent\nwritings from the Ipsilon team, the following process for a VM\nchange (using EVM to RISC-V as an example, but it could also be used for\neg. EVM to Cairo, or even RISC-V to something even better):\n\n- We require any new precompiles to be written with a\ncanonical onchain RISC-V implementation. This gets the\necosystem warmed up and started working with RISC-V as a VM.\n\n- We introduce RISC-V as an option for developers to\nwrite contracts alongside the EVM. The protocol natively\nsupports both RISC-V and EVM, and contracts written in one or\nthe other can freely interact with each other.\n\n- We replace all precompiles, except elliptic curve\noperations and KECCAK (as these require truly optimal speed),\nwith RISC-V implementations. That is, we do a hardfork\nthat removes the precompile and simultaneously changes the code of that\naddress (DAO-fork-style) from being empty to being a RISC-V\nimplementation. The RISC-V VM is so simple, that this is a net\nsimplification even if we stop here.\n\n- We implement an EVM interpreter in RISC-V (this is\nhappening anyway, because of ZK-provers) and push it onchain as a smart\ncontract. Several years after the initial release, existing EVM\ncontracts switch to being processed by being run through that\ninterpreter.\n\nOnce step 4 is done, many \"implementations of the EVM\" will remain\nand be used for optimized block building, developer tooling and chain\nanalysis purposes, but they will no longer need to be part of the\ncritical consensus spec. Ethereum consensus would\n\"natively\" understand only RISC-V.\nSimplifying by\nsharing protocol components\n\nThe third and most easily underrated way to reduce total protocol\ncomplexity is to share one standard across different parts of the stack\nas much as possible. There is typically very little or no benefit to\nusing different protocols to do the same thing in different places, but\nsuch patterns appear anyway, largely because different parts of protocol\nroadmapping don't talk to each other. Here are a few specific examples\nof places where we can simplify Ethereum by ensuring that components are\nmaximally shared across the stack.\nOne single shared erasure\ncode\n\nWe need an erasure code in three places:\n\n- Data availability sampling - clients verifying that\na block has been published\n\n- Faster P2P broadcasting - nodes being able to\naccept a block after receiving n/2 of n pieces, creating an optimal\nbalance between latency reduction and redundancy\n\n- Distributed history storage - each piece of\nEthereum's history being stored in many chunks, such that (i) the chunks\ncan be independently verified, and (ii) n/2 chunks in each group can\nrecover the remaining n/2 chunks, greatly reducing the risk that any\nsingle chunk gets lost\n\nIf we use the same erasure code (whether Reed-Solomon, random linear\ncodes, or otherwise) across the three use cases, we get some important\nadvantages:\n\n- Minimize total lines of code\n\n- Increase efficiency because if individual nodes\nhave to download individual pieces of a block (but not the whole block)\nfor one of the use cases, that data can be used for the other use\ncase\n\n- Ensure verifiability: the chunks in all three\ncontexts can be verified against the root\n\nIf different erasure codes are used, they should at least be\ncompatible erasure codes: for example, a Reed-Solomon code\nhorizontally and a random linear code vertically for DAS chunks, where\nthe two codes operate over the same field.\nOne single shared\nserialization format\n\nEthereum's serialization format is today arguably only\nsemi-enshrined, because the data can be re-serialized and broadcasted in\nany format. The only exception is signature hashes for transactions, as\nthere a canonical format is required for hashing. In the future,\nhowever, the degree of enshrinement of serialization formats will\nincrease further, for two reasons:\n\n- With full account abstraction (EIP-7701), the full\ntransaction contents will be visible to the VM\n\n- As gas limits go higher, the execution block data will need\nto be put into blobs\n\nWhen this happens, we have an opportunity to harmonize serialization\nacross the three layers of Ethereum that currently need it: (i)\nexecution layer, (ii) consensus layer, (iii) smart contract calling\nABI.\n\nI propose we use SSZ.\nSSZ is:\n\n- Easy to decode, including inside smart contracts\n(because of its 4-byte-based design and smaller number of edge\ncases)\n\n- Already widely in use in the consensus layer\n\n- Highly similar to the existing ABI, making tooling\nrelatively easy to adapt\n\nThere are efforts to migrate more fully to\nSSZ already; we should keep these efforts in mind when planning future\nupgrades, and build on them.\n\n## One single shared tree\n\nOnce we migrate from EVM to RISC-V (or an alternative minimal VM),\nthe hexary Merkle Patricia tree will become by far the largest\nbottleneck to proving block execution, even in the average case.\nMigrating to a binary\ntree based on a much more optimal hash function will greatly improve\nprover efficiency, in addition to reducing data costs for light clients\nand other use cases.\n\nWhen we do this, we should also use the same tree structure for the\nconsensus layer. This ensures that all of Ethereum, consensus and\nexecution alike, can be accessed and interpreted using the same\ncode.\n\n## From here to there\n\nSimplicity is in many ways similar to decentralization. Both are\nupstream of a goal of resilience. Explicitly valuing simplicity requires\nsome cultural change. The benefits are often illegible, and the cost of\nextra effort and turning away some shiny features is felt immediately.\nHowever, as time goes on, the benefits become more and more evident -\nand Bitcoin itself is an excellent example.\n\nI propose that we follow\nthe lead of tinygrad, and have an explicit max line\nof code target for the long-term Ethereum specification, with\nthe goal of making Ethereum consensus-critical code close to as simple\nas Bitcoin. Code that has to do with processing Ethereum's historical\nrules will continue to exist, but it should stay outside of\nconsensus-critical code paths. Alongside this, we should have a general\nethos of choosing the simpler option where possible, favoring\nencapsulated complexity over systemic complexity, and making design\nchoices that provide clearly legible properties and guarantees.",
    "contentLength": 16074,
    "summary": "Ethereum proposes replacing the EVM with simpler alternatives like RISC-V to achieve 100x performance gains and drastically reduce protocol complexity.",
    "detailedSummary": {
      "theme": "Vitalik proposes a comprehensive roadmap to dramatically simplify Ethereum's protocol complexity while maintaining backwards compatibility, drawing inspiration from Bitcoin's elegant simplicity.",
      "summary": "Vitalik argues that protocol simplicity is crucial for Ethereum's resilience and credible neutrality as a global base layer, noting that complexity increases security risks, development costs, and barriers to participation. He outlines a three-pronged approach to simplification: modernizing the consensus layer through 3-slot finality and STARK-based aggregation, transitioning from the EVM to a simpler virtual machine like RISC-V for dramatic efficiency gains, and standardizing shared components like erasure codes, serialization formats, and tree structures across the protocol stack. Vitalik proposes a backwards compatibility strategy that moves legacy code from consensus-critical paths to optional interpretation layers, similar to Apple's Rosetta translation system, allowing new implementations to ignore historical complexity while maintaining full functionality for existing applications.",
      "takeaways": [
        "Protocol simplicity is essential for resilience, reducing security risks, development costs, and barriers to participation in Ethereum's governance and development",
        "The proposed transition from EVM to RISC-V could provide 100x performance improvements while dramatically simplifying the virtual machine specification",
        "A strategic backwards compatibility approach can move legacy complexity out of consensus-critical code paths without breaking existing applications",
        "Standardizing shared components like erasure codes, SSZ serialization, and binary trees across the entire protocol stack reduces total complexity",
        "Ethereum should adopt an explicit maximum line-of-code target for consensus-critical specifications, following the philosophy of projects like tinygrad"
      ],
      "controversial": [
        "The proposal to replace the EVM with RISC-V represents a massive architectural change that could face significant resistance from the existing developer ecosystem",
        "The backwards compatibility strategy relies on eventually running EVM code through a RISC-V interpreter, which could introduce new attack vectors or performance bottlenecks"
      ]
    }
  },
  {
    "id": "general-2025-04-14-privacy",
    "title": "Why I support privacy",
    "date": "2025-04-14",
    "category": "general",
    "url": "https://vitalik.eth.limo/general/2025/04/14/privacy.html",
    "path": "general/2025/04/14/privacy.html",
    "content": "- \n\n- \n\n  \n    \n\n  \n  \n  Dark Mode Toggle\n  \n    \n\n  \n\n##  Why I support privacy \n\n 2025 Apr 14 \nSee all posts\n\n \n \n\n Why I support privacy \n\nSpecial thanks to Balvi volunteers, Paul Dylan-Ennis,\npcaversaccio, vectorized, Bruce Xu and Luozhu Zhang for discussion and\nfeedback.\n\nRecently, I have been increasingly focusing on improving\nthe state of privacy in the Ethereum ecosystem. Privacy is\nan important guarantor of decentralization: whoever has the\ninformation has the power, ergo we need to avoid centralized control\nover information. When people in the real world express concern about\ncentrally operated technical infrastructure, the concern is sometimes\nabout operators changing the rules unexpectedly or deplatforming users,\nbut just as often, it's about data collection. While the cryptocurrency\nspace has its origins in projects like Chaumian Ecash, which put\nthe preservation of digital financial privacy front and center, it has\nmore recently undervalued privacy for what is ultimately a bad\nreason: before ZK-SNARKs, we had no way to offer privacy in a\ndecentralized way, and so we downplayed it, instead focusing exclusively\non other guarantees that we could provide at the time.\n\nToday, however, privacy can no longer be ignored. AI\nis greatly increasing capabilities for centralized data collection and\nanalysis while greatly expanding the scope of data that we share\nvoluntarily. In the future, newer technologies like brain-computer\ninterfaces bring further challenges: we may be literally talking about\nAI reading our minds. At the same time, we have more powerful\ntools to preserve privacy, especially in the digital realm, than the\n1990s cypherpunks could have imagined: highly efficient zero knowledge\nproofs (ZK-SNARKs) can protect our identities while revealing enough\ninformation to prove that we are trustworthy, fully homomorphic\nencryption (FHE) can let us compute over data without seeing the data,\nand obfuscation may soon offer even more.\n\nPrivacy is not about standing apart. It's about standing\ntogether.\n\nAt this time, it's worth stepping back and reviewing the question:\nwhy do we want privacy in the first place? Each\nperson's answer will be different. In this post I will give my own,\nwhich I will break down into three parts:\n\n- Privacy is freedom: privacy gives us space to live\nour lives in the ways that meet our needs, without constantly worrying\nabout how our actions will be perceived in all kinds of political and\nsocial games\n\n- Privacy is order: a whole bunch of mechanisms that\nunderlie the basic functioning of society depend on privacy in order to\nfunction\n\n- Privacy is progress: if we gain new ways to share\nour information selectively while protecting it from being misused, we\ncan unlock a lot of value and accelerate technological and social\nprogress\n\n## Privacy is freedom\n\nBack in the early 2000s, it was popular to have viewpoints similar to\nthose epitomized by David Brin's 1998 book The\nTransparent Society: technology would make information all\naround the world much more transparent, and while this will have\ndownsides and require adaptation, on average it is a very good thing,\nand we can make it fair by making sure that the people can surveil (or\nrather, sousveil)\nthe government as well. In 1999, Sun Microsystems CEO Scott McNealy famously\nexclaimed: \"privacy is dead, get over it!\". This mentality was\ncommon in the early conception and development of Facebook, which banned\npseudonymous identities. I personally remember experiencing the tail\nend of this mentality at a presentation at a Huawei event in Shenzhen in\n2015, where a (Western) speaker casually mentioned in an offhand remark\nthat \"privacy is over\".\n\nThe Transparent Society represented the\nbest and brightest of what \"privacy is over\" ideology had to offer: it\npromised a better, more just and fair world, using the power of\ntransparency to keep governments accountable rather than repressing\nindividuals and minorities. In hindsight, however, it is clear that even\nthis approach was a product of its time, written at the peak of\nenthusiasm about global cooperation and peace and \"the end of history\",\nand it depended on a number of overly-optimistic assumptions\nabout human nature. Primarily:\n\n- The top-level layers of global politics would be generally\nwell-intentioned and sane, making vertical privacy\n(ie. not revealing information to powerful people and institutions) more\nand more unneeded. Abuses of power would generally be\nlocalized, and so the best way to fight those abuses is to\nbring them out into the sunlight.\n\n- Culture would keep improving to the point where\nhorizontal privacy (ie. not revealing information to\nother members of the public) would become unneeded. Nerds, gays, and\nultimately everyone else could stop hiding in the closet, because\nsociety would stop being harsh and judgemental toward people's unique\ntraits and instead become open-minded and accepting.\n\nToday, there is no single major country for which the first\nassumption is broadly agreed to be true, and quite a few for which it's\nbroadly agreed to be false. On the second front, cultural tolerance has\nalso been rapidly regressing - a mere twitter search for phrases like\n\"bullying\nis good\" is one piece of evidence of this, though it's easy to find\nmore.\n\nI personally have the misfortune to encounter the downsides of\n\"transparent society\" regularly, as every single action I take outside\nhas some nonzero chance of unexpectedly becoming a public media\nstory:\n\nThe worst offender was someone who took a minute-long video while I\nwas laptopping in Chiang Mai, and proceeded to post it on xiaohongshu,\nwhere it immediately got many thousands of likes and reshares. Of\ncourse, my own situation is far from the human norm - but this has\nalways been the case with privacy: privacy is less needed for\npeople whose life situations are relatively normal, and more needed for\npeople whose life situations deviate from the norm, in any\ndirection. And once you add up all of the different directions\nthat matter, the number of people who really need privacy ends\nup being quite a lot - and you never know when you will become\none of them. This is a big reason why privacy is often underrated: it's\nnot just about your situation and your information today, it's also\nabout the unknown unknowns of what happens to that information (and to\nhow it affects you) going forward forever into the future.\n\nPrivacy from corporate pricing mechanisms is a niche concern today,\neven among privacy advocates, but with the rise of AI-based analysis\ntools it is likely to become a growing issue: the more a company knows\nabout you, the more they are able to offer you a personalized price that\nmaximizes how much they can extract from you multiplied by the\nprobability that you will pay up.\n\nI can express my general argument for privacy as freedom in one\nsentence as follows:\n\nPrivacy gives you the freedom to live your life in a way that\nbest suits your personal goals and needs, without having to constantly\nbalance every action between \"the private game\" (your own needs) and\n\"the public game\" (how all kinds of other people, intermediated by all\nkinds of mechanisms including social media cascades, commercial\nincentives, politics, institutions, etc, will perceive and respond to\nyour behavior)\n\nWithout privacy, everything becomes a constant battle of\n\"what will other people (and bots) think of what I'm doing\" - powerful\npeople, companies, and peers, people today and in the future. With\nprivacy, we can preserve a balance. Today, that balance is being rapidly\neroded, especially in the physical realm, and the default path of modern\ntechno-capitalism, with its hunger for business models that find ways to\ncapture value from users without asking them to explicitly pay for\nthings, is to erode it further (even into highly sensitive domains like,\neventually, our own minds). Hence, we need to counteract this effect,\nand support privacy more explicitly, particularly in the place where we\nmost practically can: the digital realm.\nBut why not allow\ngovernment backdoors?\n\nThere is one common reply to the above reasoning: the disadvantages\nof privacy that I described are largely disadvantages of the\npublic knowing too much about our private lives, and even where\nabuse of power is concerned, it's about corporations, bosses and\npoliticians knowing too much. But we're not going to let the\npublic, corporations, bosses and politicians have all this data.\nInstead, we'll let a small group of highly trained and well-vetted law\nenforcement professionals see data taken from the security cameras on\nthe streets and the wiretaps on the internet cables and chat\napplications, enforce strict accountability procedures, and no one else\nwill find out.\n\nThis is a quietly, but widely, held position, and so it is important\nto address it explicitly. There are several reasons why, even if\nimplemented at a high standard of quality with good intentions, such\nstrategies are inherently unstable:\n\n- In practice, it's not just the government, it's also all\nkinds of corporate entities, of varying levels of quality. In\ntraditional financial systems, KYC and payment info gets held by payment\nprocessors, banks, and all kinds of other intermediaries. Email\nproviders see huge amounts of data of all sorts. Telecom companies know\nyour location, and regularly illegally\nresell it. In general, policing all of these entities at a\nsufficient level of rigor that ensures that they truly have a high level\nof care for user data is so effort intensive on both the watcher and the\nwatched that it is likely incompatible with maintaining a competitive\nfree market.\n\n- Individuals who have access will always feel the pull to\nabuse it (including by selling to third parties). In 2019,\nseveral Twitter employees were charged\nand later\nconvicted for selling personal information of dissidents to Saudi\nArabia.\n\n- The data can always get hacked. In 2024, data that\nUS telecommunication companies were legally required to collect was\nhacked, allegedly\nby state-affiliated hackers from China. In 2025, large amounts of\nsensitive personal data held by the Ukrainian government was\nhacked by Russia. And in the other direction, highly sensitive\ngovernment and corporate databases in China also\nget hacked, including by\nthe US government.\n\n- The regime can change. A government that is\ntrustworthy today may not be trustworthy tomorrow. The people in charge\ntoday may be persecuted tomorrow. A police agency that maintains\nimpeccably high standards of respect and decorum one day may find itself\nreduced to all kinds of gleeful cruelty a decade later.\n\nFrom the perspective of an individual, if data is taken from them,\nthey have no way to tell if and how it will be abused in the future.\nBy far the safest approach to handling large-scale data is to\ncentrally collect as little of it as possible in the first\nplace. Data should be maximally held by the users themselves, and\ncryptographic means used to enable aggregation of useful statistics\nwithout compromising individual privacy.\n\nThe argument that the government should have the capability to access\nanything with a warrant because that's the way that things have always\nworked misses a key point: historically, the amount of\ninformation available to be obtained with warrants was far lower than\nwhat is available today, and even what would be available if the\nstrongest proposed forms of internet privacy were universally\nadopted. In the 19\u1d57\u02b0 century, the average conversation happened\nonce, via voice, and was never recorded by anyone. For this reason: the\nentire moral\npanic around \"going dark\" is ahistorical: the average conversation,\nand even financial transaction, being fully and unconditionally\nprivate is the multi-thousand-year historical norm.\n\n An average conversation, 1950. Exactly zero words of the\nconversation were ever recorded, spied on, subject to \"lawful\nintercept\", AI analyzed, or otherwise viewed by anyone at any time other\nthan the participants in the conversation while it is happening.\n\nAnother important reason to minimize centralized data collection is\nthe inherently international nature of large portions of global\ncommunication and economic interaction. If everyone is in the\nsame country, it is at least a coherent position to say that \"the\ngovernment\" should have access to the data in their interactions. But\nwhat if people are in different countries? Certainly, in principle you\ncould try to come up with a galaxy-brained scheme where each\nperson's data is mapped to some lawful access entity that is responsible\nfor them - though even there you would have to deal with a huge number\nof edge cases involving data that relates to multiple people. But even\nif you could, it is not the realistic default outcome. The\nrealistic default outcome of government backdoors is: data becomes\nconcentrated in a small number of central jurisdictions that have\neveryone's data because they control the applications - essentially,\nglobal techno-hegemony. Strong privacy is by far the most stable\nalternative.\n\n## Privacy is order\n\nFor over a century, it has been recognized that a key technical\ncomponent making democracy work is the secret\nballot: no one knows who you voted for, and furthermore,\nyou do not have the ability to prove to anyone else who you voted for,\neven if you really want to. If secret ballots were not a default, then\nvoters would be accosted with all kinds of side incentives affecting how\nthey vote: bribes, promises of retroactive rewards, social pressure,\nthreats, and more.\n\nIt can be seen that such side incentives would completely break\ndemocracy with a simple mathematical argument: in an election with N\npeople, your probability of affecting the outcome is only roughly 1/N,\nand so any considerations related to which candidate is better and which\nis worse inherently get divided by N. Meanwhile, \"side games\"\n(eg. voter bribery, coercion, social pressure) act on you\ndirectly based on how you vote (rather than based on the outcome of the\nvote as a whole), and so they do not get divided by N. Hence, unless\nside games are tightly controlled, they by default overwhelm the entire\ngame, and drown out any consideration of which candidate's policies are\nactually better.\n\nThis applies not just to nation-scale democracy. Theoretically, it\napplies to almost any corporate or governmental principal-agent\nproblem:\n\n- A judge deciding how to rule in a case\n\n- A government official deciding which company to give a contract or\ngrant to\n\n- An immigration official deciding to grant or reject a visa\n\n- An employee in a social media company deciding on how to enforce\ncontent moderation policy\n\n- An employee in a company contributing to a business decision (eg.\nwhich supplier to buy from)\n\nThe fundamental problem in all cases is the same: if the agent acts\nhonestly, they absorb only a small share of the benefit of their action\nto the entity that they are representing, meanwhile if they follow the\nincentives of some side game, then they absorb the full share of those\nbenefits. Hence, even today, we are leaning on a lot of moral\ngoodwill to make sure all our institutions don't get completely taken\nover by a chaotic maelstrom of side games overturning side games.\nIf privacy decreases further, then these side games become even\nstronger, and the moral goodwill required to keep society functioning\nmay become unrealistically high.\n\nCould social systems be redesigned to not have this problem?\nUnfortunately, game theory pretty much explicitly says that this is\nimpossible (with one exception: total dictatorship). In the\nversion of game theory that focuses on individual choice - that is, the\nversion that assumes that each participant makes decisions independently\nand that does not allow for the possibility of groups of agents working\nas one for their mutual benefit, mechanism designers have a very wide\nlatitude to \"engineer\"\ngames to achieve all kinds of specific outcomes. In fact, there are\nmathematical\nproofs that for any game at least one stable Nash\nequilibrium must exist, and so analyzing such games becomes tractable.\nBut in the version of game theory that allows for the possibility of\ncoalitions working together (ie. \"colluding\"), called cooperative\ngame theory, we\ncan prove that there are large classes of games that do not have any\nstable outcome (called a \"core\"). In\nsuch games, whatever the current state of affairs is, there is always\nsome coalition that can profitably deviate from it.\n\nRound\n\nA\n\nB\n\nC\n\n1\n\n1/3\n\n1/3\n\n1/3\n\n2\n\n1/2\n\n1/2\n\n0\n\n3\n\n2/3\n\n0\n\n1/3\n\n4\n\n0\n\n1/3\n\n2/3\n\nIf we take the math seriously, we come to\nthe conclusion that the only way to create stable social\nstructures is to have some limits on the amount of coordination between\nparticipants that can happen - and this implies strong privacy\n(including deniability). If you do not take the math seriously\non its own, then it suffices to observe the real world, or at least\nthink through what some of the principal-agent situations described\nabove might become if they got taken over by side games, to come to the\nsame conclusion.\n\nNote that this introduces another argument for why government\nbackdoors are risky. If everyone has unlimited ability to\ncoordinate with everyone on everything, the outcome is chaos. But if\nonly a few people can do so, because they have privileged\naccess to information, then the outcome is that they dominate. One\npolitical party having backdoor access to the communications of the\nother can easily spell the end of the viability of having multiple\npolitical parties.\n\nOne other important example of a social order that depends on limits\nto collusion in order to function is intellectual and cultural\nactivity. Participation in intellectual and cultural activity\nis inherently an intrinsically-motivated public-spirited task: it's very\ndifficult to make extrinsic incentives that target positive\ncontributions to society, precisely because intellectual and\ncultural activity is, in part, the activity of determining which actions\nin society are positive actions in the first place. We can make\napproximate commercial and social incentives that point in the right\ndirection, but they too require heavy supplementation by intrinsic\nmotivation. But this also means that this kind of activity is highly\nfragile to misaligned extrinsic motivations, particularly side\ngames such as social pressure and coercion. To limit the impact of such\nmisaligned extrinsic motivations, privacy is once again required.\n\n## Privacy is progress\n\nImagine a world where public-key and symmetric-key encryption did not\nexist at all. In this world, securely sending messages across long\ndistances would inherently be vastly more difficult - not impossible,\nbut difficult. This would lead to far less international cooperation\ntaking place, and as a result much more would continue to happen via\nin-person offline channels. This would have made the world a much\npoorer, and more unequal, place.\n\nI will argue that today we are in exactly that place today,\nrelatively to a hypothetical world of tomorrow where much\nstronger forms of cryptography were widely available - in\nparticular, programmable cryptography, augmented by\nstronger forms of full-stack security and formal verification to give us\nstrong assurances that this cryptography is being correctly used.\n\n The Egyptian\ngod protocols: three powerful and highly general-purpose\nconstructions that can let us do computation on data while at the same\ntime keeping the data completely private. \n\nAn excellent source of examples is healthcare. If\nyou talk to anyone who has been working in longevity, pandemic\nresistance, or other fields in health in the past decade, they will\nuniversally tell you that the future of treatment and prevention is\npersonalized, and effective response is highly dependent on high-quality\ndata, both data about individuals and data about the environment.\nEffectively protecting people against airborne disease requires knowing\nwhere the air quality is higher and lower, and in what regions pathogens\nare emerging at any given time. The most advanced longevity clinics all\ngive customized recommendations and treatments based on data about your\nbody, food preferences and lifestyle.\n\nHowever, each of these things are simultaneously massive privacy\nrisks. I am personally aware of an incident where an air monitor was\ngiven to an employee that \"phoned home\" to a company, and the collected\ndata was sufficient to determine when that employee was having sex. For\nreasons like this, I expect that by default many of the most\nvaluable forms of data will not be collected at all, precisely because\npeople are afraid of the privacy consequences. And even when\ndata does get collected, it will almost always not be widely\nshared or made available to researchers - in part for business reasons,\nbut just as often because of the privacy concerns involved.\n\nThe same pattern is repeated in other spheres. There is a huge amount\nof information about ourselves in our actions in documents we write,\nmessages that we send across various applications, and various actions\non social media, that could all be used to more effectively predict and\ndeliver the things that we need in our daily lives. There is a huge\namount of information about how we interact with our physical\nenvironments that is not healthcare-related. Today, we lack the tools to\neffectively use this information without creating dystopian privacy\nnightmares. Tomorrow, we may have those tools.\n\nThe best way to solve these challenges is to use strong cryptography,\nwhich can let us get the benefits of sharing data without the downsides.\nThe need to gain access to data, including personal data, will only\nbecome more important in the age of AI, as there is value from being\nable to locally train and run \"digital twins\" that can make decisions on\nour behalf based on high-fidelity approximations of our preferences.\nEventually, this will also involve using brain-computer interface (BCI)\ntechnology, reading high-bandwidth inputs from our minds. For this to\nnot lead to a highly centralized global hegemony, we need ways for this\nto be done with respect for strong privacy. Programmable cryptography is\nthe most trustworthy solution.\n\n My AirValent air quality\nmonitor. Imagine a device like this that collects air quality data,\nmakes aggregate statistics publicly available on an open-data map, and\nrewards you for providing the data - all while using programmable\ncryptography to avoid revealing your personal location data and verify\nthat the data is genuine. \n\nPrivacy\ncan be progress for keeping\nour society safe\n\nProgrammable cryptography techniques like zero-knowledge proofs are\npowerful, because they are like Lego bricks for information\nflow. They can allow fine-grained control of who can see what\ninformation and, often more importantly, what information can be seen at\nall. For example, I can prove that I have a Canadian passport\nthat shows I am over 18, without revealing anything else about\nmyself.\n\nThis makes possible all kinds of fascinating combinations. I can give\na few examples:\n\n- Zero-knowledge proof of personhood: proving that\nyou are a unique person (via various forms of ID: passport, biometrics,\ndecentralized social-graph-based)\nwithout revealing anything else about which person you are.\nThis could be used for \"proof of not being a bot\", all kinds of \"maximum\nN per person use cases\", and more, while preserving full privacy beyond\nrevealing the fact that the rules are not being broken.\n\n- Privacy\nPools, a financial privacy solution that can exclude bad\nactors without requiring backdoors. When spending, users can prove that\ntheir coins came from a source that is not part of a publicly\nknown list of hacks and thefts; the hackers and thieves themselves are\nthe only ones who would not be able to generate such a proof, and so\nthey would not be able to hide. Railgun\nand privacypools.com are currently\nlive and using such a scheme.\n\n- On-device anti-fraud scanning: this does not depend\non ZKPs, but it feels like it belongs in this category. You can use\non-device filters (including LLMs) to check incoming messages, and\nautomatically identify potential misinformation and scams. If done\non-device, it does not compromise the user's privacy, and it can be done\nin a user-empowering way, giving each user a choice of which filters to\nsubscribe to.\n\n- Proof of provenance for physical items: using a\ncombination of blockchains and zero-knowledge proofs, it can be possible\nto track various properties of an item through its chain of\nmanufacturing. This could allow eg. pricing environmental externalities\nwithout publicly revealing the supply chain.\n\n Left: depiction\nof privacy pools. Right: the Taiwanese Message Checker app, which\ngives the user the choice to turn on or off multiple filters, here from\ntop to bottom: URL checking, cryptocurrency address checking, rumor\nchecking \n\n## Privacy and AI\n\nRecently, ChatGPT\nannounced that it will start feeding your past conversations into\nthe AI as context for your future conversations. That the trend will\nkeep going in this direction is inevitable: an AI looking over your past\nconversations and gleaning insights from them is fundamentally\nuseful. In the near future, we will probably see people making AI\nproducts that make even deeper intrusions into privacy: passively\ncollecting your internet browsing patterns, email and chat history,\nbiometric data, and more.\n\nIn theory, your data stays private to you. In practice, this does not\nalways seem to be the case:\n\n \"Wow!\nChatGPT has a bug, and it pushes questions asked by others\nto me! This is a big privacy leak. I asked a question, got an error, and\nthen \u2018Retry' generated a question that I would never ask.\"\n\nIt's always possible that the privacy protection worked fine, and in\nthis case the AI hallucinated by generating a question that\nBruce never asked and answering that. But there is no way to verify.\nSimilarly, there is no way to verify whether or not our conversations\nare being used for training.\n\nThis is all deeply worrying. Even more disturbing are explicit AI\nsurveillance use cases, where (physical and digital) data about people\nis being collected and analyzed on a large scale without their consent.\nFacial recognition is already helping authoritarian regimes crack\ndown on political dissent on a mass scale. And the most worrying of\nall is the inevitable final frontier of AI data collection and analysis:\nthe human mind.\n\nIn principle, brain-computer interface technology has incredible\npower to boost human potential. Take the story of Noland\nArbaugh, Neuralink's first patient as of last year:\n\nThe experimental device has given Arbaugh, now 30, a sense of\nindependence. Before, using a mouth-stick required someone to position\nhim upright. If he dropped his mouth-stick, it needed to be picked up\nfor him. And he couldn't use it for long or he'd develop sores. With the\nNeuralink device, he has nearly full control of a computer. He can\nbrowse the web and play computer games whenever he wants, and Neuralink\nsays he has set\nthe human record for cursor control with a BCI.\n\nToday, these devices are powerful enough to empower the injured and\nsick. Tomorrow, they will be powerful enough to give fully healthy\npeople an opportunity to work with computers, and communicate\ntelepathically with each other (!!), at a level of efficiency that to us\nseems unimaginable. But actually interpreting the brain signals\nto make this kind of communication possible requires AI.\n\nThere is a dark future that could arise naturally as a confluence of\nthese trends, and we get silicon super-agents that are slurping up and\nanalyzing information about everyone, including how they write, act and\nthink. But there is also a brighter future, where we get the\nbenefits of these technologies while preserving our\nprivacy.\n\nThis can be done with a combination of a few techniques:\n\n- Running computation locally whenever possible -\nmany tasks (eg. basic image analysis, translation, transcription, basic\nbrain wave analysis for BCI) are simple enough that they can be done\nfully on locally-running computation. Indeed, locally-running\ncomputation even provides advantages in reducing latency and\nimproving verifiability. If something can be done locally, it should be\ndone locally. This includes computations where various intermediate\nsteps involve accessing the internet, logging into social media\naccounts, etc.\n\n- Using cryptography to make remote computation fully\nprivate - fully\nhomomorphic encryption (FHE) could be used to perform AI computation\nremotely without allowing the remote server to see the data or the\nresults. Historically, FHE has been very expensive, but (i) recently it\nhas been rapidly improving in efficiency, and (ii) LLMs are a uniquely\nstructured form of computation, and asymptotically almost all\nof it is linear operations, making it potentially very amenable\nto ultra-efficient FHE implementations. Computation that involves the\nprivate data of multiple parties can be done with multi-party\ncomputation; the common case of two parties can be handled extremely\nefficiently with techniques like garbled\ncircuits.\n\n- Using hardware verification to extend guarantees to the\nphysical world - we could insist that hardware that can read\nour minds (whether from inside the skull or outside) must be open and\nverifiable, and use techniques\nlike IRIS to verify it. We can also do this in other domains: for\nexample, we could have security cameras that provably only save and\nforward the video stream if a local LLM flags it as physical violence or\na medical emergency, and delete it in all other cases, and have\ncommunity-driven random inspection with IRIS to verify that the cameras\nare implemented correctly.\n\n## Future imperfect\n\nIn 2008, the libertarian philosopher David Friedman wrote a book called\nFuture Imperfect, in which he gave a series of sketches about the\nchanges to society that new technologies might bring, not all of them in\nhis favor (or our favor). In one\nsection, he describes a potential future where we see a complicated\ninterplay between privacy and surveillance, where growth in digital\nprivacy counterbalances growth in surveillance in the physical\nworld:\n\nIt does no good to use strong encryption for my email if a video\nmosquito is sitting on the wall watching me type. So strong privacy in a\ntransparent society requires some way of guarding the interface between\nmy realspace body and cyberspace ... A low-tech solution is to type under\na hood. A high-tech solution is some link between mind and machine that\ndoes not go through the fingers \u2013 or anything else visible to an outside\nobserver.24\n\nThe conflict between realspace transparency and cyberspace privacy\ngoes in the other direction as well ... My pocket computer encrypts my\nmessage with your public key and transmits it to your pocket computer,\nwhich decrypts the message and displays it through your VR glasses. To\nmake sure nothing is reading the glasses over your shoulder, the goggles\nget the image to you not by displaying it on a screen but by using a\ntiny laser to write it on your retina. With any luck, the inside of your\neyeball is still private space.\n\nWe could end up in a world where physical actions are entirely\npublic, information transactions entirely private. It has some\nattractive features. Private citizens will still be able to take\nadvantage of strong privacy to locate a hit man, but hiring him may cost\nmore than they are willing to pay, since in a sufficiently transparent\nworld all murders are detected. Each hit man executes one commission\nthen goes directly to jail.\n\nWhat about the interaction between these technologies and data\nprocessing? On the one hand, it is modern data processing that makes the\ntransparent society such a threat \u2013 without that, it would not much\nmatter if you videotaped everything that happened in the world, since\nnobody could ever find the particular six inches of videotape he wanted\nin the millions of miles produced each day. On the other hand, the\ntechnologies that support strong privacy provide the possibility of\nreestablishing privacy, even in a world with modern data processing, by\nkeeping information about your transactions from ever getting to anyone\nbut you.\n\nSuch a world may well be the best of all possible worlds: if all goes\nwell, we would see a future where there is very little physical\nviolence, but at the same time preserve our freedoms online, and ensure\nthe basic functioning of political, civic, cultural and intellectual\nprocesses in society that depend on some limits to total information\ntransparency for their ongoing operation.\n\nEven if it is not ideal, it is much better than the version where\nphysical and digital privacy go to zero, eventually including\nprivacy of our own minds, and in the mid-2050s we get thinkpieces\narguing that of course it's unrealistic to expect to think\nthoughts that are not subject to lawful intercept, and responses to\nthose thinkpieces consisting of links to the most recent incident where\nan AI company's LLM got an exploit which led to a year of 30 million\npeople's private inner monologues getting leaked to the whole\ninternet.\n\nSociety has always depended on a balance between privacy and\ntransparency. In some cases I support limits to privacy too. To give an\nexample totally disconnected from the usual arguments that people give\nin this regard, I am supportive of the US government's moves to ban\nnon-compete clauses in contracts, primarily not because of their\ndirect impacts on workers, but because they are a way of forcing the\ntacit domain knowledge of companies to be partially open-source. Forcing\ncompanies to be more open than they would like is a limitation of\nprivacy - but I would argue a net-beneficial one. But from a macro\nperspective, the most pressing risk of near-future technology is that\nprivacy will approach all-time lows, and in a highly imbalanced way\nwhere the most powerful individuals and the most powerful nations get\nlots of data on everyone, and everyone else will see close to nothing.\nFor this reason, supporting privacy for everyone, and making the\nnecessary tools open source, universal, reliable and safe is one of the\nimportant challenges of our time.",
    "contentLength": 34289,
    "summary": "Vitalik argues privacy is essential for freedom, social order, and progress, especially as AI increases surveillance capabilities.",
    "detailedSummary": {
      "theme": "Vitalik argues that privacy is essential for preserving freedom, social order, and technological progress in an era of increasing AI surveillance and data collection.",
      "summary": "Vitalik presents a comprehensive case for privacy across three dimensions: freedom, order, and progress. He argues that privacy is freedom because it allows people to live authentically without constantly worrying about public perception and judgment, citing his own experiences with unwanted surveillance. Privacy is order because democratic institutions, judicial decisions, and other social mechanisms depend on secret processes (like secret ballots) to function properly without being corrupted by bribery, coercion, or side incentives. Privacy is progress because cryptographic tools like zero-knowledge proofs and fully homomorphic encryption can unlock tremendous value in healthcare, AI, and other domains by enabling data sharing and analysis while protecting individual privacy. Vitalik strongly opposes government backdoors, arguing they are inherently unstable due to hacking risks, regime changes, and abuse by individuals with access. He envisions a future where physical actions may be transparent but digital communications remain private, enabled by advanced cryptography and local computation, especially as brain-computer interfaces emerge.",
      "takeaways": [
        "Privacy enables authentic living by separating 'private games' (personal needs) from 'public games' (social perception and judgment)",
        "Democratic institutions and social order fundamentally depend on privacy mechanisms like secret ballots to prevent corruption through bribery and coercion",
        "Government backdoors are inherently unstable due to hacking risks, regime changes, and potential abuse by individuals with privileged access",
        "Advanced cryptographic tools like zero-knowledge proofs and fully homomorphic encryption can unlock progress in healthcare, AI, and other domains while preserving privacy",
        "The future may require a balance where physical actions are transparent but digital communications remain private, especially as brain-computer interfaces emerge"
      ],
      "controversial": [
        "Strong opposition to any government backdoors or lawful intercept capabilities, even with proper oversight and warrants",
        "Suggestion that some transparency measures (like banning non-compete clauses) are beneficial limitations of privacy for companies",
        "Vision of a future with extensive physical surveillance but complete digital privacy as potentially optimal",
        "Argument that historical norms of private conversations should be preserved despite modern security concerns"
      ]
    }
  },
  {
    "id": "general-2025-03-29-pubos",
    "title": "We should talk less about public goods funding and more about open source funding",
    "date": "2025-03-29",
    "category": "governance",
    "url": "https://vitalik.eth.limo/general/2025/03/29/pubos.html",
    "path": "general/2025/03/29/pubos.html",
    "content": "- \n\n- \n\n  \n    \n\n  \n  \n  Dark Mode Toggle\n  \n    \n\n  \n\n##  We should talk less about public goods funding and more about open source funding \n\n 2025 Mar 29 \nSee all posts\n\n \n \n\n We should talk less about public goods funding and more about open source funding \n\nOne topic that has been dear to me for a long time is the question of\nhow to fund public goods. If there is a project that provides\nvalue to a million people (and there's no fine-grained way to choose who\ngets the benefit and who doesn't), but each person only gets a small\namount of benefit, then it's quite possible that no single person will\nfind it in their interest to fund the project, even if the project is\nextremely valuable overall. The language of \"public goods\" has a\ncentury-long heritage in economics.\nIn digital ecosystems, especially decentralized digital\necosystems, public goods are extremely important: in fact,\nthere's a strong case that the average good that someone might want\nto produce is a public good. Open source software, academic\nresearch into cryptographic and blockchain protocols, openly available\neducation resources, and many more things are all public goods.\n\nHowever, the term \"public good\" has major challenges.\nParticularly:\n\n- The term \"public good\" often is used in public discourse to mean \"a\ngood that is produced by a government\", even if it is not a public good\nin an economic sense. This leads to confusion, as it creates a\nperception that whether or not a project is a public good is not a\nfunction of what the project is and what its properties are, but rather\na function of who is building it and what their self-described\nintentions are.\n\n- There is a general perception that public goods funding lacks rigor\nand is run on social\ndesirability bias - what sounds good, rather than what is good - and\nfavors insiders who can play the social game.\n\nTo me, these two problems are related: a big part of the\nreason why the term \"public good\" is vulnerable to social gaming is\nprecisely the fact that the definition of \"public good\" is stretched so\neasily.\n\nLet's see what happens when you search\nfor the phrase \"building a public good\" on Twitter. I did this right\nnow, and here are some of the first results:\n\nYou can keep scrolling and find many projects using the phrase \"we're\nbuilding a public good\" to describe themselves.\n\nThe point of this is not to criticize the individual projects; I know\nlittle about both of the above and they may well be excellent projects.\nHowever, both of these examples are commercial projects that have their\nown tokens. There is nothing wrong with being a commercial project, and\nthere is often nothing wrong with launching your own token. However,\nit says something about the term \"public good\" when it so easily\ngets diluted to the point where, today, it often seems to just mean\n\"project\".\n\n## Open source\n\nAs an alternative to \"public goods\", let's think about the phrase\n\"open source\". If you think about some central examples of things that\nare clearly digital public goods, you will find that they are all open\nsource:\n\n- Academic blockchain and cryptographic protocol research\n\n- Documentation, tutorials...\n\n- Open source software (eg. Ethereum clients, software\nlibraries...)\n\nAnd on the flip side, open source projects seem to be by default\npublic goods. You can certainly come up with counterexamples: if I write\na piece of software that is heavily tailored toward my personal\nworkflow, and I put it up on github, the majority of the value created\nby the project may still accrue to me personally. However, the act\nof open-sourcing (as opposed to keeping it private) is certainly a\npublic good with very diffuse benefit.\n\nA really nice thing about the term \"open source\" is that has\na clear and well-agreed definition. The FSF's Free\nSoftware Definition and the OSI's Open Source Definition have stood\nfor decades, and there are natural ways to extend these definitions to\nfields of endeavor other than software (eg. writing, research). In the\ncrypto space, the inherently stateful and multi-party nature of\napplications, and the new vectors of centralized fragility and control\nthat these things imply, do mean that we need to extend the definition\nsomewhat: open standards, the insider attack\ntest and the walkaway test introduced\nin this post can be a valuable addition to the FSF + OSI\ndefinitions.\n\nSo what is the difference between \"open source\" and \"public goods\"?\nWell, we can start off by asking the bot for examples:\n\nI personally simply disagree with the claim that the examples in the\nfirst category are not public goods. A project having a high barrier to\nentry to contribute does not preclude it from being a public\ngood, and neither does corporations benefiting from the project. Also, a\nproject can absolutely be a public good while things around it are\nprivate goods.\n\nThe second category is more interesting. First of all, we should note\nthat all five examples are in physical space, rather than\ndigital space. Hence, if we want to focus on digital\npublic goods, the above examples give no reason to oppose just focusing\non \"open source\". But what if we do want to also cover\nphysical goods? Even the crypto space has its share of enthusiasm for\nbetter governing physical things and not just digital things; in a\nsense, that's the whole point of network\nstates.\nOpen source and\nphysical local public goods\n\nHere, we can make an observation: while providing these things at\nlocal scale is an \"infrastructure building\" problem, and can be\ndone open source or closed source, the most efficient way to provide\nthese things at global scale generally ends up involving...\nactual open source. Clean air is the most obvious example: there has\nbeen lots\nof research\nand development,\nmuch of it open source, to help people worldwide enjoy cleaner air. Open\nsource can help make any kind of public infrastructure easier to deploy\nworldwide. The problem of how to provide physical infrastructure at a\nlocal scale effectively is still important - but the problem applies\nequally to democratically run communities and to corporations.\n\nNational defense is an interesting case. Here, I would argue the\nfollowing: if you build a project for a national defense reason that you\nwould not feel comfortable open-sourcing, then chances are that\nwhile it may be a public good at the local scale, it's likely not a\npublic good at the global scale. Innovation in weapons is the most\nobvious example. Sometimes, one side in a war has a much stronger moral\ncase than the other, and it's justified to help it with offensive\noperations, but on average, building technology to improve military\ncapabilities does not improve the world. The exceptions\n(national-defense projects that one would want to open source)\nwould likely be \"defense\" capabilities that are actually about\ndefense; one example might be decentralized agriculture,\nelectricity and internet infrastructure that can help people stay fed,\nfunctional and connected in challenging environments.\n\nHence, here too it feels like shifting focus from \"public goods\" to\n\"open source\" is actually the best thing to do. Open source should not\nmean \"it's equally virtuous to build whatever as long as it's open\nsource\"; it should be about building and open-sourcing things that are\nmaximally valuable to humanity. But distinguishing which projects are\nworth supporting and which projects are not is already well-understood\nto be the primary task of public goods funding mechanisms.",
    "contentLength": 7478,
    "summary": "Vitalik argues we should fund \"open source\" projects instead of \"public goods\" because the term has clearer definitions and avoids misuse by commercial projects claiming to build public goods.",
    "detailedSummary": {
      "theme": "Vitalik argues that the cryptocurrency and digital ecosystem communities should shift their focus from 'public goods funding' to 'open source funding' due to clearer definitions and reduced susceptibility to social gaming.",
      "summary": "Vitalik contends that while public goods funding has been important for supporting projects that benefit many people with diffuse benefits, the term 'public goods' has become problematic and diluted. He points out two main issues: the term is often confused with government-produced goods, and it's vulnerable to social gaming where projects claim to be public goods without meeting the economic definition. Vitalik demonstrates this by showing how many commercial projects with their own tokens now describe themselves as 'building a public good.' He proposes that 'open source' is a superior framework because it has clear, well-established definitions through the FSF and OSI, and most genuine digital public goods are inherently open source. Even for physical public goods like clean air or infrastructure, Vitalik argues that the most effective global-scale solutions typically involve open source research and development, making the open source framework more comprehensive and rigorous than the current public goods discourse.",
      "takeaways": [
        "The term 'public goods' has become diluted and is frequently misused by commercial projects to appear more socially desirable",
        "Open source has clear, established definitions that are harder to game and more rigorously defined than 'public goods'",
        "Most genuine digital public goods (academic research, documentation, software libraries) are inherently open source",
        "Even physical public goods benefit most from open source approaches when scaled globally",
        "The crypto space should extend open source definitions to include open standards and tests for decentralization and user control"
      ],
      "controversial": [
        "Dismissing projects with tokens as not being true public goods may be overly restrictive, as some tokenized projects could still provide genuine public benefit",
        "The suggestion that national defense projects that can't be open-sourced are likely not global public goods could be debated, as some classified defense innovations may have broader humanitarian applications"
      ]
    }
  },
  {
    "id": "general-2025-03-29-treering",
    "title": "The tree ring model of culture and politics",
    "date": "2025-03-29",
    "category": "society",
    "url": "https://vitalik.eth.limo/general/2025/03/29/treering.html",
    "path": "general/2025/03/29/treering.html",
    "content": "- \n\n- \n\n  \n    \n\n  \n  \n  Dark Mode Toggle\n  \n    \n\n  \n\n##  The tree ring model of culture and politics \n\n 2025 Mar 29 \nSee all posts\n\n \n \n\n The tree ring model of culture and politics \n\nWhen I was growing up, one of the things that often puzzled me was\nthe often-repeated claim that we live in a \"deeply neoliberal society\"\nthat highly valued \"deregulation\". I was confused because while I could\nsee a fair share of people arguing for neoliberalism and deregulation,\nit seemed clear that on the whole, the actual state of government\nregulation was very very different from anything that could remotely be\nconstrued as reflecting such values. The total number of federal\nregulations has kept\ncontinuously going up. KYC, copyright, airport security and all\nkinds of other rules were continuously tightening. US federal tax\nreceipts as a percentage of GDP have been roughly\nconstant since WW2.\n\nIf you told someone in 2020 that in five years, either the USA or\nChina would be leading in open-source AI and the other would be leading\nin closed-source AI, and asked which would be leading where, they would\nhave probably stared at you as though you were asking a trick question.\nThe USA is the country that values openness, China is the country that\nvalues closure and control, USA tech in general leans much more toward\nopen source than Chinese tech, come on, it's obvious! And yet, they\nwould have been completely wrong.\n\nWhat's going on here? In this post, I will propose a simple\nexplanation, which I call the tree ring model of politics and\nculture:\n\nThe model is as follows:\n\n- How a culture treats new things is a product of the\nattitudes and incentives prevalent in that culture at that particular\ntime.\n\n- How a culture treats old things is primarily driven\nby status quo bias.\n\nEach period of time adds a new ring to the tree, and while that new\nring is forming there are new attitudes around new things being formed.\nSoon, however, the lines are frozen in place and become much more\ndifficult to change, and a new ring starts growing overtop, shaping\nattitudes about the next wave of topics.\n\nWe can analyze the above situations, and others, through this\nlens:\n\n- There really was a deregulatory trend in the USA, but it was\nstrongest in the 1990s (if you look carefully you can actually see this\nin the charts!). By the 2000s, the tone was already shifting toward more\nregulation and control. However, if you look at specific things that\n\"came of age\" in the 1990s (eg. the internet), they ended up being\nregulated based on principles that had the upper hand in the 1990s,\ngiving the USA (and, due to imitation, much of the world) decades of\nrelative internet freedom.\n\n- Taxes are constrained by budget needs, which are largely set by the\nneeds of healthcare and welfare programs. The \"red lines\" in this regard\nwere already set 50 years ago.\n\n- All kinds of moderately dangerous activities involving modern\ntechnology are viewed more suspiciously, both by law and by culture,\nthan eg. risky forms of mountain climbing, which can have very high\nmortality rates. This is explainable by the fact that risky forms of\nmountain climbing is something that people have done for centuries, and\nattitudes solidified when general risk tolerance was much higher.\n\n- Social media came of age in the 2010s, and has been treated by\nculture and by politics in part as a part of the internet, but also in\npart as a distinct thing. Hence, restrictive attitudes toward social\nmedia generally do not also carry over to the early internet - despite\ngrowing internet authoritarianism generally, we have not seen\nparticularly stronger attempts to crack down on unauthorized file\nsharing, for example.\n\n- AI came of age in the 2020s, and at this point in time the USA is\nthe leading power and China is the following power, hence it is in\nChina's interest to play a \"commoditize the complement\"\nstrategy on AI. This intersects with a favorable mood among many\ndevelopers toward open-source in general. The result is a favorable\nenvironment for open-source AI that is very genuine, but is also fairly\nspecific to AI; older spheres of technology remain closed and\nwalled-garden-like.\n\nMore generally, the implication here is that it is difficult to\nchange how a culture treats things that already exist and where\nattitudes have already solidified. What is easier is to invent new\npatterns of behavior that outcompete the old, and work to maximize the\nchance that we get good norms around those. This could be done in\nmultiple ways: developing new technologies is one, using (physical or\ndigital) communities on the internet to experiment with new social norms\nis another. This is also to me one of the attractions of the crypto\nspace: it presents an independent technological and cultural ground to\ndo new things without being overly burdened by existing status quo bias.\nRather than growing the same old trees, we can also bring life to the\nforest by planting and growing new trees.",
    "contentLength": 4965,
    "summary": "The \"tree ring model\" explains how cultures regulate new things based on current attitudes when they emerge, but old things remain governed by outdated frozen norms.",
    "detailedSummary": {
      "theme": "Vitalik proposes a 'tree ring model' explaining how cultural and political attitudes toward new versus old things are shaped by the zeitgeist of when those things first emerged, with new attitudes forming around new phenomena while old attitudes remain frozen by status quo bias.",
      "summary": "Vitalik argues that apparent contradictions in cultural attitudes - like living in a supposedly 'neoliberal' society while regulations continuously increase, or China leading in open-source AI while the US leads in closed-source - can be explained through his tree ring model. This model suggests that how cultures treat new things reflects current attitudes and incentives, while how they treat old things is primarily driven by status quo bias. Each time period creates a new 'ring' with distinct attitudes that become frozen once established. Vitalik illustrates this with examples like internet regulation being shaped by 1990s deregulatory attitudes, social media facing 2010s skepticism, and AI development reflecting 2020s geopolitical dynamics where China benefits from open-source strategies. He concludes that rather than trying to change entrenched attitudes about existing phenomena, it's more effective to create new technologies and social experiments that can establish better norms from the ground up, citing crypto as an example of building on fresh technological and cultural ground.",
      "takeaways": [
        "Cultural attitudes toward phenomena are largely determined by the prevailing zeitgeist when those phenomena first emerge, then become frozen by status quo bias",
        "The internet enjoys relative freedom because it 'came of age' during the deregulatory 1990s, while social media faces more restrictions having emerged during the more control-oriented 2010s",
        "China's leadership in open-source AI reflects a strategic 'commoditize the complement' approach that benefits a following power, intersecting with 2020s pro-open-source sentiment",
        "Changing established cultural attitudes about existing things is extremely difficult due to status quo bias",
        "Creating new technologies and social structures offers better opportunities to establish positive norms than trying to reform attitudes about existing phenomena"
      ],
      "controversial": [
        "The characterization of China's open-source AI strategy as primarily driven by geopolitical 'commoditize the complement' tactics rather than genuine technological philosophy",
        "The implication that trying to reform existing systems is largely futile compared to building new alternatives"
      ]
    }
  },
  {
    "id": "general-2025-02-28-aihumans",
    "title": "AI as the engine, humans as the steering wheel",
    "date": "2025-02-28",
    "category": "general",
    "url": "https://vitalik.eth.limo/general/2025/02/28/aihumans.html",
    "path": "general/2025/02/28/aihumans.html",
    "content": "- \n\n- \n\n  \n    \n\n  \n  \n  Dark Mode Toggle\n  \n    \n\n  \n\n##  AI as the engine, humans as the steering wheel \n\n 2025 Feb 28 \nSee all posts\n\n \n \n\n AI as the engine, humans as the steering wheel \n\nSpecial thanks to Devansh Mehta, Davide Crapis and Julian\nZawistowski for feedback and review, Tina Zhen, Shaw Walters and others\nfor discussion.\n\nIf you ask people what they like about democratic structures, whether\ngovernments, workplaces, or blockchain-based DAOs, you will often hear\nthe same arguments: they avoid concentration of power, they give their\nusers strong guarantees because there isn't a single person who can\ncompletely change the system's direction on a whim, and they can make\nhigher-quality decisions by gathering the perspectives and wisdom of\nmany people.\n\nIf you ask people what they dislike about democratic\nstructures, they will often give the same complaints: average voters are\nnot sophisticated, because each voter only has a small chance of\naffecting the outcome, few voters put high-quality thought into their\ndecisions, and you often get either low participation (making the system\neasy to attack) or de-facto centralization because everyone just\ndefaults to trusting and copying the views of some influencer.\n\nThe goal of this post will be to explore a paradigm that could\nperhaps use AI to get us the benefits of democratic structures without\nthe downsides. \"AI as the engine, humans as the steering\nwheel\". Humans provide only a small amount of information into\nthe system, perhaps only a few hundred bits, but each of those bits is a\nwell-considered and very high-quality bit. AI treats this data as an\n\"objective function\", and tirelessly makes a very large number of\ndecisions doing a best-effort at fitting these objectives. In\nparticular, this post will explore an interesting question: can we do\nthis without enshrining a single AI at the center, instead\nrelying on a competitive open market that any AI (or human-AI hybrid) is\nfree to participate in?\n\n## Table of contents\n\n- Why not just put a single AI in charge?\n\n- Futarchy\n\n- Distilled human judgement\n\n- Deep funding\n\n- Adding privacy\n\n- Benefits of engine + steering wheel designs\n\nWhy not just put a\nsingle AI in charge?\n\nThe easiest way to insert human preferences into an AI-based\nmechanism is to make a single AI model, and have humans feed their\npreferences into it somehow. There are easy ways to do this: you can\njust put a text file containing a list of people's instructions into the\nsystem prompt. Then you use one of many \"agentic AI frameworks\" to give\nthe AI the ability to access the internet, hand it the keys to your\norganization's assets and social media profiles, and you're done.\n\nAfter a few iterations, this may end up good enough for many use\ncases, and I fully expect that in the near future we are going to see\nmany structures involving AIs reading instructions given by a group (or\neven real-time reading a group chat) and taking actions as a result.\n\nWhere this structure is not ideal is as a governing\nmechanism for long-lasting institutions. One valuable property for\nlong-lasting institutions to have is credible\nneutrality. In my post introducing this concept, I listed four\nproperties that are valuable for credible neutrality:\n\n- Don't write specific people or specific outcomes into the\nmechanism\n\n- Open source and publicly verifiable execution\n\n- Keep it simple\n\n- Don't change it too often\n\nAn LLM (or AI agent) satisfies 0/4. The model inevitably has a\nhuge amount of specific people and outcome preferences encoded\nthrough its training process. Sometimes this leads to the AI having\npreferences in surprising directions, eg. see this recent\nresearch suggesting that major LLMs value lives in Pakistan far more\nhighly than lives in the USA (!!). It can be open-weights, but\nthat's far\nfrom open-source; we really don't know what\ndevils are hiding in the depths of a model. It's the opposite of\nsimple: the Kolmogorov complexity of an LLM is in the tens of billions\nof bits, about the same as that of all\nUS law (federal + state + local) put together. And because of how\nrapidly AI is evolving, you'll have to change it every three months.\n\nFor this reason, an alternative approach that I favor exploring for\nmany use cases is to make a simple mechanism be the rules of the\ngame, and let AIs be the players. This is the same insight that\nmakes markets so effective: the rules are a relatively dumb system of\nproperty rights, with edge cases decided by a court system that slowly\naccumulates and adjusts precedents, and all of the intelligence comes\nfrom entrepreneurs operating \"at the edge\".\n\nThe individual \"game players\" can be LLMs, swarms of LLMs interacting\nwith each other and calling into various internet services, various AI +\nhuman combinations, and many other constructions; as a mechanism\ndesigner, you do not need to know. The ideal goal is to have a mechanism\nthat functions as an automaton - if the goal of the mechanism is\nchoosing what to fund, then it should feel as much as possible like\nBitcoin or Ethereum block rewards.\n\nThe benefits of this approach are:\n\n- It avoids enshrining any single model into the\nmechanism; instead, you get an open market of many different\nparticipants and architectures, all with their own different biases.\nOpen models, closed models, agent swarms, human + AI hybrids, cyborgs,\ninfinite\nmonkeys, etc, are all fair game; the mechanism does not\ndiscriminate.\n\n- The mechanism is open source. While the\nplayers are not, the game is - and this is a pattern\nthat is already reasonably well-understood (eg. political parties and\nmarkets both work this way)\n\n- The mechanism is simple, and so there are\nrelatively few routes for a mechanism designer to encode their own\nbiases into the design\n\n- The mechanism does not change, even if the\narchitecture of the underlying players will need to be redesigned every\nthree months from here until the singularity.\n\nThe goal of the steering mechanism is to provide a faithful\nrepresentation of the participants' underlying goals. It only needs to\nprovide a small amount of information, but it should be high-quality\ninformation.\n\nYou can think of the mechanism as exploiting an asymmetry\nbetween coming up with an answer and verifying the answer. This is\nsimilar to how a sudoku is difficult to solve, but it's easy to verify\nthat a solution is correct. You (i) create an open market of\nplayers to act as \"solvers\", and then (ii) maintain a human-run\nmechanism that performs the much simpler task of verifying solutions\nthat have been presented.\n\n## Futarchy\n\nFutarchy was originally introduced by Robin Hanson as \"vote values, but bet\nbeliefs\". A voting mechanism chooses a set of goals (which can be\nanything, with the caveat that they need to be measurable) which get\ncombined into a metric M. When you need to make a decision (for\nsimplicity, let's say it's YES/NO), you set up conditional\nmarkets: you ask people to bet on (i) whether YES or NO will be\nchosen, (ii) value of M if YES is chosen, otherwise zero, (iii) value of\nM if NO is chosen, otherwise zero. Given these three variables, you can\nfigure out if the market thinks YES or NO is more bullish for the value\nof M.\n\n\"Price of the company share\" (or, for a cryptocurrency, a token) is\nthe most commonly cited metric, because it's so easy to understand and\nmeasure, but the mechanism can support many kinds of metrics: monthly\nactive users, median self-reported happiness of some group of\nconstituents, some quantifiable measure of decentralization, etc.\n\nFutarchy was originally invented in the pre-AI era. However,\nfutarchy fits very naturally in the \"sophisticated solver, easy\nverifier\" paradigm described in the previous section, and\ntraders in a futarchy can be AI (or human+AI combinations) too. The role\nof the \"solvers\" (prediction market traders) is to determine how each\nproposed plan will affect the value of a metric in the future. This is\nhard. The solvers make money if they are right, and lose money if they\nare wrong. The verifiers (the people voting on the metric, adjusting the\nmetric if they notice that it is being \"gamed\" or is otherwise becoming\noutdated, and determining the actual value of the metric at some future\ntime) need only answer the simpler question \"what is the value of the\nmetric now?\"\n\n## Distilled human judgement\n\nDistilled human judgement is a class of mechanisms that works as\nfollows. There is a very large number (think: 1 million) of\nquestions that need to be answered. Natural examples\ninclude:\n\n- How much credit does each person in this list deserve for their\ncontributions to some project or task?\n\n- Which of these comments violate the rules of a social media platform\n(or sub-community)?\n\n- Which of these given Ethereum addresses represent a real and unique\nhuman being?\n\n- Which of these physical objects contributes positively or negatively\nto the aesthetics of its environment?\n\nYou have a jury that can answer such questions, though at the cost of\nspending a lot of effort on each answer. You ask the jury to\nonly a small number of the questions (eg. if the total list has\n1 million items, the jury perhaps only provides answers on 100 of them).\nYou can even ask the jury indirect questions: instead of asking \"what\npercent of total credit does Alice deserve?\", you can ask \"does Alice or\nBob deserve more credit, and how many times more?\". When designing the\njury mechanism, you can reuse time-tested mechanisms from the real world\nlike grants committees, courts (determining value of a judgement),\nappraisals, etc, though of course the jury participants are\nthemselves welcome to use new-fangled AI research tools to help\nthem come to an answer.\n\nYou then allow anyone to submit a list of numerical responses\nto the entire set of questions (eg. providing an estimate for\nhow much credit each participant in the entire list deserves).\nParticipants are encouraged to use AI to do this, though they can use\nany technique: AI, human-AI hybrid, AI with access to internet search\nand the ability to autonomously hire other human or AI workers,\ncybernetically enhanced monkeys, etc.\n\nOnce the full-list providers and the jurors have both submitted their\nanswers, the full lists are checked against the jury answers, and some\ncombination of the full lists that are most compatible with the\njury answers is taken as the final answer.\n\nThe distilled human judgement mechanism is different from futarchy,\nbut has some important similarities:\n\n- In futarchy, the \"solvers\" are\nmaking predictions, and the \"ground-truth\ndata\" that their predictions get checked against (to reward or\npenalize solvers) is the oracle that outputs the value of the\nmetric, which is run by the jury.\n\n- In distilled human judgement, the\n\"solvers\" are providing answers to a very large\nquantity of questions, and the \"ground-truth\ndata\" that their predictions get checked against is\nhigh-quality answers to a small subset of those\nquestions, provided by a jury.\n\n Toy example of distilled human judgement for credit\nassignment, see python\ncode here. The script asks you to be the jury, and contains\nsome AI-generated (and human-generated) full lists pre-included in the\ncode. The mechanism identifies the linear combination of full lists that\nbest-fits the jury answers. In this case, the winning combination is\n0.199 * Claude's answer + 0.801 * Deepseek's answer; this\ncombination matches the jury answers better than any single model does.\nThese coefficients would also be the rewards given to the\nsubmitters. \n\nThe \"humans as a steering wheel\" aspect in this \"defeating Sauron\"\nexample is reflected in two places. First, there is high-quality human\njudgement being applied on each individual question, though this is\nstill leveraging the jury as \"technocratic\" evaluators of performance.\nSecond, there is an implied voting mechanism that determines if\n\"defeating Sauron\" is even the right goal (as opposed to, say, trying to\nally with him, or offering him all the territory east of some critical\nriver as a concession for peace). There are other distilled human\njudgement use cases where the jury task is more directly values-laden:\nfor example, imagine a decentralized social media platform (or\nsub-community) where the jury's job is to label randomly selected forum\nposts as following or not following the community's rules.\n\nThere are a few open variables within the distilled human judgement\nparadigm:\n\n- How do you do the sampling? The role of the full\nlist submitters is to provide a large quantity of answers; the role of\nthe jurors is to provide high-quality answers. We need to choose jurors,\nand choose questions for jurors, in such a way that a model's ability to\nmatch jurors' answers is maximally indicative of its performance in\ngeneral. Some considerations include:\n\n- Expertise vs bias tradeoff: skilled jurors are\ntypically specialized in their domain of expertise, so you will get\nhigher quality input by letting them choose what to rate. On the other\nhand, too much choice could lead to bias (jurors favoring content from\npeople they are connected to), or weaknesses in sampling (some content\nis systematically left unrated)\n\n- Anti-Goodharting:\nthere will be content that tries to \"game\" AI mechanisms, eg.\ncontributors that generate large amounts of impressive-looking but\nuseless code. The implication is that the jury can detect this, but\nstatic AI models do not unless they try hard. One possible way to catch\nsuch behavior is to add a challenge mechanism by which individuals can\nflag such attempts, guaranteeing that the jury judges them (and thus\nmotivating AI developers to make sure to correctly catch them). The\nflagger gets a reward if the jury agrees with them or pays a penalty if\nthe jury disagrees.\n\n- What scoring function do you use? One idea that is\nbeing used in the current deep funding pilots is to ask jurors \"does A\nor B deserve more credit, and how much more?\". The scoring function is\nscore(x) = sum((log(x[B]) - log(x[A]) - log(juror_ratio)) ** 2 for (A, B, juror_ratio) in jury_answers)\n: that is, for each jury answer, it asks how far away the ratio in the\nfull list is from the ratio provided by the juror, and adds a penalty\nproportional to the square of the distance (in log space). This is to\nshow that there is a rich design space of scoring functions, and the\nchoice of scoring functions is connected to the choice of which\nquestions you ask the jurors.\n\n- How do you reward the full list submitters?\nIdeally, you want to often give multiple participants a nonzero reward,\nto avoid monopolization of the mechanism, but you also want to satisfy\nthe property that an actor cannot increase their reward by submitting\nthe same (or slightly modified) set of answers many times. One promising\napproach is to directly compute the linear combination (with\ncoefficients non-negative and summing to 1) of full lists that best fits\nthe jury answers, and use those same coefficients to split rewards.\nThere could also be other approaches.\n\nIn general, the goal is to take human judgement mechanisms that are\nknown to be effective and bias-minimizing and have stood the test of\ntime (eg. think of how the adversarial structure of a court system\nincludes both the two parties to a dispute, who have high information\nbut are biased, and a judge, who has low information but is probably\nunbiased), and use an open market of AIs as a reasonably high-fidelity\nand very low-cost predictor of these mechanisms (this is similar to how\n\"distillation\" of LLMs works).\n\n## Deep funding\n\nDeep funding is the application of distilled human judgement to the\nproblem of filling in the weights of edges on a graph representing \"what\npercent of the credit for X belongs to Y?\"\n\nIt's easiest to show this directly with an example:\n\nOutput of two-level deep funding example: the ideological origins\nof Ethereum. See python\ncode here.\n\nHere, the goal is to distribute the credit for philosophical\ncontributions that led to Ethereum. Let's look at an example:\n\n- The simulated deep funding round shown here has assigned 20.5% of\nthe credit to the Cypherpunk Movement and 9.2% to\nTechno-Progressivism.\n\n- Within each of those nodes, you ask the question: to what\nextent is it an original contribution (so it deserves credit for\nitself), and to what extent is it a recombination of other upstream\ninfluences? For the Cypherpunk Movement, it's 40% new and 60%\ndependencies.\n\n- You can then look at influences further upstream of those nodes:\nLibertarian minarchism and anarchism gets 17.3% of the credit for the\nCypherpunk Movement but Swiss direct democracy only gets 5%.\n\n- But note that Libertarian minarchism and anarchism also inspired\nBitcoin's monetary philosophy, so there are two pathways by which it\ninfluenced Ethereum's philosophy.\n\n- To compute the total share of contribution of Libertarian minarchism\nand anarchism to Ethereum, you would multiply up the edges along each\npath, and add the paths:\n0.205 * 0.6 * 0.173 + 0.195 * 0.648 * 0.201 ~= 0.0466. And\nso if you had to donate $100 to reward everyone who contributes to the\nphilosophies that motivated Ethereum, according to this simulated deep\nfunding round, Libertarian minarchists and anarchists would get\n$4.66.\n\nThis approach is designed to work in domains where work is built on\ntop of previous work and the structure of this is highly legible.\nAcademia (think: citation graphs) and open source software (think:\nlibrary dependencies and forking) are two natural examples.\n\nThe goal of a well-functioning deep funding system would be to create\nand maintain a global graph, where any funder that is interested in\nsupporting one particular project would be able to send funds to an\naddress representing that node, and funds would automatically propagate\nto its dependencies (and recursively to their dependencies etc) based on\nthe weights on the edges of the graph.\n\nYou could imagine a decentralized protocol using a built-in\ndeep funding gadget to issue its token: some in-protocol\ndecentralized governance would choose a jury, and the jury would run the\ndeep funding mechanism, as the protocol automatically issues tokens and\ndeposits them into the node corresponding to itself. By doing so, the\nprotocol rewards all of its direct and indirect contributors in a\nprogrammatic way reminiscent of how Bitcoin or Ethereum block rewards\nrewarded one specific type of contributor (miners). By influencing the\nweights of the edges, the jury gets a way to continuously define what\ntypes of contributions it values. This mechanism could function as a\ndecentralized and long-term-sustainable alternative to mining, sales or\none-time airdrops.\n\n## Adding privacy\n\nOften, making good judgements on questions like those in the examples\nabove requires having access to private information: an organization's\ninternal chat logs, information confidentially submitted by community\nmembers, etc. One benefit of \"just using a single AI\", especially for\nsmaller-scale contexts, is that it's much more acceptable to give one AI\naccess to the information than to make it public for everyone.\n\nTo make distilled human judgement or deep funding work in these\ncontexts, we could try to use cryptographic techniques to securely give\nAIs access to private information. The idea is to use multi-party\ncomputation (MPC), fully homomorphic encryption (FHE), trusted execution\nenvironments (TEEs) or similar mechanisms to make the private\ninformation available, but only to mechanisms whose only output is a\n\"full list submission\" that gets directly put into the mechanism.\n\nIf you do this, then you would have to restrict the set of mechanisms\nto just being AI models (as opposed to humans or AI + human\ncombinations, as you can't let humans see the data), and in particular\nmodels running in some specific substrate (eg. MPC, FHE, trusted\nhardware). A major research direction is figuring out near-term\npractical versions of this that are efficient enough to make sense.\n\nBenefits of engine +\nsteering wheel designs\n\nDesigns like this have a number of promising benefits. By far the\nmost important one is that they allow for the construction of\nDAOs where human voters are in control of setting the direction, but\nthey are not overwhelmed with an excessively large number of decisions\nto make. They hit the happy medium where each person doesn't\nhave to make N decisions, but they have more power than just making one\ndecision (how delegation typically works), and in a way that is more\ncapable of eliciting rich preferences that are difficult to express\ndirectly.\n\nAdditionally, mechanisms like this seem to have an incentive\nsmoothing property. What I mean here by \"incentive smoothing\"\nis a combination of two factors:\n\n- Diffusion: no single action that the voting\nmechanism takes has an overly large impact on the interests of any one\nsingle actor.\n\n- Confusion: the connection between voting decisions\nand how they affect actors' interests is more complex and difficult to\ncompute.\n\nThe terms confusion and diffusion here are taken from\ncryptography, where they are key properties of what makes ciphers\nand hash functions secure.\n\nA good example of incentive smoothing in the real world today is the\nrule of law: the top level of the government does not regularly take\nactions of the form \"give Alice's company $200M\", \"fine Bob's company\n$100M\", etc, rather it passes rules that are intended to apply evenly to\nlarge sets of actors, which then get interpreted by a separate class of\nactors. When this works, the benefit is that it greatly reduces the\nbenefits of bribery and other forms of corruption. And when it's\nviolated (as it often is in practice), those issues quickly become\ngreatly magnified.\n\nAI is clearly going to be a very large part of the future, and this\nwill inevitably include being a large part of the future of governance.\nHowever, if you are involving AI in governance, this has obvious risks:\nAI has biases, it could be intentionally corrupted during the training\nprocess, and AI technology is evolving so quickly that \"putting\nan AI in charge\" may well realistically mean \"putting whoever is\nresponsible for upgrading the AI in charge\". Distilled human\njudgement offers an alternative path forward, which lets us harness the\npower of AI in an open free-market way while keeping a human-run\ndemocracy in control.\n\nAnyone interested in more deeply exploring and participating in these\nmechanisms today is highly encouraged to check out the currently active\ndeep funding round at https://cryptopond.xyz/modelfactory/detail/2564617.",
    "contentLength": 22524,
    "summary": "AI systems should act as engines executing decisions while humans provide high-quality steering through simple mechanisms like futarchy and distilled judgement.",
    "detailedSummary": {
      "theme": "Vitalik proposes using AI as computational engines while maintaining human control through democratic steering mechanisms, avoiding the risks of putting a single AI in charge of governance.",
      "summary": "Vitalik argues that while democratic structures have benefits like avoiding power concentration and gathering diverse perspectives, they suffer from voter sophistication issues and low participation. He proposes a paradigm where humans provide high-quality steering decisions (a few hundred bits of information) while AI systems serve as the engine, processing these objectives through competitive market mechanisms. Rather than enshrining a single AI model with embedded biases and complexity, Vitalik advocates for simple, open-source mechanisms where multiple AIs compete as players in the game. He explores three specific implementations: futarchy (betting on outcomes based on measurable metrics), distilled human judgement (using small jury samples to guide AI evaluation of large datasets), and deep funding (distributing credit through dependency graphs). These approaches aim to harness AI's computational power while maintaining human democratic control and avoiding the credible neutrality problems of single AI systems.",
      "takeaways": [
        "Democratic governance can benefit from AI engines while maintaining human steering, combining the computational power of AI with human judgment and values",
        "Using competitive AI markets rather than single AI models avoids enshrining specific biases and maintains credible neutrality through simple, open-source mechanisms",
        "Futarchy, distilled human judgement, and deep funding represent practical frameworks for implementing human-AI collaborative governance",
        "The approach exploits an asymmetry between solving complex problems (AI's strength) and verifying solutions (humans' comparative advantage)",
        "Privacy-preserving techniques like multi-party computation could enable these mechanisms to work with confidential information while maintaining transparency"
      ],
      "controversial": [
        "The assumption that humans are better at 'verifying' solutions than generating them may not hold as AI capabilities advance, potentially undermining the core premise",
        "The complexity of implementing these mechanisms with cryptographic privacy protections may create new centralization risks around technical infrastructure",
        "Market-based governance mechanisms could still be manipulated by wealthy actors or sophisticated gaming strategies that exploit the AI competition dynamics"
      ]
    }
  },
  {
    "id": "general-2025-02-14-l1scaling",
    "title": "Reasons to have higher L1 gas limits even in an L2-heavy Ethereum",
    "date": "2025-02-14",
    "category": "protocol",
    "url": "https://vitalik.eth.limo/general/2025/02/14/l1scaling.html",
    "path": "general/2025/02/14/l1scaling.html",
    "content": "- \n\n- \n\n  \n    \n\n  \n  \n  Dark Mode Toggle\n  \n    \n\n  \n\n##  Reasons to have higher L1 gas limits even in an L2-heavy Ethereum \n\n 2025 Feb 14 \nSee all posts\n\n \n \n\n Reasons to have higher L1 gas limits even in an L2-heavy Ethereum \n\nSpecial thanks to Ansgar Dietrichs for feedback and\nreview\n\nOne important near-term debate in the Ethereum roadmap is the\nquestion of how much to increase the L1 gas limit. Recently, the L1 gas\nlimit was increased from 30 million to 36 million, increasing capacity\nby 20%. Many support following up with much larger increases in the near\nfuture. These increases are made safe by recent and upcoming\nimprovements in technology: efficiency improvements to Ethereum clients,\nreduced need to store old history due to EIP-4444 (see roadmap), and\nlater on stateless\nclients.\n\nHowever, before we go down this path, it's important to ask a\nquestion: in the context of the rollup-centric\nroadmap, are higher L1 gas limits the right thing to do in the long\nterm? Gas limits are easy to increase, but difficult to decrease - and\neven if you do decrease them later, the consequences to centralization\nmay well be permanent. We do not want to end up with the centralization\nrisks of heavy L1 usage without actually being sure that we will benefit\nfrom that usage.\n\nThis post will argue that, even in a world where most usage\nand applications are on L2, there is value in significantly scaling,\nbecause it enables simpler and more secure patterns of application\ndevelopment. This post will not attempt to argue for\nor against a claim that more applications in general should be on L1\neven in the long term. Rather, the goal is to argue that eg. ~10x\nscaling on L1 has long-term value regardless of the outcome of that\ndebate.\n\n## Censorship resistance\n\n The goal is to resist censorship. \n\nOne of the core value propositions of a blockchain is\ncensorship resistance: if a transaction is valid, and\nyou have the funds to pay a market-rate fee, you should be able to\nreliably get that transaction included onchain, quickly.\n\nIn some cases, censorship resistance is needed even on short\ntimescales: if you have a position in a defi protocol, and prices are\nchanging very quickly, then even a 5 minute delay in getting a\ntransaction included could be enough to get you liquidated.\n\nThe staker set of the L1 is highly\ndecentralized, making it very difficult to censor a transaction for\nmore than a few slots. There are proposals\nto improve this property of Ethereum even further, guaranteeing\ncensorship resistance even in cases where eg. block building is highly\ncentralized and outsourced. L2s, on the other hand, rely on either a\nmuch more concentrated set of block producers, or a centralized\nsequencer, which can easily choose to censor users. Some L2s (eg. see Optimism,\nArbitrum\ndocumentation) do have a force-inclusion mechanism to allow\nusers to submit transactions directly through the L1. Hence, the\npractical value of the censorship resistance guarantee is dependent on\n(i) L1 fees being sufficiently low, and (ii) L1 having enough space that\nusers can send bypass transactions even if an L2 censors a large number\nof users en masse.\nBasic mathematical\nassumptions\n\nWe can do some math to compute how expensive it is to actually use\nthe force-inclusion mechanism. First, let's state some assumptions,\nwhich we will also reuse in other sections:\n\n- An L1 \u2192 L2 deposit transaction today costs around 120,000 L1\ngas. Here\nis an example from Optimism.\n\n- An ultra-minimal L1 operation such as changing the value of a\nparticular storage slot costs 7500 L1 gas (cold SSTORE\nplus calldata cost of the address plus a little more for\ncomputation)\n\n- The ETH price is $2500\n\n- The gas price is 15 gwei, a reasonable\napproximation for the long-term\naverage\n\n- Demand elasticity is close to 1 (ie. doubling the\ngas limit would halve prices). This is\nweakly supported by earlier analyses of data, though in practice we\nshould note that the actual elasticity could end up very different in\neither direction\n\n- We want responding to attacks to cost less than $1.\n\"Normal\" operations should\nnot cost more than $0.05 per tx. Transactions whose\nlevel of exceptionalness is somewhere in between (eg. key changes)\nshould cost less than $0.25. This is admittedly just an\nintuitive value judgement\n\nGiven these assumptions, today bypassing censorship would cost\n120000 * 15 * 10**-9 * 2500 = $4.5 . To push it below our\ntarget, we would need to scale L1 by 4.5x (though note that this is a\nvery rough estimate, because elasticity is so hard to estimate, and even\nabsolute usage levels are hard to estimate).\nNeed to move assets between\nL2s\n\nOften, users will need to move assets from one L2 to another. For\ncommonly-traded high-volume assets, the most practical way to do this is\nintent protocols such as ERC-7683. Only a small number of\nmarket makers need to actually do direct movements from one L2 to\nanother; everyone else simply trades against the market makers. For\nlow-volume assets or NFTs, however, this is not possible, and so to move\nsuch assets from one L2 to another, individual users would need to send\ntransactions through L1.\n\nToday, a withdrawal costs ~250,000\nL1 gas and a deposit another 120,000\nL1 gas. Theoretically, this flow can be optimized quite a bit. To\nmove an NFT eg. from Ink to Arbitrum, the underlying ownership of the\nNFT has to be transferred from the Ink bridge to the Arbitrum bridge on\nL1. This is a storage operation and costs only ~5000 gas. Everything\nelse is \"just\" calls and proofs and with the right logic can be made\ncheap; let's say a total cost of 7500 gas.\n\nLet's calulate the cost in both cases.\n\nToday: 370000 * 15 * 10**-9 * 2500 = $13.87\n\nWith ideal design: 7500 * 15 * 10**-9 * 2500 = $0.28\n\nOur ideal goal is $0.05, so this implies a need to scale 5.5x.\n\nAlternatively, we can analyze more directly based on capacity.\nSuppose that each user needs to do a cross-L2 transfer of an NFT (or\nrare ERC20) on average once a month. Ethereum's total gas capacity for a\nmonth is 18000000 * (86400 * 30 / 12) = 3.88 trillion , or\nenough for 518 million such transfers. Hence, if Ethereum wanted to\nserve the whole world (eg. take Facebook's user count of 3.1 billion),\nit would need to expand capacity by ~6x, and that's if that's the\nonly thing L1 was for.\n\n## L2 mass exits\n\nOne of the important properties that L2s have, that \"alt L1s\" do not,\nis the ability to exit to the L1 if the L2 breaks. What if all users are\nnot able to get out within a one-week window? In optimistic rollups,\nthis may actually be fine: a single honest actor can prevent bad state\nroots from being confirmed indefinitely. In plasma\nsystems, however, there is often a need to get out within one week if\ndata becomes unavailable. And even in optimistic rollups, a hostile\ngovernance upgrade gives users a 30 day timeline (see: stage\n2 definition) to withdraw their assets.\n\nWhat does this imply? Well, suppose that a single Plasma chain\nbreaks, and an exit costs 120000 gas. How many users will be able to\nexit within a week? We can compute:\n86400 * 7 / 12 * 18000000 / 120000 = 7.56 million users. If\nit's an optimistic rollup with a hostile 30-day-delayed governance\nupgrade, that increases to 32.4 million users. Conceivably,\nyou could create a mass-exit protocol that allows many users to exit at\nthe same time. Suppose that we push efficiency to the limit, and you\nonly need to do one single SSTORE and a little more (so, 7500 gas) per\nuser. Then, the two numbers increase to 121 million and\n518 million , respectively.\n\nSony has an L2 on Ethereum\ntoday. Sony's Playstation has about\n116 million monthly active users. If all those users were to become\nSoneium users, then Ethereum today would not be scalable enough to\nsupport a mass exit event. However, if we implement much more clever\nmass exit protocols, it just barely would be.\n\nIf we want to avoid technically complex hash-commit protocols, we may\nwant to have space for 7500 gas per asset. I currently have 9\nassets of significant value on my primary wallet on Arbitrum; if you\ntake that as an estimate, then L1 potentially needs to scale by ~9x.\n\nThe other concern for users is that even if they can scale safely,\nthey would lose a lot of money to very high gas costs.\n\nLet's analyze the gas costs, using both present-day and \"ideal\" costs\nfor an exit:\n\n120000 * 15 * 10**-9 * 2500 = $4.5\n\n7500 * 15 * 10*-9 * 2500 = $0.28\n\nThe problem with these estimates, however, is that in a mass exit\nsituation, everyone would be trying to exit at the same time,\nand so gas costs would be significantly higher. We have seen entire days\nwhere the L1's average daily gas cost goes above 100 gwei. If we take\n100 gwei as a baseline, then we get a withdrawal cost of $1.88, implying\na need for L1 to scale 1.9x to handle exits affordably (under $1). Note\nalso that if you want users to be able to exit all their assets\nat once, without needing technically complex hash-commit protocols, then\nthat may imply 7500 gas per asset., then withdrawal costs increase to\neither $2.5 or $16.8, depending on your parameters, with corresponding\nimplications to how much L1 needs to scale to keep withdrawals\naffordable.\n\n## Issuing ERC20s on L1\n\nMany tokens are being launched on L2s today. This has an underrated\nsecurity concern: if an L2 goes through a hostile governance upgrade,\nthen an ERC20 launched on that L2 could start issuing an unlimited\nnumber of new tokens, and there would be no way to stop those tokens\nfrom leaking into the rest of the ecosystem. If a token is issued on L1,\nthe consequences of one L2 going astray are mostly bounded to that\nL2.\n\nOver 200,000 ERC20\ntokens have been launched on L1 so far. Supporting even 100x that\nwould be feasible. However, for launching ERC20s on L1 to be a popular\noption, it needs to be cheap. Let's take eg. the Railgun token (a major\nprivacy\nprotocol). Here\nis its deployment transaction. It cost 1.647 million gas, which is\n$61.76 under our assumptions. For a company, this cost is fine as-is. In\nprinciple, this could be optimized a lot, especially for projects that\nlaunch lots of tokens with the same logic. However, even if we get the\ncost down to 120000 gas, it's still $4.5.\n\nIf we give ourselves the goal of eg. bringing Polymarket to L1 (at least asset\nissuance; trading can still happen on L2s), and we want lots\nof micro-markets happening, then following our target goal above of\n$0.25, we would need to scale L1 by ~18x.\n\n## Keystore wallet operations\n\nKeystore\nwallets are a type of wallet that has modifiable verification logic\n(for changing keys, signature algorithms, etc) that automatically\npropagates across all L2s. The verification logic sits on L1, and L2s\nuse synchronous reads (eg. L1SLOAD,\nREMOTESTATICCALL)\nto read the logic. Keystore wallets can be done with the\nverification logic on an L2, but this adds a lot more complexity.\n\nSuppose that each user needs to do a key change or account upgrade\noperation once a year, and we have 3.1 billion users. If each operation\ncosts 50,000 gas, then we get a gas consumption per slot of\n50000 * 3100000000 / (31556926 / 12) ~= 59 million , about\n3.3x the current target.\n\nWe could optimize very hard, but making key chance\noperations initiated on L2, but stored on L1 (credit\nthe\nScroll team for this idea). This would reduce gas consumption to\npotentially a storage write and a little more (let's once again say 7500\ngas), which would allow keystore updates to be made with about half of\nEthereum's current gas capacity.\n\nWe can also estimate the cost of a keystore operation:\n\n7500 * 15 * 10**-9 * 2500 = $0.28\n\nFrom this perspective, a 1.1x increase would be sufficient to make\nkeystore wallets sufficiently affordable.\n\n## L2 proof submission\n\nFor cross-L2 interoperability to be fast and general-purpose\nand trustless, we need L2s to frequently post to L1, so that\nthey can be directly aware of each other's state. To get optimally low\nlatency, L2s need to commit to L1 every slot.\n\nWith today's technology (ZK-SNARKs), this is a cost of ~500,000 per\nL2, and so Ethereum would only be able to support 36 L2s (compare:\nL2beat tracks about\n150, including validiums and optimiums). But what's more important\nis that it is too economically unviable to do this: at an approximate\nlong-term average gas price of 15 gwei and an ETH price of $2500,\nthe cost per year of submitting is\n500000 * 15 * 10**-9 * (31556926 / 12) * 2500 = $49M per year\n. If we used aggregation\nprotocols, the cost could again drop, in the limit perhaps about\n10,000 gas per submission because the aggregation mechanism is somewhat\nmore complex than just updating a single storage slot. This would make\nsubmission cost about $1M per year per L2.\n\nIdeally, we want submitting to L1 every slot to be a no-brainer.\nDoing that would again require significant L1 capacity increases. $100k\nper year is a reasonably small cost for an L2 team, $1m per year is\nnot.\n\n## Conclusion\n\nWe can put the above use cases into a table as follows:\n\nUse case\nL1 gas needs with present-day tech\nL1 gas needs with more ideal tech\nL1 gas needs (to be affordable)\n\nCensorship resistance\n< 0.01x\n< 0.01x\n~4.5x\n\nCross-L2 asset movements\n278x\n5.5x\n~6x\n\nL2 mass exits\n3 - 117x\n1 - 9x\n~1 - 16.8x\n\nIssuing ERC20s\n< 0.01x\n< 0.01x\n~1 - 18x\n\nKeystore wallet operations\n3.3x\n0.5x\n~1.1x\n\nL2 proof submission\n4x\n0.08x\n~10x\n\nKeep in mind that the first and second columns are additive, eg. if\nkeystore wallet operations are taking up half the current gas\nconsumption, there needs to be enough space to run an L2 mass exit\non top of that.\n\nAdditionally, keep in mind once again that the cost-based estimates\nare extremely approximate. Demand elasticity (how much gas\ncosts respond to gas limit changes, especially in the long run) are very\nhard to estimate, and on top of that there is a lot of uncertainty in\nhow the fee market will evolve even given a fixed level of usage.\n\nAltogether, this analysis shows that there is significant value to\n~10x scaling of L1 gas even in an L2-dominated world. This in turn\nimplies that short-term L1 scaling that can be done in the next 1-2\nyears is valuable regardless of what the long-term picture ends up\nlooking like.",
    "contentLength": 14178,
    "summary": "Higher L1 gas limits enable censorship resistance bypassing, cross-L2 asset transfers, and mass exits even in an L2-focused Ethereum ecosystem.",
    "detailedSummary": {
      "theme": "Vitalik argues that Ethereum's Layer 1 should significantly increase gas limits even in a rollup-centric future to enable critical functions like censorship resistance, cross-L2 interoperability, and mass exits.",
      "summary": "Vitalik presents a detailed technical and economic analysis arguing that Ethereum's Layer 1 needs approximately 10x scaling even as most applications move to Layer 2 rollups. He examines six key use cases through mathematical modeling: censorship resistance via force-inclusion mechanisms, cross-L2 asset transfers, mass exit scenarios when L2s fail, ERC20 token issuance on L1 for security, keystore wallet operations, and frequent L2 proof submissions for interoperability. Using assumptions about gas costs, ETH prices, and user behavior, Vitalik calculates current and optimized gas requirements for each scenario. His analysis reveals that while some functions like keystore operations need modest scaling (1.1x), others like cross-L2 movements and mass exits require much more capacity (6-17x) to remain affordable and practical. The post concludes that short-term L1 scaling efforts over the next 1-2 years provide significant value regardless of long-term architectural decisions, as they enable simpler and more secure application development patterns in a multi-L2 ecosystem.",
      "takeaways": [
        "Even in an L2-dominant future, Ethereum L1 needs approximately 10x gas limit scaling to support critical infrastructure functions effectively",
        "Censorship resistance through L2 force-inclusion mechanisms requires L1 scaling of about 4.5x to keep bypass costs under $1",
        "Cross-L2 asset transfers for NFTs and rare tokens need 5.5-6x scaling with optimized protocols to serve global user bases",
        "Mass exit scenarios from failing L2s require significant L1 capacity, with Sony's PlayStation user base (116M) barely fitting current limits even with optimized protocols",
        "L2 proof submission for fast cross-chain interoperability could cost $49M annually per L2 without scaling and protocol improvements"
      ],
      "controversial": [
        "The assumption that demand elasticity is close to 1 (doubling gas limits halves prices) is acknowledged as weakly supported and could be very different in practice",
        "The cost targets for various operations ($0.05 for normal transactions, $0.25 for exceptional ones, $1 for attack responses) are described as 'intuitive value judgements' without strong empirical backing",
        "The push for 10x L1 scaling conflicts with Ethereum's rollup-centric roadmap philosophy that most activity should occur on L2s"
      ]
    }
  },
  {
    "id": "general-2025-01-23-l1l2future",
    "title": "Scaling Ethereum L1 and L2s in 2025 and beyond",
    "date": "2025-01-23",
    "category": "protocol",
    "url": "https://vitalik.eth.limo/general/2025/01/23/l1l2future.html",
    "path": "general/2025/01/23/l1l2future.html",
    "content": "- \n\n- \n\n  \n    \n\n  \n  \n  Dark Mode Toggle\n  \n    \n\n  \n\n##  Scaling Ethereum L1 and L2s in 2025 and beyond \n\n 2025 Jan 23 \nSee all posts\n\n \n \n\n Scaling Ethereum L1 and L2s in 2025 and beyond \n\nSpecial thanks to Tim Beiko, Justin Drake, and developers from\nvarious L2 teams for feedback and review\n\nThe goal of Ethereum is the same as what it has been from day 1:\nbuilding a global, censorship-resistant permissionless\nblockchain. A free and open platform for decentralized\napplications, built upon the same principles (what we might call today\nthe regen\nand cypherpunk\nethos) as GNU + Linux, Mozilla, Tor, Wikipedia, and many other great\nfree and open source software projects that came before it.\n\nOver the past ten years, Ethereum has also evolved another property\nthat I have come to greatly appreciate: in addition to the innovation in\ncryptography and economics, Ethereum is also an innovation in\nsocial technology. Ethereum as an ecosystem is a working, live\ndemonstration of a new, more open and decentralized way of building\nthings together. Political philosopher Ahmed Gatnash describes his\nexperience at Devcon as\nfollows:\n\n... A glimpse of what an alternative world could look like - one mostly\nfree of gatekeeping, and with no attachment to legacy systems. In its\ninversion of society's standard status systems, the people who are held\nin highest social status here are the nerds who spend all their time\nhyper focused on independently solving a problem that they really deeply\ncare about, not playing a game to climb the hierarchies of legacy\ninstitutions and amass power. Almost all the power here was soft power.\nI found it beautiful and very inspiring - it makes you feel like\nanything would be possible in a world like this, and that a world like\nthis is actually within reach.\n\nThe technical project and the social project are inherently\nintertwined. If you have a decentralized technical system at\ntime T, but a centralized social process maintaining it, there is no\nguarantee that your technical system will still be decentralized at time\nT+1. Similarly, the social process is kept alive in many ways by the\ntechnology: the tech brings in users, the ecosystem made possible by the\ntech provides incentives for developers to come and stay, it keeps the\ncommunity grounded and focused on building rather than just socializing,\nand so on.\n\n Where you can use Ethereum to pay for things around the\nworld, Oct 2024. Source.\n\nAs a result of ten years of hard work governed by this mix of\ntechnical and social properties, Ethereum has come to embody another\nimportant quality: Ethereum does useful things for people, at\nscale. Millions of people hold ETH or stablecoins as a form of\nsavings, and many more use these assets for payment: I'm one of them. It\nhas effective, working privacy\ntools that I use to pay for VPNs to protect my internet data. It has\nENS, a robust decentralized alternative to DNS and more generally public\nkey infrastructure. It has working and easy-to-use Twitter alternatives.\nIt has defi tools that offer millions of people higher-yielding low-risk\nassets than what they can access in tradfi.\n\nFive years ago, I was not comfortable talking about the latter use\ncase for one primary reason: the infrastructure and the code were not\nmature, we were only a few years removed from the massive and highly\ntraumatic smart contract hacks of 2016-17, and there is no point in\nhaving a 7% APY instead of a 5% APY if every year there is a 5% chance\nyou will\ninstead get\na -100%\nAPY. On top of this, transaction fees were too high to make these\nthings usable at scale. Today, these tools have shown their resilience\nover time, the quality of auditing tools has increased, and we are\nincreasingly confident in their security. We know what\nnot to do.\nL2 scaling is working.\nTransaction fees have been very\nlow for almost a year.\n\nWe need to continue building up the technical and social\nproperties, and the utility, of Ethereum. If we have the\nformer, but not the latter, then we devolve into a\nmore-and-more-ineffective \"decel\" community that can howl into the wind\nabout how various mainstream actors are immoral and bad, but has no\nposition to actually offer a better alternative. If wed have the latter,\nbut not the former, then we have exactly the Wall Street greed-is-good\nmentality that many of us came here\nprecisely to escape.\n\nThere are many implications of the duality that I have just\ndescribed. In this post, I want to focus on a specific one, which\nmatters greatly Ethereum's users in the short and medium term:\nEthereum's scaling strategy.\n\n## The rise of layer 2s\n\nToday, the path that we are taking to scale Ethereum is layer\n2 protocols (L2s). The L2s of 2025 are a far cry from the early\nexperiments they were in 2019: they have reached key decentralization\nmilestones, they are securing billions of dollars of value, and they\nare currently scaling\nEthereum's transaction capacity by a factor of 17x, dropping fees by a\nsimilar amount.\n\n Left: stage 1 and stage 2 rollups. On Jan 22, Ink has\njoined as the sixth stage 1+ rollup (and third full-EVM stage 1+\nrollup). Right, top rollups by TPS, with Base leading at roughly 40% of\nEthereum's capacity. \n\nThis is all happening just in time for a wave of successful\napplications: various defi platforms, social networks, prediction\nmarkets, exotic contraptions like Worldchain (now with 10\nmillion users) and more. The \"enterprise blockchain\" movement,\nwidely viewed as a dead end after the failure of consortium blockchains\nin 2010s, is coming back to life with L2s, with Soneium providing a leading\nexample.\n\nThese successes are also a testament to the social\nside of Ethereum's decentralized and modular approach to scaling:\ninstead of the Ethereum Foundation having to seek out all of these users\nitself, there are dozens of independent entities who are motivated to do\nso. These entities have also made crucial contributions to the\ntechnology, without which Ethereum would not be anywhere close to as far\nas it is today. And as a result, we are finally approaching escape\nvelocity.\nChallenges:\nscale and dealing with heterogeneity\n\nThere are two primary challenges facing L2s today:\n\n- Scale: our blob space is barely\ncovering the L2s and the usecases of today, and we have far\nfrom enough\nfor the needs\nof tomorrow.\n\n- Challenges of heterogeneity. The early vision\nfor how Ethereum could scale involved creating a blockchain that contains\nmany shards, each shard being a copy of the EVM that gets processed\nby a small fraction of the nodes. L2s are, in theory, an\nimplementation of exactly this approach. In practice, however, there\nis a key difference: each shard (or set of shards) is created by a\ndifferent actor, is treated by infrastructure as being a different\nchain, and often follows different standards. Today, this translates\ninto composability and user experience problems for developers and\nusers.\n\nThe first problem is an easy-to-understand technical challenge, and\nhas an easy-to-describe (but hard-to-implement) technical solution: give\nEthereum more blobs. In addition to this, the L1 can also do moderate amount of scaling\nin the short term, as well as improvements to proof\nof stake, stateless\nand light verification, storage,\nthe EVM and\ncryptography.\n\nThe second problem, which has received the bulk of public attention,\nis a coordination problem. Ethereum is no stranger to\nperforming complex technical tasks between multiple teams: after all, we\ndid the merge. Here, the coordination problem is more challenging,\nbecause of the greater number and diversity of actors and goals andt the\nfact that the process is starting much later in the game. But even\nstill, our ecosystem has solved difficult problems before, and we can do\nso again.\n\nOne possible shortcut for scaling is to give up on L2s, and\ndo everything through L1 with a much higher gas limit (either across\nmany shards, or on one shard). However, this approach compromises too\nmuch of the benefits of Ethereum's current social structure,\nwhich has been so effective at getting the benefits of different forms\nof research, development and ecosystem-building culture at the same\ntime. Hence, instead we should stay the course, continue to\nscale primarily through L2s, but make sure that L2s actually fulfill the\npromise that they were meant to fulfill.\n\nThis means the following:\n\n- L1 needs to accelerate scaling blobs.\n\n- L1 also needs to do a moderate amount of scaling the EVM and\nincreasing the gas limit, to be able to handle the activity\nthat it will continue to have even in an L2-dominated world (eg. proofs,\nlarge-scale defi, deposits and withdrawals, exceptional mass exit\nscenarios, keystore\nwallets, asset issuance).\n\n- L2s need to continue improving security. The same\nsecurity guarantees that one would expect from sharding (including eg.\ncensorship resistance, light client verifiability, lack of enshrined\ntrusted parties) should be available on L2s.\n\n- L2s, and wallets need to accelerate improving and\nstandardizing interoperability. This includes chain-specific\naddresses, message-passing and bridge standards, efficient\ncross-chain payments, on-chain configs and more. Using Ethereum should\nfeel like using a single ecosystem, not 34 different blockchains.\n\n- L2 deposit and withdraw times need to become much\nfaster.\n\n- As long as basic interoperability needs are met, L2\nheterogeneity is good. Some L2s will be governance-minimized\nbased rollups that run exact copies of the L1 EVM. Others will\nexperiment with different VMs. Others will act more like servers that\nuse Ethereum to give users extra security guarantees. We need L2s at\neach part of that spectrum.\n\n- We should think explicitly about economics of ETH.\nWe need to make sure that ETH continues to accrue value even in an\nL2-heavy world, ideally solving for a variety of models of how value\naccrual happens.\n\nLet us now go through each of these topic areas in more detail.\n\n## Scaling: blobs, blobs, blobs\n\nWith EIP-4844, we now have 3 blobs per slot, or a data bandwidth of\n384 kB per slot. Quick napkin math suggests that this is 32 kB per\nsecond, and each transaction takes about 150\nbytes onchain, so we get ~210 tx/sec.\u00a0L2beat data gives us almost exactly this\nnumber.\n\nWith Pectra, scheduled for release\nin March, we plan to double this to 6 blobs per slot.\n\nThe current goal\nof Fusaka is to focus primarily on PeerDAS,\nideally having nothing other than PeerDAS and EOF. PeerDAS could increase the\nblob count immediately by another 2-4x, and then 8x or more over\ntime.\n\nAfter that point, the goal is to keep improving the technology to\nincrease the blob count further. When we get to 2D\nsampling, we can reach 128 blobs per slot, and then keep going\nfurther. With this, and improvements\nto data compression, we can reach 100,000 TPS onchain.\n\nSo far, the above is all a re-statement of the pre-2025 status quo\nroadmap. The key question is: what can we actually change to\nmake this go faster? My answers are the following:\n\n- We should be more willing to explicitly deprioritize features that\nare not blobs.\n\n- We should be clearer that blobs are the goal, and make relevant p2p\nR&D a talent acquisition priority.\n\n- We can make the blob target adjusted directly by stakers, similar to\nthe gas limit. This would allow the blob target to increase more quickly\nin response to technology improvements, without waiting for a hard\nfork.\n\n- We can consider more radical\napproaches that get us more blobs faster with more trust assumptions\nfor lower-resourced stakers, though we should be careful about\nthis.\n\nImproving\nsecurity: proof systems and native rollups\n\nToday, there are three stage 1 rollups (Optimism, Arbitrum, Ink) and\nthree stage 2 rollups (DeGate, zk.money, Fuel). The majority of activity\nstill happens on stage 0 rollups (ie. multisigs). This needs to change.\nA big reason why this has not changed faster, is that building a\nproof system, and getting enough confidence in it to be willing to give\nup training wheels and rely fully on it for security, is\nhard.\n\nThere are two paths toward getting there:\n\n- Stage 2 + multi-provers + formal verification: use\nmultiple proving systems for redundancy, and use formal verification\n(see: the verified ZK-EVM\ninitiative) to get confidence that they are secure.\n\n- Native rollups: make EVM state transition function\nverification part of the protocol itself, eg. through a precompile (see:\n[1] [2]\n[3]\nfor research)\n\nToday, we should work on both in parallel. For stage 2 +\nmulti-provers + formal verification, the roadmap is relatively\nwell-understood. The main practical place where we can accelerate is to\ncooperate more on software stacks, reducing the need for duplicate work\nwhile increasing interoperability as a by-product.\n\nNative rollups are still an early-stage idea. There is a lot\nof active thinking to be done, particularly on the topic of how\nto make a native rollup precompile maximally flexible. An ideal goal\nwould be for it to support not just exact clones of the EVM, but also\nEVMs with various arbitrary changes, in such a way that an L2 with a\nmodified EVM could still use the native rollup precompile, and \"bring\nits own prover\" only for the modifications. This could be done for\nprecompiles, opcodes, the state tree, and potentially other pieces.\nInteroperability and\nstandards\n\nThe goal is to make it so that moving assets between and using\napplications on different L2s has the same experience as you would have\nif they were different \"shards\" of the same blockchain. There has for a\nfew months been a pretty well-understood roadmap for how to do this:\n\n- Chain-specific\naddresses: the address should include both the account on the chain,\nand some kind of identifier for the chain itself. ERC-3770 is an early\nattempt at this, there are now more sophisticated ideas, which also move\nthe registry for L2s to the Ethereum L1 itself.\n\n- Standardized cross-chain bridges and cross-chain message\npassing: there should be standard ways to verify proofs and\npass messages between L2s, and these standards should not require\ntrusting anything except for the proof systems of the L2s themselves.\nAn ecosystem relying on multisig bridges is NOT\nacceptable. If it's a trust assumption that would not exist if\nwe had done 2016-style\nsharding, it's not acceptable today, full stop.\n\n- Speeding up deposit and withdraw times, so that\n\"native\" messages can take minutes (and eventually one slot) rather than\nweeks. This involves faster ZK-EVM provers, and proof aggregation.\n\n- Synchronous read of L1 from L2. See: L1SLOAD,\nREMOTESTATICCALL.\nThis makes cross-L2 interoperability significantly easier, and also\nhelps keystore\nwallets.\n\n- Shared sequencing, and other longer-term work.\nBased rollups are valuable in part because they may be\nable to do this more effectively.\n\nAs long as standards like these are satisfied, there is still a lot\nof room for L2s to have very different properties from each other:\nexperimenting with different virtual machines, different sequencing\nmodels, scale vs security tradeoffs, and other differences. However, it\nmust be clear to users and application developers what level of security\nthey are getting.\n\nTo make faster progress, a large share of the work can be done by\nentities that operate across the ecosystem: the Ethereum Foundation,\nclient development teams, major application teams, etc. This will reduce\ncoordination effort and make adopting standards more of a no-brainer,\nbecause the work that will be done by each individual L2 and wallet will\nbe reduced. However, L2s and wallets, as extensions of Ethereum, both\nstill need to step up work on the last mile of actually implementing\nthese features and bringing them to users.\n\n## Economics of ETH\n\n ETH as\ntriple-point asset \n\nWe should pursue a multi-pronged strategy, to cover all major\npossible sources of the value of ETH as a\ntriple-point asset. Some key planks of that strategy could be the\nfollowing:\n\n- Agree broadly to cement ETH as the primary asset of the\ngreater (L1 + L2) Ethereum economy, support applications using\nETH as the primary collateral, etc\n\n- Encourage L2s supporting ETH with some percentage of\nfees. This could be done through burning a portion of fees,\npermanently staking them and donating proceeds to Ethereum ecosystem\npublic goods, or a number of other formulas.\n\n- Support based rollups in part as a path for L1 to capture\nvalue through MEV, but do not attempt to force all rollups to\nbe based (because it does not work for all applications), and do not\nassume that this alone will solve the problem.\n\n- Raise the blob count, consider a minimum blob\nprice, and keep blobs in mind as another possible revenue generator. As\nan example possible future, if you take the average blob fee of the last\n30 days, and suppose it stays the same (due to induced\ndemand) while blob count increases to 128, then Ethereum would burn\n713,000 ETH per year. However, such a favorable demand curve is not\nguaranteed, so also do not assume that this alone will solve the\nproblem.\n\n## Conclusion: The Road Ahead\n\nEthereum has matured as a technology stack and a social ecosystem,\nbringing us closer to a more free and open future where hundreds of\nmillions of people can benefit from crypto assets and decentralized\napplications. However, there is a lot of work to be done, and now is the\ntime to double down.\n\nIf you're an L2 developer, contribute to the tooling to make blobs\nscale more safely, the code to scale the execution of your EVM, and the\nfeatures and standards to make the L2 interoperable. If you are a wallet\ndeveloper, be similarly engaged in contributing to and implementing\nstandards to make the ecosystem more seamless for users, and at the same\ntime as secure and decentralized as it was when Ethereum was just an L1.\nIf you are an ETH holder or community member, actively participate in\nthese discussions; there are many areas that still require active\nthought and brainstorming. The future of Ethereum depends on every one\nof us playing an active role.",
    "contentLength": 18011,
    "summary": "Ethereum will scale primarily through L2s by increasing blob capacity and improving interoperability, while L1 handles limited scaling.",
    "detailedSummary": {
      "theme": "Vitalik outlines Ethereum's scaling roadmap for 2025 and beyond, emphasizing the need to accelerate blob scaling, improve L2 interoperability, and maintain ETH's value while preserving Ethereum's decentralized social and technical foundations.",
      "summary": "Vitalik argues that Ethereum has evolved beyond just a technical project into a successful social technology that demonstrates a new, decentralized way of building together, now serving millions of users with practical applications like payments, DeFi, and privacy tools. He identifies that while Layer 2 solutions have successfully scaled Ethereum by 17x and reduced fees dramatically, two major challenges remain: insufficient blob space for future needs and heterogeneity problems that create poor user experience across different L2s. Vitalik proposes a comprehensive solution involving accelerated blob scaling (potentially reaching 100,000 TPS), improved L2 security through stage 2 rollups and native rollup precompiles, standardized interoperability features like chain-specific addresses and cross-chain bridges, and a multi-pronged strategy to ensure ETH maintains value in an L2-dominated world. He emphasizes that abandoning L2s for simple L1 scaling would compromise Ethereum's beneficial social structure, and calls for continued commitment to the L2-centric approach while solving its current limitations.",
      "takeaways": [
        "Ethereum should prioritize blob scaling above other features, potentially allowing stakers to adjust blob targets directly and aiming for 100,000+ TPS through technologies like PeerDAS and 2D sampling",
        "L2s must improve security by advancing to stage 2 with multi-prover systems and formal verification, while exploring native rollup precompiles integrated into Ethereum's protocol",
        "Standardized interoperability is crucial, requiring chain-specific addresses, trustless cross-chain bridges, faster deposit/withdrawal times, and L1 read capabilities from L2s",
        "ETH's value must be preserved through multiple strategies including cementing ETH as the primary ecosystem asset, encouraging L2 fee sharing, supporting based rollups for MEV capture, and considering minimum blob pricing",
        "The success depends on active participation from L2 developers, wallet developers, and the broader community to implement standards and maintain Ethereum's decentralized principles"
      ],
      "controversial": [
        "The suggestion to deprioritize non-blob features and make blob scaling the primary focus could be contentious among developers working on other important Ethereum improvements",
        "The proposal for more radical approaches to blob scaling with increased trust assumptions for lower-resourced stakers may conflict with Ethereum's decentralization principles",
        "The assertion that multisig bridges are 'NOT acceptable' and the hard stance against trust assumptions that wouldn't exist in 2016-style sharding could be seen as overly rigid by some L2 teams"
      ]
    }
  },
  {
    "id": "general-2025-01-05-dacc2",
    "title": "d/acc: one year later",
    "date": "2025-01-05",
    "category": "society",
    "url": "https://vitalik.eth.limo/general/2025/01/05/dacc2.html",
    "path": "general/2025/01/05/dacc2.html",
    "content": "- \n\n- \n\n  \n    \n\n  \n  \n  Dark Mode Toggle\n  \n    \n\n  \n\n##  d/acc: one year later \n\n 2025 Jan 05 \nSee all posts\n\n \n \n\n d/acc: one year later \n\nSpecial thanks to Liraz Siri, Janine Leger and Balvi volunteers\nfor feedback and review\n\nAbout a year ago, I wrote an\narticle on techno-optimism, describing my general enthusiasm for\ntechnology and the massive benefits that it can bring, as well as my\ncaution around a few specific concerns, largely centered around\nsuperintelligent AI, and the risk that it may bring about either doom,\nor irreversible human disempowerment, if the technology is built in the\nwrong ways. One of the core ideas in my post was the philosophy of\n: decentralized and democratic, differential defensive\nacceleration. Accelerate technology, but differentially focus\non technologies improve our ability to defend, rather than our ability\nto cause harm, and in technologies that distribute power rather than\nconcentrating it in the hands of a singular elite that decides what is\ntrue, false, good or evil on behalf of everyone. Defense like in\ndemocratic Switzerland\nand historically quasi-anarchist Zomia,\nnot like the lords and castles of medieval feudalism.\n\nIn the year since then, the philosophy and ideas have matured\nsignificantly. I talked about the ideas on 80,000\nHours, and have seen many responses, largely positive and some\ncritical. The work itself is continuing and bearing fruit: we're seeing\nprogress in verifiable open-source\nvaccines, growing recognition of the value of healthy indoor air,\nCommunity Notes continuing to shine, a breakout year for prediction\nmarkets as an info tool, ZK-SNARKs in government\nID and social media (and securing Ethereum wallets through\naccount abstraction), open-source imaging tools with\napplications in medicine and BCI, and more. In the fall, we had the\nfirst significant d/acc event: \"d/acc Discovery Day\"\n(d/aDDy) at Devcon, which featured a full day of speakers from all\npillars of d/acc (bio, physical, cyber, info defense, plus neurotech).\nPeople who have been working on these technologies for years are\nincreasingly aware of each other's work, and people outside are\nincreasingly aware of the larger story: the same kinds of values that\nmotivated Ethereum and crypto can be\napplied to the wider world.\n\n## Table of contents\n\n- What d/acc is and is not\n\n- The third dimension: survive and thrive\n\n- The hard question: AI safety, short timelines and\nregulation\n\n- The role of crypto in d/acc\n\n- d/acc and public goods funding\n\n- The future\n\n## What d/acc is and is not\n\nIt's the year 2042. You're seeing reports in the media about a new\npandemic potentially in your city. You're used to these: people get\nover-excited about every animal disease mutation, and most of them come\nto nothing. The previous two actual potential pandemics were\ndetected very early through wastewater\nmonitoring and open-source\nanalysis of social media, and stopped completely in their tracks.\nBut this time, prediction markets are showing a 60% chance of at least\n10,000 cases, so you're more worried.\n\nThe sequence for the virus was identified yesterday. Software updates\nfor your pocket air tester to allow\nit to detect the new virus (from a single breath, or from 15 minutes of\nexposure to indoor air in a room) are available already. Open-source\ninstructions and code for generating a vaccine using equipment that can\nbe found in any modern medical facility worldwide should be available\nwithin weeks. Most people are not yet taking any action at all, relying\nmostly on widespread adoption of air filtering and ventilation to\nprotect them. You have an immune condition so you're more cautious: your\nopen-source locally-running personal assistant AI, which handles among\nother tasks navigation and restaurant and event recommendation, is also\ntaking into account real-time air tester and CO2 data to only recommend\nthe safest venues. The data is provided by many thousands of\nparticipants and devices using ZK-SNARKs\nand differential\nprivacy to minimize the risk that the data can be leaked or abused\nfor any other purpose (if you want to contribute data to these\ndatasets, there's other personal assistant AIs that verify\nformal proofs that these cryptographic gadgets actually work).\n\nTwo months later, the pandemic disappeared: it seems like 60% of\npeople following the basic protocol of putting on a mask if the air\ntester beeps and shows the virus present, and staying home if they test\npositive personally, was enough to push the transmission rate, already\nheavily reduced due to passive heavy air filtering, to below 1. A\ndisease that simulations show might have been five times worse than\nCovid twenty years ago turns out to be a non-issue today.\n\n Devcon d/acc day \n\nOne of the most positive takeaways from the d/acc event at Devcon was\nthe extent to which the d/acc umbrella successfully brought people\ntogether from very different fields, and got them to actually\nbe interested in each other's work.\n\nCreating events with \"diversity\" is easy, but making different people\nwith different backgrounds and interests actually relate to each other\nis hard. I still have memories of being forced to watch long operas in\nmiddle school and high school, and personally finding them boring. I\nknew that I was \"supposed to\" appreciate them, because if I did not then\nI would be an uncultured computer science slob, but I did not connect\nwith the content on a more genuine level. d/acc day did not feel like\nthat at all: it felt like people actually enjoyed learning about very\ndifferent kinds of work in different fields.\n\nIf we want to create a brighter alternative to domination,\ndeceleration and doom, we need this kind of broad coalition building.\nd/acc seemed to be actually succeeding at it, and that alone shows the\nvalue of the idea.\n\nThe core idea of d/acc is simple: decentralized and\ndemocratic differential defensive acceleration. Build\ntechnologies that shift the offense/defense balance toward defense, and\ndo so in a way that does not rely on handing over more power to\ncentralized authorities. There is an inherent tie between these two\nsides: any kind of decentralized, democratic or liberal political\nstructure thrives best when defense is easy, and suffers the most\nchallenge when defense is hard - in those cases, the far more likely\noutcome is some period of war of all against all, and eventually an\nequilibrium of rule by the strongest.\n\nThe core principle of d/acc extends across many domains:\n\n Chart from My\nTechno-Optimism, last year \n\nOne way to understand the importance of trying to be decentralized,\ndefensive and acceleration-minded at the same time, is to contrast it\nwith the philosophy that you get when you give up each of the three.\n\n- \n\nDecentralized acceleration, but don't care about the\n\"differential defensive\" part. Basically, be an e/acc,\nbut decentralized. There are plenty of people who take this\napproach, some who label\nthemselves d/acc but helpfully describe their focus as \"OFFENSE\", but\nalso plenty of others who are excited about \"decentralized AI\" and\nsimilar topics in a more moderate way, but in my view put insufficient\nattention on the \"defensive\" aspect. \n\n In my view, this approach\nmay avoid the risk of global human dictatorship by the specific tribe\nyou're worried about, but it doesn't have an answer to the underlying\nstructural problem: in an offense-favoring environment, there's constant\nongoing risk of either catastrophe, or someone positioning themselves as\na protector and permanently establishing themselves at the top. In the\nspecific case of AI, it also doesn't have a good answer to the risk of\nhumans as a whole being disempowered compared to AIs.\n\n- \n\nDifferential defensive acceleration, but don't care about\n\"decentralized and democratic\". Embracing centralized control\nfor the sake of safety has permanent appeal to a subset of people, and\nreaders are undoubtedly already familiar with many examples, and the\ndownsides of them. Recently, some have worried that extreme centralized\ncontrol is the only solution to the extremes of future technologies: see\nthis\nhypothetical scenario where \"Everybody is fitted with a \u2018freedom\ntag' \u2013 a sequent to the more limited wearable surveillance devices\nfamiliar today, such as the ankle tag used in several countries as a\nprison alternative ... encrypted video and audio is continuously uploaded\nand machine-interpreted in real time\". However, centralized control is a\nspectrum. One milder version of centralized control that's usually\noverlooked, but is still harmful, is resistance to public scrutiny in\nbiotech (eg. food,\nvaccines),\nand the closed source norms that allow this resistance to go\nunchallenged. \n\n The risk of this approach is, of course, that the\ncenter is often itself the source of risk. We saw this in Covid, where\ngain-of-function research funded by multiple major world\ngovernments may have been the source of the pandemic, centralized\nepistemology led to the WHO not\nacknowledging for years that\nCovid is airborne, and coercive social\ndistancing and vaccine\nmandates led to political backlash that may reverberate for decades. A\nsimilar situation may well happen around any risks to do with AI, or\nother risky technologies. A decentralized approach would better address\nrisks from the center itself.\n\n- \n\nDecentralized defense, but don't care about\nacceleration - basically, attempting to slow down technological\nprogress, or economic degrowth. \n\n The\nchallenge with this strategy is twofold. First, on balance technology\nand economic growth have been massively good for humanity, and\nany delay to it imposes\ncosts that are hard\nto overstate. Second, in a non-totalitarian world, not advancing is\nunstable: whoever \"cheats\" the most and finds plausibly-deniable ways to\nadvance anyway will get ahead. Decelerationist strategies can work to\nsome extent in some contexts: European food being healthier than\nAmerican food is one example, the success of nuclear non-proliferation\nso far is another. But they cannot work forever.\n\nWith d/acc, we want to:\n\n- Be principled at a time when much of the world is becoming tribal,\nand not just build whatever - rather, we want to build\nspecific things that make the world safer and better.\n\n- Acknowledge that exponential technological progress means that\nthe world is going to get very very weird, and that\nhumanity's total \"footprint\" on the universe will only increase. Our\nability to keep vulnerable animals, plants and people out of harm's way\nmust improve, but the only way out is forward.\n\n- Build technology that keeps us safe without assuming that\n\"the good guys (or good AIs) are in charge\". We do this by\nbuilding tools that are naturally\nmore effective when used to build and to protect than when used to\ndestroy.\n\nAnother way to think about d/acc is to go back to a frame from the\nPirate Party movements in Europe in the late 00s:\nempowerment.\n\nThe goal is to build a world where we preserve human agency,\nachieving both\nthe negative freedom of avoiding active interference (whether from other\npeople acting as private citizens, or from governments, or from\nsuperintelligent bots) with our ability to shape our own destinies, and\nthe positive freedom of ensuring that we have the knowledge and\nresources to. This echoes a centuries-long classical liberal tradition,\nwhich also includes Stewart Brand's focus on \"access\nto tools\" and John Stuart Mill's emphasis\non education alongside liberty as\nkey components of human progress - and perhaps, one might add,\nBuckminster Fuller's desire to see the process of global solving be participatory\nand widely distributed. We can see d/acc as a way of achieving these\nsame goals given the technological landscape of the 21\u02e2\u1d57 century.\n\nThe third dimension:\nsurvive and thrive\n\nIn my post last year, d/acc specifically focused on the defensive\ntechnologies: physical defense, bio defense, cyber defense and info\ndefense. However, decentralized defense is not enough to make the world\ngreat: you also need a forward-thinking positive vision for what\nhumanity can use its newfound decentralization and safety to\naccomplish.\n\nLast year's post did contain a positive vision, in two places:\n\n- Focusing on the challenges of superintelligence, I proposed a path\n(far from original to me) of how we can have superintelligence without\ndisempowerment:\n\n- Today, build AI-as-tools rather than\nAI-as-highly-autonomous-agents\n\n- Tomorrow use tools like virtual\nreality, myoelectrics\nand brain-computer interfaces to create tighter and tighter feedback\nbetween AI and humans\n\n- Over time proceed toward an eventual endgame where the\nsuperintelligence is a tightly coupled combination of machines and\nus.\n\n- When talking about info-defense, I also tangentially mentioned that\nin addition to _defensiv_e social technology that tries to help\ncommunities maintain cohesion and have high-quality discourse in the\nface of attackers, there is also progressive social technology\nthat can help communities more readily make high-quality judgements: pol.is is one example, and prediction markets are\nanother.\n\nBut these two points felt disconnected from the d/acc argument: \"here\nare some ideas for creating a more democratic and defense-favoring world\nat the base layer, and by the way here are some unrelated ideas for how\nwe might do superintelligence\".\n\nHowever, I think in reality there are some very important\nconnections between what labelled above as \"defensive\" and \"progressive\"\nd/acc technology. Let's expand the d/acc chart from last year's post, by\nadding this axis (also, let's relabel it \"survive\nvs thrive\") to the chart and seeing what comes out:\n\nThere is a consistent pattern, across all domains, that the\nscience, ideas and tools that can help us \"survive\" in one domain are\nclosely related to the science, ideas and tools that can help us\n\"thrive\". Some examples:\n\n- A lot of recent anti-Covid research focuses on the\nrole of viral persistence in the body as a\nmechanism for why Long Covid is such a problem. Recently, there are also\nsigns that viral persistence may\nbe responsible for Alzheimer's disease - if true, this implies that\naddressing viral persistence across all tissue types may be key to\nsolving aging.\n\n- Low-cost and miniature imaging\ntools such as those being built by Openwater can be\npowerful for treating microclots,\nor viral persistence, cancers, and they can also be used for BCI.\n\n- Very similar ideas motivate the construction of social\ntools built for highly adversarial environments, such as Community\nNotes, and social tools built for reasonably cooperative\nenvironments, such as pol.is.\n\n- Prediction markets are valuable\nin both high-cooperation\nand high-adversity\nenvironments.\n\n- Zero knowledge proofs and similar technologies for doing computation\non data while preserving privacy both increase the data available for\nuseful work such as science, and increase privacy.\n\n- Solar power and batteries are great for supercharging\nthe next wave of clean economic growth, but they are also amazing\nfor decentralization and physical resilience.\n\nIn addition to this, there are also important cross-dependencies\nbetween subject areas:\n\n- BCI is very relevant as an info-defense and collaboration\ntechnology, because it could enable much more detailed\ncommunication of our thoughts and intentions. BCI is not just\nbot-to-consciousness: it can also be\nconsciousness-to-bot-to-consciousness. This echoes Plurality\nideas about the value of BCI.\n\n- A lot of biotech depends on info-sharing, and in\nmany contexts people will only be comfortable sharing info if they are\nconfident that it will be used for one application and one application\nonly. This depends on privacy technology (eg. ZKP, FHE,\nobfuscation...)\n\n- Collaboration technology can be used to coordinate\nfunding for any of the other technology areas\n\nThe\nhard question: AI safety, short timelines and regulation\n\n Different people have very different AI timelines. Chart\nfrom Zuzalu in Montenegro, 2023. \n\nThe argument against my post last year that I found most compelling\nwas a critique from the AI safety community. The argument goes: \"sure,\nif we have half a century until we get strong AI, we can concentrate our\nenergies and build all of these good things. But actually it's looking\nlikely we have three year timelines until AGI, and another three years\nuntil superintelligence. And so if we don't want the world to be\ndestroyed or otherwise fall into an irreversible trap, we can't\njust accelerate the good, we also have to slow down the bad,\nand this means passing powerful regulations that may make powerful\npeople upset\". In my post last year, I did indeed not call for any\nspecific strategy to \"slow down the bad\", beyond vague appeals to not\nbuild risky forms of superintelligence. And so here, it's worth\naddressing the question directly: if we are living in the least\nconvenient world, where AI risk is high and timelines are\npotentially five years away, what regulation would I support?\nFirst, the\ncase for caution around new regulations\n\nLast year, the main proposed AI regulation was the SB-1047\nbill in California. SB-1047 required developers of the most powerful\nmodels (those that take over $100M to train, or over $10M in the case of\nfine-tunes) to take some safety-testing measures before releasing. In\naddition, it imposed liability on developers of AI models if they take\ninsufficient care. Many detractors argued that the bill was \"a\nthreat to open source\"; I disagreed, because the cost thresholds\nmeant that it affected only the most powerful models: even LLama3 was probably\nunder the threshold. Looking back, however, I think there was a\nlarger issue with the bill: like most\nregulation, it was overfitted to the present-day situation. The\nfocus on training cost is proving fragile in the face of new technology\nalready: the recent state-of-the-art quality Deepseek v3 model was\ntrained at a cost of only $6\nmillion, and in new models like o1 costs are shifting from training to\ninference more generally.\n\nSecond, the most likely actors who would actually be responsible for\nan AI superintelligence doom scenario are realistically militaries. As\nwe have\nseen in the last half-century of biosecurity (and beyond),\nmilitaries are willing to do scary things, and they can easily make\nmistakes. AI military use is advancing rapidly today (see Ukraine,\nGaza).\nAnd any safety regulation that a government passes, by default would\nexempt their own military, and corporations that cooperate closely with\nthe military.\n\nThat said, these arguments are not reasons to throw up our hands and\ndo nothing. Rather, we can use them as a guide, and try to come up with\nrules that would trigger these concerns the least.\n\n## Strategy 1: Liability\n\nIf someone acts in some way that causes legally actionable damage,\nthey could\nbe\nsued.\nThis does not solve the problem of risks from militaries and other\n\"above-the-law\" actors, but it is a very general-purpose approach that\navoids overfit, and is often supported by\nlibertarian-leaning economists\nfor this exact reason.\n\nThe primary targets for liability that have been considered so far\nare:\n\n- Users - the people who use the AI\n\n- Deployers - intermediaries who offer AI as a\nservice for users\n\n- Developers - the people who build the AI\n\nPutting liability on users feels most incentive-compatible. While the\nlink between how a model is developed and how it ends up being used is\noften unclear, the user decides exactly how the AI is used.\nLiability on users creates a strong pressure to do AI in what I\nconsider the right way: focus on building mecha suits for the human\nmind, not on creating new forms of self-sustaining intelligent\nlife. The former responds regularly to user intent, and so\nwould not cause catastrophic actions unless the user wanted them to. The\nlatter would have the greatest risk of going off and creating a classic\n\"AI going rogue\" scenario. Another benefit of putting liability as close\nto end usage as possible is that it minimizes the risk that liability\nwill lead to people taking actions that are harmful in other ways (eg.\nclosed source, KYC and surveillance, state/business collusion to\nclandestinely restrict users as with eg. debanking, locking out large\nregions of the world).\n\nThere is a classic argument against putting liability solely on\nusers: users may be regular individuals without too much money, or even\nanonymous, leaving no one that could actually pay for a catastrophic\nharm. This argument can be overstated: even if some users are\ntoo small to be held liable, the average customer of an AI\ndeveloper is not, and so AI developers would still be incentivized\nto build products that can give their users assurance that they won't\nface high liability risk. That said, it is still a valid argument, and\nneeds to be addressed. You need to incentivize someone in the\npipeline who has the resources to take the appropriate level of care to\ndo so, and deployers and developers are both easily available targets\nwho still have\na lot of influence over how safe or unsafe a model is.\n\nDeployer liability seems reasonable. A commonly cited concern is that\nit would not work for open-source models, but this seems manageable,\nespecially since there is a high chance that the most powerful models\nwill be closed source anyway (and if they turn out to be open, then\nwhile deployer liability does not end up very useful, it also does not\ncause much harm). Developer liability has the same concern (though with\nopen source models there is some speed-bump of needing to\nfine-tune a model to cause it to do some originally disallowed thing),\nbut the same counterargument applies. As a general principle,\nputting a \"tax\" on control, and essentially saying \"you\ncan build\nthings you don't control, or you can build things you do control,\nbut if you build things you do control, then 20% of the control has to\nbe used for our purposes\", seems like a reasonable position for legal\nsystems to have.\n\nOne idea that seems under-explored is putting liability on other\nactors in the pipeline, who are more guaranteed to be well-resourced.\nOne idea that is very d/acc friendly is to put liability on\nowners or operators of any equipment that an AI takes over (eg.\nby hacking) in the process of executing some catastrophically harmful\naction. This would create a very broad incentive to do the hard work to\nmake the world's (especially computing and bio) infrastructure as secure\nas possible.\nStrategy\n2: Global \"soft pause\" button on industrial-scale hardware\n\nIf I was convinced that we need something more \"muscular\"\nthan liability rules, this is what I would go for. The goal would be to\nhave the capability to reduce worldwide available compute by ~90-99% for\n1-2 years at a critical period, to buy more time for humanity to\nprepare. The value of 1-2 years should not be overstated: a year of\n\"wartime mode\" can easily be worth a hundred years of work under\nconditions of complacency. Ways to implement a \"pause\" have been explored, including\nconcrete proposals like requiring\nregistration and verifying\nlocation of hardware.\n\nA more advanced approach is to use clever cryptographic trickery: for\nexample, industrial-scale (but not consumer) AI hardware that gets\nproduced could be equipped with a trusted hardware chip that only allows\nit to continue running if it gets 3/3 signatures once a week from major\ninternational bodies, including at least one non-military-affiliated.\nThe signatures would be device-independent (if desired, we could even\nrequire a zero-knowledge proof that they were published on a\nblockchain), so it would be all-or-nothing: there would be no practical\nway to authorize one device to keep running without authorizing all\nother devices.\n\nThis feels like it \"checks the boxes\" in terms of maximizing benefits\nand minimizing risks:\n\n- It's a useful capability to have: if we get warning signs that\nnear-superintelligent AI is starting to do things that risk catastrophic\ndamage, we will want to take the transition more slowly.\n\n- Until such a critical moment happens, merely having the\ncapability to soft-pause would cause little harm to\ndevelopers.\n\n- Focusing on industrial-scale hardware, and only aiming for 90-99% as\na goal, would avoid some dystopian effort of putting spy chips or kill\nswitches in consumer laptops or forcing draconian measures on small\ncountries against their will.\n\n- Focusing on hardware seems very robust to changes in technology. We\nhave seen across multiple generations of AI that quality depends heavily\non available compute, especially so in the early versions of a new\nparadigm. Hence, reducing available compute by 10-100x can easily make\nthe difference between a runaway superintelligent AI winning or losing a\nfast-paced battle against humans trying to stop it.\n\n- The inherent annoyingness of needing to get online once a week for a\nsignature would create a strong pressure against extending the scheme to\nconsumer hardware.\n\n- It could be verified with random inspection, and doing it at\nhardware level would make it difficult to exempt specific users\n(approaches that are based on legally forcing shutdown, rather\nthan technically, do not have this all-or-nothing property,\nwhich makes them have much greater risk of slippery-sloping into\nexemptions for militaries etc)\n\nHardware regulation is being strongly considered already, though\ngenerally through the frame of export\ncontrols, which inherently have a \"we trust our side, but not the\nother side\" philosophy. Leopold Aschenbrenner has famously advocated\nthat the US should race\nto get a decisive advantage and then essentially\nforce China to sign a protocol limiting how many\nboxes they are allowed to run. To me, this approach seems\nrisky, and could combine the flaws of multipolar\nraces and centralization. If we have to limit people, it seems\nbetter to limit everyone on an equal footing, and do the hard work of\nactually trying to cooperate to organize that instead of one party\nseeking to dominate everyone else.\n\n## d/acc technologies in AI risk\n\nBoth of these strategies (liability and the hardware pause button)\nhave holes in them, and it's clear that they are only temporary\nstopgaps: if something becomes possible to do on a supercomputer at time\nT, it will likely be possible on a laptop at time T + 5 years. And so we\nneed something more stable to buy time for. Many d/acc\ntechnologies are relevant here. We can look at the role of d/acc tech as\nfollows: if AI takes over the world, how would it do so?\n\n- It hacks our computers \u2192 cyber-defense\n\n- It creates a super-plague \u2192 bio-defense\n\n- It convinces us (either to trust it, or to distrust each\nother) \u2192 info-defense\n\nAs briefly mentioned above, liability rules are a naturally\nd/acc-friendly style of regulation, because they can very\nefficiently motivate all parts of the world to adopt these defenses and\ntake them seriously. Taiwan has been\nexperimenting with liability for false advertising recently, which\ncan be viewed as one example of using liability to encourage info\ndefense. We should not be too enthusiastic about putting\nliability everywhere, and remember the benefits of plain old freedom in\nenabling the little guy to participate in innovation without fear of\nlawsuits, but where we do want a stronger push to be secure, liability\ncan be quite flexible and effective.\n\n## The role of crypto in d/acc\n\nMuch of d/acc goes far beyond typical blockchain topics: biosecurity,\nBCI and collaborative discourse tools seem far away from things that a\ncrypto person normally talks about. However, I think there are some\nimportant ties between crypto and d/acc, particularly:\n\n- d/acc is an extension of the underlying values of\ncrypto (decentralization, censorship resistance, open global\neconomy and society) to other areas of technology.\n\n- Because crypto users are natural early adopters, and there is an\nalignment of values, crypto communities are natural early users\nof d/acc technology. The heavy emphasis on community (both\nonline and offline, eg. events and popups), and the fact that these\ncommunities actually do high-stakes things instead of just talking to\neach other, makes crypto communities particularly appealing incubators\nand testbeds for d/acc technologies that fundamentally work on groups\nrather than individuals (eg. a large fraction of info defense and bio\ndefense). Crypto people just do things, together.\n\n- Many crypto technologies can be used in d/acc\nsubject areas: blockchains for building more robust and decentralized\nfinancial, governance and social media infrastructure, zero knowledge\nproofs for protecting privacy, etc. Today, many of the largest\nprediction markets are built on blockchains, and they are gradually\nbecoming more sophisticated, decentralized and democratic.\n\n- There are also win-win opportunities to collaborate on\ncrypto-adjacent technologies that are very useful to crypto\nprojects, but are also key to achieving d/acc goals: formal verification, computer\nsoftware and hardware\nsecurity, and adversarially-robust governance technology. These\nthings make the Ethereum blockchain, wallets and DAOs more secure and\nrobust, and they also accomplish important civilizational defense goals\nlike reducing our vulnerability to cyberattacks, including potentially\nfrom superintelligent AI.\n\n Cursive, an app that uses fully homomorphic encryption\n(FHE) to allow users to identify areas of common interest with other\nusers, while preserving privacy. This was used at Edge City, one of the\nmany offshoots of Zuzalu,\nin Chiang Mai. \n\nIn addition to these direct intersections, there is also another\ncrucial shared point of interest: funding\nmechanisms.\n\nd/acc and public goods\nfunding\n\nOne of my ongoing interests is coming up with better mechanisms to\nfund public goods: projects that are valuable to very\nlarge groups of people, but that do\nnot have a naturally accessible business model. My past work on this\nincludes my contributions to quadratic\nfunding and its use in Gitcoin\nGrants, retro\nPGF, and more recently deep\nfunding.\n\nMany people are skeptical of public goods as a concept. The\nskepticism generally comes from two sources:\n\n- The fact that public goods have historically been used as a\njustification for heavy-handed central planning and government\nintervention in society and economy.\n\n- A general perception that public goods funding lacks rigor and is\nrun on social\ndesirability bias - what sounds good, rather than what is good - and\nfavors insiders who can play the social game.\n\nThese are important critiques, and good critiques. However, I would\nargue that strong decentralized public goods funding is essential to a\nd/acc vision, because a key d/acc goal (minimizing central points of\ncontrol) inherently frustrates many traditional business models. It is\npossible to build successful businesses on open source - several\nBalvi grantees\nare doing so - but in some situations it is hard enough that important\nprojects needs extra ongoing support. Hence, we have to do the\nhard thing, and figure out how to do public goods funding in a way that\naddresses both of the above critiques.\n\nThe solution to the first problem is basically credible neutrality\nand decentralization.\nCentral planning is problematic because it gives control to elites who\nmight turn abusive, and because it often overfits\nto the present-day situation and becomes less and less effective\nover time. Quadratic funding and similar mechanisms were precisely about\nfunding public goods in a way that is as credibly neutral and\n(architecturally and politically) decentralized as possible.\n\nThe second problem is more challenging. With quadratic funding, a\ncommon critique has been that it quickly\nbecomes a popularity contest, requiring project funders to spend a\nlot of effort publicly campaigning. Furthermore, projects that are \"in\nfront of people's eyeballs\" (eg. end-user applications) get funded, but\nprojects that are more in the background (the archetypal \"dependency maintained by a guy in\nNebraska\") don't get any funding at all. Optimism retro funding\nrelies on a smaller number of expert badge holders; here, popularity\ncontest effects are diminished, but social effects of having close\npersonal ties with the badge holders are magnified.\n\nDeep funding\nis my own latest effort to solve this problem. Deep funding has two\nprimary innovations:\n\n- The dependency graph. Instead of asking each juror\na global question (\"what is the value of project A to\nhumanity?\"), we ask a local question (\"is project A or project\nB more valuable to outcome C? And by how much?\"). Humans are notoriously\nbad at global questions: in a famous study,\nwhen asked how much money they would pay to save N birds, responders\nanswered roughly $80 for N=2,000, N=20,000 and N=200,000. Local\nquestions are much more tractable. We then combine local answers into a\nglobal answer by maintaining a \"dependency graph\": for each project,\nwhat other projects contributed to its success, and how much?\n\n- AI as distilled\nhuman judgement. Jurors are only each assigned a small\nrandom sample of all questions. There is an open competition through\nwhich anyone can submit AI models that try to efficiently fill in\nall the edges in the graph. The final answer is the weighted\nsum of models that is most compatible with the jury answers. See here for a code\nexample. This approach allows the mechanism to scale to a very large\nsize, while requiring the jury to submit only a small number of \"bits\"\nof information. This reduces opportunity for corruption, and ensures\nthat each bit is high-quality: jurors can afford to think for a long\ntime on each question, instead of quickly clicking through hundreds. By\nusing an open competition of AIs, we reduce the bias from any one single\nAI training and administration process. Open market of AIs as\nthe engine, humans as the steering wheel.\n\nBut deep funding is only the latest example; there have been other\npublic goods funding mechanism ideas before, and there will be many more\nin the future. allo.expert does a\ngood job of cataloguing them. The underlying goal is to create a\nsocietal gadget that can fund public goods with a level of accuracy,\nfairness and open entry that at least approximates the way that markets\nfund private goods. It does not have to be perfect; after all,\nmarkets are far from perfect themselves. But it should be effective\nenough that developers working on top-quality open-source projects that\nbenefit everyone can afford to keep doing so without feeling the need to\nmake unacceptable compromises.\n\nToday, the leading projects in most d/acc subject areas: vaccines, BCI,\n\"borderline BCI\" like wrist\nmyoelectrics and eye tracking, anti-aging medicines, hardware, etc,\nare proprietary. This has big downsides in terms of securing public\ntrust, as we have seen in many\nof the above\nareas already.\nIt also shifts attention toward competitive dynamics (\"OUR TEAM must win\nthis critical industry!\"), and away from the larger competition of\nmaking sure these technologies come fast enough to be there to protect\nus in a world of superintelligent AI. For these reasons, robust public\ngoods funding can be a strong booster of openness and freedom. This is\nanother way in which the crypto community can help d/acc: by putting\nserious effort into exploring these funding mechanisms and making them\nwork well within its own context, preparing them for much wider adoption\nfor open-source science and technology more generally.\n\n## The future\n\nThe next few decades bring important challenges. There are two\nchallenges that have recently been on my mind:\n\n- Powerful new waves of technology, especially strong AI, are\ncoming quickly, and these technologies come with important traps that we\nneed to avoid. It may take five years for \"artificial\nsuperintelligence\" to get here, it may take fifty. Either way, it's not\nclear that the default outcome is automatically positive, and as\ndescribed in this post and the\nprevious one, there are multiple traps to avoid.\n\n- The world is becoming less cooperative. Many\npowerful actors that before seemed to at least sometimes act on\nhigh-minded principles (cosmopolitanism, freedom, common humanity... the\nlist goes on) are now more openly, and aggressively, pursuing personal\nor tribal self-interest.\n\nHowever, each of these challenges has a silver lining. First,\nwe now have very powerful tools to do our remaining work more\nquickly:\n\n- Present-day and near-future AI can be used to build other\ntechnologies, and can be used as an ingredient in governance (as in deep\nfunding or info\nfinance). It's also very relevant to BCI, which can itself provide\nfurther productivity gains.\n\n- Large-scale coordination is now possible at much greater scales than\nbefore. The internet and social media extended the reach of\ncoordination, global finance (including crypto) increased its\npower, and now info defense and collaboration tools can\nincrease its quality and perhaps soon BCI in its\nhuman-to-computer-to-human form can increase its depth.\n\n- Formal verification, sandboxing (web browsers, Docker, Qubes, GrapheneOS, and much more), secure\nhardware modules, and other technologies are improving to the point of\nmaking much better cybersecurity possible.\n\n- Writing any kind of software is significantly easier than it was two\nyears ago.\n\n- Recent basic research on understanding how viruses work, especially\nthe simple understanding that the most important form of transmission to\nguard against is airborne, is showing a much clearer path for how to\nimprove bio defense.\n\n- Recent advances in biotech (eg. CRISPR, advances\nin bio-imaging) are making\nall kinds of biotech, whether for defense, or longevity,\nor super-happiness,\nor exploring multiple\nnovel bio hypotheses,\nor simply doing\nreally cool things, much more accessible.\n\n- Advances in computing and biotech together are enabling synthetic\nbio tools you can use to adapt, monitor, and improve your health.\nCyber-defense tech such as cryptography makes the personalized dimension\nof this much more viable.\n\nSecond, now that many principles that we hold dear are no\nlonger occupied by a few particular segments of the old guard, they can\nbe reclaimed by a broad coalition that anyone in the world is welcome to\njoin. This is probably the biggest upside of recent political\n\"realignments\" around the world, and one that is worth taking advantage\nof. Crypto has already done an excellent job taking advantage of this\nand finding global appeal; d/acc can do the same.\n\nAccess to tools means that we are able to adapt and improve our\nbiologies and our environments, and the \"defense\" part of d/acc means\nthat we are able to do this without infringing on others' freedom to do\nthe same. Liberal\npluralist principles mean we can have a lot of diversity on how this\nis done, and our commitment to common humanity goals means it should get\ndone.\n\nWe, humans, continue to be the brightest star. The task ahead of us,\nof building an even brighter 21\u02e2\u1d57 century that preserves human survival\nand freedom and agency as we head toward the stars, is a challenging\none. But I am confident that we are up to it.",
    "contentLength": 39090,
    "summary": "One year after proposing d/acc (decentralized/democratic differential defensive acceleration), Vitalik reviews progress in defensive tech.",
    "detailedSummary": {
      "theme": "Vitalik reflects on the evolution of d/acc (decentralized, democratic, differential defensive acceleration) philosophy one year later, advocating for technology that enhances defense capabilities while distributing rather than concentrating power.",
      "summary": "Vitalik revisits his d/acc philosophy a year after its introduction, emphasizing the principle of accelerating technology development while focusing on defensive capabilities and decentralized power structures rather than offensive capabilities or centralized control. He expands the framework to include not just 'survival' technologies but also 'thrive' technologies, arguing that the tools and ideas that help humanity survive often overlap with those that help it flourish. The post addresses AI safety concerns by proposing regulatory approaches like liability frameworks and global hardware pause mechanisms, while maintaining that d/acc principles can help defend against AI risks through cyber-defense, bio-defense, and info-defense. Vitalik also explores the connection between crypto and d/acc values, highlighting shared principles of decentralization and censorship resistance, and discusses public goods funding mechanisms like deep funding as essential infrastructure for supporting open-source d/acc technologies.",
      "takeaways": [
        "d/acc philosophy emphasizes building technologies that favor defense over offense while maintaining decentralized rather than centralized power structures",
        "The framework now includes both 'survive' and 'thrive' dimensions, recognizing that defensive and progressive technologies often share common foundations",
        "For AI safety, Vitalik proposes liability-based regulation and global hardware pause capabilities rather than broad deceleration or centralized control",
        "Crypto communities serve as natural early adopters and testbeds for d/acc technologies due to aligned values and collaborative culture",
        "Robust public goods funding mechanisms are essential for supporting open-source d/acc development and avoiding proprietary capture of critical technologies"
      ],
      "controversial": [
        "Vitalik's proposal for hardware-level pause mechanisms requiring international coordination may be seen as technically unfeasible or politically naive",
        "The liability-based approach to AI regulation could be viewed as insufficient given short AI timelines and catastrophic risk scenarios",
        "The emphasis on decentralized solutions over government regulation may be criticized as idealistic in the face of existential AI risks"
      ]
    }
  },
  {
    "id": "general-2024-12-03-wallets",
    "title": "What I would love to see in a wallet",
    "date": "2024-12-03",
    "category": "applications",
    "url": "https://vitalik.eth.limo/general/2024/12/03/wallets.html",
    "path": "general/2024/12/03/wallets.html",
    "content": "- \n\n- \n\n  \n    \n\n  \n  \n  Dark Mode Toggle\n  \n    \n\n  \n\n##  What I would love to see in a wallet \n\n 2024 Dec 03 \nSee all posts\n\n \n \n\n What I would love to see in a wallet \n\nSpecial thanks to Liraz Siri, Yoav Weiss, and ImToken, Metamask\nand OKX developers for feedback and review.\n\nOne critical layer of the Ethereum infrastructure stack, often\nunderappreciated by core L1 researchers and developers, is the wallet.\nWallets are the window between a user and the Ethereum world, and a user\nonly benefits from any decentralization, censorship resistance,\nsecurity, privacy, or other properties that Ethereum and its\napplications offer to the extent that the wallet itself also has these\nproperties.\n\nRecently, we have seen a lot of progress on improving user\nexperience, security and functionality of Ethereum wallets. The goal of\nthis post is to give my own views of some of the properties that an\nideal Ethereum wallet would have. This is not intended to be a complete\nlist; reflecting my cypherpunk leanings, it focuses on security and\nprivacy, and it is almost certainly incomplete on the user experience\nfront. However, I would argue that wishlists are less effective for\noptimizing user experience than simply deploying and iterating based on\nfeedback, and so I think it is most valuable to focus on the security\nand privacy properties.\nUser experience of\ncross-L2 transactions\n\nThere is now an increasingly detailed roadmap for improving cross-L2\nuser experience, which has a short-term part and a long-term part. Here,\nI will talk about the short-term part: ideas which are theoretically\nimplementable even today.\n\nThe core ideas are (i) built-in cross-L2 sends, and\n(ii) chain-specific addresses and payment requests.\nYour wallet should be able to give you an address that (following the\nstyle of this draft\nERC) looks like this:\n\n0xd8dA6BF26964aF9D7eEd9e03E53415D37aA96045@optimism.eth\n\nWhen someone (or some application) gives you an address of this\nformat, you should be able to paste it into a wallet's \"to\" field, and\nclick \"send\". The wallet should automatically process that send in\nwhatever way it can:\n\n- If you already have enough coins of the needed type on the\ndestination chain, send the coins directly\n\n- If you have coins of the needed type on another chain (or\nmultiple other chains), use a protocol like ERC-7683\n(effectively a cross-chain DEX) to send the coins\n\n- If you have coins of a different type on the same or other chains,\nuse decentralized exchanges to convert them into the\nright type of coin on the right chain and send them. This should require\nexplicit permission from the user: the user would see how much of what\nthey are paying, and how much the recipient is getting.\n\n Mockup of possible wallet interface with cross-chain\naddress support \n\nThe above is for the \"you copy-paste an address (or ENS, eg. vitalik.eth@optimism.eth) for\nsomeone to pay you\" use case. If a dapp is requesting a deposit (eg. see\nthis\nPolymarket example) then the ideal flow is to extend the\nweb3 API and allow the dapp to make a chain-specific payment request.\nYour wallet would then be able to satisfy that request in whatever way\nit needs to. Making the user experience work well would also require\nstandardizing a getAvailableBalance request, and wallets would need to\nput significant thought into which chains they store users' assets on by\ndefault to maximize security and ease of transfers.\n\nChain-specific payment requests could also be put into QR codes,\nwhich mobile wallets could scan. In an in-person (or online) consumer\npayments scenario, the recipient would make a QR code or web3 API call\nthat says \"I want X units of token Y on chain\nZ, with reference ID or callback W\", and the\nwallet would be free to satisfy that request in whatever way it can.\nAnother option is a claim link protocol, where the\nuser's wallet generates a QR code or URL that contains an authorization\nto claim a certain quantity of funds from their onchain contract, and\nit's the recipient's job to figure out how to then move those funds to\ntheir own wallet.\n\nAnother related topic is gas payments. If you receive assets on an L2\nwhere you do not yet have ETH, and you need to send a transaction on\nthat L2, a wallet should be able to automatically use a protocol (eg. RIP-7755)\nto pay the gas on a chain where you do have ETH. If the wallet\nexpects you to make more transactions on that L2 in the future, it\nshould also just use a DEX to send over eg. a few million gas worth of\nETH, so that future transactions can spend gas there directly (as this\nis cheaper).\n\n## Account security\n\nOne way that I conceptualize the account security challenge is that a\ngood wallet should simultaneously be good in two areas: (i)\nprotecting the user from the wallet developer being hacked or malicious,\nand (ii) protecting the user from their own mistakes.\n\n The typo \"mistakles\" on the left was unintentional.\nHowever, upon seeing it I realized that it's perfectly appropriate for\nthe context, so I decided to keep it. \n\nMy preferred solution to this, for over\nten years,\nhas been social recovery and multisig wallets, with graded access\ncontrol. A user's account has two layers of keys: a primary key,\nand N guardians (eg. N = 5). The primary key is able to do\nlow-value and non-financial operations. A majority of the guardians is\nrequired to do either (i) high-value operations, like sending away the\nentire value in the account, or (ii) change the primary key or any of\nthe guardians. If desired, the primary key can be allowed to do\nhigh-value operations with a timelock.\n\nThe above is a basic design, and can be augmented. Session keys, and\npermissions mechanisms like ERC-7715,\ncan help support different balances between convenience and security for\ndifferent applications. More complicated guardian architectures, such as\nhaving multiple timelock durations at different thresholds, can help\nmaximize the chance of successful legitimate account recovery while\nminimizing the risk of theft.\nWho or what should the\nguardians be?\n\nFor an experienced crypto user who is inside a community of\nexperienced crypto users, a viable option is the keys of your\nfriends and family. If you ask each one to provide you with a\nfresh address, then no one needs to know who they are - in fact, your\nguardians don't even need to know who each other are. The chance that\nthey will collude without one of them tipping you off is tiny. For most\nnew users, however, this option is not available.\n\nA second option is institutional guardians: firms\nthat specialize in performing the service of only signing a transaction\nif they get some other confirmation that a request is coming from you:\neg. a confirmation code, or for high-value users a video call. People\nhave attempted to make these for a long time, eg. I\nprofiled CryptoCorp in 2013. However, so far such firms have not\nbeen very successful.\n\nA third option is multiple personal devices (eg.\nphone, desktop, hardware wallet). This can work, but also is difficult\nto set up and manage for inexperienced users. There is also the risk of\ndevices being lost or stolen at the same time, especially if they are at\nthe same location.\n\nRecently, we have started to see more wallets based on\npasskeys. Passkeys can be backed up on your devices\nonly, making them a type of personal-device solution, or backed up in\nthe cloud, making their security dependent on a complicated hybrid\nof password security, institutional and trusted hardware assumptions.\nRealistically, passkeys are a valuable security gain for ordinary users,\nbut they alone are not strong enough to protect a user's life\nsavings.\n\nFortunately, with ZK-SNARKs, we have a fourth option:\nZK-wrapped centralized ID. This genre includes zk-email, Anon Aadhaar, Myna\nWallet, and many others. Basically, you can take many forms of\n(corporate or governmental) centralized ID, and turn it into an Ethereum\naddress, which you can only send transactions from by generating a\nZK-SNARK proving possession of the centralized ID.\n\nWith this addition, we now have a wide array of options, and\nZK-wrapped centralized ID is uniquely \"noob-friendly\".\n\nFor this to work, it needs to be implemented with a streamlined and\nintegrated UI: you should be able to just specify that you want\n\"example@gmail.com\" as a guardian, and it should automatically generate\nthe corresponding zk-email Ethereum address under the hood. Advanced\nusers should be able to enter their email (along with perhaps a salt\nvalue for privacy, that would be saved in that email) into an\nopen-source third-party application, and confirm that the address\ngenerated is correct. The same should be true for any other supported\nguardian type.\n\n Mockup of possible Safe interface \n\nNote that today, one practical challenge with zk-email specifically\nis that it depends on DKIM\nsignatures, which use keys that are rotated once every few months,\nand these keys are not themselves signed by any other authority. This\nmeans that zk-email today has some level of trust requirement beyond the\nprovider themselves; this could be reduced if zk-email used TLSNotary inside trusted hardware to\nverify updated keys, but it's not ideal. Hopefully, email providers will\nstart signing their DKIM keys directly. Today, I would recommend using\nzk-email for one guardian, but not for a majority of your guardians: do\nnot store funds in a setup where zk-email breaking means that you lose\naccess to your funds.\n\n## New users and in-app wallets\n\nNew users realistically will not want to have to enter a large number\nof guardians in their first signup experience. Hence,\nwallets should offer them a very simple option. One natural route is a\n2-of-3 using zk-email on their email address, a key stored locally on\nthe user's device (which could be a passkey), and a backup key held by\nthe provider. As a user becomes more experienced, or accrues more\nassets, at some point they should be prompted to add more guardians.\n\nWallets integrated in applications are inevitable,\nbecause applications trying to appeal to non-crypto users do not want\nthe confusing user experience of asking their users to download two\nnew applications (the app itself, plus an Ethereum wallet) at the\nsame time. However, a user of many application wallets should be able to\nlink all of their wallets together, so that they only have one \"access\ncontrol thing\" to worry about. The simplest way to do this is a\nhierarchical scheme, where there is a fast \"linking\" process that allows\na user to set their primary wallet to be the guardian of all of their\nin-app wallets. The Farcaster client Warpcast supports this already:\n\n By default, your Warpcast account's recovery is\ncontrolled by the Warpcast team. However, you can \"take sovereignty\nover\" your Farcaster account, and change the recovery to your own\naddress. \n\nProtecting\nusers from scams and other external threats\n\nIn addition to account security, wallets today do a lot to\nidentify fake addresses, phishing, scams and other external threats, and\ntry to protect their users from such threats. At the same time, many of\nthe countermeasures are still quite primitive: for example, requiring a\nclickthrough to send ETH or other tokens to any new address,\nregardless of whether you're sending $100 or $100,000. Here, there is no\nsingle magic-bullet solution; it's a series of slow ongoing fixes and\nimprovements to different categories of threats. However, there is a lot\nof value in continuing to do the hard work to improve here.\n\n## Privacy\n\nNow is the time to start taking privacy on Ethereum much more\nseriously. ZK-SNARK technology is now very advanced, privacy\ntechnologies that mitigate regulatory risks without relying on\nbackdoors, such as Privacy\nPools, are growing more mature, and secondary infrastructure like Waku and ERC-4337 mempools is slowly\nbecoming more stable. However, up until now, making private transfers on\nEthereum has required users to explicitly download and use a \"privacy\nwallet\", such as Railway (or Umbra for stealth\naddresses). This adds great inconvenience and reduces the number of\npeople who are willing to make private transfers. The solution is that\nprivate transfers need to be integrated directly into\nwallets.\n\nA simple implementation is as follows. A wallet could store some\nportion of a user's assets as a \"private balance\" in a privacy pool.\nWhen a user makes a transfer, it would automatically withdraw from the\nprivacy pool first. If a user needs to receive funds, the wallet could\nautomatically generate a stealth address.\n\nAdditionally, a wallet could automatically generate a new address for\neach application that a user participates in (eg. a defi protocol).\nDeposits would come from the privacy pool, and withdrawals would go\nstraight into the privacy pool. This allows a user's activity in any one\napplication to be unlinked from their activity in other\napplications.\n\nOne advantage of this technique is that it is a natural pathway to\nnot just privacy-preserving asset transfer, but also\nprivacy-preserving identity. Identity happens onchain already:\nany application that uses proof-of-personhood gating (eg. Gitcoin\nGrants), any token-gated chat, the Ethereum\nFollow Protocol, and much more are all onchain identity. We want\nthis ecosystem to also be privacy-preserving. This means that a user's\nactivity onchain should not be collected in one place: each item should\nbe stored separately, and the user's wallet should be the only thing\nwith a \"global view\" that sees all of your attestations at the same\ntime. A natively many-accounts-per-user ecosystem helps accomplish this,\nas do offchain attestation protocols like EAS and Zupass.\n\nThis represents a pragmatic vision for Ethereum privacy in the\nmedium-term future. It can be implemented today, although there are\nfeatures that can be introduced at L1 and L2 to make privacy-preserving\ntransfers more efficient and reliable. Some privacy advocates argue that\nthe only acceptable thing is total privacy of everything: encrypting the\nentire EVM. I would argue that this may be ideal as a long-term outcome,\nbut it requires a much more fundamental rethink of programming models,\nand it's currently not at the level of maturity where it's ready to go\nand deploy across Ethereum. We do need privacy-by-default to\nget sufficiently large anonymity sets. However, focusing first on making\n(i) transfers between accounts, and (ii) identity and identity-adjacent\nuse cases like attestations private is a pragmatic first step that is\nfar easier to implement, and which wallets can get started on today.\nEthereum\nwallets need to also become data wallets\n\nOne consequence of any effective privacy solution, whether for\npayments or for identity or other use cases, is that it creates a need\nfor the user to store offchain data. This was obvious in Tornado Cash,\nwhich required users to save each individual \"note\" representing a\n0.1-100 ETH deposit. More modern privacy protocols sometimes save the\ndata encrypted onchain, and use a single private key to decrypt it. This\nis risky, because if the key is ever leaked, or if quantum computers\never become viable, the data all becomes public. Offchain attestations\nlike EAS and Zupass have an even more obvious need for offchain data\nstorage.\n\nWallets need to become not just software to store onchain\naccess permissions, but also software to store your private\ndata. This is something that the non-crypto world is\nincreasingly recognizing as well, eg. see Tim Berners-Lee's recent work in\npersonal data stores. All of the problems that we need to solve\naround robustly guaranteeing control of access permissions, we also need\nto solve around robustly guaranteeing accessibility and non-leakage of\ndata. Perhaps the solutions could be overlaid together: if you have N\nguardians, use M-of-N secret sharing between those same N guardians to\nstore your data. Data is inherently harder to secure, because you can't\nrevoke someone's share of it, but we should come up with decentralized\ncustody solutions that are as secure as we can.\n\n## Secure chain access\n\nToday, wallets trust their RPC providers to tell them any information\nabout a chain. This is a vulnerability in two ways:\n\n- The RPC provider could attempt to steal money by\nfeeding them false information, eg. about market prices\n\n- The RPC provider could extract private information\nabout what applications and other accounts a user is interacting\nwith\n\nIdeally, we want to plug both of these holes. To plug the first, we\nneed standardized light clients for L1 and L2s, which directly verify\nthe blockchain consensus. Helios already does this for\nL1, and has been doing some preliminary work to support some specific\nL2s. To properly cover all L2s, what we need is a standard by which a\nconfig contract representing an L2 (also used for\nchain-specific addresses) can declare a function, perhaps in a manner\nsimilar to ERC-3668, containing\nthe logic for obtaining recent state roots, and verifying proofs of\nstate and receipts against those state roots. This way we could have a\nuniversal light client, allowing wallets to securely verify any\nstate or events on L1 and L2.\n\nFor privacy, today the only realistic approach is to run your own\nfull node. However, now that L2s are entering the picture, running a\nfull node of everything is getting increasingly hard. The equivalent to\na light client here is private\ninformation retrieval (PIR). PIR involves a server holding a copy of\nall the data, and a client sending the server an encrypted request. The\nserver performs a computation over all the data, which returns the\nclient's desired data, encrypted to the client's key, without revealing\nto the server which piece of data the client accessed.\n\nTo keep the server honest, the individual database items would\nthemselves be Merkle branches, so the client could verify them using\ntheir light client.\n\nPIR is very computationally expensive. There are several routes\naround this problem:\n\n- Brute force: improvements in algorithms, or\nspecialized hardware, could potentially make PIR fast enough to run.\nThese techniques may depend on pre-processing: servers\ncould store encrypted-and-shuffled data for each client, and clients\ncould query that data. The main challenge in the Ethereum setting is\nadapting these techniques to datasets that change rapidly (as the state\ndoes). This makes real-time computational costs lower, but may well make\ntotal computational and storage costs higher.\n\n- Weaken the privacy requirement: for example, each\nlookup could only have 1 million \"mixins\", so the server would know a\nmillion possible values that the client could have accessed, but not any\nfiner granularity\n\n- Multi-server PIR: PIR algorithms are often much\nfaster if you use multiple servers with a 1-of-N honesty assumption\nbetween those servers.\n\n- Anonymity instead of confidentiality: instead of\nhiding the contents of the request, the request could be sent\nthrough a mixnet, hiding the sender of the request. However,\ndoing this effectively inevitably increases latency, worsening the user\nexperience.\n\nFiguring out the right combination of techniques to maximize privacy\nwhile maintaining practicality in the Ethereum context is an open\nresearch problem, and I welcome cryptographers trying their hand at\nit.\n\n## Ideal keystore wallets\n\nAside from transfers and state access, one other important workflow\nthat needs to work smoothly in a cross-L2 context is changing an\naccount's validation configuration: whether changing its keys (eg.\nrecovery), or a deeper change to the account's entire logic. Here, there\nare three tiers of solutions, in increasing order of how difficult they\nare:\n\n- Replayed updates: when a user changes their\nconfiguration, a message authorizing this change is replayed on every\nchain where the wallet detects that a user has assets. Potentially, the\nmessage format and validation rules can be made chain-independent, so it\ncan be automatically replayed on as many chains as possible.\n\n- Keystores on L1: the config information lives on\nL1, and the wallet on L2 reads it with an L1SLOAD\nor REMOTESTATICCALL.\nThis way, updating the config needs to be done only on L1, and the\nconfig becomes effective automatically.\n\n- Keystores on L2: the config information lives on\nL2, and the wallet on L2 reads it with a ZK-SNARK. This is the same as\n(2), except keystore updates can be cheaper, though on the other hand\nreads are more expensive.\n\nSolution (3) is particularly powerful because it stacks well with\nprivacy. In a normal \"privacy solution\", a user has a\nsecret s , a \"leaf value\" L is published on\nchain, and a user proves that L = hash(s, 1) and\nN = hash(s, 2) for some (never-revealed) secret that they\ncontrol. The nullifier N gets published, making\nsure that future spends of the same leaf fail, without ever revealing\nL. This depends on the user keeping s safe. A\nrecovery-friendly privacy solution would instead say:\ns is a location (eg. address and storage slot)\nonchain, and the user must prove a state query:\nL = hash(sload(s), 1) .\n\n## Dapp security\n\nThe weakest link in a user's security is often the dapp. Most of the\ntime, a user interacts with an application by going to a website, which\nimplicitly downloads the user interface code in real-time from a server\nand then executes it in-browser. If the server is hacked, or if the DNS\nis hacked, the user will get a fake copy of the interface, which could\ntrick the user into doing arbitrary things. Wallet features like\ntransaction simulations are very helpful in mitigating the risks, but\nthey are far from perfect.\n\nIdeally, we would move the ecosystem to on-chain content versioning:\na user would access a dapp via its ENS name, which would contain the\nIPFS hash of the interface. An onchain transaction from a multisig or\nDAO would be needed to update the interface. Wallets would show users if\nthey're interacting with a more-secure onchain interface, or a\nless-secure web2 interface. Wallets can also show users if they're\ninteracting with a secure chain (eg. stage 1+, multiple\nsecurity audits).\n\nFor privacy-conscious users, wallets can also add a paranoid\nmode, which requires users to clickthrough accept HTTP requests,\nand not just web3 operations:\n\n Mockup of possible interface for paranoid mode\n\nA more advanced approach would be to move beyond HTML + Javascript,\nand write the business logic of dapps in a dedicated language, perhaps a\nrelatively thin overlay over Solidity or Vyper. Browsers could then\nautomatically generate a UI for any needed functionality. OKContract is doing this already.\n\nAnother direction is cryptoeconomic info-defense:\ndapp developers, security firms, chain deployers and others can put up a\nbond that would get paid out to affected users if a dapp was hacked or\notherwise harmed users by acting in a highly misleading way, as\ndetermined by some onchain adjudication DAO. The wallet could show a\nuser a score that is based on the size of the bond.\n\n## The longer-term future\n\nThe above was all in the context of conventional interfaces, which\ninvolve pointing and clicking on things and entering things into text\nfields. However, we are also on the cusp of paradigms changing much more\ndeeply:\n\n- AI, which could lead to us moving away from a\npoint-and-click-and-type paradigm to a paradigm of \"say what you want to\ndo, and the bot figures it out\"\n\n- Brain-computer interfaces, both \"mild\" approaches\nlike eye tracking as well as more direct and even invasive techniques\n(see: first\nNeuralink patient this year)\n\n- Clients engaging in active defense: the Brave\nbrowser actively protects users\nagainst ads, trackers and many other undesirable objects. Many\nbrowsers, plugins and crypto wallets have entire teams actively working\nto protect users against all kinds of security and privacy threats.\nThese kinds of \"active guardians\" will become only more powerful in the\ncoming decade.\n\nThese three trends together will lead to much deeper rethinking of\nhow interfaces work. Through natural language input, eye tracking, or\neventually more direct BCI, together with knowledge of your history\n(perhaps including text messages, as long as all data is processed\nlocally), a \"wallet\" could get a clear intuitive idea of what you want\nto do. AI could then translate that intuition into a concrete \"action\nplan\": a series of onchain and offchain interactions that accomplish\nwhat you want. This could greatly reduce the need for third-party user\ninterfaces entirely. If a user does interact with a third-party\napplication (or another user), the AI should think adversarially on the\nuser's behalf, and identify any threats and suggest action plans for\navoiding them. Ideally, there would be an open ecosystem of these AIs,\nproduced by different groups with different biases and incentive\nstructures.\n\nThese more radical ideas depend on technology that is\nextremely immature today, and so I would not put my assets\ntoday into a wallet that relies on them. However, something like this\nseems to be pretty clearly the future, and so it's worth starting to\nmore actively explore in that direction.",
    "contentLength": 25052,
    "summary": "Vitalik wants wallets to support seamless cross-L2 transactions, social recovery with diverse guardians, and ZK-based security features.",
    "detailedSummary": {
      "theme": "Vitalik outlines his vision for an ideal Ethereum wallet that prioritizes security, privacy, cross-chain functionality, and user protection while maintaining decentralization principles.",
      "summary": "Vitalik presents a comprehensive wishlist for improving Ethereum wallets, focusing on security and privacy features that he believes are more important than user experience optimizations. He advocates for built-in cross-L2 transaction capabilities using chain-specific addresses, social recovery multisig wallets with guardian systems, and integrated privacy features using ZK-SNARKs and privacy pools. Vitalik emphasizes that wallets should become 'data wallets' that store private information securely, implement light clients for secure chain access, and protect users from dapp security vulnerabilities through on-chain content versioning and transaction simulation.",
      "takeaways": [
        "Wallets should support cross-L2 transactions with chain-specific addresses (like address@optimism.eth) and automatic routing of funds across different chains and protocols",
        "Account security should use social recovery with multiple guardians, including ZK-wrapped centralized ID options like zk-email for user-friendly guardian management",
        "Privacy features should be integrated directly into wallets through privacy pools, stealth addresses, and separate addresses per application to prevent transaction linking",
        "Wallets need to evolve into data storage systems for private information, using techniques like M-of-N secret sharing among guardians",
        "Future wallets may incorporate AI, brain-computer interfaces, and active defense mechanisms to create more intuitive and secure user experiences"
      ],
      "controversial": [
        "Vitalik's preference for ZK-wrapped centralized ID (like Gmail-based guardians) as a security solution may conflict with decentralization ideals and introduce single points of failure",
        "His dismissal of total EVM encryption as 'not ready' while advocating for partial privacy solutions may disappoint privacy maximalists who want stronger guarantees",
        "The suggestion that wallets should store private data locally creates tension between user control and the practical challenges of data backup and recovery"
      ]
    }
  },
  {
    "id": "general-2024-11-09-infofinance",
    "title": "From prediction markets to info finance",
    "date": "2024-11-09",
    "category": "applications",
    "url": "https://vitalik.eth.limo/general/2024/11/09/infofinance.html",
    "path": "general/2024/11/09/infofinance.html",
    "content": "- \n\n- \n\n  \n    \n\n  \n  \n  Dark Mode Toggle\n  \n    \n\n  \n\n##  From prediction markets to info finance \n\n 2024 Nov 09 \nSee all posts\n\n \n \n\n From prediction markets to info finance \n\nSpecial thanks to Robin Hanson and Alex Tabarrok for feedback and\nreview\n\nOne of the Ethereum applications that has always excited me the most\nare prediction markets. I wrote\nabout futarchy, a model of prediction-based governance conceived by Robin\nHanson, in 2014. I was an active user and supporter of Augur back in 2015 (look, mommy, my\nname is in the Wikipedia\narticle!). I earned $58,000 betting\non the election in 2020. And this year, I have been a close\nsupporter and follower of Polymarket.\n\nTo many people, prediction markets are about betting on elections,\nand betting on elections is gambling - nice if it helps people enjoy\nthemselves, but fundamentally not more interesting than buying random\ncoins on pump.fun. From this perspective, my interest in prediction\nmarkets may seem confusing. And so in this post I aim to explain what it\nis about the concept that excites me. In short, I believe that (i)\nprediction markets even as they exist today are a very useful\ntool for the world, but furthermore (ii) prediction\nmarkets are only one example of a much larger incredibly powerful\ncategory, with potential to create better implementations of\nsocial media, science, news, governance, and other fields. I shall label\nthis category \"info finance\".\nThe\ntwo faces of Polymarket: a betting site for the participants, a news\nsite for everyone else\n\nIn the past week, Polymarket has been a very\neffective source of information about the US election. Not only did\nPolymarket predict Trump would win with 60/40 odds while other sources\npredicted 50/50 (not\ntoo impressive by itself), it also showed other virtues: when the\nresults were coming out, while many pundits and news sources kept\nstringing viewers along with hope of some kind of favorable news for\nKamala, Polymarket showed the direct truth: Trump had a greater than 95%\nchance of victory, and a greater than 90% chance of seizing control of\nall branches of government at the same time.\n\n Two screenshots both taken at 3:40 AM EST, Nov 6\n\nBut to me this is not even the best example of why Polymarket is\ninteresting. So let us go to a different example: the\nelections in Venezuela in July. The day after the election happened,\nI remember seeing out of the corner of my eye something about people\nprotesting a highly\nmanipulated election result in Venezuela. At first, I thought\nnothing of it. I knew that Maduro was one of those \"basically a\ndictator\" figures already, and so I figured, of course he would\nfake every election outcome to keep himself in power, of course\nsome people would protest, and of course the protest would fail\n- as, unfortunately, so many others do. But then I was scrolling\nPolymarket, and I saw this:\n\nPeople were willing to put over a hundred thousand dollars on the\nline, betting that there is a 23% chance that this election\nwould be the one where Maduro would actually get struck down.\nNow I was paying attention.\n\nOf course, we know the unfortunate result of this situation.\nUltimately, Maduro did stay in power. However, the markets clued\nme in to the fact that this time, the attempt to unseat Maduro\nwas serious. There were huge protests, and the opposition played\na surprisingly well-executed strategy to prove to the world just how\nfraudulent the elections were. Had I not received the initial signal\nfrom Polymarket that \"this time, there is something to pay attention\nto\", I would not have even started paying that much attention.\n\nYou should never trust the charts entirely: if everyone\ntrusts the charts, then anyone with money can manipulate the charts and\nno one will dare to bet against them. On the other hand, trusting the\nnews entirely is also a bad idea. News has an incentive to be\nsensational, and play up the consequences of anything for clicks.\nSometimes, this is justified, sometimes it's not. If you see a\nsensational article, but then you go to the market and you see that\nprobabilities on relevant events have not changed at all, it makes sense\nto be suspicious. Alternatively, if you see an unexpectedly high or low\nprobability on the market, or an unexpectedly sudden change, that's a\nsignal to read through the news and see what might have caused it.\nConclusion: you can be more informed by reading the news\nand the charts, than by reading either one alone.\n\nLet's recap that's going on here. If you are a bettor, then\nyou can deposit to Polymarket, and for you it's a betting site. If you\nare not a bettor, then you can read the charts, and for you it's a news\nsite. You should never trust the charts entirely, but I\npersonally have already incorporated reading the charts as one step in\nmy information-gathering workflow (alongside traditional media and\nsocial media), and it has helped me become more informed more\nefficiently.\n\n## Info finance, more broadly\n\nNow, we get to the important part: predicting the election is\njust the first app. The broader concept is that you can\nuse finance as a way to align incentives in order to provide\nviewers with valuable information. Now, one natural response\nis: isn't all finance fundamentally about information?\nDifferent actors make different buy and sell decisions because of\ndifferent opinions about what will happen in the future (in addition to\npersonal needs like risk preferences and desire to hedge), and you can\nread market prices to infer a lot of knowledge about the world.\n\nTo me, info finance is that, but correct\nby construction. Similar to the concept of correct-by-construction\nin software engineering, info finance is a discipline where you\n(i) start from a fact that you want to know, and then (ii)\ndeliberately design a market to optimally elicit that information from\nmarket participants.\n\n Info finance as a three-sided market: bettors make\npredictions, readers read predictions. The market outputs predictions\nabout the future as a public good (because that's what it was designed\nto do). \n\nOne example of this is prediction markets: you want\nto know a specific fact that will take place in the future, and so you\nset up a market for people to bet on that fact. Another example is\ndecision markets: you want to know whether decision A\nor decision B will produce a better outcome according to some metric M.\nTo achieve this, you set up conditional markets: you ask people\nto bet on (i) which decision will be chosen, (ii) value of M if decision\nA is chosen, otherwise zero, (iii) value of M if decision B is chosen,\notherwise zero. Given these three variables, you can figure out if the\nmarket thinks decision A or decision B is more bullish for the value of\nM.\n\nOne technology that I expect will turbocharge info finance in\nthe next decade is AI (whether LLMs or some future technology).\nThis is because many of the most interesting applications of info\nfinance are on \"micro\" questions: millions of mini-markets for decisions\nthat individually have relatively low consequence. In practice, markets\nwith low volume often do not work effectively: it does not make sense\nfor a sophisticated participant to spend the time to make a detailed\nanalysis just for the sake of a few hundred dollars of profit, and many\nhave even argued that without subsidies such\nmarkets won't work at all because on all but the most large and\nsensational questions, there are not enough naive traders for\nsophisticated traders to take profit from. AI changes that equation\ncompletely, and means that we could potentially get reasonably\nhigh-quality info elicited even on markets with $10 of volume. Even if\nsubsidies are required, the size of the subsidy per question\nbecomes extremely affordable.\nInfo finance for\ndistilled human judgement\n\nSuppose that you have a human judgement mechanism that you trust, and\nthat has the legitimacy\nof a whole community trusting it, but which takes a long time and a high\ncost to make a judgement. However, you want access to at least an\napproximate copy of that \"costly mechanism\" cheaply and in real\ntime. Here is Robin Hanson's\nidea for what you can do: every time you need to make a decision,\nyou set up a prediction market on what outcome the costly mechanism\nwould make on the decision if it was called. You let the\nprediction market run, and put in a small amount of money to subsidize\nmarket makers.\n\n99.99% of the time, you don't actually call the costly mechanism:\nperhaps you \"revert the trades\" and give everyone back what they put in,\nor you just give everyone zero, or you see if the average price was\ncloser to 0 or 1 and treat that as the ground truth. 0.01% of the time -\nperhaps randomly, perhaps for the highest-volume markets, perhaps some\ncombination of both - you actually run the costly mechanism, and\ncompensate participants based on that.\n\nThis gives you a credibly neutral\nfast and cheap \"distilled version\" of your original highly trustworthy\nbut highly costly mechanism (using the word \"distilled\" as an analogy to\nLLM\ndistillation). Over time, this distilled mechanism roughly mirrors\nthe original mechanism's behavior - because only the participants that\nhelp it have that outcome make money, and the others lose money.\n\n Mockup of possible prediction markets + Community Notes\ncombo. \n\nThis has applications not just in social media, but also for\nDAOs. A major problem of DAOs is that there is such a large\nnumber of decisions that most people are not willing to participate in\nmost of them, leading to either widespread use of delegation, with risk\nof the same kinds of centralization and principal-agent failures we see\nin representative democracy, or vulnerability to attack. A DAO where\nactual votes only happen very rarely, and most things are decided by\nprediction markets with some combination of humans and AI predicting the\nvotes, could work well.\n\nJust as we saw in the decision markets example, info finance contains\nmany potential paths to solving important problems in decentralized\ngovernance. The key is the balance between market and\nnon-market: the market is the \"engine\", and some other non-financialized\ntrustworthy mechanism is the \"steering wheel\".\nOther use cases of info\nfinance\n\nPersonal tokens - the genre of projects such as Bitclout (now deso), friend.tech and many others that\ncreate a token for each person and make it easy to speculate on these\ntokens - are a category that I would call \"proto info-finance\". They are\ndeliberately creating market prices for specific variables - namely,\nexpectations of future prominence of a person - but the exact\ninformation being uncovered by the prices is too unspecific and subject\nto reflexivity\nand bubble dynamics. There is a possibility to create improved versions\nof such protocols, and use them to solve important problems like talent\ndiscovery, by being more careful about the economic design of a token,\nparticularly where its ultimate value comes from. Robin Hanson's idea\nof prestige futures is one possible end state here.\n\nAdvertising - the ultimate \"expensive but\ntrustworthy signal\" is whether or not you will buy a product. Info\nfinance based off of that signal could be used to help people to\nidentify what to buy.\n\nScientific peer review - there is an ongoing \"replication\ncrisis\" in science where famous results that have in some cases\nbecome part of folk wisdom end up not being reproduced at all by newer\nstudies. We can try to identify results that need re-checking with a\nprediction market. Before the re-checking is done, such a market would\nalso give readers a quick estimate of how much they should trust any\nspecific result. Experiments of this idea have been\ndone, and so far seem successful.\n\nPublic goods funding - one of the main problems with\npublic goods funding mechanisms used in Ethereum is the \"popularity\ncontest\" nature of them. Each contributor needs to run their own\nmarketing operation on social media in order to get recognized, and\ncontributors who are not well-equipped to do this, or who have\ninherently more \"background\" roles, have a hard time getting significant\namounts of money. An appealing solution to this is to try to track an\nentire dependency graph: for each positive outcome, which\nprojects contributed how much to it, and then for each of those\nprojects, which projects contributed how much to that, and so\non. The main challenge in this kind of design is figuring out the\nweights of the edges in a way that is resistant to manipulation - after\nall, such manipulation happens\nall the time already. A distilled human judgement mechanism could\npotentially help.\n\n## Conclusions\n\nThese ideas have been theorized about for a long time: the earliest\nwritings about prediction markets and even decision markets are decades\nold, and theory of finance saying similar things is even older. However,\nI would argue that the current decade presents a unique opportunity, for\nseveral key reasons:\n\n- Info finance solves trust problems that people actually\nhave. A common concern of this era is the lack of knowledge\n(and worse, lack of consensus) about whom to trust, in political,\nscientific and commercial contexts. Info finance applications could help\nbe part of the solution.\n\n- We now have scalable blockchains as the substrate.\nUp until very recently, fees were too high to actually implement most of\nthese ideas. Now, they are no longer too high.\n\n- AIs as participants. Info finance is relatively\ndifficult to make work when it must depend on humans to participate on\neach question. AIs greatly improve this situation, enabling effective\nmarkets even on small-scale questions. Many markets will likely have a\ncombination of AI and human participants, especially as volume on\nspecific questions suddenly switches from small to large.\n\nTo take full advantage of this opportunity, it's time to go beyond\njust predicting elections, and explore the rest of what info finance can\nbring us.",
    "contentLength": 13916,
    "summary": "Polymarket functions as both a betting site for participants and a news site for everyone else, exemplifying \"info finance\" - markets designed to extract valuable information.",
    "detailedSummary": {
      "theme": "Vitalik argues that prediction markets are just the beginning of a broader category called 'info finance' that can revolutionize how we gather information across social media, science, governance, and other fields.",
      "summary": "Vitalik presents prediction markets like Polymarket not just as betting platforms, but as powerful information tools that serve dual purposes - betting sites for participants and news sources for everyone else. He demonstrates this through examples like the US election and Venezuela, where market probabilities provided clearer signals than traditional media about what was actually happening. Vitalik introduces the concept of 'info finance' - deliberately designed markets that start with a fact you want to know and create optimal mechanisms to elicit that information from participants. He envisions applications ranging from 'distilled human judgment' systems that could improve social media content moderation and DAO governance, to scientific peer review, public goods funding, and advertising. The key insight is using finance as an incentive alignment tool to produce valuable information as a public good, with AI participation potentially making even small-scale markets viable.",
      "takeaways": [
        "Prediction markets function as dual-purpose platforms: betting sites for participants and information sources for non-participants",
        "Info finance is 'correct by construction' - deliberately designing markets to elicit specific information rather than just reading existing market signals",
        "'Distilled human judgment' systems could provide fast, cheap approximations of slow, expensive but trustworthy mechanisms like community voting",
        "AI participants could make info finance viable even for small-scale questions by reducing the need for sophisticated human analysis",
        "The current decade presents unique opportunities for info finance due to scalable blockchains, trust problems people actually face, and AI capabilities"
      ],
      "controversial": [
        "The claim that financial markets can provide more accurate information than traditional journalism and expert analysis",
        "The proposal to replace human decision-making in DAOs with prediction markets that only occasionally verify with actual votes",
        "The suggestion that speculation on personal tokens could be used for talent discovery and human resource allocation"
      ]
    }
  },
  {
    "id": "general-2024-10-29-futures6",
    "title": "Possible futures of the Ethereum protocol, part 6: The Splurge",
    "date": "2024-10-29",
    "category": "protocol",
    "url": "https://vitalik.eth.limo/general/2024/10/29/futures6.html",
    "path": "general/2024/10/29/futures6.html",
    "content": "- \n\n- \n\n  \n    \n\n  \n  \n  Dark Mode Toggle\n  \n    \n\n  \n\n##  Possible futures of the Ethereum protocol, part 6: The Splurge \n\n 2024 Oct 29 \nSee all posts\n\n \n \n\n Possible futures of the Ethereum protocol, part 6: The Splurge \n\nSpecial thanks to Justin Drake, Tim Beiko and Yoav Weiss for\nfeedback and review\n\nSome things are just not easy to put into a single category. There\nare lots of \"little things\" in Ethereum protocol design that are very\nvaluable for Ethereum's success, but don't fit nicely into a larger\nsub-category. In practice, about half of which has ended up being about\nEVM improvements of various kinds, and the rest is made up of various\nniche topics. This is what \"the Splurge\" is for.\n\n The Splurge, 2023 roadmap \n\n## The Splurge: key goals\n\n- Bring the EVM to a performant and stable \"endgame state\"\n\n- Bring account abstraction in-protocol, allowing all users to benefit\nfrom much more secure and convenient accounts\n\n- Optimize transaction fee economics, increasing scalability while\nreducing risks\n\n- Explore advanced cryptography that could make Ethereum far better in\nthe long run\n\n## In this chapter\n\n- EVM improvements\n\n- Account abstraction\n\n- EIP-1559 improvements\n\n- VDFs\n\n- Obfuscation and one-shot signatures: the far future of\ncryptography\n\n## EVM improvements\n\n## What problem does it solve?\n\nThe EVM today is difficult to statically analyze, making it difficult\nto create highly efficient implementations, formally verify code, and\nmake further extensions to over time. Additionally, it is highly\ninefficient, making it difficult to implement many forms of advanced\ncryptography unless they are explicitly supported through\nprecompiles.\nWhat is it, and how does it\nwork?\n\nThe first step in the current EVM improvement roadmap, scheduled to\nbe included in the next hard fork, is the EVM Object Format (EOF). EOF is\na series of EIPs\nthat specifies a new version of EVM code that has a number of distinct\nfeatures, most notably:\n\n- Separation between code (executable, but not\nreadable from the EVM) and data (readable, but not\nexecutable)\n\n- Dynamic jumps banned, static jumps only.\n\n- EVM code can no longer observe gas-related\ninformation.\n\n- A new explicit subroutine mechanism is added.\n\n Structure of EOF code \n\nOld-style contracts would continue to exist and be createable,\nalthough there is a possible path to deprecate old-style contracts (and\nperhaps even force-convert them to EOF code) eventually. New-style\ncontracts would benefit from efficiency gains created by EOF - first,\nfrom slightly smaller bytecode taking advantage of the subroutine\nfeature, and later from new EOF-specific features, or EOF-specific gas\ncost decreases.\n\nAfter EOF is introduced, it becomes easier to introduce further\nupgrades. The most well-developed today is the EVM Modular Arithmetic\nExtensions (EVM-MAX). EVM-MAX creates a new set of operations\nspecifically designed for modular arithmetic, and puts them into a new\nmemory space that cannot be accessed with other opcodes. This enables\nthe use of optimizations, such as Montgomery\nmultiplication.\n\nA newer idea is to combine EVM-MAX with a\nsingle-instruction-multiple-data (SIMD) feature. SIMD has been around as\nan idea for Ethereum for a long time starting with Greg Colvin's EIP-616.\nSIMD can be used to speed up many forms of cryptography, including hash\nfunctions, 32-bit STARKs, and lattice-based cryptography. EVM-MAX plus\nSIMD make for a natural pair of performance-oriented extensions to the\nEVM.\n\nAn approximate design for a combined EIP would be to take EIP-6690 as a\nstarting point, and then:\n\n- \nAllow (i) any odd number or (ii) any power of 2 up to 2768 as\na modulus\n\n- \n\nFor each EVMMAX opcode (add, sub,\nmul) add a version which, instead of taking 3 immediates\nx, y, z, takes 7 immediates:\nx_start, x_skip, y_start,\ny_skip, z_start, z_skip,\ncount . In python code, these opcodes would do something\nequivalent to:\n\nfor i in range(count):\n    mem[z_start + z_skip * count] = op(\n        mem[x_start + x_skip * count],\n        mem[y_start + y_skip * count]\n    )\nExcept in an actual implementation, it would be processed in parallel.\n\n- \nPotentially, add XOR, AND, OR,\nNOT and SHIFT (both cyclic and noncyclic), at\nleast for power-of-two moduli. Also add ISZERO (which\npushes the output to EVM main stack)\n\nThis would be powerful enough to implement elliptic curve\ncryptography, small-field cryptography (eg. Poseidon, circle STARKs),\nconventional hash functions (eg. SHA256, KECCAK, BLAKE), and\nlattice-based cryptography.\n\nOther EVM upgrades may also be possible, but so far they have seen\nmuch less attention.\nWhat are some links to\nexisting research?\n\n- EOF: https://evmobjectformat.org/\n\n- EVM-MAX: https://eips.ethereum.org/EIPS/eip-6690\n\n- SIMD: https://eips.ethereum.org/EIPS/eip-616\n\nWhat is left to\ndo, and what are the tradeoffs?\n\nCurrently, EOF is scheduled to be included in the next hard fork.\nWhile there is always a possibility to remove it - features have been\nlast-minute-removed from hard forks before - doing so would be an uphill\nbattle. Removing EOF would imply making any future upgrades to the EVM\nwithout EOF, which can be done but may be more difficult.\n\nThe main tradeoff in EVM is L1 complexity versus infrastructure\ncomplexity. EOF is a significant amount of code to add to EVM\nimplementations, and the static code checks are pretty complex. In\nexchange, however, we get simplifications to higher-level languages,\nsimplifications to EVM implementations, and other benefits. Arguably, a\nroadmap which prioritizes continued improvement to the Ethereum L1 would\ninclude and build on EOF.\n\nOne important piece of work to do is to implement something like\nEVM-MAX plus SIMD and benchmark how much gas various cryptographic\noperations would take.\nHow does\nit interact with other parts of the roadmap?\n\nThe L1 adjusting its EVM makes it easier for L2s to do the same. One\nadjusting without the other creates some incompatibilities, which has\nits own downsides. Additionally, EVM-MAX plus SIMD can reduce gas costs\nfor many proof systems, enabling more efficient L2s. It also makes it\neasier to remove more precompiles, by replacing them with EVM code that\ncan perform the same task perhaps without a large penalty to\nefficiency.\n\n## Account abstraction\n\n## What problem does it solve?\n\nToday, a transaction can only be verified in one way: ECDSA\nsignatures. Originally, account abstraction was meant to expand beyond\nthis, and allow an account's verification logic to be arbitrary EVM\ncode. This could enable a range of applications:\n\n- Switching to quantum-resistant cryptography\n\n- Rotating out old keys (widely understood to be a recommended\nsecurity practice)\n\n- Multisig wallets and social\nrecovery wallets\n\n- Signing with one key for low-value operations and another key (or\nset of keys) for high-value operations\n\n- Allowing privacy protocols to work without relayers, significantly\nlowering their complexity and removing a key central point of\ndependency\n\nSince account abstraction began in 2015, the goals have expanded to\nalso include a large set of \"convenience goals\", such as an account that\nhas no ETH but has some ERC20 being able to pay gas in that ERC20.\nInstead of the account abstraction roadmap just abstracting validation,\nit aims to abstract everyghing: authentication (who can perform an\naction), authorization (what can they do), replay protection, gas\npayment and execution. One summary of these goals is the following\nchart:\n\nMPC here is multi-party computation: a 40-year-old\ntechnique to split a key into multiple pieces that are stored on\nmultiple devices, and use cryptographic techniques to generate a\nsignature without combining the pieces of the key directly.\n\nEIP-7702 is an\nEIP planned to be introduced in the next hard fork. EIP-7702 is the\nresult of the growing recognition of a need to give the convenience\nbenefits of account abstraction to all users, including EOA users, to\nimprove user experience for everyone in the short term, and in a way\nthat avoids bifurcation into two ecosystems. This work started with EIP-3074, and\nculminated in EIP-7702. EIP-7702\nmakes the \"convenience features\" of account abstraction available to all\nusers, including EOAs (externally-owned\naccounts, ie. accounts controlled by ECDSA signatures),\ntoday.\n\nAs we can see from the chart, while some challenges (especially the\n\"convenience\" challenges) can be solved with incremental techniques such\nas multi-party computation or EIP-7702, the bulk of the security goals\nthat motivated the original account abstraction proposal can only be\nsolved by going back and solving the original problem: allowing smart\ncontract code to control transaction verification. The reason why this\nhas not been done so far is that implementing it safely is a\nchallenge.\nWhat is it, and how does it\nwork?\n\nAt the core, account abstraction is simple: allow transactions to be\ninitiated by smart contracts, and not just EOAs. The entire complexity\ncomes from doing this in a way that is friendly to maintaining a\ndecentralized network and protecting against denial of service\nattacks.\n\nOne illustrative example of a key challenge is the multi-invalidation\nproblem:\n\nIf there are 1000 accounts whose validation function all depends on\nsome single value S , and there are transactions in the\nmempool that are valid given the current value of S , then\none single transaction flipping the value of S could\ninvalidate all of the other transactions in the mempool. This allows for\nan attacker to spam the mempool, clogging up the resources of nodes on\nthe network, at a very low cost.\n\nYears of effort trying to expand functionality while limiting DoS\nrisks have led to convergence on one solution for how to implement\n\"ideal account abstraction\": ERC-4337.\n\nERC-4337 works by dividing processing of user operations into two\nphases: validation and execution. All\nvalidations are processed first, and all executions are processed\nsecond. In the mempool, a user operation is only accepted if its\nvalidation phase only touches its own account (plus a few special-case\nextensions, see \"associated storage\" in ERC-7562), and does\nnot read environmental variables. This prevents multi-invalidation\nattacks. A strict gas limit on the validation step is also enforced.\n\nERC-4337 was designed as an extra-protocol standard (an ERC), because\nat the time the Ethereum client developers were focused on the Merge,\nand did not have any spare capacity to work on other features. This is\nwhy ERC-4337 uses its own object called user operations, instead of\nregular transactions. More recently, however, we have been realizing\nthat there is a need to enshrine at least parts of it in the protocol.\nTwo key reasons are:\n\n- The inherent inefficiencies of the EntryPoint being a contract: a\nflat ~100k gas overhead per bundle and thousands extra per user\noperation\n\n- The need to make sure Ethereum properties such as inclusion\nguarantees created by inclusion lists carry over to account abstraction\nusers.\n\nAdditionally, ERC-4337 has been extended by two features:\n\n- Paymasters: a feature that allows an account to pay\nfees on behalf of another account. This violates the rule that only the\nsender account itself can be accessed during the validation phase, so\nspecial handling is introduced to allow the paymaster mechanism and\nensure that it is safe.\n\n- Aggregators: a feature that supports signature\naggregation, such as BLS aggregation or SNARK-based aggregation. This is\nneeded to enable the highest level of data efficiency on rollups.\n\nWhat are some links\nto existing research?\n\n- Presentation on history of account abstraction: https://www.youtube.com/watch?v=iLf8qpOmxQc\n\n- ERC-4337: https://eips.ethereum.org/EIPS/eip-4337\n\n- EIP-7702: https://eips.ethereum.org/EIPS/eip-7702\n\n- BLSWallet code (uses aggregation feature): https://github.com/getwax/bls-wallet\n\n- ERC-7562 (enshrined account abstraction mempool rules): https://eips.ethereum.org/EIPS/eip-7562\n\n- EIP-7701 (EOF-based enshrined AA): https://eips.ethereum.org/EIPS/eip-7701\n\nWhat is left to\ndo, and what are the tradeoffs?\n\nThe main remaining thing to figure out is how to fully bring account\nabstraction into the protocol. A recently popular enshrined account\nabstraction EIP is EIP-7701, which\nimplements account abstraction on top of EOF. An account can have a\nseparate code section for validation, and if an account has that code\nsection set, that is the code gets executed during the validation step\nof a transaction from that account.\n\n EOF code structure for an EIP-7701 account\n\nWhat is fascinating about this approach is that it makes it clear\nthat there are two equivalent ways to view native account\nabstraction:\n\n- EIP-4337, but as part of the protocol\n\n- A new type of EOA, where the signature algorithm is EVM code\nexecution\n\nIf we start with strict bounds on the complexity of code that can be\nexecuted during validation - allowing no external state access, and even\nat first setting a gas limit too low to be useful for quantum-resistant\nor privacy-preserving applications - then the safety of this approach is\nvery clear: it's just swapping out ECDSA verification for an EVM code\nexecution that takes a similar amount of time. However, over time we\nwould need to loosen these bounds, because allowing\nprivacy-preserving applications to work without relayers, and quantum\nresistance, are both very important. And in order to do this,\nwe do need to find ways to address the DoS risks in a more flexible way,\nwithout requiring the validation step to be ultra-minimalistic.\n\nThe main tradeoff seems to be \"enshrine something that fewer people\nare happy with, sooner\" versus \"wait longer, and perhaps get a more\nideal solution\". The ideal approach will likely be some hybrid approach.\nOne hybrid approach is to enshrine some use cases more quickly, and\nleave more time to figure out others. Another is to deploy more\nambitious versions of account abstraction on L2s first. However, this\nhas the challenge that for an L2 team to be willing to do the work to\nadopt a proposal, they need to be confident that L1 and/or other L2s\nwill adopt something compatible later on.\n\nAnother application that we need to think about explicitly is\nkeystore\naccounts, which store account-related state on either L1 or\na dedicated L2, but can be used both L1 and any compatible L2. Doing\nthis effectively likely requires L2s to support opcodes such as L1SLOAD\nor REMOTESTATICCALL,\nthough it also requires account abstraction implementations on L2 to\nsupport it.\nHow does\nit interact with other parts of the roadmap?\n\nInclusion lists need to support account abstracted transactions. In\npractice, the needs of inclusion lists and the needs of decentralized\nmempools end up being pretty similar, though there is slightly more\nflexibility for inclusion lists. Additionally, account abstraction\nimplementations should ideally be harmonized on L1 and L2 as much as\npossible. If, in the future, we expect most users to be using keystore\nrollups, the account abstraction designs should be built with this in\nmind. Gas payment abstraction should also be designed with cross-chain\nuse cases in mind (see eg. RIP-7755).\n\n## EIP-1559 improvements\n\n## What problem does it solve?\n\nEIP-1559\nactivated on Ethereum in 2021, and led to significant improvements in\naverage block inclusion time.\n\nHowever, the current implementation of EIP-1559 is imperfect in\nseveral ways:\n\n- The formula is slightly flawed: instead of targeting 50% blocks it\ntargets ~50-53% full blocks depending on variance (this has to do with\nwhat mathematicians call the \"AM-GM\ninequality\")\n\n- It doesn't\nadjust fast enough in extreme conditions.\n\nThe formula later used for blobs (EIP-4844) was explicitly designed to\naddress the first concern, and is overall cleaner. Neither EIP-1559\nitself, nor EIP-4844, attempt to address the second problem. As a\nresult, the status quo is a confusing halfway state involving two\ndifferent mechanisms, and there is even a case that over time both will\nneed to be improved.\n\nIn addition to this, there are other weaknesses of Ethereum resource\npricing that are independent of EIP-1559, but which could be solved by\ntweaks to EIP-1559. A major one is average case vs worst case\ndiscrepancies: resource prices in Ethereum have to be set to be\nable to handle the worst case, where a block's entire gas consumption\ntakes up one resource, but average-case use is much less than this,\nleading to inefficiencies.\n\nWhat is it, and how does it\nwork?\n\nA solution to these inefficiencies is multidimensional\ngas: having separate prices and limits for separate resources. This\nconcept is technically independent from EIP-1559, but EIP-1559 makes it\neasier: without EIP-1559, optimally packing a block with multiple\nresource constraints is a complicated multidimensional\nknapsack problem. With EIP-1559, most blocks are not at full\ncapacity on any resource, and so the simple algorithm of \"accept\nanything that pays a sufficient fee\" suffices.\n\nWe have multidimensional gas for execution and blobs today; in\nprinciple, we could increase this to more dimensions:\ncalldata, state reads/writes, and\nstate size expansion.\n\nEIP-7706\nintroduces a new gas dimension for calldata. At the same time, it\nstreamlines the multidimensional gas mechanism by making all three types\nof gas fall under one (EIP-4844-style) framework, thus also solving the\nmathematical flaws with EIP-1559.\n\nEIP-7623 is a\nmore surgical solution to the average case vs worst case resource\nproblem that more strictly bounds max calldata without introducing a\nwhole new dimension.\n\nA further direction to go would be to tackle the update rate problem,\nand find a basefee calculation algorithm that is faster, and at the same\ntime preserves the key invariants introduced by the EIP-4844 mechanism\n(namely: in the long run average usage approaches exactly the\ntarget).\nWhat are some links\nto existing research?\n\n- EIP-1559 FAQ: https://notes.ethereum.org/@vbuterin/eip-1559-faq\n\n- Empirical analysis on EIP-1559: https://dl.acm.org/doi/10.1145/3548606.3559341\n\n- Proposed improvements to allow rapid adjustment: https://kclpure.kcl.ac.uk/ws/portalfiles/portal/180741021/Transaction_Fees_on_a_Honeymoon_Ethereums_EIP_1559_One_Month_Later.pdf\n\n- EIP-4844 FAQ, section on the basefee mechanism: https://notes.ethereum.org/@vbuterin/proto_danksharding_faq#How-does-the-exponential-EIP-1559-blob-fee-adjustment-mechanism-work\n\n- EIP-7706: https://eips.ethereum.org/EIPS/eip-7706\n\n- EIP-7623: https://eips.ethereum.org/EIPS/eip-7623\n\n- Multidimensional gas: https://vitalik.eth.limo/general/2024/05/09/multidim.html\n\nWhat is left to\ndo, and what are the tradeoffs?\n\nMultidimensional gas has two primary tradeoffs:\n\n- It adds complexity to the protocol\n\n- It adds complexity to the optimal algorithm needed to fill a block\nto capacity\n\nProtocol complexity is a relatively small issue for calldata, but\nbecomes a larger issue for gas dimensions that are \"inside the EVM\",\nsuch as storage reads and writes. The problem is that it's not just\nusers that set gas limits: it's also contracts that set limits when they\ncall other contracts. And today, the only way they have to set limits is\none-dimensional.\n\nOne easy way to eliminate this problem is to make multidimensional\ngas only available inside EOF, because EOF does not allow contracts to\nset gas limits in calls to other contracts. Non-EOF contracts would have\nto pay a fee in all types of gas when making a storage operation (eg. if\nan SLOAD costs 0.03% of a block's storage access gas limit,\nthe non-EOF user would also be charged 0.03% of the execution gas\nlimit)\n\nMore research on multidimensional gas would be very helpful in\nunderstanding the tradeoffs and figuring out the ideal balance.\nHow does\nit interact with other parts of the roadmap?\n\nA successful implementation of multidimensional gas can greatly\nreduce certain \"worst-case\" resource usages, and thus reduce pressure on\nthe need to optimize performance in order to support eg. STARKed\nhash-based binary trees. Having a hard target for state size growth\nwould make it much easier for client developers to plan and estimate\ntheir requirements going forward into the future.\n\nAs described above, EOF makes more extreme versions of\nmultidimensional gas significantly easier to implement due to its gas\nnon-observability properties.\n\nVerifiable delay functions\n(VDFs)\n\n## What problem does it solve?\n\nToday, Ethereum uses RANDAO-based\nrandomness to choose proposers. RANDAO-based randomness works by\nasking each proposer to reveal a secret that they committed to ahead of\ntime, and mixing each revealed secret into the randomness. Each proposer\nthus has \"1 bit of manipulation\": they can change the randomness (at a\ncost) by not showing up. This is reasonably okay for finding proposers,\nbecause it's very rare that you can give yourself two new proposal\nopportunities by giving up one. But it's not okay for on-chain\napplications that need randomness. Ideally, we would find a more robust\nsource of randomness.\nWhat is it, and how does it\nwork?\n\nVerifiable delay\nfunctions are a type of function that can only be computed\nsequentially, with no speedups from parallelization. A simple example is\nrepeated hashing: compute\nfor i in range(10**9): x = hash(x). The output, proven with\na SNARK proof of correctness, could be used as a random value. The idea\nis that the input is selected based on information available at time T,\nand the output is not yet known at time T: it only becomes available\nsome time after T, once someone fully runs the computation. Because\nanyone can run the computation, there is no possibility to withhold the\nresult, and so there is no ability to manipulate the outcome.\n\nThe main risk to a verifiable delay function is unexpected\noptimization: someone figures out how to run the function much\nfaster than expected, allowing them to manipulate the information they\nreveal at time T based on the future output. Unexpected optimization can\nhappen in two ways:\n\n- Hardware acceleration: someone makes an ASIC that\nruns the computation loop much faster than existing hardware.\n\n- Unexpected parallelization: someone finds a way to\nrun the function faster by parallelizing it, even if doing so requires\n100x more resources.\n\nThe tasks of creating a successful VDF is to avoid these two issues,\nwhile at the same time keeping efficiency practical (eg. one problem\nwith the hash-based approach is that SNARK-proving over hashing in real\ntime has heavy hardware requirements). Hardware acceleration is\ntypically solved by having a public-good actor create and distribute\nreasonably-close-to-optimal ASICs for the VDF by itself.\nWhat are some links\nto existing research?\n\n- vdfresearch.org: https://vdfresearch.org/\n\n- Thinking on attacks against VDFs used in Ethereum, 2018: https://ethresear.ch/t/verifiable-delay-functions-and-attacks/2365\n\n- Attacks against MinRoot, a proposed VDF: https://inria.hal.science/hal-04320126/file/minrootanalysis2023.pdf\n\nWhat is left to\ndo, and what are the tradeoffs?\n\nCurrently, there is no VDF construction that fully satisfies Ethereum\nresearchers on all axes. More work is left to find such a function. If\nwe have it, the main tradeoff is simply whether or not to include it: a\nsimple tradeoff of functionality versus protocol complexity and risk to\nsecurity. If we think a VDF is secure, but it ends up being insecure,\nthen depending on how it's implemented security degrades to either the\nRANDAO assumption (1 bit of manipulation per attacker) or something\nslightly worse. Hence, even a broken VDF would not break the protocol,\nthough it would break applications or any new protocol features that\nstrongly depend on it.\nHow does\nit interact with other parts of the roadmap?\n\nThe VDF is a relatively self-contained ingredient of the Ethereum\nprotocol, though in addition to increasing the security of proposer\nselection it also has uses in (i) onchain applications that depend on\nrandomness, and potentially (ii) encrypted mempools, though making\nencrypted mempools based on a VDF still depends on additional\ncryptographic discoveries which have not yet happened.\n\nOne point to keep in mind is that given uncertainty in hardware,\nthere will be some \"slack\" between when a VDF output is produced and\nwhen it becomes needed. This means that information will be accessible a\nfew blocks ahead. This can be an acceptable cost, but should be taken\ninto account in eg. single-slot finality or committee selection\ndesigns.\n\nObfuscation\nand one-shot signatures: the far future of cryptography\n\n## What problem does it solve?\n\nOne of Nick Szabo's most famous posts is a 1997 essay on \"God\nprotocols\". In this essay, he points out that often, multi-party\napplications depend on a \"trusted third party\" to manage the\ninteraction. The role of cryptography, in his view, is to create a\nsimulated trusted third party that does the same job, without actually\nrequiring any trust in any specific actor.\n\n \"Mathematically trustworthy protocol\", diagram by Nick\nSzabo \n\nSo far, we have only been able to partially approach this ideal. If\nall we need is a transparent virtual computer, where the data\nand computation cannot be shut down, censored or tampered with, but\nprivacy is not a goal, then blockchains can do it, though with limited\nscalability. If privacy is a goal, then up until recently we\nhave only been able to make a few specific protocols for specific\napplications: digital signatures for basic authentication, ring signatures\nand linkable\nring signatures for primitive forms of anonymity, identity-based\nencryption to enable more convenient encryption under specific\nassumptions about a trusted issuer, blind\nsignatures for Chaumian e-cash, and so\non. This approach requires lots of work for every new application.\n\nIn the 2010s, we saw the first glimpse of a different, and more\npowerful approach, based on programmable\ncryptography. Instead of creating a new protocol for each new\napplication, we could use powerful new protocols - specifically,\nZK-SNARKs - to add cryptographic guarantees to\narbitrary programs. ZK-SNARKs allow a user to prove any\narbitrary statement about data that they hold, in a way that the\nproof (i) is easy to verify, and (ii) does not leak any data other than\nthe statement itself. This was a huge step forward for privacy and\nscalability at the same time, that I have likened to the effect\nof transformers\nin AI. Thousands of man-years of application-specific work were\nsuddenly swept away by a general-purpose solution that you can just plug\nin to solve a surprisingly wide range of problems.\n\nBut ZK-SNARKs are only the first in a trio of similar extremely\npowerful general-purpose primitives. These protocols are so powerful\nthat when I think of them, they remind me of a set of extremely powerful\ncards in Yu-Gi-Oh, a card game and a TV show that I used to play and\nwatch when I was a young child: the Egyptian god\ncards. The Egyptian god cards are a trio of extremely powerful\ncards, which according to legend are potentially deadly to manufacture,\nand are so powerful that they are not allowed in duels. Similarly, in\ncryptography, we have the trio of Egyptian god protocols:\n\nWhat is it, and how does it\nwork?\n\nZK-SNARKs are one of these three protocols that we\nalready have, to a high level of maturity. After large improvements\nto prover speed and developer-friendliness in the last five years,\nZK-SNARKs have become the bedrock of Ethereum's scalability and privacy\nstrategy. But ZK-SNARKs have an important limitation: you need to know\nthe data to make proofs about it. Each piece of state in a ZK-SNARK\napplication must have a single \"owner\", who must be around to approve\nany reads or writes to it.\n\nThe second protocol, which does not have this limitation, is fully\nhomomorphic encryption (FHE). FHE lets you\ndo any computation on encrypted data without seeing the\ndata. This lets you do computations on a user's data for the\nuser's benefit while keeping the data and the algorithm private. It also\nlets you extend voting systems such as\nMACI to have almost-perfect security and privacy guarantees. FHE was\nfor a long time considered too inefficient for practical use, but now\nit's finally becoming efficient enough that we are starting to see\napplications.\n\n Cursive, an\napplication that uses two-party computation and FHE to do\nprivacy-preserving discovery of common interests. \n\nBut FHE too has its limits: any FHE-based technology still requires\nsomeone to hold the decryption key. This could be a M-of-N\ndistributed setup, and you can even use TEEs to add a second layer of\ndefense, but it's still a limitation.\n\nThis gets us to the third protocol, which is more powerful than the\nother two combined: indistinguishability\nobfuscation. While it's still\nvery far from maturity, as of 2020 we have\ntheoretically valid protocols for it based on standard security\nassumptions, and work is recently\nstarting on implementations. Indistinguishability obfuscation lets\nyou create an \"encrypted program\" that performs an arbitrary\ncomputation, in such a way that all internal details of the program are\nhidden. As a simple example, you can put a private key into an\nobfuscated program which only lets you use it to sign prime numbers, and\ndistribute this program to other people. They can use the program to\nsign any prime number, but cannot take the key out. But it's far more\npowerful than that: together with hashes, it can be used to implement\nany other cryptographic primitive, and more.\n\nThe only thing that an obfuscated program can't do, is prevent itself\nfrom being copied. But for that, there is something even more powerful\non the horizon, though it depends on everyone having quantum computers:\nquantum one-shot\nsignatures.\n\nWith obfuscation and one-shot signatures together, we can build\nalmost perfect trustless third parties. The only thing we can't do with\ncryptography alone, and that we would still need a blockchain for, is\nguaranteeing censorship resistance. These technologies would allow us to\nnot only make Ethereum itself much more secure, but also build much more\npowerful applications on top of it.\n\nTo see how each of these primitives adds additional power, let us go\nthrough a key example: voting. Voting is a fascinating\nproblem because it has so many tricky security properties that need to\nbe satisfied, including very strong forms of both verifiability and\nprivacy. While voting protocols with strong security properties have existed for decades, let\nus make the problem harder for ourselves by saying that we want a design\nthat can handle arbitrary voting protocols: quadratic\nvoting, pairwise-bounded\nquadratic funding, cluster-matching\nquadratic funding, and so on. That is, we want the \"tallying\" step\nto be an arbitrary program.\n\n- First, suppose we put votes publicly on a\nblockchain. This gets us public\nverifiability (anyone can verify that the final outcome is\ncorrect, including tallying rules and eligibility rules) and\ncensorship resistance (can't stop people from voting).\nBut we have no privacy.\n\n- Then, we add ZK-SNARKs. Now, we have\nprivacy: each vote is anonymous, while ensuring that\nonly authorized voters can vote, and every voter can only vote\nonce.\n\n- Now, we add the MACI mechanism.\nVotes are encrypted to a central server's decryption key. The central\nserver is required to run the tallying process, including throwing out\nduplicate votes, and it publishes a ZK-SNARK proving the answer. This\nkeeps the previous guarantees (even if the server is cheating!), but if\nthe server is honest it adds a coercion-resistance\nguarantee: a user can't prove how they voted, even if they want to. This\nis because while a user can prove the vote that they made, they have no\nway to prove that they did not make another vote that cancels it out.\nThis prevents bribery and other attacks.\n\n- We run the tallying inside FHE, and then have an\nN/2-of-N threshold-decryption\ncomputation decrypt it. This makes the coercion-resistance\nguarantee N/2-of-N,\ninstead of 1-of-1.\n\n- We make the tallying program obfuscated, and we\ndesign the obfuscated program so that it can only give an output if\ngiven permission to do so, either by a proof of blockchain consensus, or\nby some quantity of proof of work, or both. This makes the\ncoercion-resistance guarantee almost perfect: in the blockchain\nconsensus case, you would need 51% of validators to collude to break it,\nand in the proof of work case, even if everyone colludes, re-running the\ntally with different subsets of voters to try to extract the behavior of\na single voter would be extremely expensive. We can even make the\nprogram make a small random adjustment to the final tally, to make it\neven harder to extract the behavior of an individual voter.\n\n- We add one-shot signatures, a primitive that\ndepends on quantum computing that allows signatures that can only be\nused to sign a message of a certain type once. This makes the\ncoercion-resistance guarantee truly perfect.\n\nIndistinguishability obfuscation also allows for other powerful\napplications. For example:\n\n- DAOs, on-chain auctions, and other applications with\narbitrary internal secret state.\n\n- A truly universal trusted\nsetup: someone can create an obfuscated program that\ncontains a key, and can run any program and provide the output, putting\nhash(key, program) in as an input into the program. Given\nsuch a program, anyone can also put the program into itself, combining\nthe program's pre-existing key with their own key, and in doing so\nextend the setup. This can be used to generate a 1-of-N trusted setup\nfor any protocol.\n\n- ZK-SNARKs whose verification is just a signature.\nImplementing this is simple: have a trusted setup where someone creates\nan obfuscated program that only signs a message with a key if it's a\nvalid ZK-SNARK.\n\n- Encrypted mempools. It becomes trivially easy to\nencrypt transactions in such a way that they only get decrypted when\nsome onchain event in the future happens. This could even include the\nsuccessful execution of a VDF.\n\nWith one-shot signatures, we can make blockchains immune to\nfinality-reverting 51% attacks, though censorship attacks continue to be\npossible. Primitives similar to one-shot signatures enable quantum money,\nsolving the double-spend problem without a blockchain, though many more\ncomplex applications would still require a chain.\n\nIf these primitives can be made efficient enough, then most\napplications in the world can be made decentralized. The main bottleneck\nwould be verifying the correctness of implementations.\nWhat are some links\nto existing research?\n\n- Indistinguishability obfuscation protocol from 2021: https://eprint.iacr.org/2021/1334.pdf\n\n- How obfuscation can help Ethereum: https://ethresear.ch/t/how-obfuscation-can-help-ethereum/7380\n\n- First known construction of one-shot signatures: https://eprint.iacr.org/2020/107.pdf\n\n- Attempted implementation of obfuscation (1): https://mediatum.ub.tum.de/doc/1246288/1246288.pdf\n\n- Attempted implementation of obfuscation (2): https://github.com/SoraSuegami/iOMaker/tree/main\n\nWhat is left to\ndo, and what are the tradeoffs?\n\nThere is a heck of a lot left to do.\nIndistinguishability obfuscation is incredibly immature, and candidate\nconstructions are millions of times too slow (if not more) to be usable\nin applications. Indistinguishability obfuscation is famous for having\nruntimes that are \"theoretically\" polynomial-time, but take longer than\nthe lifetime of the universe to run in practice. More recent protocols\nhave made runtimes less extreme, but the overhead is still far too high\nfor regular use: one implementer expects a runtime of one year.\n\nQuantum computers do not even exist: all constructions you might read\nabout on the internet today are either prototypes not capable of doing\nany computation larger than 4 bits, or are not real quantum computers,\nin the sense that while they may have quantum parts in them, they cannot\nrun actually-meaningful computations like Shor's\nalgorithm or Grover's\nalgorithm. Recently, there have been signs that \"real\" quantum\ncomputers are no longer\nthat far away. However, even if \"real\" quantum computers come soon,\nthe day when regular people have quantum computers on their laptops or\nphones may well be decades after the day when powerful institutions get\none that can crack elliptic curve cryptography.\n\nFor indistinguishability obfuscation, one key tradeoff is in security\nassumptions. There are more aggressive designs that use\nexotic assumptions.\nThese often have more realistic runtimes, but the exotic assumptions\nsometimes end up\nbroken. Over time, we may end up understanding lattices enough to\nmake assumptions that do not get broken. However, this path is more\nrisky. The more conservative path is to insist on protocols whose\nsecurity provably reduces to \"standard\" assumptions, but this may mean\nthat it takes much longer until we get protocols that run fast\nenough.\nHow does\nit interact with other parts of the roadmap?\n\nExtremely powerful cryptography could change the game completely. For\nexample:\n\n- If we get ZK-SNARKs which are as easy to verify as a signature, we\nmay not need any aggregation protocols; we can just verify onchain\ndirectly.\n\n- One-shot signatures could imply much more secure proof-of-stake\nprotocols.\n\n- Many complicated privacy protocols could be replaced with \"just\"\nhaving a privacy-preserving EVM.\n\n- Encrypted mempools become much easier to implement.\n\nAt first, the benefits will come on the application layer, because\nthe Ethereum L1 inherently needs to be conservative on security\nassumptions. However, even application-layer use alone could be\ngame-changing, by as much as the advent of ZK-SNARKs has been.",
    "contentLength": 37674,
    "summary": "The Splurge focuses on bringing Ethereum's EVM to its \"endgame state\" with EOF improvements, account abstraction via EIP-7702, and advanced crypto.",
    "detailedSummary": {
      "theme": "Vitalik outlines The Splurge roadmap focusing on EVM improvements, account abstraction, transaction fee optimization, and advanced cryptographic primitives that don't fit neatly into other Ethereum upgrade categories.",
      "summary": "In this comprehensive roadmap post, Vitalik details The Splurge's four main goals: bringing the EVM to a stable endgame state, implementing native account abstraction, optimizing transaction fee economics, and exploring advanced cryptography. The EVM improvements center around the EVM Object Format (EOF) scheduled for the next hard fork, which separates code from data and enables static analysis, followed by EVM-MAX for modular arithmetic and SIMD capabilities. Account abstraction aims to move beyond ECDSA-only transaction validation through EIP-7702 for immediate convenience features and eventual full protocol integration via EIP-7701, allowing smart contract code to control transaction verification while managing DoS risks through ERC-4337's validation/execution separation model. EIP-1559 improvements focus on multidimensional gas pricing through proposals like EIP-7706 to better handle resource constraints and eliminate average-case versus worst-case inefficiencies. Perhaps most ambitiously, Vitalik explores advanced cryptographic primitives including verifiable delay functions for better randomness, and the theoretical potential of indistinguishability obfuscation and quantum one-shot signatures as 'Egyptian god protocols' that could enable truly trustless applications with perfect privacy and security guarantees, though these remain far from practical implementation.",
      "takeaways": [
        "EOF (EVM Object Format) is scheduled for the next hard fork and will enable static analysis, separate code from data, and pave the way for EVM-MAX and SIMD performance improvements",
        "Account abstraction will be implemented incrementally, starting with EIP-7702 for convenience features, then moving toward full protocol integration through EIP-7701 built on EOF",
        "Multidimensional gas pricing could significantly improve Ethereum's resource utilization by having separate limits and prices for execution, calldata, storage operations, and state expansion",
        "Advanced cryptographic primitives like indistinguishability obfuscation and quantum one-shot signatures could eventually enable almost perfect trustless applications, though they remain years or decades from practical implementation",
        "The Splurge represents necessary protocol improvements that don't fit other categories but are crucial for Ethereum's long-term success, balancing complexity additions with functionality gains"
      ],
      "controversial": [
        "The decision to proceed with EOF despite its significant complexity and the possibility of last-minute removal from hard forks, with Vitalik noting it would be an 'uphill battle' to remove it",
        "The tradeoff between enshrining account abstraction features quickly versus waiting for more ideal solutions, particularly around loosening validation bounds to support quantum-resistant and privacy-preserving applications",
        "The emphasis on highly speculative and immature cryptographic primitives like indistinguishability obfuscation, which currently have runtimes measured in years and may never become practical"
      ]
    }
  },
  {
    "id": "general-2024-10-26-futures5",
    "title": "Possible futures of the Ethereum protocol, part 5: The Purge",
    "date": "2024-10-26",
    "category": "protocol",
    "url": "https://vitalik.eth.limo/general/2024/10/26/futures5.html",
    "path": "general/2024/10/26/futures5.html",
    "content": "- \n\n- \n\n  \n    \n\n  \n  \n  Dark Mode Toggle\n  \n    \n\n  \n\n##  Possible futures of the Ethereum protocol, part 5: The Purge \n\n 2024 Oct 26 \nSee all posts\n\n \n \n\n Possible futures of the Ethereum protocol, part 5: The Purge \n\nSpecial thanks to Justin Drake, Tim Beiko, Matt Garnett, Piper\nMerriam, Marius van der Wijden and Tomasz Stanczak for feedback and\nreview\n\nOne of Ethereum's challenges is that by default, any blockchain\nprotocol's bloat and complexity grows over time. This happens in two\nplaces:\n\n- Historical data: any transaction made and any\naccount created at any point in history needs to be stored by all\nclients forever, and downloaded by any new clients making a full sync to\nthe network. This causes client load and sync time to keep increasing\nover time, even as the chain's capacity remains the same.\n\n- Protocol features: it's much easier to add a new\nfeature than to remove an old one, causing code complexity to increase\nover time.\n\nFor Ethereum to sustain itself into the long term, we need a strong\ncounter-pressure against both of these trends, reducing\ncomplexity and bloat over time. But at the same time, we need\nto preserve one of the key properties that make blockchains\ngreat: their permanence. You can put an NFT, a love note in\ntransaction calldata, or a smart contract containing a million dollars\nonchain, go into a cave for ten years, come out and find it still there\nwaiting for you to read and interact with. For dapps to feel comfortable\ngoing fully decentralized and removing their upgrade keys, they need to\nbe confident that their dependencies are not going to upgrade\nin a way that breaks them - especially the L1 itself.\n\n The Purge, 2023 roadmap. \n\nBalancing between these two needs, and minimizing or reversing bloat,\ncomplexity and decay while preserving continuity, is absolutely possible\nif we put our minds to it. Living organisms can do it: while most age\nover time, a lucky few\ndo not. Even social systems can have\nextreme longevity. On a few occasions, Ethereum has already shown\nsuccesses: proof of work is gone, the SELFDESTRUCT opcode\nis mostly gone, and beacon chain nodes already store old data up to only\nsix months. Figuring out this path for Ethereum in a more generalized\nway, and moving toward an eventual outcome that is stable for the long\nterm, is the ultimate challenge of Ethereum's long term scalability,\ntechnical sustainability and even security.\n\n## The Purge: key goals\n\n- Reducing client storage requirements by reducing or\nremoving the need for every node to permanently store all history, and\nperhaps eventually even state\n\n- Reducing protocol complexity by eliminating\nunneeded features\n\n## In this chapter\n\n- History expiry\n\n- State expiry\n\n- Feature cleanup\n\n## History expiry\n\n## What problem does it solve?\n\nAs of the time of this writing, a full-synced Ethereum node requires\nroughly\n1.1 terabytes of disk space for the execution\nclient, plus another few hundred gigabytes for the consensus client.\nThe great majority of this is history: data about historical\nblocks, transactions and receipts, the bulk of which are many years old.\nThis means that the size of a node keeps increasing by hundreds of\ngigabytes each year, even if the gas limit does not increase at all.\nWhat is it, and how does it\nwork?\n\nA key simplifying feature of the history storage problem is that\nbecause each block points to the previous block via a hash link (and other\nstructures),\nhaving consensus on the present is enough to have consensus on history.\nAs long as the network has consensus on the latest block, any historical\nblock or transaction or state (account balance, nonce, code, storage)\ncan be provided by any single actor along with a Merkle proof, and the\nproof allows anyone else to verify its correctness. While consensus is\nan N/2-of-N trust model, history is a 1-of-N\ntrust model.\n\nThis opens up a lot of options for how we can store the history. One\nnatural option is a network where each node only stores a small\npercentage of the data. This is how torrent networks have worked for\ndecades: while the network altogether stores and distributes millions of\nfiles, each participant only stores and distributes a few of them.\nPerhaps counterintuitively, this approach does not even necessarily\ndecrease the robustness of the data. If, by making node running\nmore affordable, we can get to a network with 100,000 nodes, where each\nnode stores a random 10% of the history, then each piece of data would\nget replicated 10,000 times - exactly the same replication factor as a\n10,000-node network where each node stores everything.\n\nToday, Ethereum has already started to move away from the model of\nall nodes storing all history forever. Consensus blocks (ie. the parts\nrelated to proof of stake consensus) are only stored for ~6 months.\nBlobs are only stored for ~18 days. EIP-4444 aims to\nintroduce a one-year storage period for historical blocks and receipts.\nA long-term goal is to have a harmonized period (which could be ~18\ndays) during which each node is responsible for storing everything, and\nthen have a peer-to-peer network made up of Ethereum nodes storing older\ndata in a distributed way.\n\nErasure codes can be used to increase robustness while keeping the\nreplication factor the same. In fact, blobs already come erasure-coded\nin order to support data availability sampling. The simplest solution\nmay well be to re-use this erasure coding, and put execution and\nconsensus block data into blobs as well.\nWhat are some links to\nexisting research?\n\n- EIP-4444: https://eips.ethereum.org/EIPS/eip-4444\n\n- Torrents and EIP-4444: https://ethresear.ch/t/torrents-and-eip-4444/19788\n\n- Portal network: https://ethereum.org/en/developers/docs/networking-layer/portal-network/\n\n- Portal network and EIP-4444: https://github.com/ethereum/portal-network-specs/issues/308\n\n- Distributed storage and retrieval of SSZ objects in Portal: https://ethresear.ch/t/distributed-storage-and-cryptographically-secured-retrieval-of-ssz-objects-for-portal-network/19575\n\n- How to raise the gas limit (Paradigm): https://www.paradigm.xyz/2024/05/how-to-raise-the-gas-limit-2\n\nWhat is left to\ndo, and what are the tradeoffs?\n\nThe main remaining work involves building out and integrating a\nconcrete distributed solution for storing history - at least execution\nhistory, but ultimately also consensus and blobs. The easiest solutions\nfor this are (i) to simply introduce an existing torrent library, and\n(ii) an Ethereum-native solution called the Portal network. Once either of\nthese is introduced, we can turn EIP-4444 on. EIP-4444 itself does\nnot require a hard fork, though it does require a new network\nprotocol version. For this reason, there is value in enabling it for all\nclients at the same time, because otherwise there are risks of clients\nmalfunctioning from connecting to other nodes expecting to download the\nfull history but not actually getting it.\n\nThe main tradeoff involves how hard we try to make \"ancient\"\nhistorical data available. The easiest solution would be to simply stop\nstoring ancient history tomorrow, and rely on existing archive nodes and\nvarious centralized providers for replication. This is easy, but this\nweakens Ethereum's position as a place to make permanent records. The\nharder, but safer, path is to first build out and integrate the torrent\nnetwork for storing history in a distributed way. Here, there are two\ndimensions of \"how hard we try\":\n\n- How hard do we try to make sure that a maximally large set of nodes\nreally is storing all the data?\n\n- How deeply do we integrate the historical storage into the\nprotocol?\n\nA maximally paranoid approach for (1) would involve proof\nof custody: actually requiring each proof of stake validator to\nstore some percentage of history, and regularly cryptographically\nchecking that they do so. A more moderate approach is to set a voluntary\nstandard for what percentage of history each client stores.\n\nFor (2), a basic implementation involves just taking the work that is\nalready done today: Portal already stores ERA files containing the\nentire Ethereum history. A more thorough implementation would involve\nactually hooking this up to the syncing process, so that if someone\nwanted to sync a full-history-storing node or an archive node, they\ncould do so even if no other archive nodes existed online, by syncing\nstraight from the Portal network.\nHow does\nit interact with other parts of the roadmap?\n\nReducing history storage requirements is arguably even more important\nthan statelessness if we want to make it extremely easy to run or spin\nup a node: out of the 1.1 TB that a node needs to have, ~300 GB is\nstate, and the remaining ~800 GB is history. The vision of an Ethereum\nnode running on a smart watch and taking only a few minutes to set up is\nonly achievable if both statelessness and EIP-4444 are\nimplemented.\n\nLimiting history storage also makes it more viable for newer Ethereum\nnode implementations to only support recent versions of the\nprotocol, which allows them to be much simpler. For example, many lines\nof code can be safely removed now that empty storage slots created\nduring the 2016 DoS attacks have all been removed. Now that the\nswitch to proof of stake is ancient history, clients can safely remove\nall proof-of-work-related code.\n\n## State expiry\n\n## What problem does it solve?\n\nEven if we remove the need for clients to store history, a client's\nstorage requirement will continue to grow, by around 50 GB per year,\nbecause of ongoing growth to the state:\naccount balances and nonces, contract code and contract storage. Users\nare able to pay a one-time cost to impose a burden on present and future\nEthereum clients forever.\n\nState is much harder to \"expire\" than history, because the EVM is\nfundamentally designed around an assumption that once a state object is\ncreated, it will always be there and can be read by any transaction at\nany time. If we introduce statelessness, there is an argument that maybe\nthis problem is not that bad: only a specialized class of block builders\nwould need to actually store the state, and all other nodes (even inclusion\nlist production!) can run statelessly. However, there is an argument\nthat we don't want to lean on statelessness too much, and\neventually we may want to expire state to keep Ethereum\ndecentralized.\nWhat is it, and how does it\nwork?\n\nToday, when you create a new state object (which can happen in one of\nthree ways: (i) sending ETH to a new account, (ii) creating a new\naccount with code, (iii) setting a previously-untouched storage slot),\nthat state object is in the state forever. What we want instead, is for\nobjects to automatically expire over time. The key challenge is doing\nthis in a way that accomplishes three goals:\n\n- Efficiency: don't require huge amounts of extra\ncomputation to run the expiry process\n\n- User-friendliness: if someone goes into a cave for\nfive years and comes back, they should not lose access to their ETH,\nERC20s, NFTs, CDP positions...\n\n- Developer-friendliness: developers should not have\nto switch to a completely unfamiliar mental model. Additionally,\napplications that are ossified today and do not update should continue\nto work reasonably well.\n\nIt's easy to solve the problem without satisfying these goals. For\nexample, you could have each state object also store a counter for its\nexpiry date (which could be extended by burning ETH, which could happen\nautomatically any time it's read or written), and have a process that\nloops through the state to remove expired state objects. However, this\nintroduces extra computation (and even storage requirements), and it\ndefinitely does not satisfy the user-friendliness requirement.\nDevelopers too would have a hard time reasoning about edge cases\ninvolving storage values sometimes resetting to zero. If you make the\nexpiry timer contract-wide, this makes life technically easier\nfor developers, but it makes the economics harder: developers\nwould have to think about how to \"pass through\" the ongoing costs of\nstorage to their users.\n\nThese are problems that the Ethereum core development community\nstruggled with for many years, including proposals like \"blockchain rent\"\nand \"regenesis\".\nEventually, we combined the best parts of the proposals and converged on\ntwo categories of \"known least bad solutions\":\n\n- Partial state-expiry solutions\n\n- Address-period-based state expiry proposals.\n\n## Partial state expiry\n\nPartial state expiry proposals all work along the same principle. We\nsplit the state into chunks. Everyone permanently stores the \"top-level\nmap\" of which chunks are empty or nonempty. The data within\neach chunk is only stored if that data has been recently accessed. There\nis a \"resurrection\" mechanism where if a chunk is no longer stored,\nanyone can bring that data back by providing a proof of what the data\nwas.\n\nThe main distinctions between these proposals are: (i) how do we\ndefine \"recently\", and (ii) how do we define \"chunk\"? One concrete\nproposal is EIP-7736, which\nbuilds upon the \"stem-and-leaf\" design introduced\nfor Verkle trees (though compatible with any form of statelessness,\neg. binary trees). In this design, header, code and storage slots that\nare adjacent to each other are stored under the same \"stem\". The data\nstored under a stem can be at most 256 * 31 = 7,936 bytes.\nIn many cases, the entire header and code, and many key storage slots,\nof an account will all be stored under the same stem. If the data under\na given stem is not read or written for 6 months, the data is no longer\nstored, and instead only a 32-byte commitment (\"stub\") to the data is\nstored. Future transactions that access that data would need to\n\"resurrect\" the data, with a proof that would be checked against the\nstub.\n\nThere are other ways to implement a similar idea. For example, if\naccount-level granularity is not enough, we could make a scheme where\neach 1/232 fraction of the tree is governed by a similar\nstem-and-leaf mechanism.\n\nThis is trickier because of incentives: an attacker could force\nclients to permanently store a very large amount of state by putting a\nvery large amount of data into a single subtree and sending a single\ntransaction every year to \"renew the tree\". If you make the renewal cost\nproportional (or renewal duration inversely-proportional) to the tree\nsize, then someone could grief another user by putting a very large\namount of data into the same subtree as them. One could try to limit\nboth problems by making the granularity dynamic based on the subtree\nsize: for example, each consecutive 216 = 65536 state objects\ncould be treated as a \"group\". However, these ideas are more complex;\nthe stem-based approach is simple, and it aligns incentives, because\ntypically all the data under a stem is related to the same application\nor user.\nAddress-period-based\nstate expiry proposals\n\nWhat if we wanted to avoid any permanent state growth at\nall, even 32-byte stubs? This is a hard problem because of resurrection\nconflicts: what if a state object gets removed, later EVM execution\nputs another state object in the exact same position, but then after\nthat someone who cares about the original state object comes back and\ntries to recover it? With partial state expiry, the \"stub\" prevents new\ndata from being created. With full state expiry, we cannot afford to\nstore even the stub.\n\nThe address-period-based design is the best known idea for solving\nthis. Instead of having one state tree storing the whole state, we have\na constantly growing list of state trees, and any state that gets read\nor written gets saved in the most recent state tree. A new empty state\ntree gets added once per period (think: 1 year). Older state trees are\nfrozen solid. Full nodes are only expected to store the most recent two\ntrees. If a state object was not touched for two periods and thus falls\ninto an expired tree, it still can be read or written to, but the\ntransaction would need to prove a Merkle proof for it - and once it\ndoes, a copy will be saved in the latest tree again.\n\nA key idea for making this all user and developer-friendly is the\nconcept of address periods. An address period is a\nnumber that is part of an address. A key rule is that an address\nwith address period N can only be read or written to during or after\nperiod N (ie. when the state tree list reaches length N). If\nyou're saving a new state object (eg. a new contract, or a new\nERC20 balance), if you make sure to put the state object into a contract\nwhose address period is either N or N-1, then you can save it\nimmediately, without needing to provide proofs that there was nothing\nthere before. Any additions or edits to state in older address periods,\non the other hand, do require a proof.\n\nThis design preserves most of Ethereum's current properties, is very\nlight on extra computation, allows applications to be written almost as\nthey are today (ERC20s will need to rewrite, to ensure that balances of\naddresses with address period N are stored in a child contract which\nitself has address period N), and solves the \"user goes into a cave for\nfive years\" problem. However, it has one big issue: addresses\nneed to be expanded beyond 20 bytes to fit address periods.\n\n## Address space extension\n\nOne proposal is to introduce a new 32-byte address format, which\nincludes a version number, an address period number and an expanded\nhash.\n\n0x01000000000157aE408398dF7E5f4552091A69125d5dFcb7B8C2659029395bdF\n\nThe red is a version number. The four zeroes colored orange here are\nintended as empty space, which could fit a shard number in the future.\nThe green is an address period number. The blue is a 26-byte hash.\n\nThe key challenge here is backwards compatibility. Existing contracts\nare designed around 20 byte addresses, and often use tight byte-packing\ntechniques that explicitly assume addresses are exactly 20 bytes long.\nOne\nidea for solving this involves a translation map, where old-style\ncontracts interacting with new-style addresses would see a 20-byte hash\nof the new-style address. However, there are significant complexities\ninvolved in making this safe.\n\n## Address space contraction\n\nAnother approach goes the opposite direction: we immediately forbid\nsome 2128-sized sub-range of addresses (eg. all addresses\nstarting with 0xffffffff), and then use that range to\nintroduce addresses with address periods and 14-byte hashes.\n\n0xffffffff000169125d5dFcb7B8C2659029395bdF\n\nThe key sacrifice that this approach makes, is that it introduces\nsecurity risks for counterfactual addresses:\naddresses that hold assets or permissions, but whose code has not yet\nbeen published to chain. The risk involves someone creating an address\nwhich claims to have one piece of (not-yet-published) code, but also has\nanother valid piece of code which hashes to the same address. Computing\nsuch a collision requires 280 hashes today; address space\ncontraction would reduce this number to a very accessible 256\nhashes.\n\nThe key risk area, counterfactual addresses that are not\nwallets held by a single owner, is a relatively rare case today, but is\nlikely to become more common as we enter a multi-L2 world. The only\nsolution is to simply accept this risk, but identify all common use\ncases where this may be an issue, and come up with effective\nworkarounds.\nWhat are some links\nto existing research?\n\n- Early proposals\n\n- Blockchain rent: https://github.com/ethereum/EIPs/issues/35\n\n- Regenesis: https://ethresear.ch/t/regenesis-resetting-ethereum-to-reduce-the-burden-of-large-blockchain-and-state/7582\n\n- A theory of Ethereum state size management: https://hackmd.io/@vbuterin/state_size_management\n\n- A few possible paths to statelessness and state expiry: https://hackmd.io/@vbuterin/state_expiry_paths\n\n- Partial state expiry proposals\n\n- EIP-7736: https://eips.ethereum.org/EIPS/eip-7736\n\n- Address space extension documents\n\n- Original proposal: https://ethereum-magicians.org/t/increasing-address-size-from-20-to-32-bytes/5485\n\n- Ipsilon review: https://notes.ethereum.org/@ipsilon/address-space-extension-exploration\n\n- Blog post review: https://medium.com/@chaisomsri96/statelessness-series-part2-ase-address-space-extension-60626544b8e6\n\n- What would break if we lose collision resistance: https://ethresear.ch/t/what-would-break-if-we-lose-address-collision-resistance/11356\n\nWhat is left to\ndo, and what are the tradeoffs?\n\nI see four viable paths for the future:\n\n- We do statelessness, and never introduce state\nexpiry. State is ever-growing (albeit slowly: we may not see it\nexceed 8 TB for decades), but only needs to be held by a relatively\nspecialized class of users: not even PoS validators would need the\nstate. \n\n The one function that needs access to parts of\nthe state is inclusion list production, but we can\naccomplish this in a decentralized way: each user is responsible for\nmaintaining the portion of the state tree that contains their own\naccounts. When they broadcast a transaction, they broadcast it with a\nproof of the state objects accessed during the verification\nstep (this works for both EOAs and ERC-4337 accounts). Stateless\nvalidators can then combine these proofs into a proof for the whole\ninclusion list.\n\n- We do partial state expiry, and accept a much lower\nbut still nonzero rate of permanent state size growth. This outcome is\narguably similar to how history expiry proposals involving peer-to-peer\nnetworks accept a much lower but still nonzero rate of permanent history\nstorage growth from each client having to store a low but fixed\npercentage of the historical data.\n\n- We do state expiry, with address space\nexpansion. This will involve a multi-year process of making\nsure that the address format conversion approach works and is safe,\nincluding for existing applications.\n\n- We do state expiry, with address space\ncontraction. This will involve a multi-year process of making\nsure that all of the security risks involving address collisions,\nincluding cross-chain situations, are handled.\n\nOne important point is that the difficult issues around\naddress space expansion and contraction will eventually have to be\naddressed regardless of whether or not state expiry schemes that depend\non address format changes are ever implemented. Today, it takes\nroughly 280 hashes to generate an address collision, a\ncomputational load that is already feasible for extremely well-resourced\nactors: a GPU can do around 227 hashes, so running for a year\nit can compute 252, so all ~230\nGPUs in the world could compute a collision in ~1/4 of a year, and\nFPGAs and ASICs could accelerate this further. In the future, such\nattacks will become open to more and more people. Hence, the actual cost\nof implementing full state expiry may not be as high as it seems, since\nwe have to solve this very challenging address problem regardless.\nHow does\nit interact with other parts of the roadmap?\n\nDoing state expiry potentially makes transitions from one state tree\nformat to another easier, because there will be no need for a transition\nprocedure: you could simply start making new trees using a new format,\nand then later do a hard fork to convert the older trees. Hence, while\nstate expiry is complex, it does have benefits in simplifying other\naspects of the roadmap.\n\n## Feature cleanup\n\n## What problems does it solve?\n\nOne of the key preconditions of security, accessibility and credible neutrality\nis simplicity. If a protocol is beautiful and simple, it reduces the\nchance that there will be bugs. It increases the chance that new\ndevelopers will be able to come in and work with any part of it. It's\nmore likely to be fair and easier to defend against special interests.\nUnfortunately, protocols, like any social system, by default become more\ncomplex over time. If we do not want Ethereum to go into a black hole of\never-increasing complexity, we need to do one of two things: (i) stop\nmaking changes and ossify the protocol, (ii) be able to\nactually remove features and reduce\ncomplexity. An intermediate route, of making fewer\nchanges to the protocol, and also removing at least a little\ncomplexity over time, is also possible. This section will talk how we\ncan reduce or remove complexity.\nWhat is it, and how does it\nwork?\n\nThere is no big single fix that can reduce protocol complexity; the\ninherent nature of the problem is that there are many little fixes.\n\nOne example that is mostly finished already, and can serve as a\nblueprint for how to handle the others, is the removal of the\nSELFDESTRUCT opcode. The SELFDESTRUCT opcode was the only\nopcode that could modify an unlimited number of storage slots within a\nsingle block, requiring clients to implement significantly more\ncomplexity to avoid DoS attacks. The opcode's original purpose was to\nenable voluntary state clearing, allowing the state size to decrease\nover time. In practice, very few ended up using it. The opcode was nerfed to\nonly allow self-destructing accounts created in the same transaction in\nthe Dencun hardfork. This solves the DoS issue and allows for\nsignificant simplification in client code. In the future, it likely\nmakes sense to eventually remove the opcode completely.\n\nSome key examples of protocol simplification opportunities that have\nbeen identified so far include the following. First, some examples that\nare outside the EVM; these are relatively non-invasive, and thus easier\nto get consensus on and implement in a shorter timeframe.\n\n- RLP \u2192 SSZ transition: originally, Ethereum objects\nwere serialized using an encoding called RLP.\nRLP is untyped, and needlessly complex. Today, the beacon chain uses SSZ,\nwhich is significantly better in many ways, including supporting not\njust serialization but also hashing. Eventually, we want to get rid of\nRLP entirely, and move all data types into being SSZ structs, which\nwould in turn make upgradability much easier. Current EIPs for this\ninclude [1] [2] [3].\n\n- Removal of old transaction types: there are too\nmany transaction types today, many of them could potentially be removed.\nA more moderate alternative to full removal is an account abstraction\nfeature by which smart accounts could include the code to process and\nverify old-style transactions if they so choose.\n\n- LOG reform: logs create bloom filters and other\nlogic that adds complexity to the protocol, but is not actually used by\nclients because it is too slow. We could remove these\nfeatures, and instead put effort into alternatives, such as\nextra-protocol decentralized log reading tools that use modern\ntechnology like SNARKs.\n\n- Eventual removal of the beacon chain sync committee\nmechanism: the sync\ncommittee mechanism was originally introduced to enable light client\nverification of Ethereum. However, it adds significant complexity to the\nprotocol. Eventually, we will be able to verify\nthe Ethereum consensus layer directly using SNARKs, which would\nremove the need for a dedicated light client verification protocol.\nPotentially, changes to consensus could enable us to remove sync\ncommittees even earlier, by creating a more \"native\" light client\nprotocol that involves verifying signatures from a random subset of the\nEthereum consensus validators.\n\n- Data format harmonization: today, execution state\nis stored in a Merkle Patricia tree, consensus state is stored in an SSZ\ntree, and blobs are committed to with KZG\ncommitments. In the future, it makes sense to make a single unified\nformat for block data and a single unified format for state. These\nformats would cover all important needs: (i) easy proofs for stateless\nclients, (ii) serialization and erasure coding for data, (iii)\nstandardized data structures.\n\n- Removal of beacon chain committees: this mechanism\nwas originally introduced to support a particular\nversion of execution sharding. Instead, we ended up doing sharding\nthrough\nL2s and blobs. Hence, committees are unnecessary, and so there is an\nin-progress move\ntoward removing them.\n\n- Removal of mixed-endianness: the EVM is big-endian\nand the consensus layer is little-endian. It may make sense to\nre-harmonize and make everything one or the other (likely big-endian,\nbecause the EVM is harder to change)\n\nNow, some examples that are inside the EVM:\n\n- Simplification of gas mechanics: the current gas\nrules are not quite well-optimized to give clear limits to the quantity\nof resources required to verify a block. Key examples of this include\n(i) storage read/write costs, which are meant to bound\nthe number of reads/writes in a block but are currently pretty\nhaphazard, and (ii) memory filling rules, where it is\ncurrently hard to estimate the max memory consumption of the EVM.\nProposed fixes include statelessness gas cost\nchanges, which harmonize all storage-related costs into a simple\nformula, and this proposal\nfor memory pricing.\n\n- Removal of precompiles: many of the precompiles\nthat Ethereum has today are both needlessly complex and relatively\nunused, and make up a large percentage of consensus failure near-misses\nwhile not actually being used by any applications. Two ways of dealing\nwith this are (i) just removing the precompile, and (ii) replacing it\nwith a (inevitably more expensive) piece of EVM code that implements the\nsame logic. This draft EIP\nproposes to do this for the identity precompile as a first step;\nlater on, RIPEMD160, MODEXP and BLAKE may be candidates for\nremoval.\n\n- Removal of gas observability: make the EVM\nexecution no longer able to see how much gas it has left. This would\nbreak a few applications (most notably, sponsored transactions), but\nwould enable much easier upgrading in the future (eg. for more advanced\nversions of multidimensional\ngas). The EOF spec\nalready makes gas unobservable, though to be useful for protocol\nsimplification EOF would need to become mandatory.\n\n- Improvements to static analysis: today EVM code is\ndifficult to statically analyze, particularly because jumps can be\ndynamic. This also makes it more difficult to make optimized EVM\nimplementations that pre-compile EVM code into some other language. We\ncan potentially fix this by removing dynamic\njumps (or making them much more expensive, eg. gas cost linear in\nthe total number of JUMPDESTs in a contract). EOF does this, though\ngetting protocol simplification gains out of this would require making\nEOF mandatory.\n\nWhat are some links\nto existing research?\n\n- Next steps in the Purge: https://notes.ethereum.org/I_AIhySJTTCYau_adoy2TA\n\n- SELFDESTRUCT: https://hackmd.io/@vbuterin/selfdestruct\n\n- SSZ-ification EIPS: [1] [2] [3]\n\n- Statelessness gas cost changes: https://eips.ethereum.org/EIPS/eip-4762\n\n- Linear memory pricing: https://notes.ethereum.org/ljPtSqBgR2KNssu0YuRwXw\n\n- Precompile removal: https://notes.ethereum.org/IWtX22YMQde1K_fZ9psxIg\n\n- Bloom filter removal: https://eips.ethereum.org/EIPS/eip-7668\n\n- A way to do off-chain secure log retrieval using incrementally\nverifiable computation (read: recursive STARKs): https://notes.ethereum.org/XZuqy8ZnT3KeG1PkZpeFXw\n\nWhat is left to\ndo, and what are the tradeoffs?\n\nThe main tradeoff in doing this kind of feature simplification is (i)\nhow much we simplify and how quickly vs (ii) backwards compatibility.\nEthereum's value as a chain comes from it being a platform where you can\ndeploy an application and be confident that it will still work many\nyears from now. At the same time, it's possible to take that ideal too\nfar, and, to paraphrase\nWilliam Jennings Bryan, \"crucify Ethereum on a cross of backwards\ncompatibility\". If there are only two applications in all of Ethereum\nthat use a given feature, and one has had zero users for years and the\nother is almost completely unused and secures a total of $57 of value,\nthen we should just remove the feature, and if needed pay the victims\n$57 out of pocket.\n\nThe broader social problem is in creating a standardized pipeline for\nmaking non-emergency backwards-compatibility-breaking changes. One way\nto approach this is to examine and extend existing precedents, such as\nthe SELFDESTRUCT process. The pipeline looks something as follows:\n\n- Step 1: start talking about removing feature X\n\n- Step 2: do analysis to identify how much removing X\nbreaks applications, depending on the results either (i) abandon the\nidea, (ii) proceed as planned, or (iii) identify a modified\n\"least-disruptive\" way to remove X and proceed with that\n\n- Step 3: make a formal EIP to deprecate X. Make sure\nthat popular higher-level infrastructure (eg. programming languages,\nwallets) respect this and stop using that feature.\n\n- Step 4: finally, actually remove X\n\nThere should be a multi-year-long pipeline between step 1 and step 4,\nwith clear information about which items are at which step. At that\npoint, there is a tradeoff between how vigorous and fast the\nfeature-removal pipeline is, versus being more conservative and putting\nmore resources into other areas of protocol development, but we are\nstill far from the Pareto frontier.\n\n## EOF\n\nA major set of changes that has been proposed to the EVM is the EVM Object Format (EOF). EOF\nintroduces a large number of changes, such as banning gas observability,\ncode observability (ie. no CODECOPY), allowing static jumps only. The\ngoal is to allow the EVM to be upgraded more, in a way that has stronger\nproperties, while preserving backwards compatibility (as the pre-EOF EVM\nwill still exist).\n\nThis has the advantage that it creates a natural path to adding new\nEVM features and encouraging migration to a more restrictive EVM with\nstronger guarantees. It has the disadvantage that it significantly\nincreases protocol complexity, unless we can find a way to\neventually deprecate and remove the old EVM. One major question is:\nwhat role does EOF play in EVM simplification proposals,\nespecially if the goal is to reduce the complexity of the EVM as a\nwhole?\nHow does\nit interact with other parts of the roadmap?\n\nMany of the \"improvement\" proposals in the rest of the roadmap are\nalso opportunities to do simplifications of old features. To repeat some\nexamples from above:\n\n- Switching to single-slot finality gives us an opportunity to remove\ncommittees, rework economics, and do other proof-of-stake-related\nsimplifications.\n\n- Fully implementing account abstraction lets us remove a lot of\nexisting transaction-handling logic, by moving it into a piece of\n\"default account EVM code\" that all EOAs could be replaced by.\n\n- If we move the Ethereum state to binary hash trees, this could be\nharmonized with a new version of SSZ, so that all Ethereum data\nstructures could be hashed in the same way.\n\nA\nmore radical approach: turn big parts of the protocol into contract\ncode\n\nA more radical Ethereum simplification strategy is to keep the\nprotocol as is, but move large parts of it from being protocol features\nto being contract code.\n\nThe most extreme version of this would be to make the Ethereum L1\n\"technically\" be just the beacon chain, and introduce a minimal VM (eg.\nRISC-V, Cairo, or something even more\nminimal specialized for proving systems) which allows anyone else to\ncreate their own rollup. The EVM would then turn into the first one of\nthese rollups. This is ironically exactly the same outcome as the execution\nenvironment proposals from 2019-20, though SNARKs make it\nsignificantly more viable to actually implement.\n\nA more moderate approach would be to keep the relationship between\nthe beacon chain and the current Ethereum execution environment as-is,\nbut do an in-place swap of the EVM. We could choose RISC-V, Cairo or\nanother VM to be the new \"official Ethereum VM\", and then force-convert\nall EVM contracts into new-VM code that interprets the logic of the\noriginal code (by compiling or interpreting it). Theoretically, this\ncould even be done with the \"target VM\" being a version of EOF.",
    "contentLength": 35706,
    "summary": "Ethereum's \"Purge\" phase aims to reduce client storage from 1.1TB+ and protocol complexity through history expiry and state expiry mechanisms.",
    "detailedSummary": {
      "theme": "Vitalik outlines strategies for reducing Ethereum's growing complexity and storage requirements while preserving the blockchain's permanence and backwards compatibility.",
      "summary": "In this fifth installment of his Ethereum protocol futures series, Vitalik addresses one of Ethereum's fundamental long-term challenges: the inevitable growth of bloat and complexity over time. He identifies two main sources of this problem - historical data that accumulates indefinitely and protocol features that are easier to add than remove. Vitalik proposes 'The Purge' as a comprehensive strategy to counter these trends through three main approaches: history expiry (reducing storage of old blocks and transactions through distributed networks), state expiry (preventing indefinite growth of account and contract data), and feature cleanup (removing unnecessary protocol complexity). The challenge lies in balancing simplification with Ethereum's core value proposition of permanence - the guarantee that applications, smart contracts, and data will remain accessible and functional for years to come. Vitalik emphasizes that while organisms age and most systems decay over time, some can achieve longevity through active maintenance and renewal, which is the goal for Ethereum's technical sustainability and security.",
      "takeaways": [
        "History expiry through EIP-4444 and distributed storage networks like Portal can reduce node storage requirements from 1.1TB to manageable levels while maintaining data availability",
        "State expiry solutions face the complex tradeoff between preventing unlimited state growth and maintaining user/developer friendliness, with partial state expiry and address-period-based approaches as leading candidates",
        "Feature cleanup requires establishing a standardized pipeline for backwards-compatibility-breaking changes, balancing protocol simplification against Ethereum's value as a stable platform",
        "Address space modifications (either expansion to 32 bytes or contraction with collision risks) may be necessary regardless of state expiry implementation due to growing feasibility of address collision attacks",
        "The Purge interacts synergistically with other roadmap elements like statelessness and could enable more radical simplifications like moving protocol features into contract code"
      ],
      "controversial": [
        "The proposal to potentially remove features used by only a few applications, even suggesting paying victims 'out of pocket' for minimal value applications, raises questions about Ethereum's commitment to immutability",
        "Address space contraction introduces significant security risks for counterfactual addresses, reducing collision resistance from 2^80 to 2^56 hashes",
        "The suggestion that state expiry might eventually be necessary even with statelessness challenges the assumption that specialized block builders can indefinitely handle growing state requirements",
        "The radical proposal to turn the EVM into a rollup on top of a minimal beacon chain VM represents a fundamental architectural shift that could be seen as abandoning Ethereum's current execution model"
      ]
    }
  },
  {
    "id": "general-2024-10-23-futures4",
    "title": "Possible futures of the Ethereum protocol, part 4: The Verge",
    "date": "2024-10-23",
    "category": "protocol",
    "url": "https://vitalik.eth.limo/general/2024/10/23/futures4.html",
    "path": "general/2024/10/23/futures4.html",
    "content": "- \n\n- \n\n  \n    \n\n  \n  \n  Dark Mode Toggle\n  \n    \n\n  \n\n##  Possible futures of the Ethereum protocol, part 4: The Verge \n\n 2024 Oct 23 \nSee all posts\n\n \n \n\n Possible futures of the Ethereum protocol, part 4: The Verge \n\nSpecial thanks to Justin Drake, Hsiao-wei Wang, Guillaume Ballet,\nIgnacio, Josh Rudolf, Lev Soukhanov, Ryan Sean Adams and Uma Roy for\nfeedback and review.\n\nOne of the most powerful things about a blockchain is the fact that\nanyone can run a node on their computer and verify that the\nchain is correct. Even if 95% of the nodes running the chain\nconsensus (PoW, PoS...) all immediately agreed to change the rules, and\nstarted producing blocks according to the new rules, everyone running a\nfully-verifying node would refuse to accept the chain. The stakers who\nare not part of such a cabal would automatically converge on,\nand continue building, a chain that continues to follow the old rules,\nand fully-verifying users would follow that chain.\n\nThis is a key difference between blockchains and centralized systems.\nHowever, for this property to hold, running a fully-verifying node needs\nto be actually feasible for a critical mass of people. This applies both\nto stakers (as if stakers are not verifying the chain,\nthey are not actually contributing to enforcing the protocol rules), and\nto regular users. Today, running a node is possible on\na consumer laptop (including the one being used to write this post), but\ndoing so is difficult. The Verge is about changing this, and\nmaking fully-verifying the chain so computationally affordable\nthat every mobile wallet, browser wallet, and even smart watch is doing\nit by default.\n\n The Verge, 2023 roadmap. \n\nOriginally, the \"Verge\" referred to the idea of moving Ethereum state\nstorage to Verkle\ntrees - a tree structure that allows for much more compact proofs,\nenabling stateless validation of Ethereum blocks. A\nnode could verify an Ethereum block without having any of the Ethereum\nstate (account balances, contract code, storage...) on its hard drive, at\na cost of spending a few hundred kilobytes of proof data and a few\nhundred extra milliseconds verifying a proof. Today, the Verge\nrepresents a much larger vision focused on enabling maximally\nresource-efficient verification of the Ethereum chain,\nwhich includes not just stateless validation technology, but also\nverifying all Ethereum execution with SNARKs.\n\nIn addition to the added long-term focus on SNARK-verifying the whole\nchain, another new question has to do with whether or not Verkle\ntrees are even the best technology in the first place. Verkle\ntrees are vulnerable to quantum computers, and so if we replace the\ncurrent KECCAK Merkle\nPatricia tree with Verkle trees, we will later have to replace the\ntrees again. The natural alternative to Merkle trees is skipping\nstraight to using a STARK\nof Merkle branches in a binary tree.\nHistorically, this has been considered non-viable due to overhead and\ntechnical complexity. More recently, however, we have seen Polygon prove 1.7\nmillion Poseidon hashes per second on a laptop with circle\nSTARKs, and proving times for more \"conventional\" hashes are also\nrapidly improving thanks to techniques like GKR.\n\nAs a result, over the past year the Verge has become much more\nopen-ended, and there are several possibilities.\n\n## The Verge: key goals\n\n- Stateless clients: fully-verifying clients, and staking nodes,\nshould not need more than a few GB of storage\n\n- (Longer term) fully verify the chain (consensus and execution) on a\nsmart watch. Download some data, verify a SNARK, done.\n\n## In this chapter\n\n- Stateless verification: Verkle or STARKs\n\n- Validity proofs of EVM execution\n\n- Validity proofs of consensus\n\nStateless verification:\nVerkle or STARKs\nWhat problem are we trying\nto solve?\n\nToday, an Ethereum client needs to store hundreds\nof gigabytes of state data in order to verify blocks, and this\namount continues to increase with each passing year. The raw state data\nincreases by ~30\nGB per year, and individual clients have to store some extra data on\ntop to be able to update the trie efficiently.\n\nThis reduces the number of users who can run fully-verifying Ethereum\nnodes: even though hard drives large enough to store all Ethereum state\nand even history for many years are readily available,\nthe computers that people buy by default tend to only have a few hundred\ngigabytes of storage. The state size also introduces a great friction\ninto the process of setting up a node for the first time: the node needs\nto download the entire state, which can take hours or days. This has all\nkinds of knockon effects. For example, it makes it significantly harder\nfor a staker to upgrade their staking setup. Technically, it's possible\nto do this with no downtime - start a new client, wait for it to sync,\nthen shut down the old client and transfer the key - but in practice\nit's technically complicated.\nWhat is it and how does it\nwork?\n\nStateless verification is a technology that allows nodes to verify\nblocks without having the entire state. Instead, each block comes with a\nwitness, which includes (i) the values (eg.\ncode, balances, storage) at the specific locations in the state\nthat the block will access, and (ii) a cryptographic\nproof that those values are correct.\n\nActually implementing stateless verification requires changing the\nEthereum state tree structure. This is because the current Merkle\nPatricia tree is extremely unfriendly to implementing any\ncryptographic proof scheme, especially in the worst case. This is true\nfor both \"raw\" Merkle branches, and the possibility of \"wrapping\" the\nMerkle branches in a STARK. The key difficulties stem from two\nweaknesses of the MPT:\n\n- It's a hexary tree (ie. each node has 16 children). This means that\non average, a proof in a size-N tree has\n32 * (16 - 1) * log16(N) = 120 * log2(N) bytes, or about\n3840 bytes in a 232-item tree. With a binary tree, you only\nneed 32 * (2 - 1) * log2(N) = 32 * log2(N) bytes, or about\n1024 bytes.\n\n- The code is not Merkelized. This means that proving any access of\naccount code requires providing the entire code, which is a maximum of\n24000 bytes.\n\nWe can compute a worst-case scenario as follows:\n\n30,000,000 gas / 2,400 (\"cold\" account read cost) * (5 * 480 + 24,000) = 330,000,000\nbytes\n\nThe branch cost is slightly decreased ( 5 * 480 instead\nof 8 * 480 ) because the top parts of branches are repeated\nwhen there are many of them. But even still, this works out to a\ncompletely unrealistic amount of data to download within one slot. If we\ntry to wrap it in a STARK, we get two problems: (i) KECCAK is relatively\nSTARK-unfriendly, and (ii) 330 MB of data means we have to prove 5\nmillion calls to the KECCAK round function, which is way too\nmuch to prove on all but the most powerful consumer hardware, even\nif we could make STARK-proving KECCAK much more efficient.\n\nIf we just replace the hexary tree with a binary tree, and we\nadditionally Merkelize code, then the worst case becomes roughly\n30,000,000 / 2,400 * 32 * (32 - 14 + 8) = 10,400,000 bytes\n(the 14 is a subtraction for redundant bits of ~214 branches,\nand the 8 is the length of a proof going into a leaf in a chunk). Note\nthat this requires a change in gas costs, to charge for accessing each\nindividual chunk of code; EIP-4762 does this.\n10.4 MB is much better, but it's still too much data for many nodes to\ndownload within one slot. And so we need to introduce some more powerful\ntechnology. For this, there are two leading solutions: Verkle\ntrees, and STARKed binary hash trees.\n\n## Verkle trees\n\nVerkle trees use elliptic curve-based vector commitments to make much\nshorter proofs. The key unlock is that the piece of the proof\ncorresponding to each parent-child relationship is only 32 bytes,\nregardless of the width of the tree. The only limit to the\nwidth of the tree is that if the tree gets too wide, proofs become\ncomputationally inefficient. The implementation proposed for Ethereum\nhas a width of 256.\n\nThe size of a single branch in a proof thus becomes\n32 * log256(N) = 4 * log2(N) bytes. The theoretical max\nproof size thus becomes roughly\n30,000,000 / 2,400 * 32 * (32 - 14 + 8) / 8 = 1,300,000\nbytes (the math works out slightly differently in practice because of\nuneven distribution of state chunks, but this is fine as a first\napproximation).\n\nAs an additional caveat, note that in all of the above examples, this\n\"worst case\" is not quite a worst case: an even worse case is\nwhen an attacker intentionally \"mines\" two addresses to have a long\ncommon prefix in the tree and reads from one of them, which can extend\nthe worst-case branch length by another ~2x. But even with this\ncaveat, Verkle trees get us to ~2.6 MB worst-case proofs, which roughly\nmatches today's worst-case calldata.\n\nWe also take advantage of this caveat to do another thing: we make it\nvery cheap to access \"adjacent\" storage: either many chunks of code of\nthe same contract, or adjacent storage slots. EIP-4762 provides a\ndefinition of adjacency, and charges only 200 gas for adjacent access.\nWith adjacent accesses, the worst-case proof size becomes\n30,000,000 / 200 * 32 = 4,800,800 bytes, which is still\nroughly within tolerances. If we want this value decreased for safety,\nwe can increase the adjacent access costs slightly.\n\n## STARKed binary hash trees\n\nThe technology here is very self-explanatory: you do a binary tree,\ntake the max-10.4-MB proof that you need to prove the values in a block,\nand replace the proof with a STARK of the proof. This gets us to the\npoint where the proof itself consists of only the data being proven,\nplus ~100-300 kB fixed overhead from the actual STARK.\n\nThe main challenge here is prover time. We can make basically the\nsame calculations as above, except instead of counting bytes, we count\nhashes. A 10.4 MB block means 330,000 hashes. If we add in the\npossibility of an attacker \"mining\" addresses with a long common prefix\nin the tree, the real worst case becomes around 660,000 hashes.\nSo if we can prove ~200,000 hashes per second, we're fine.\n\nThese numbers have already been reached on a consumer laptop with the\nPoseidon hash function,\nwhich has been explicitly designed for STARK-friendliness. However,\nPoseidon is relatively immature, and so many do not yet trust it for\nsecurity. There are thus two realistic paths forward:\n\n- Do lots and lots of security analysis on Poseidon quickly, and get\ncomfortable enough with it to deploy it at L1\n\n- Use a more \"conservative\" hash function, such as SHA256 or\nBLAKE\n\nStarkware's circle STARK provers at the time of this writing can only\nprove ~10-30k hashes per second on a consumer laptop if they are proving\nconservative hash functions. However, STARK technology is improving\nquickly. Even today, GKR-based techniques show promise in potentially\nincreasing this to the ~100-200k range.\nUse cases of\nwitnesses other than verifying blocks\n\nIn addition to verifying blocks, there are three other key use cases\nfor more efficient stateless validation:\n\n- Mempools: when a transaction gets broadcasted,\nnodes in the p2p network need to verify that the transaction is valid\nbefore re-broadcasting it. Today, validation involves verifying the\nsignature, but also checking that the balance is sufficient and the\nnonce is correct. In the future (eg. with native account abstraction,\nsuch as EIP-7701),\nthis may involve running some EVM code, which does some state accesses.\nIf nodes are stateless, the transaction will need to come with a proof\nproving the state objects.\n\n- Inclusion lists: this is a proposed\nfeature which allows (potentially small and unsophisticated)\nproof-of-stake validators to force the next block to contain a\ntransaction, regardless of the (potentially large and sophisticated)\nblock builder's wishes. This would reduce the ability of powerful actors\nto manipulate the blockchain by delaying transactions. However, this\nrequires validators to have a way to verify the validity of transactions\nin the inclusion list.\n\n- Light clients: if we want users accessing the chain\nthrough wallets (eg. Metamask, Rainbow, Rabby...) to do so without\ntrusting centralized actors, they need to run light clients (eg. Helios). The core Helios\nmodule gives the user verified state roots. However, for a fully\ntrustless experience, the user needs proofs for each individual RPC call\nthey make (eg. for an eth_call request,\nthe user would need a proof of all the state accessed during the\ncall)\n\nOne thing that all of these use cases have in common is that they\nrequire a fairly large number of proofs, but each proof is small. For\nthis reason, STARK proofs do not actually make sense for them; instead,\nit's most realistic to just use Merkle branches directly. Another\nadvantage of Merkle branches is that they are updateable: given a proof\nof a state object X, rooted in block B, if you receive a child block B2\nwith its witness, you can update the proof to make it rooted in block\nB2. Verkle proofs are also natively updateable.\nWhat are some links to\nexisting research?\n\n- Verkle trees: https://vitalik.eth.limo/general/2021/06/18/verkle.html\n\n- Original Verkle tree paper by John Kuszmaul: https://math.mit.edu/research/highschool/primes/materials/2018/Kuszmaul.pdf\n\n- Starkware proving data: https://x.com/StarkWareLtd/status/1807776563188162562\n\n- Polygon proving data: https://x.com/dlubarov/status/1845862467315920940\n\n- Poseidon2 paper: https://eprint.iacr.org/2023/323\n\n- Ajtai (alternative fast hash function based on lattice hardness): https://www.wisdom.weizmann.ac.il/~oded/COL/cfh.pdf\n\n- Verkle.info: https://verkle.info/\n\nWhat is left to\ndo, and what are the tradeoffs?\n\nThe main remaining work to do is:\n\n- More analysis on the consequences of EIP-4762\n(statelessness gas cost changes)\n\n- More work finalizing and testing the transition procedure, which is\na large part of the complexity of any statelessness EIP\n\n- More security analysis of Poseidon, Ajtai and other \"STARK-friendly\"\nhash functions\n\n- More development of ultra-efficient STARK protocols for\n\"conservative\" (or \"traditional\") hash functions, eg. based on ideas\nfrom Binius\nor GKR.\n\nWe also will soon have a decision point of which of three options to\ntake: (i) Verkle trees, (ii) STARK-friendly\nhash functions, and (iii) conservative hash\nfunctions. Their properties can be approximately summarized in\nthis table:\n\nAlgorithm\nProof size\nSecurity assumptions\nWorst-case prover time (today)\n\nVerkle\nData plus ~100-2,000 kB\nElliptic curve (not quantum-resistant)\n< 1s\n\nSTARK over conservative hash functions (eg. SHA256,\nBLAKE)\nData plus ~100-300 kB\nConservative hash functions\n> 10 s\n\nSTARK over aggressive hash functions (Poseidon,\nAjtai)\nData plus ~100-300 kB\nRelatively new and less-tested hash functions\n1-2s\n\nIn addition to these \"headline numbers\", there are a few other\nimportant considerations:\n\n- Today, the Verkle tree code is quite mature. Using\nanything but Verkle would realistically delay deployment, likely by one\nhard fork. This can be okay, especially if we need the extra time anyway\nto work on hash function analysis or prover implementations, and if we\nhave other important features we want to get included in Ethereum\nearlier.\n\n- Updating the state root is faster with hashes than\nwith Verkle trees. This means that hash-based approaches can lead to\nlower sync time for full nodes.\n\n- Verkle trees have interesting witness update\nproperties - Verkle tree witnesses are updateable. This\nproperty is useful for mempools, inclusion lists, and other use cases,\nand it also can potentially help with making implementations more\nefficient: if a state object is updated, you can update the witness on\nthe second last level without even reading the last level.\n\n- Verkle trees are more difficult to SNARK-prove. If\nwe want to reduce the proof size all the way to a few kilobytes, Verkle\nproofs introduce some difficulty. This is because the verification of a\nVerkle proof introduces a large number of 256-bit operations, which\nrequires the proof system to either have a lot of overhead, or itself\nhave a custom internal construction with a 256-bit part for the Verkle\nproof. This is not a problem for statelessness itself, but introduces\nmore difficulties later.\n\nIf we want the Verkle witness updateability properties in a way\nthat's quantum-safe, and reasonably efficient, one other possible path\nis lattice-based Merkle\ntrees.\n\nIf proof systems end up being not efficient enough in the worst case,\none other \"rabbit out of a hat\" that we could use to compensate for such\nan inadequacy is multidimensional\ngas: have separate gas limits for (i) calldata, (ii) computation,\n(iii) state accesses, and possibly other distinct resources.\nMultidimensional gas adds complexity, but in exchange it much more\ntightly bounds the ratio between average case and worst case. With\nmultidimensional gas, the theoretical max number of branches to prove\ncould plausibly decrease from 30,000,000 / 2400 = 12,500 to\neg. 3000. This would make BLAKE3 (just barely) sufficient even\ntoday, with no further prover improvements.\n\n Multidimensional gas allows the resource limits of a\nblock to much more closely replicate the resource limits of the\nunderlying hardware. \n\nAnother \"rabbit out of a hat\" is this\nproposal to delay state root computation until the slot\nafter a block. This would give us a full 12 seconds to\ncompute the state root, meaning that only ~60,000 hashes/sec proving\ntime is sufficient even in the most extreme cases, again putting us in\nthe range of BLAKE3 being just barely sufficient.\n\nThis approach has the downside that it would increase light client\nlatency by a slot, though there are more clever versions of the\ntechnique that reduce this delay to just the proof generation\nlatency. For example, the proof could be broadcasted across the network\nas soon as any node generates it, instead of waiting for the next\nblock.\nHow does\nit interact with other parts of the roadmap?\n\nSolving statelessness greatly increases the ease of solo staking.\nThis becomes much more valuable if technologies that reduce the minimum\nbalance of solo staking, such as Orbit SSF or application-layer\nstrategies like squad staking,\nbecome available.\n\nMultidimensional gas becomes easier if EOF is also introduced. This\nis because a key\ncomplexity of multidimensional gas for execution is handling\nsub-calls which don't pass along the parent call's full gas, and EOF\nmakes this problem trivial by simply making such sub-calls illegal (and\nnative account abstraction would provide an in-protocol alternative for\nthe current primary use case of partial-gas sub-calls).\n\nOne other important synergy is between stateless validation and\nhistory expiry. Today, clients have to store nearly a\nterabyte of history data; this data is several times larger than the\nstate. Even if clients are stateless, the dream of\nnearly-storage-free clients will not be realized unless we can relieve\nclients of the responsibility to store history as well. The\nfirst step in this regard is EIP-4444, which also\nimplies storing historical data in torrents or the Portal network.\n\nValidity proofs of EVM\nexecution\nWhat problem are we\ntrying to solve?\n\nThe long-term goal for Ethereum block verification is clear: you\nshould be able to verify an Ethereum block by (i) downloading\nthe block, or perhaps even only small parts of the block with\ndata availability sampling, and (ii) verifying a small\nproof that the block is valid. This would be an extremely\nlow-resource operation, and could be done on a mobile client, inside a\nbrowser wallet, or even (without the data availability part) in another\nchain.\n\nGetting to this point requires having SNARK or STARK proofs of (i)\nthe consensus layer (ie. the proof of stake), and (ii)\nthe execution layer (ie. the EVM). The former is itself\na challenge, and it should be addressed during the process of making\nfurther ongoing improvements to the consensus layer (eg. for single slot\nfinality). The latter requires proofs of EVM execution.\nWhat is it and how does it\nwork?\n\nFormally, in the Ethereum specifications, the EVM is defined as a\nstate transition function: you have some\npre-state S, a block\nB, and you are computing a post-state\nS' = STF(S, B). If a user is using a light client, they do\nnot have S and S' or even B in\ntheir entirety; instead, they have a pre-state root\nR, and a post-state root R'\nand a block hash H. The full statement that needs to be\nproven is approximately:\n\n- Public inputs: pre-state root R,\npost-state root R' , block hash H\n\n- Private inputs: block body B , the\nobjects in the state accessed by the block Q, the same\nobjects after executing the block Q' , the state proof (eg.\nMerkle branches) P\n\n- Claim 1: P is a valid proof that\nQ contains some portion of the state represented by\nR\n\n- Claim 2: if you run STF on Q , (i) the\nexecution only accesses objects inside of Q, (ii) the block\nis valid, and (iii) the outcome is Q'\n\n- Claim 3: If you recompute the new state root, using\ninformation from Q' and P , you get\nR'\n\nIf this exists, you can have a light client that fully verifies the\nEthereum EVM execution. This allows clients to be quite low-resource\nalready. To get to true fully-verifying Ethereum clients, you\nalso need to do the same for the consensus side.\n\nImplementations of validity proofs for EVM computation already exist,\nand are being used heavily by layer 2s. However, there is still a lot to\ndo to make EVM validity proofs viable for L1.\nWhat are some links\nto existing research?\n\n- EF PSE ZK-EVM (now sunsetted because better options exist): https://github.com/privacy-scaling-explorations/zkevm-circuits\n\n- Zeth, which works by compiling the EVM into the RISC-0 ZK-VM: https://github.com/risc0/zeth\n\n- ZK-EVM formal verification project: https://verified-zkevm.org/\n\nWhat is left to\ndo, and what are the tradeoffs?\n\nToday, validity proofs for the EVM are inadequate in two dimensions:\nsecurity and prover time.\n\nA secure validity proof involves having an assurance that the SNARK\nactually verifies the EVM computation, and does not have a bug in it.\nThe two leading techniques for increasing security are multi-provers and formal\nverification. Multi-provers means having multiple\nindependently-written validity proof implementations, much like there\nare multiple clients, and having clients accept a block if it's proven\nby a sufficiently large subset of these implementations. Formal\nverification involves using tools often used to prove mathematical\ntheorems, such as Lean4, to prove that a\nvalidity proof only accepts inputs that are correct executions of the\nunderlying EVM specification (eg. the EVM K\nSemantics or the Ethereum execution\nlayer specifications (EELS) written in python).\n\nSufficiently fast prover time means that any Ethereum block can be\nproven in less than ~4 seconds. Today, we are still far from this,\nthough we are much closer than was imagined possible even two years ago.\nTo get to this goal, we need to advance in three directions:\n\n- \n\nParallelization - the fastest EVM prover today\ncan prove an average Ethereum block in ~15 seconds. It does\nthis by parallelizing between several hundred GPUs, and then aggregating\ntheir work together at the end. Theoretically, we know exactly how to\nmake an EVM prover that can prove computation in O(log(N)) time: have\none GPU do each step, and then do an \"aggregation tree\":\n\nThere are challenges in implementing this. To work even in worst-case\nsituations, where a very large transaction takes up an entire block, the\nsplitting of the computation cannot be per-tx; it must be per-opcode (of\nthe EVM or of an underlying VM like RISC-V). A key implementation\nchallenge that makes this not completely trivial is the need to\nmake sure that the \"memory\" of the VM is consistent between different\nparts of the proof. However, if we can make this kind of\nrecursive proof, then we know that at least the prover latency problem\nis solved, even without any improvements on any of the other\naxes.\n\n- \n\nProof system optimization - new proof systems\nlike Orion, Binius,\nGKR,\nand many more, are likely to lead to yet another large reduction in\nprover time for general-purpose computation.\n\n- \n\nEVM gas cost other changes - many things in the\nEVM could be optimized to be much more prover-friendly, especially in\nthe worst case. Being able to prove an average Ethereum\nblock in 4 seconds is not enough, if an attacker can construct a block\nthat will clog up provers' time for ten minutes. The EVM changes\nrequired can largely be broken down into two categories:\n\n- Gas cost changes - if an operation takes a long\ntime to prove, it should have a high gas cost, even if it is relatively\nfast to compute. EIP-7667 is one\nproposed EIP to handle the worst offenders in this regard: it\nsignificantly increases the gas costs of (conventional) hash functions\nexposed as relatively cheap opcodes and precompiles. To compensate for\nthese gas cost increases, we can reduce the gas cost of EVM opcodes that\nare relatively cheap to prove, so as to keep average throughput the\nsame.\n\n- Data structure replacements - in addition to\nreplacing the state tree with a more STARK-friendly alternative, we need\nto replace the transaction list, receipt tree, and other structures that\nare expensive to prove. Etan Kissling's EIPs that move transaction and\nreceipt structures to SSZ ([1] [2] [3]) are one step in\nthat direction.\n\nIn addition to this, the two \"rabbits out of a hat\" mentioned in the\nprevious section (multidimensional gas, and\ndelayed state root) can also help here. However, it's\nworth noting that unlike stateless validation, where using either rabbit\nout of a hat means that we have sufficient technology to do what we need\ntoday, even with these techniques full ZK-EVM validation will\ntake more work - it will just take less work.\n\nOne thing that was not mentioned above is prover\nhardware: using GPUs, FPGAs and ASICs to generate proofs\nfaster. Fabric\nCryptography, Cysic and Accseal are three companies pushing\nahead on this. This will be extremely valuable for layer 2s, but it's\nunlikely to become a decisive consideration for layer 1, because there\nis a strong desire to keep layer 1 highly decentralized, which implies\nthat proof generation must be within the capabilities of a reasonably\nlarge subset of Ethereum users, and should not be bottlenecked by a\nsingle company's hardware. Layer 2s can make more aggressive\ntradeoffs.\n\nMore work remains to be done in each of these areas:\n\n- Parallelized proving requires proof systems where different parts of\nthe proof can \"share a memory\" (eg. lookup tables). We know techniques\nfor doing this, but they need to be implemented.\n\n- We need more analysis to figure out the ideal set of gas cost\nchanges to minimize worst-case prover time.\n\n- We need more work on proving systems\n\nLikely tradeoffs here include:\n\n- Security vs prover time: it may be possible to cut\ndown prover time using aggressive choices for hash functions, proof\nsystems with more complexity or more aggressive security assumptions, or\nother design choices.\n\n- Decentralization vs prover time: the community\nneeds to agree on what is the \"spec\" for prover hardware that it is\ntargeting. Is it okay to require provers to be large-scale entities? Do\nwe want a high-end consumer laptop to be able to prove an Ethereum block\nin 4 seconds? Something in between?\n\n- Degree of breaking backwards compatibility:\ninsufficiencies in other areas can be compensated for by making much\nmore aggressive gas cost changes, but this is more likely to\ndisproportionately increase the cost of some applications over others,\nand force developers to rewrite and redeploy code in order to remain\neconomically viable. Similarly, the \"rabbits out of a hat\" have their\nown complexity and downsides.\n\nHow does\nit interact with other parts of the roadmap?\n\nThe core tech needed to make EVM validity proofs at layer 1 happen is\nheavily shared with two other areas:\n\n- Validity proofs at layer 2 (ie. \"ZK rollups\")\n\n- The \"STARK a binary hash proof\" approach to statelessness\n\nA successful implementation of validity proofs at layer 1 allows for\nthe ultimate in easy solo staking: even the weakest computer (including\nphone or smart watch) would be able to stake. This further increases the\nvalue of addressing the other limitations to solo staking (eg. the 32\nETH minimum).\n\nAdditionally, EVM validity proofs at L1 can enable\nconsiderable L1 gas limit increases.\n\n## Validity proofs of consensus\n\nWhat problem are we\ntrying to solve?\n\nIf we want it to be possible to fully verify an Ethereum block with a\nSNARK, then the EVM execution is not the only part we need to prove. We\nalso need to prove the consensus: the part of the system that\nhandles deposits, withdrawals, signatures, validator balance updates,\nand other elements of the proof-of-stake part of Ethereum.\n\nThe consensus is considerably simpler than the EVM, but it has the\nchallenge that we don't have layer 2 EVM rollups as a reason why most of\nthe work is going to be done anyway. Hence, any implementation of\nproving Ethereum consensus would need to be done \"from scratch\",\nalthough the proof systems themselves, are shared work that can be built\non top of.\nWhat is it and how does it\nwork?\n\nThe beacon chain is defined as a state\ntransition function, just like the EVM. The state transition\nfunction is dominated by three things:\n\n- ECADDs (for verifying BLS signatures)\n\n- Pairings (for verifying BLS signatures)\n\n- SHA256 hashes (for reading and updating the state)\n\nIn each block, we need to prove 1-16 BLS12-381\nECADDs per validator (potentially more than one,\nbecause signatures can get included in multiple aggregates). This can be\ncompensated for by subset precomputation techniques, so altogether we\ncan say that it's one BLS12-381 ECADD per validator. Today, there are\n~30,000 validators signing in each slot. In the future, with single slot\nfinality, this could change in either direction (see the\nexposition here): if we take the \"brute force\" route, this could\nincrease to 1 million validators per slot. Meanwhile, with Orbit SSF, it\nwould stay at 32,768, or even decrease to 8,192.\n\n How BLS aggregation works. Verifying the aggregate\nsignature only requires an ECADD per participant, instead of an ECMUL.\nBut 30,000 ECADDs is still a lot to prove. \n\nFor pairings, there is a current maximum of 128 attestations per\nslot, implying the need to verify 128 pairings. With EIP-7549 and further\nchanges, this can plausibly decrease to 16 per slot. Pairings are few in\nnumber, but they are extremely expensive: each one takes thousands of\ntimes longer to run (or prove) than an ECADD.\n\nA major challenge with proving BLS12-381 operations is that there is\nno convenient curve whose curve order equals the BLS12-381 field size,\nwhich adds considerable overhead to any proving system. The Verkle trees\nproposed for Ethereum, on the other hand, were built with the Bandersnatch curve,\nwhich makes BLS12-381 itself the natural curve to use in a SNARK system\nto prove a Verkle branch. A fairly naive implementation can prove ~100\nG1 additions per second; clever techniques like GKR would almost\ncertainly be required to make proving fast enough.\n\nFor SHA256 hashes, today the worst case is the epoch\ntransition block, where the entire validator short-balance tree, and a\nsignificant number of validator balances, gets updated. The validator\nshort-balance tree has one byte per validator, so ~1 MB of data gets\nre-hashed. This corresponds to 32,768 SHA256 calls. If a thousand\nvalidators' balances fall above or below a threshold that requires the\neffective balance, in the validator record, to be updated, that\ncorresponds to a thousand Merkle branches, so perhaps another ten\nthousand hashes. The shuffling\nmechanism requires 90 bits per validator (so, 11 MB of data), but\nthis can be computed at any time over the course of an epoch. With\nsingle-slot finality, these numbers may again increase or decrease\ndepending on the details. Shuffling becomes unnecessary, though Orbit\nmay bring back the need for some degree of it.\n\nAnother challenge is the need to read all validator\nstate, including public keys, in order to verify a block.\nReading the public keys alone takes 48 million bytes for 1 million\nvalidators, together with Merkle branches. This requires millions of\nhashes per epoch. If we had to prove the proof-of-stake validation\ntoday, a realistic approach would be some form of incrementally\nverifiable computation: store a separate data structure inside the proof\nsystem that is optimized for efficient lookups, and prove updates to\nthis structure.\n\nTo summarize, there are a lot of challenges.\n\nFixing these challenges most efficiently may well require a deep\nredesign of the beacon chain, which could happen at the same time as a\nswitch to single-slot finality. Features of this redesign could\ninclude:\n\n- Hash function change: today, the \"full\" SHA256 hash\nfunction gets used, so due to padding each call corresponds to two\nunderlying compression function calls. At the very least, we can get a\n2x gain by switching to the SHA256 compression function. If we switch to\nPoseidon, we can get a potentially ~100x gain, which could solve all of\nour problems (at least for hashes) completely: at 1.7 million hashes (54\nMB) per second, even a million validator records can be \"read\" into a\nproof in a few seconds.\n\n- If Orbit, store shuffled validator records\ndirectly: if you choose some number of validators (eg. 8,192 or\n32,768) to be the committee for a given slot, put them directly into the\nstate beside each other, so that the minimum amount of hashing is needed\nto read all validator pubkeys into a proof. This would also allow all\nbalance updates to be done efficiently.\n\n- Signature aggregation: any high-performance\nsignature aggregation scheme will realistically involve some kind of\nrecursive-proving, where intermediate proofs of subsets of signatures\nwill get made by various nodes in the network. This naturally splits the\nload of proving across many nodes in the network, making the work of the\n\"final prover\" much smaller.\n\n- Other signature schemes: For a Lamport+Merkle\nsignature, we need 256 + 32 hashes to verify a signature; multiplying by\n32,768 signers gives 9,437,184 hashes. Optimizations to the signature\nscheme can improve this further by a small constant factor. If we use\nPoseidon, this is within range to prove within a single slot.\nRealistically, though, this would be made much faster with recursive\naggregation schemes.\n\nWhat are some links\nto existing research?\n\n- Succinct, proof of Ethereum consensus (sync committee only): https://github.com/succinctlabs/eth-proof-of-consensus\n\n- Succinct, Helios inside SP1: https://github.com/succinctlabs/sp1-helios\n\n- Succinct BLS12-381 precompiles: https://blog.succinct.xyz/succinctshipsprecompiles/\n\n- Halo2-based verification of BLS aggregate signatures: https://ethresear.ch/t/zkpos-with-halo2-pairing-for-verifying-aggregate-bls-signatures/14671\n\nWhat is left to\ndo, and what are the tradeoffs?\n\nRealistically, it will take years before we have a validity proof of\nthe Ethereum consensus. This is roughly the same timeline that we need\nto implement single slot finality, Orbit, changes to the signature\nalgorithm, and potentially security analysis needed to become\nsufficiently confident to use \"aggressive\" hash functions like Poseidon.\nHence, it makes the most sense to work on these other problems, and\nwhile doing that work keep STARK-friendliness in mind.\n\nThe main tradeoff may well be in order of operations, between a more\nincremental approach to reforming the Ethereum consensus layer and a\nmore radical \"many changes at once\" approach. For the EVM, an\nincremental approach makes sense, because it minimizes disruption to\nbackwards compatibility. For the consensus layer, backwards\ncompatibility concerns are smaller, and there are benefits to a more\n\"holistic\" re-think in various details of how the beacon chain is\nconstructed, to best optimize for SNARK-friendliness.\nHow does\nit interact with other parts of the roadmap?\n\nSTARK-friendliness needs to be a primary concern in long-term\nredesigns of the Ethereum proof of stake consensus, most notably\nsingle-slot finality, Orbit, changes to the signature scheme, and\nsignature aggregation.",
    "contentLength": 36259,
    "summary": "The Verge aims to make Ethereum verification so efficient that mobile wallets and smartwatches can run full nodes using Verkle trees or STARKs.",
    "detailedSummary": {
      "theme": "Vitalik outlines The Verge roadmap for making Ethereum block verification so computationally efficient that any device, including mobile wallets and smartwatches, can fully verify the blockchain by default.",
      "summary": "Vitalik presents The Verge as a comprehensive vision for achieving maximally resource-efficient verification of the Ethereum chain, moving beyond the original focus on Verkle trees to include SNARK verification of all Ethereum execution. The core challenge is that running a fully-verifying Ethereum node currently requires hundreds of gigabytes of storage and complex setup processes, limiting decentralization. Vitalik explores three main technical approaches: stateless validation (using either Verkle trees or STARKed binary hash trees), validity proofs of EVM execution, and validity proofs of consensus mechanisms. For stateless validation, he compares Verkle trees (faster deployment, quantum-vulnerable) against STARK-based approaches (quantum-resistant, longer development timeline). The ultimate goal is enabling verification on resource-constrained devices like smartphones and smartwatches, which would dramatically improve Ethereum's decentralization and accessibility while maintaining security guarantees.",
      "takeaways": [
        "The Verge aims to make Ethereum verification so lightweight that mobile wallets and smartwatches can verify blocks by default, requiring only a few GB of storage instead of hundreds",
        "Three competing approaches exist for stateless validation: Verkle trees (mature but quantum-vulnerable), STARKs with conservative hash functions (secure but slow), and STARKs with new hash functions (fast but unproven)",
        "Current Ethereum state storage grows by ~30GB annually and requires complex setup, creating barriers to running fully-verifying nodes and limiting decentralization",
        "Achieving full SNARK verification requires proving both EVM execution and consensus mechanisms, with consensus proving being more challenging due to BLS signature verification complexity",
        "Technical solutions like multidimensional gas limits and delayed state root computation could serve as backup options if primary approaches prove insufficient"
      ],
      "controversial": [
        "The potential adoption of relatively new and less-tested hash functions like Poseidon for critical blockchain infrastructure, which Vitalik acknowledges may lack sufficient security analysis",
        "The tradeoff between deployment speed and quantum resistance, where choosing Verkle trees enables faster implementation but creates future technical debt requiring another migration",
        "The suggestion that breaking backwards compatibility through aggressive gas cost changes might be necessary to achieve verification efficiency goals"
      ]
    }
  },
  {
    "id": "general-2024-10-20-futures3",
    "title": "Possible futures of the Ethereum protocol, part 3: The Scourge",
    "date": "2024-10-20",
    "category": "protocol",
    "url": "https://vitalik.eth.limo/general/2024/10/20/futures3.html",
    "path": "general/2024/10/20/futures3.html",
    "content": "- \n\n- \n\n  \n    \n\n  \n  \n  Dark Mode Toggle\n  \n    \n\n  \n\n##  Possible futures of the Ethereum protocol, part 3: The Scourge \n\n 2024 Oct 20 \nSee all posts\n\n \n \n\n Possible futures of the Ethereum protocol, part 3: The Scourge \n\nSpecial thanks to Justin Drake, Caspar Schwarz-Schilling, Phil\nDaian, Dan Robinson, Charlie Noyes and Max Resnick for feedback and\nreview, and the ethstakers community for discussion.\n\nOne of the biggest risks to the Ethereum L1 is proof-of-stake\ncentralizing due to economic pressures. If there are economies-of-scale\nin participating in core proof of stake mechanisms, this would naturally\nlead to large stakers dominating, and small stakers dropping out to join\nlarge pools. This leads to higher risk of 51% attacks, transaction\ncensorship, and other crises. In addition to the centralization risk,\nthere are also risks of value extraction: a small group\ncapturing value that would otherwise go to Ethereum's users.\n\nOver the last year, our understanding of these risks has increased\ngreatly. It's well understood that there are two key places where this\nrisk exists: (i) block construction, and (ii)\nstaking capital provision. Larger actors can afford to\nrun more sophisticated algorithms (\"MEV extraction\") to generate blocks,\ngiving them a higher revenue per block. Very large actors can also more\neffectively deal with the inconvenience of having their capital locked\nup, by releasing it to others as a liquid staking token (LST). In\naddition to the direct questions of small vs large stakers, there is\nalso the question of whether or not there is (or will be) too\nmuch staked ETH.\n\n The Scourge, 2023 roadmap \n\nThis year, there have been significant advancements on block\nconstruction, most notably convergence on \"committee inclusion lists\nplus some targeted solution for ordering\" as the ideal solution, as well\nas significant research on proof of stake economics, including ideas\nsuch as two-tiered staking models and reducing issuance to cap the\npercent of ETH staked.\n\n## The Scourge: key goals\n\n- Minimize centralization risks at Ethereum's staking layer (notably,\nin block construction and capital provision, aka. MEV and staking\npools)\n\n- Minimize risks of excessive value extraction from users\n\n## In this chapter\n\n- Fixing the block construction pipeline\n\n- Fixing staking economics\n\n- Application-layer solutions\n\nFixing the block\nconstruction pipeline\n\n## What problem are we solving?\n\nToday, Ethereum block construction is largely done through\nextra-protocol propser-builder separation with MEVBoost. When a validator gets\nan opportunity to propose a block, they auction off the job of choosing\nblock contents to specialized actors called builders. The task of\nchoosing block contents that maximize revenue is very economies-of-scale\nintensive: specialized algorithms are needed to determine which\ntransactions to include, in order to extract as much value as possible\nfrom on-chain financial gadgets and users' transactions interacting with\nthem (this is what is called \"MEV extraction\"). Validators are left with\nthe relatively economies-of-scale-light \"dumb pipe\" task of listening\nfor bids and accepting the highest bid, as well as other\nresponsibilities like attesting.\n\n Stylized diagram of what MEVBoost is doing: specialized\nbuilders take on the tasks in the red, and stakers take on the tasks in\nblue. \n\nThere are various versions of this, including \"proposer-builder\nseparation\" (PBS) and \"attester-proposer separation\" (APS). The\ndifference between these has to do with fine-grained details around\nwhich responsibilities go to which of the two actors: roughly, in PBS,\nvalidators still propose blocks, but receive the payload from builders,\nand in APS, the entire slot becomes the builder's responsibility.\nRecently, APS is preferred over PBS, because it further reduces\nincentives for proposers to co-locate with builders. Note that APS would\nonly apply to execution blocks, which contain transactions;\nconsensus blocks, which contain proof-of-stake-related data\nsuch as attestations, would still be randomly assigned to\nvalidators.\n\nThis separation of powers helps keep validators decentralized, but it\nhas one important cost: the actors that are doing the \"specialized\"\ntasks can easily become very centralized. Here's Ethereum block\nbuilding today:\n\nTwo actors are choosing the contents of roughly 88% of Ethereum\nblocks. What if those two actors decide to censor a transaction? The\nanswer is not quite as bad as it might seem: they are not able to reorg\nblocks, and so you don't need 51% censoring to prevent a transaction\nfrom getting included at all: you need 100%. With 88% censoring, a user\nwould need to wait an average of 9 slots to get included (technically,\nan average of 114 seconds, instead of 6 seconds). For some use cases,\nwaiting for two or even five minutes for certain transactions is fine.\nBut for other use cases, eg. defi liquidations, even the ability to\ndelay inclusion of someone else's transaction by a few blocks is a\nsignificant market manipulation risk.\n\nThe strategies that block builders can employ to maximize revenue can\nalso have other negative consequences for users. A \"sandwich\nattack\" could cause users making token swaps to suffer significant\nlosses from slippage. The transactions introduced to make these attacks\nclog the chain, increasing gas prices for other users.\nWhat is it, and how does it\nwork?\n\nThe leading solution is to break down the block production task\nfurther: we give the task of choosing transactions back to the proposer\n(ie. a staker), and the builder can only choose the ordering and insert\nsome transactions of their own. This is what inclusion\nlists seek to do.\n\nAt time T, a randomly selected staker creates an inclusion list, a\nlist of transactions that are valid given the current state of the\nblockchain at that time. At time T+1, a block builder, perhaps chosen\nthrough an in-protocol auction mechanism ahead of time,\ncreates a block. This block is required to include every transaction in\nthe inclusion list, but they can choose the order, and they can add in\ntheir own transactions.\n\nFork-choice-enforced inclusion lists (FOCIL)\nproposals involve a committee of multiple inclusion list\ncreators per block. To delay a transaction by one block, k\nof k inclusion list creators (eg. k = 16 )\nwould have to censor the transaction. The combination of FOCIL with a\nfinal proposer chosen by auction that is required to include the\ninclusion lists, but can reorder and add new transactions, is often\ncalled \"FOCIL + APS\".\n\nA different approach to the problem is multiple concurrent\nproposers (MCP) schemes such as BRAID. BRAID\nseeks to avoid splitting up the block proposer role into a\nlow-economies-of-scale part and a high-economies-of-scale part, and\ninstead tries to distribute the block production process among many\nactors, in such a way that each proposer only needs to have a medium\namount of sophistication to maximize their revenue. MCP works by having\nk parallel proposers generate lists of transactions, and\nthen using a deterministic algorithm (eg. order by highest-to-lowest\nfee) to choose the order.\n\nBRAID does not seek to attain the goal of dumb-pipe block proposers\nrunning default software being optimal. Two easy-to-understand reasons\nwhy it cannot do so are:\n\n- Last-mover arbitrage attacks: suppose that the\naverage time that proposers submit is T, and the last possible\ntime you can submit and still get included is around T+1. Now, suppose\nthat on centralized exchanges, the ETH/USDC price moves from $2500 to\n$2502 between T and T+1. A proposer can wait an extra second and add an\nadditional transaction to arbitrage on-chain decentralized exchanges,\nclaiming up to $2 per ETH in profit. Sophisticated proposers who are\nvery well-connected to the network have more ability to do this.\n\n- Exclusive order flow: users have the incentive to\nsend transactions directly to one single proposer, to minimize their\nvulnerability to front-running and other attacks. Sophisticated\nproposers have an advantage because they can set up infrastructure to\naccept these direct-from-user transactions, and they have stronger\nreputations so users who send them transactions can trust that the\nproposer will not betray and front-run them (this can be mitigated with\ntrusted hardware, but then trusted hardware has trust assumptions of its\nown)\n\nIn BRAID, attesters can still be separated off and run as a dumb-pipe\nfunctionality.\n\nIn addition to these two extremes, there is a spectrum of\npossible designs in between. For example, you could auction off\na role that only has the right to append to a block, and not to\nreorder or prepend. You could even let them append or prepend, but not\ninsert in the middle or reorder. The attraction of these techniques is\nthat the winners of the auction market are likely to be very concentrated,\nand so there is a lot of benefit to reducing their authority.\n\n## Encrypted mempools\n\nOne technology that is crucial to the successful implementation of\nmany of these designs (specifically, either BRAID or a version of APS\nwhere there are strict limits on the capability being auctionef off) is\nencrypted mempools. Encrypted mempools are a technology\nwhere users broadcast their transactions in encrypted form, along with\nsome kind of proof of their validity, and the transactions are included\ninto blocks in encrypted form, without the block builder knowing the\ncontents. The contents of the transactions are revealed later.\n\nThe main challenge in implementing encrypted mempools is coming up\nwith a design that ensures that transactions do all get revealed later:\na simple \"commit and reveal\" scheme does not work, because if revealing\nis voluntary, the act of choosing to reveal or not reveal is itself a\nkind of \"last-mover\" influence on a block that could be exploited. The\ntwo leading techniques for this are (i) threshold\ndecryption, and (ii) delay encryption, a primitive closely related\nto verifiable delay functions\n(VDFs).\nWhat are some links to\nexisting research?\n\n- Explainer on MEV and builder centralization: https://vitalik.eth.limo/general/2024/05/17/decentralization.html#mev-and-builder-dependence\n\n- MEVBoost: https://github.com/flashbots/mev-boost\n\n- Enshrined PBS (an earlier proposed solution to these problems): https://ethresear.ch/t/why-enshrine-proposer-builder-separation-a-viable-path-to-epbs/15710\n\n- Mike Neuder's list of inclusion list-related readings: https://gist.github.com/michaelneuder/dfe5699cb245bc99fbc718031c773008\n\n- Inclusion list EIP: https://eips.ethereum.org/EIPS/eip-7547\n\n- FOCIL: https://ethresear.ch/t/fork-choice-enforced-inclusion-lists-focil-a-simple-committee-based-inclusion-list-proposal/19870\n\n- Presentation on BRAID by Max Resnick: https://www.youtube.com/watch?v=mJLERWmQ2uw\n\n- \"Priority is All You Need\", by Dan Robinson: https://www.paradigm.xyz/2024/06/priority-is-all-you-need\n\n- On multi-proposer gadgets and protocols: https://hackmd.io/xz1UyksETR-pCsazePMAjw\n\n- VDFresearch.org: https://vdfresearch.org/\n\n- Verifiable delay functions and attacks (focuses on the RANDAO\nsetting, but also applicable to encrypted mempools): https://ethresear.ch/t/verifiable-delay-functions-and-attacks/2365\n\n- MEV Capture and Decentralization in Execution Tickets: https://www.arxiv.org/pdf/2408.11255\n\n- Centralization in APS: https://arxiv.org/abs/2408.03116\n\n- Multi-block MEV and inclusion lists: https://x.com/%5fcharlienoyes/status/1806186662327689441\n\nWhat is left to\ndo, and what are the tradeoffs?\n\nWe can think of all of the above schemes as being different ways of\ndividing up the authority involved in staking, arranged on a spectrum\nfrom lower economies of scale (\"dumb-pipe\") to higher economies of scale\n(\"specialization-friendly\"). Pre-2021, all of these authorities were\nbundled together in one actor:\n\nThe core conundrum is this: any meaningful authority that\nremains in the hands of stakers, is authority that could end up being\n\"MEV-relevant\". We want a highly decentralized set of actors to\nhave as much authority as possible; this implies (i) putting a lot of\nauthority in the hands of stakers, and (ii) making sure stakers are as\ndecentralized as possible, meaning that they have few\neconomies-of-scale-driven incentives to consolidate. This is a difficult\ntension to navigate.\n\nOne particular challenge is multi-block MEV: in some cases,\nexecution auction winners can make even more money if they capture\nmultiple slots in a row, and do not allow any MEV-relevant\ntransactions in blocks other than the last one that they control. If\ninclusion lists force them to, then they can try to bypass that by not\npublishing any block at all during those slots. One could make\nunconditional inclusion lists, which directly become the block\nif the builder does not provide one, but this makes the\ninclusion list MEV-relevant. The solution here may involve some\ncompromise that involves accepting some low degree of incentive to bribe\npeople to include transactions in an inclusion list, and hoping that\nit's not high enough to lead to mass outsourcing.\n\nWe can view FOCIL + APS as follows. Stakers continue to have the\nauthority on the left part of the spectrum, while the right part of the\nspectrum gets auctioned off to the highest bidder.\n\nBRAID is quite different. The \"staker\" piece is larger, but it gets\nsplit into two pieces: light stakers and heavy stakers. Meanwhile,\nbecause transactions are ordered in decreasing order of priority fee,\nthe top-of-block choice gets de-facto auctioned off via the fee market,\nin a scheme that can be viewed as analogous to enshrined\nPBS.\n\nNote that the safety of BRAID depends heavily on encrypted mempools;\notherwise, the top-of-block auction mechanism becomes vulnerable to\nstrategy-stealing attacks (essentially: copying other people's\ntransactions, swapping the recipient address, and paying a 0.01% higher\nfee). This need for pre-inclusion privacy is also the reason why\nenshrined PBS is so tricky to implement.\n\nFinally, more \"aggressive\" versions of FOCIL + APS, eg. the option\nwhere APS only determines the end of the block, look like this:\n\nThe main remaining task is to (i) work on solidifying the various\nproposals and analyzing their consequences, and (ii) combine this\nanalysis with an understanding of the Ethereum community's goals in\nterms of what forms of centralization it will tolerate. There is also\nwork to be done on each individual proposal, such as:\n\n- Continuing work on encrypted mempool designs, and\ngetting to the point where we have a design that is both robust and\nreasonably simple, and plausibly ready for inclusion.\n\n- Optimizing the design of multiple inclusion lists\nto make sure that (i) it does not waste data, particularly in the\ncontext of inclusion lists covering blobs, and (ii) it\nis friendly to stateless validators.\n\n- More work on the optimal auction design for\nAPS.\n\nAdditionally, it's worth noting that these different proposals are\nnot necessarily incompatible forks on the road from each other. For\nexample, implementing FOCIL + APS could easily serve as a stepping stone\nto implementing BRAID. A valid conservative strategy would be a\n\"wait-and-see\" approach where we first implement a solution where\nstakers' authority is limited and most of the authority is auctioned\noff, and then slowly increase stakers' authority over time as we learn\nmore about the MEV market operation on the live network.\nHow does\nit interact with other parts of the roadmap?\n\nThere are positive interactions between solving one staking\ncentralization bottleneck and solving the others. To give an analogy,\nimagine a world where starting your own company required growing your\nown food, making your own computers and having your own army. In this\nworld, only a few companies could exist. Solving one of the three\nproblems would help the situation, but only a little. Solving two\nproblems would help more than twice as much as solving one. And\nsolving three would be far more than three times as helpful - if you're\na solo entrepreneur, either 3/3 problems are solved or you stand no\nchance.\n\nIn particular, the centralization bottlenecks for staking are:\n\n- Block construction centralization (this section)\n\n- Staking centralization for economic reasons (next section)\n\n- Staking centralization because of the 32 ETH minimum (solved with\nOrbit or other techniques; see the post on the Merge)\n\n- Staking centralization because of hardware requirements (solved in\nthe Verge, with stateless clients and later ZK-EVMs)\n\nSolving any one of the four increases the gains from solving any of\nthe others.\n\nAdditionally, there are interactions between the block construction\npipeline and the single slot finality design, particularly in the\ncontext of trying to reduce slot times. Many block construction\npipeline designs end up increasing slot times. Many block\nconstruction pipelines involve roles for attesters at multiple steps in\nthe process. For this reason, it can be worth thinking about the block\nconstruction pipelines and single slot finality simultaneously.\n\n## Fixing staking economics\n\n## What problem are we solving?\n\nToday, about 30% of the ETH supply is\nactively staking. This is far more than enough to protect Ethereum\nfrom 51% attacks. If the percent of ETH staked grows much larger,\nresearchers fear a different scenario: the risks that would arise if\nalmost all ETH becomes staked. These risks include:\n\n- Staking turns from being a profitable task for specialists into a\nduty for all ETH holders. Hence, the average staker would be much more\nunenthusiastic, and would choose the easiest approach (realistically,\ndelegating their tokens to whichever centralized operator offers the\nmost convenience)\n\n- Credibility of the slashing mechanism weakens if almost all ETH is\nstaked\n\n- A single liquid staking token could take over the bulk of the stake\nand even taking over \"money\" network effects from ETH itself\n\n- Ethereum needlessly issuing an extra ~1m ETH/year. In the case where\none liquid staking token gets dominant network effect, a large portion\nof this value could potentially even get captured by the LST.\n\nWhat is it, and how does it\nwork?\n\nHistorically, one class of solution has been: if everyone staking is\ninevitable, and a liquid staking token is inevitable, then let's make\nstaking friendly to having a liquid staking token that is actually\ntrustless, neutral and maximally decentralized. One simple way to do\nthis is to cap staking penalties at eg. 1/8, which would make 7/8 of\nstaked ETH unslashable, and thus eligible to be put into the same liquid\nstaking token. Another option is to explicitly create two\ntiers of staking: \"risk-bearing\" (slashable) staking, which would\nsomehow be capped to eg. 1/8 of all ETH, and \"risk-free\" (unslashable)\nstaking, which everyone could participate in.\n\nHowever, one criticism of this approach is that it seems\neconomically equivalent to something much simpler: massively reduce\nissuance if the stake approaches some pre-determined\ncap. The basic argument is: if we end up in a world\nwhere the risk-bearing tier has 3.4% returns and the risk-free tier\n(which everyone participates in) has 2.6% returns, that's actually the\nsame thing as a world where staking ETH has 0.8% returns and just\nholding ETH has 0% returns. The dynamics of the risk-bearing tier,\nincluding both total quantity staked and centralization, would be the\nsame in both cases. And so we should just do the simple thing and reduce\nissuance.\n\nThe main counterargument to this line of argument would be if we can\nmake the \"risk-free tier\" still have some useful role and\nsome level of risk (eg. as proposed by\nDankrad here).\n\nBoth of these lines of proposals imply changing the issuance curve,\nin a way that makes returns prohibitively low if the amount of stake\ngets too high.\n\n Left: one proposal for an adjusted issuance curve, by\nJustin Drake. Right: another set of proposals, by Anders Elowsson.\n\nTwo-tier staking, on the other hand, requires setting two\nreturn curves: (i) the return rate for \"basic\" (risk-free or low-risk)\nstaking, and (ii) the premium for risk-bearing staking. There are\ndifferent ways to set these parameters: for example, if you set a hard\nparameter that 1/8 of stake is slashable, then market dynamics will\ndetermine the premium on the return rate that slashable stake gets.\n\nAnother important topic here is MEV capture. Today,\nrevenue from MEV (eg. DEX arbitrage, sandwiching...) goes to proposers,\nie. stakers. This is revenue that is completely \"opaque\" to the\nprotocol: the protocol has no way of knowing if it's 0.01% APR, 1% APR\nor 20% APR. The existence of this revenue stream is highly inconvenient\nfrom multiple angles:\n\n- It is a volatile revenue source, as each individual\nstaker only gets it when they propose a block, which is once every ~4\nmonths today. This creates an incentive to join pools for more stable\nincome.\n\n- It leads to an unbalanced allocation of incentives:\ntoo much for proposing, too little for attesting.\n\n- It makes stake capping very difficult to implement:\neven if the \"official\" return rate is zero, the MEV revenue alone may be\nenough to drive all ETH holders to stake. As a result, a realistic stake\ncapping proposal would in fact have to have returns approach\nnegative infinity, as eg. proposed\nhere. This, needless to say, creates more risk for stakers,\nespecially solo stakers.\n\nWe can solve these problems by finding a way to make MEV revenue\nlegible to the protocol, and capturing it. The earliest proposal was Francesco's\nMEV smoothing; today, it's widely understood that any mechanism for\nauctioning off block proposer rights (or, more generally, sufficient\nauthority to capture almost all MEV) ahead of time accomplishes the same\ngoal.\nWhat are some links\nto existing research?\n\n- Issuance.wtf: https://issuance.wtf/\n\n- Endgame staking economics, a case for targeting: https://ethresear.ch/t/endgame-staking-economics-a-case-for-targeting/18751\n\n- Properties of issuance level, Anders Elowsson: https://ethresear.ch/t/properties-of-issuance-level-consensus-incentives-and-variability-across-potential-reward-curves/18448\n\n- Validator set size capping: https://notes.ethereum.org/@vbuterin/single_slot_finality?type=view#Economic-capping-of-total-deposits\n\n- Thoughts on multi-tier staking ideas: https://notes.ethereum.org/@vbuterin/staking_2023_10?type=view\n\n- Rainbow staking: https://ethresear.ch/t/unbundling-staking-towards-rainbow-staking/18683\n\n- Dankrad's liquid staking proposal: https://notes.ethereum.org/Pcq3m8B8TuWnEsuhKwCsFg\n\n- MEV smoothing, by Francesco: https://ethresear.ch/t/committee-driven-mev-smoothing/10408\n\n- MEV burn, by Justin Drake: https://ethresear.ch/t/mev-burn-a-simple-design/15590\n\nWhat is left to\ndo, and what are the tradeoffs?\n\nThe main remaining task is to either agree to do nothing, and accept\nthe risks of almost all ETH being inside LSTs, or finalize and agree on\nthe details and parameters of one of the above proposals. An approximate\nsummary of the benefits and risks is:\n\nPolicy\nNeed to decide\nRisks to analyze\n\nDo nothing\n* MEV burn implementation, if any\n* Almost 100% of ETH staked, likely in LSTs (perhaps a single\ndominant one)\n* Macroeconomic risks\n\nStake capping (via changing issuance curve)\n* Reward function and parameters (esp.\u00a0what the cap is)\n* MEV\nburn implementation\n* Open question of which stakers enter and leave, possibility that\nremaining staker set is centralized\n\n* Two-tiered staking\n* The role of the risk-free tier\n * Parameters (eg. the economics\nthat determine the amount staked in the risk-bearing tier)\n * MEV\nburn implementation\n* Open question of which stakers enter and leave, possibility that\nrisk-bearing set is centralized\n\nHow does\nit interact with other parts of the roadmap?\n\nOne important point of intersection has to do with solo\nstaking. Today, the cheapest VPSes that can run an Ethereum\nnode cost about $60 per month, primarily due to hard disk storage costs.\nFor a 32 ETH staker ($84,000 at the time of this writing), this\ndecreases APY by (60 * 12) / 84000 ~= 0.85% . If total\nstaking returns drop below 0.85%, solo staking will be unviable for many\npeople at these levels.\n\nIf we want solo staking to continue to be viable, this puts further\nemphasis on the need to reduce node operation costs, which will be done\nin the Verge: statelessness will remove storage space requirements,\nwhich may be sufficient on its own, and then L1 EVM validity proofs will\nmake costs completely trivial.\n\nOn the other hand, MEV burn arguably helps solo staking.\nAlthough it decreases returns for everyone, it more importantly\ndecreases variance, making staking less like a lottery.\n\nFinally, any change in issuance interacts with other fundamental\nchanges to the staking design (eg. rainbow staking). One particular\npoint of concern is that if staking returns become very low, this means\nwe have to choose between (i) making penalties also low,\nreducing disincentives against bad behavior, and (ii) keeping penalties\nhigh, which would increase the set of circumstances in which even\nwell-meaning validators accidentally end up with negative returns if\nthey get unlucky with technical issues or even attacks.\n\n## Application layer solutions\n\nThe above sections focused on changes to the Ethereum L1 that can\nsolve important centralization risks. However, Ethereum is not just an\nL1, it is an ecosystem, and there are also important application-layer\nstrategies that can help mitigate the above risks. A few examples\ninclude:\n\n- Specialized staking hardware solutions - some\ncompanies, such as Dappnode, are\nselling hardware that is specifically designed to make it as easy as\npossible to operate a staking node. One way to make this solution more\neffective, is to ask the question: if a user is already spending the\neffort to have a box running and connected to the internet 24/7, what\nother services could it provide (to the user or to others) that benefit\nfrom decentralization? Examples that come to mind include (i) running\nlocally hosted LLMs, for self-sovereignty and privacy reasons, and (ii)\nrunning nodes for a decentralized VPN.\n\n- Squad staking - this solution from Obol allows multiple\npeople to stake together in an M-of-N format. This will likely get more\nand more popular over time, as statelessness and later L1 EVM validity\nproofs will reduce the overhead of running more nodes, and the benefit\nof each individual participant needing to worry much less about being\nonline all the time starts to dominate. This is another way to reduce\nthe cognitive overhead of staking, and ensure solo staking prospers in\nthe future.\n\n- Airdrops - Starknet gave an airdrop\nto solo stakers. Other projects wishing to have a decentralized and\nvalues-aligned set of users may also consider giving airdrops or\ndiscounts to validators that are identified as probably being solo\nstakers.\n\n- Decentralized block building marketplaces - using a\ncombination of ZK, MPC and TEEs, it's possible to create a decentralized\nblock builder that participates in, and wins, the APS auction game, but\nat the same time provides pre-confirmation privacy and censorship\nresistance guarantees to its users. This is another path toward\nimproving users' welfare in an APS world.\n\n- Application-layer MEV minimization - individual\napplications can be built in a way that \"leaks\" less MEV to L1, reducing\nthe incentive for block builders to create specialized algorithms to\ncollect it. One simple strategy that is universal, though inconvenient\nand composability-breaking, is for the contract to put all incoming\noperations into a queue and execute them in the next block, and auction\noff the right to jump the queue. Other more sophisticated approaches\ninclude doing more work offchain eg. as Cowswap does. Oracles can also be\nredesigned to minimize oracle-extractable\nvalue.",
    "contentLength": 27823,
    "summary": "Ethereum's \"Scourge\" tackles centralizing PoS risks through inclusion lists + encrypted mempools to fix block construction.",
    "detailedSummary": {
      "theme": "Vitalik outlines comprehensive strategies to address Ethereum's centralization risks in staking and block construction through protocol changes and application-layer solutions.",
      "summary": "Vitalik identifies two critical centralization risks facing Ethereum: block construction dominated by specialized builders who extract MEV, and staking capital provision increasingly concentrated in liquid staking tokens. For block construction, Vitalik proposes solutions like inclusion lists (FOCIL) that give transaction selection back to validators while allowing builders to handle ordering, or multi-concurrent proposer schemes like BRAID that distribute block production among many actors. These approaches require encrypted mempools to prevent manipulation. For staking economics, Vitalik discusses reducing ETH issuance when stake levels get too high (currently 30% of supply) and implementing MEV capture mechanisms to make this revenue visible to the protocol rather than creating additional centralization pressures. Vitalik emphasizes that solving multiple centralization bottlenecks simultaneously creates compounding benefits, and notes that upcoming improvements like stateless clients will reduce solo staking costs. Beyond protocol changes, Vitalik highlights application-layer solutions including specialized staking hardware, squad staking for shared validation duties, targeted airdrops to solo stakers, and MEV minimization techniques at the application level.",
      "takeaways": [
        "Two builders currently control 88% of Ethereum block construction, creating significant centralization risks that could enable transaction censorship and value extraction",
        "Inclusion lists (FOCIL) represent the leading solution to decentralize block building by separating transaction selection from ordering and MEV extraction",
        "With 30% of ETH already staked, Ethereum risks having almost all ETH in liquid staking tokens, potentially undermining decentralization and protocol credibility",
        "MEV capture through protocol-level auctions could address multiple problems by making MEV revenue visible and reducing the volatile, pool-favoring nature of current MEV distribution",
        "Solving staking centralization requires addressing multiple bottlenecks simultaneously: block construction, economic incentives, the 32 ETH minimum, and hardware requirements"
      ],
      "controversial": [
        "Implementing stake capping through reduced issuance could potentially make staking returns negative, creating risks especially for solo stakers who face higher operational costs",
        "The proposal to capture and burn MEV revenue represents a significant change to Ethereum's economic model that could face resistance from current stakers benefiting from MEV",
        "Some solutions like BRAID acknowledge that 'dumb-pipe' staking may not be achievable, potentially accepting some degree of centralization as inevitable"
      ]
    }
  },
  {
    "id": "general-2024-10-17-futures2",
    "title": "Possible futures of the Ethereum protocol, part 2: The Surge",
    "date": "2024-10-17",
    "category": "protocol",
    "url": "https://vitalik.eth.limo/general/2024/10/17/futures2.html",
    "path": "general/2024/10/17/futures2.html",
    "content": "- \n\n- \n\n  \n    \n\n  \n  \n  Dark Mode Toggle\n  \n    \n\n  \n\n##  Possible futures of the Ethereum protocol, part 2: The Surge \n\n 2024 Oct 17 \nSee all posts\n\n \n \n\n Possible futures of the Ethereum protocol, part 2: The Surge \n\nSpecial thanks to Justin Drake, Francesco, Hsiao-wei Wang, @antonttc and Georgios\nKonstantopoulos\n\nAt the beginning, Ethereum had two scaling strategies in its roadmap.\nOne (eg. see this\nearly paper from 2015) was \"sharding\": instead of\nverifying and storing all of the transactions in the chain, each node\nwould only need to verify and store a small fraction of the\ntransactions. This is how any other peer-to-peer network (eg.\nBitTorrent) works too, so surely we could make blockchains work the same\nway. Another was layer 2 protocols: networks that would\nsit on top of Ethereum in a way that allow them to fully benefit from\nits security, while keeping most data and computation off the main\nchain. \"Layer 2 protocols\" meant state\nchannels in 2015, Plasma in 2017, and\nthen rollups\nin 2019. Rollups are more powerful than state channels or Plasma, but\nthey require a large amount of on-chain data bandwidth. Fortunately, by\n2019 sharding research had solved the\nproblem of verifying \"data availability\" at scale. As a result, the\ntwo paths converged, and we got the rollup-centric\nroadmap which continues to be Ethereum's scaling strategy today.\n\nThe Surge, 2023 roadmap edition.\n\nThe rollup-centric roadmap proposes a simple division of labor: the\nEthereum L1 focuses on being a robust and decentralized base layer,\nwhile L2s take on the task of helping the ecosystem scale. This is a\npattern that recurs everywhere in society: the court system (L1) is not\nthere to be ultra-fast and efficient, it's there to protect contracts\nand property rights, and it's up to entrepreneurs (L2) to build on top\nof that sturdy\nbase layer\nand take humanity to (metaphorical and literal) Mars.\n\nThis year, the rollup-centric roadmap has seen important successes:\nEthereum L1 data bandwidth has increased greatly with EIP-4844 blobs, and multiple EVM\nrollups are now at stage\n1. A very heterogeneous\nand pluralistic implementation of sharding, where each L2 acts as a\n\"shard\" with its own internal rules and logic, is now reality. But as we\nhave seen, taking this path has some unique challenges of its own. And\nso now our task is to bring the rollup-centric roadmap to\ncompletion, and solve these problems, while preserving the robustness\nand decentralization that makes the Ethereum L1 special.\n\n## The Surge: key goals\n\n- 100,000+ TPS on L1+L2\n\n- Preserve decentralization and robustness of L1\n\n- At least some L2s fully inherit Ethereum's core properties\n(trustless, open, censorship resistant)\n\n- Maximum interoperability between L2s. Ethereum should feel like one\necosystem, not 34 different blockchains.\n\n## In this chapter\n\n- Aside: the scalability trilemma\n\n- Further progress in data availability sampling\n\n- Data compression\n\n- Generalized Plasma\n\n- Maturing L2 proof systems\n\n- Cross-L2 interoperability and UX improvements\n\n- Scaling execution on L1\n\nAside: the scalability\ntrilemma\n\nThe scalability trilemma was an idea introduced\nin 2017, which argued that there is a tension between three\nproperties of a blockchain: decentralization (more\nspecifically: low cost to run a node), scalability\n(more specifically: high number of transactions processed), and\nsecurity (more specifically: an attacker needing to\ncorrupt a large portion of the nodes in the whole network to make even a\nsingle transaction fail).\n\nNotably, the trilemma is not a theorem, and the post\nintroducing the trilemma did not come with a mathematical\nproof. It did give a heuristic mathematical argument: if a\ndecentralization-friendly node (eg. consumer laptop) can verify N\ntransactions per second, and you have a chain that processes k*N\ntransactions per second, then either (i) each transaction is only seen\nby 1/k of nodes, which implies an attacker only needs to corrupt a few\nnodes to push a bad transaction through, or (ii) your nodes are going to\nbe beefy and your chain not decentralized. The purpose of the\npost was never to show that breaking the trilemma is impossible; rather,\nit was to show that breaking the trilemma is hard - it requires\nsomehow thinking outside of the box that the argument implies.\n\nFor many years, it has been common for some high-performance chains\nto claim that they solve the trilemma without doing anything clever at a\nfundamental architecture level, typically by using software engineering\ntricks to optimize the node. This is always misleading, and running a\nnode in such chains always ends up far more difficult than in Ethereum.\nThis\npost gets into some of the many subtleties why this is the case (and\nhence, why L1 client software engineering alone cannot scale Ethereum\nitself).\n\nHowever, the combination of data availability sampling and\nSNARKs does solve the trilemma: it allows a client to verify\nthat some quantity of data is available, and some number of steps of\ncomputation were carried out correctly, while downloading only a small\nportion of that data and running a much smaller amount of computation.\nSNARKs are trustless. Data availability sampling has a nuanced few-of-N\ntrust model, but it preserves the fundamental property that\nnon-scalable chains have, which is that even a 51% attack cannot\nforce bad blocks to get accepted by the network.\n\nAnother way to solve the trilemma is Plasma architectures, which use\nclever techniques to push the responsibility to watch for data\navailability to the user in an incentive-compatible way. Back in\n2017-2019, when all we had to scale computation was fraud proofs, Plasma\nwas very limited in what it could safely do, but the mainstreaming of\nSNARKs makes Plasma architectures far\nmore viable for a wider array of use cases than before.\n\nFurther progress\nin data availability sampling\n\n## What problem are we solving?\n\nAs of 2024 March 13, when the Dencun upgrade\nwent live, the Ethereum blockchain has three ~125 kB \"blobs\" per\n12-second slot, or ~375 kB per slot of data\navailability bandwidth. Assuming transaction data is published onchain\ndirectly, an ERC20 transfer is ~180 bytes, and so the maximum TPS of\nrollups on Ethereum is:\n\n375000 / 12 / 180 = 173.6 TPS\n\nIf we add Ethereum's calldata (theoretical max: 30 million gas per\nslot / 16 gas per byte = 1,875,000 bytes per slot), this becomes\n607 TPS. With PeerDAS, the plan is to increase the blob\ncount target to 8-16, which would give us 463-926 TPS\nin calldata.\n\nThis is a major increase over the Ethereum L1, but it is not enough.\nWe want much more scalability. Our medium-term target is 16 MB\nper slot, which if combined with improvements in rollup data\ncompression would give us ~58,000 TPS.\nWhat is it and how does it\nwork?\n\nPeerDAS is a relatively simple implementation of \"1D sampling\". Each\nblob in Ethereum is a degree-4096 polynomial over a 253-bit prime field.\nWe broadcast \"shares\" of the polynomial, where each share consists of 16\nevaluations at an adjacent 16 coordinates taken from a total set of 8192\ncoordinates. Any 4096 of the 8192 evaluations (with current proposed\nparameters: any 64 of the 128 possible samples) can recover the\nblob.\n\nPeerDAS works by having each client listen on a small number of\nsubnets, where the i'th subnet broadcasts the i'th sample of any blob,\nand additionally asks for blobs on other subnets that it needs by asking\nits peers in the global p2p network (who would be listening to different\nsubnets). A more conservative version, SubnetDAS,\nuses only the subnet mechanism, without the additional layer of\nasking peers. A current proposal is for nodes participating in proof of\nstake to use SubnetDAS, and for other nodes (ie. \"clients\") to use\nPeerDAS.\n\nTheoretically, we can scale 1D sampling pretty far: if we increase\nthe blob count maximum to 256 (so, the target to 128), then we would get\nto our 16 MB target while data availability sampling would only cost\neach node 16 samples * 128 blobs * 512 bytes per sample per blob = 1 MB\nof data bandwidth per slot. This is just barely within our reach of\ntolerance: it's doable, but it would mean bandwidth-constrained clients\ncannot sample. We could optimize this somewhat by decreasing blob count\nand increasing blob size, but this would make reconstruction more\nexpensive.\n\nAnd so ultimately we want to go further, and do 2D\nsampling, which works by random sampling not just\nwithin blobs, but also between blobs. The linear\nproperties of KZG commitments are used to \"extend\" the set of blobs in a\nblock with a list of new \"virtual blobs\" that redundantly encode the\nsame information.\n\n 2D sampling. Source:\na16z crypto \n\nCrucially, computing the extension of the commitments does not\nrequire having the blobs, so the scheme is fundamentally friendly to\ndistributed block construction. The node actually constructing the block\nwould only need to have the blob KZG commitments, and can themslves rely\non DAS to verify the availability of the blobs. 1D DAS is also\ninherently friendly to distributed block construction.\nWhat are some links to\nexisting research?\n\n- Original post introducing data availability (2018): https://github.com/ethereum/research/wiki/A-note-on-data-availability-and-erasure-coding\n\n- Follow-up paper: https://arxiv.org/abs/1809.09044\n\n- Explainer post on DAS, paradigm: https://www.paradigm.xyz/2022/08/das\n\n- 2D availability with KZG commitments: https://ethresear.ch/t/2d-data-availability-with-kate-commitments/8081\n\n- PeerDAS on ethresear.ch: https://ethresear.ch/t/peerdas-a-simpler-das-approach-using-battle-tested-p2p-components/16541\n, and paper: https://eprint.iacr.org/2024/1362\n\n- Presentation on PeerDAS by Francesco: https://www.youtube.com/watch?v=WOdpO1tH_Us\n\n- EIP-7594: https://eips.ethereum.org/EIPS/eip-7594\n\n- SubnetDAS on ethresear.ch: https://ethresear.ch/t/subnetdas-an-intermediate-das-approach/17169\n\n- Nuances of recoverability in 2D sampling: https://ethresear.ch/t/nuances-of-data-recoverability-in-data-availability-sampling/16256\n\nWhat is left to\ndo, and what are the tradeoffs?\n\nThe immediate next step is to finish the implementation and rollout\nof PeerDAS. From there, it's a progressive grind to keep increasing the\nblob count on PeerDAS while carefully watching the network and improving\nthe software to ensure safety. At the same time, we want more academic\nwork on formalizing PeerDAS and other versions of DAS and its\ninteractions with issues such as fork choice rule safety.\n\nFurther into the future, we need much more work figuring out the\nideal version of 2D DAS and proving its safety properties. We also want\nto eventually migrate away from KZG to a quantum-resistant,\ntrusted-setup-free alternative. Currently, we do not know of candidates\nthat are friendly to distributed block building. Even the expensive\n\"brute force\" technique of using recursive STARKs to generate proofs of\nvalidity for reconstructing rows and columns does not suffice, because\nwhile technically a STARK is O(log(n) * log(log(n)) hashes in\nsize (with STIR), in\npractice a STARK is almost as big as a whole blob.\n\nThe realistic paths I see for the long term are:\n\n- Implement ideal 2D DAS\n\n- Stick with 1D DAS, sacrificing sampling bandwidth\nefficiency and accepting a lower data cap for the sake of simplicity and\nrobustness\n\n- (Hard pivot) abandon DA, and fully embrace Plasma\nas a primary layer 2 architecture we are focusing on\n\nWe can view these along a tradeoff spectrum:\n\nNote that this choice exists even if we decide to scale\nexecution on L1 directly. This is because if L1 is to process\nlots of TPS, L1 blocks will become very big, and clients will want an\nefficient way to verify that they are correct, so we would have to use\nthe same technology that powers rollups (ZK-EVM and DAS) at L1.\nHow does\nit interact with other parts of the roadmap?\n\nThe need for 2D DAS is somewhat lessened, or at least delayed, if\ndata compression (see below) is implemented, and it's lessened even\nfurther if Plasma is widely used. DAS also poses a challenge to\ndistributed block building protocols and mechanisms: while DAS is\ntheoretically friendly to distributed reconstruction, this needs to be\ncombined in practice with inclusion\nlist proposals and their surrounding fork choice mechanics.\n\n## Data compression\n\n## What problem are we solving?\n\nEach transaction in a rollup takes a significant amount of data space\nonchain: an ERC20 transfer takes about 180 bytes. Even with ideal data\navailability sampling, this puts a cap on scalability of layer 2\nprotocols. With 16 MB per slot, we get:\n\n16000000 / 12 / 180 = 7407 TPS\n\nWhat if in addition to tackling the numerator, we can also tackle the\ndenominator, and make each transaction in a rollup take fewer bytes\nonchain?\nWhat is it and how does it\nwork?\n\nThe best explanation in my opinion is this\ndiagram from two years ago:\n\nThe simplest gains are just zero-byte compression: replacing each\nlong sequence of zero bytes with two bytes representing how many zero\nbytes there are. To go further, we take advantage of the specific\nproperties of transactions:\n\n- Signature aggregation - we switch from ECDSA\nsignatures to BLS signatures, which have the property that many\nsignatures can be combined together into a single signature that attests\nfor the validity of all of the original signatures. This is not\nconsidered for L1 because the computational costs of verification, even\nwith aggregation, are higher, but in a data-scarce environment like L2s,\nthey arguably make sense. The aggregation feature of ERC-4337 presents one\npath for implementing this.\n\n- Replacing addresses with pointers - if an address\nwas used before, we can replace the 20-byte address with a 4-byte\npointer to a location in history. This is needed to achieve the biggest\ngains, though it takes effort to implement, because it requires (at\nleast a portion of) the blockchain's history to effectively become part\nof the state.\n\n- Custom serialization for transaction values - most\ntransaction values have very few digits, eg. 0.25 ETH is represented as\n250,000,000,000,000,000 wei. Gas max-basefees and priority fees work\nsimilarly. We can thus represent most currency values very compactly\nwith a custom decimal floating point format, or even a dictionary of\nespecially common values.\n\nWhat are some links\nto existing research?\n\n- Exploration from sequence.xyz: https://sequence.xyz/blog/compressing-calldata\n\n- Calldata-optimized contracts for L2s, from ScopeLift: https://github.com/ScopeLift/l2-optimizoooors\n\n- An alternative strategy - validity-proof-based rollups (aka ZK\nrollups) post state diffs instead of transactions: https://ethresear.ch/t/rollup-diff-compression-application-level-compression-strategies-to-reduce-the-l2-data-footprint-on-l1/9975\n\n- BLS wallet - an implementation of BLS aggregation through ERC-4337:\nhttps://github.com/getwax/bls-wallet\n\nWhat is left to\ndo, and what are the tradeoffs?\n\nThe main thing left to do is to actually implement the above schemes.\nThe main tradeoffs are:\n\n- Switching to BLS signatures takes significant effort, and reduces\ncompatibility with trusted hardware chips that can increase security. A\nZK-SNARK wrapper around other signature schemes could be used to replace\nthis.\n\n- Dynamic compression (eg. replacing addresses with pointers)\ncomplicates client code.\n\n- Posting state diffs to chain instead of transactions reduces\nauditability, and makes a lot of software (eg. block explorers) not\nwork.\n\nHow does\nit interact with other parts of the roadmap?\n\nAdoption of ERC-4337, and eventually the enshrinement of parts of it\nin L2 EVMs, can greatly hasten the deployment of aggregation techniques.\nEnshrinement of parts of ERC-4337 on L1 can hasten its deployment on\nL2s.\n\n## Generalized Plasma\n\n## What problem are we solving?\n\nEven with 16 MB blobs and data compression, 58,000 TPS is not\nnecessarily enough to fully take over consumer payments, decentralized\nsocial or other high-bandwidth sectors, and this becomes especially true\nif we start taking privacy into account, which could drop\nscalability by 3-8x. For high-volume, low-value applications, one option\ntoday is a validium,\nwhich keeps data off-chain and has an interesting security model where\nthe operator cannot steal users' funds, but they can disappear\nand temporarily or permanently freeze all users' funds. But we\ncan do better.\nWhat is it and how does it\nwork?\n\nPlasma is a scaling solution that involves an operator publishing\nblocks offchain, and putting the Merkle roots of those blocks onchain\n(as opposed to rollups, where the full block is put onchain). For each\nblock, the operator sends to each user a Merkle branch proving what\nhappened, or did not happen, to that user's assets. Users can withdraw\ntheir assets by providing a Merkle branch. Importantly, this branch does\nnot have to be rooted in the latest state - for this reason,\neven if data availability fails, the user can still recover their assets\nby withdrawing the latest state they have that is available. If a user\nsubmits an invalid branch (eg. exiting an asset that they already sent\nto someone else, or the operator themselves creating an asset out of\nthin air), an onchain challenge mechanism can adjudicate who the asset\nrightfully belongs to.\n\n A diagram of a Plasma Cash chain. Transactions spending\ncoin i are put into the i'th\nposition in the tree. In this example, assuming all previous trees are\nvalid, we know that Eve currently owns coin 1, David owns coin 4 and\nGeorge owns coin 6. \n\nEarly versions of Plasma were only able to handle the payments use\ncase, and were not able to effectively generalize further. If we require\neach root to be verified with a SNARK, however, Plasma becomes much more\npowerful. Each challenge game can be simplified significantly, because\nwe take away most possible paths for the operator to cheat. New paths\nalso open up to allow Plasma techniques to be extended to a much more\ngeneral class of assets. Finally, in the case where the operator does\nnot cheat, users can withdraw their funds instantly, without needing to\nwait for a one-week challenge period.\n\n One way (not the only way) to make an EVM plasma chain:\nuse a ZK-SNARK to construct a parallel UTXO tree that reflects the\nbalance changes made by the EVM, and defines a unique mapping of what is\n\"the same coin\" at different points in history. A Plasma construction\ncan then be built on top of that. \n\nOne key insight is that the Plasma system does not need to be\nperfect. Even if you can only protect a subset of assets (eg. even just\ncoins that have not moved in the past week), you've already greatly\nimproved on the status quo of ultra-scalable EVM, which is a\nvalidium.\n\nAnother class of constructions is hybrid plasma/rollups,\nsuch as Intmax. These constructions\nput a very small amount of data per user onchain (eg. 5 bytes), and by\ndoing so, get properties that are somewhere between plasma and rollups:\nin the Intmax case, you get a very high level of scalability and\nprivacy, though even in the 16 MB world capacity is theoretically capped\nto roughly 16,000,000 / 12 / 5 = 266,667 TPS.\nWhat are some links\nto existing research?\n\n- Original Plasma paper: https://plasma.io/plasma-deprecated.pdf\n\n- Plasma Cash: https://ethresear.ch/t/plasma-cash-plasma-with-much-less-per-user-data-checking/1298\n\n- Plasma Cashflow: https://hackmd.io/DgzmJIRjSzCYvl4lUjZXNQ?view#\ud83d\udeaa-Exit\n\n- Intmax (2023): https://eprint.iacr.org/2023/1082\n\nWhat is left to\ndo, and what are the tradeoffs?\n\nThe main remaining task is to bring Plasma systems to production. As\nmentioned above, \"plasma vs validium\" is not a binary: any\nvalidium can have its safety properties improved at least a little bit\nby adding Plasma features into the exit mechanism. The research part is\nin getting optimal properties (in terms of trust requirements, and\nworst-case L1 gas cost, and vulnerability to DoS) for an EVM, as well as\nalternative application specific constructions. Additionally, the\ngreater conceptual complexity of Plasma relative to rollups needs to be\naddressed directly, both through research and through construction of\nbetter generalized frameworks.\n\nThe main tradeoff in using Plasma designs is that they depend more on\noperators and are harder to make \"based\",\nthough hybrid plasma/rollup designs can often avoid this weakness.\nHow does\nit interact with other parts of the roadmap?\n\nThe more effective Plasma solutions can be, the less pressure there\nis for the L1 to have a high-performance data availability\nfunctionality. Moving activity to L2 also reduces MEV pressure on\nL1.\n\n## Maturing L2 proof systems\n\n## What problem are we solving?\n\nToday, most rollups are not yet actually trustless; there is a\nsecurity council that has the ability to override the behavior of the\n(optimistic or validity) proof\nsystem. In some cases, the proof system is not even live at all, or\nif it is it only has an \"advisory\" functionality. The furthest ahead are\n(i) a few application-specific rollups, such as Fuel, which are\ntrustless, and (ii) as of the time of this writing, Optimism and\nArbitrum, two full-EVM rollups that have achieved a\npartial-trustlessness milestone known as \"stage 1\". The reason why\nrollups have not gone further is concern about bugs in the code. We need\ntrustless rollups, and so we need to tackle this problem head on.\nWhat is it and how does it\nwork?\n\nFirst, let us recap the \"stage\" system, originally introduced in this\npost. There are more detailed requirements, but the summary is:\n\n- Stage 0: it must be possible for a user to run a\nnode and sync the chain. It's ok if validation is fully\ntrusted/centralized.\n\n- Stage 1: there must be a (trustless) proof\nsystem that ensures that only valid transactions get accepted.\nIt's allowed for there to be a security council that can\noverride the proof system, but only with a 75%\nthreshold vote. Additionally, a quorum-blocking portion of the\ncouncil (so, 26%+) must be outside the main company building the rollup.\nAn upgrade mechanism with weaker features (eg. a DAO) is allowed, but it\nmust have a delay long enough that if it approves a malicious upgrade,\nusers can exit their funds before it comes online.\n\n- Stage 2: there must be a (trustless) proof\nsystem that ensures that only valid transactions get accepted.\nSecurity councils are only allowed to intervene in the event of\nprovable bugs in the code, eg. if two redundant proof systems\ndisagree with each other or if one proof system accepts two different\npost-state roots for the same block (or accepts nothing for a\nsufficiently long period of time eg. a week). An upgrade mechanism is\nallowed, but it must have a very long delay.\n\nThe goal is to reach Stage 2. The main challenge in reaching\nstage 2 is getting enough confidence that the proof system actually is\ntrustworthy enough. There are two major ways to do this:\n\n- Formal verification: we can use modern mathematical\nand computational techniques to prove that an (optimistic or validity)\nproof system only accept blocks that pass the EVM specification. These\ntechniques have existed for decades, but recent advancements such as Lean 4 have\nmade them much more practical, and advancements in AI-assisted proving\ncould potentially accelerate this trend further.\n\n- Multi-provers: make multiple proof systems, and put\nfunds into a 2-of-3 (or larger) multisig between those proof systems and\na security council (and/or other gadget with trust assumptions, eg.\nTEEs). If the proof systems agree, the security council has no power; if\nthey disagree, the security council can only choose between one of them,\nit can't unilaterally impose its own answer.\n\n Stylized diagram of a multi-prover, combining one\noptimistic proof system, one validity proof system and a security\ncouncil. \n\nWhat are some links\nto existing research?\n\n- EVM K Semantics (formal verification work from 2017): https://github.com/runtimeverification/evm-semantics\n\n- Presentation on the idea of multi-provers (2022): https://www.youtube.com/watch?v=6hfVzCWT6YI\n\n- Taiko plans to use multi-proofs: https://docs.taiko.xyz/core-concepts/multi-proofs/\n\nWhat is left to\ndo, and what are the tradeoffs?\n\nFor formal verification, a lot. We need to create a formally\nverified version of an entire SNARK prover of an EVM. This is an\nincredibly complex project, though it is one that we have already started. There is\none trick that significantly simplifies the task: we can make a formally\nverified SNARK prover of a minimal VM, eg. RISC-V or Cairo, and then write\nan implementation of the EVM in that minimal VM (and formally prove its\nequivalence to some other EVM specification).\n\nFor multi-provers, there are two main remaining pieces. First, we\nneed to get enough confidence in at least two different proof systems,\nboth that they are reasonably safe individually and that if they break,\nthey would break for different and unrelated reasons (and so they would\nnot break at the same time). Second, we need to get a very high level of\nassurance in the underlying logic that merges the proof systems. This is\na much smaller piece of code. There are ways to make it\nextremely small - just store funds in a Safe multisig contract whose\nsigners are contracts representing individual proof systems - but this\nhas the tradeoff of high onchain gas costs. Some balance between\nefficiency and safety will need to be found.\nHow does\nit interact with other parts of the roadmap?\n\nMoving activity to L2 reduces MEV pressure on L1.\n\nCross-L2\ninteroperability improvements\n\n## What problem are we solving?\n\nOne major challenge with the L2 ecosystem today is that it is\ndifficult for users to navigate. Furthermore, the easiest ways of doing\nso often re-introduce trust assumptions: centralized bridges, RPC\nclients, and so forth. If we are serious about the idea that L2s are\npart of Ethereum, we need to make using the L2 ecosystem feel like using\na unified Ethereum ecosystem.\n\n An example of pathologically bad (and even dangerous: I\npersonally lost $100 to a chain-selection mistake here) cross-L2 UX -\nthough this is not Polymarket's fault, cross-L2 interoperability should\nbe the responsibility of wallets and the Ethereum standards (ERC)\ncommunity. In a well-functioning Ethereum ecosystem, sending coins from\nL1 to L2, or from one L2 to another, should feel just like sending coins\nwithin the same L1. \n\nWhat is it and how does it\nwork?\n\nThere are many categories of cross-L2 interoperability improvements.\nIn general, the way to come up with these is to notice that in theory,\na\nrollup-centric Ethereum is the same thing as L1 execution sharding,\nand then ask where the current Ethereum L2-verse falls short of that\nideal in practice. Here are a few:\n\n- Chain-specific addresses: the chain (L1, Optimism,\nArbitrum...) should be part of the address. Once this is implemented,\ncross-L2 sending flows can be implemented by just putting the address\ninto the \"send\" field, at which point the wallet can figure out how to\ndo the send (including using bridging protocols) in the background.\n\n- Chain-specific payment requests: it should be easy\nand standardized to make a message of the form \"send me X tokens of type\nY on chain Z\". This has two primary use cases: (i) payments, whether\nperson-to-person or person-to-merchant-service, and (ii) dapps\nrequesting funds, eg. the Polymarket example above.\n\n- Cross-chain swaps and gas payment: there should be\na standardized open protocol for expressing cross-chain operations such\nas \"I am sending 1 ETH on Optimism to whoever sends me 0.9999 ETH on\nArbitrum\", and \"I am sending 0.0001 ETH on Optimism to whoever includes\nthis transaction on Arbitrum\". ERC-7683 is one\nattempt at the former, and RIP-7755\nis one attempt at the latter, though both are also more general than\njust these specific use cases.\n\n- Light clients: users should be able to actually\nverify the chains that they are interacting with, and not just trust RPC\nproviders. A16z crypto's Helios does this for Ethereum\nitself, but we need to extend this trustlessness to L2s. ERC-3668 (CCIP-read)\nis one strategy for doing this.\n\n \n\n How a light client can update its view of the Ethereum\nheader chain. Once you have the header chain, you can use Merkle proofs\nto validate any state object. And once you have the right L1 state\nobjects, you can use Merkle proofs (and possibly signatures, if you want\nto check preconfirmations) to validate any state object on L2. Helios\ndoes the former already. Extending to the latter is a standardization\nchallenge. \n\n- Keystore wallets: today, if you want to update the\nkeys that control your smart contract wallet, you have to do it on all N\nchains on which that wallet exists. Keystore wallets are a technique\nthat allow the keys to exist in one place (either on L1, or later\npotentially on an L2), and then be read from any L2 that has a copy of\nthe wallet. This means that updates only need to happen once. To be\nefficient, keystore wallets require L2s to have a standardized way to\ncostlessly read L1; two proposals for this are L1SLOAD\nand REMOTESTATICCALL.\n\n A stylized diagram of how keystore wallets work.\n\n- \n\nMore radical \"shared token bridge\" ideas:\nimagine a world where all L2s are validity proof rollups, that commit to\nEthereum every slot. Even in this world, moving assets from one L2 to\nanother L2 \"natively\" would require withdrawaing and depositing, which\nrequires paying a substantial amount of L1 gas. One way to solve this is\nto create a shared minimal rollup, whose only function would be to\nmaintain the balances of how many tokens of which type are owned by\nwhich L2, and allow those balances to be updated en masse by a series of\ncross-L2 send operations initiated by any of the L2s. This would allow\ncross-L2 transfers to happen without needing to pay L1 gas per transfer,\nand without needing liquidity-provider-based techniques like\nERC-7683.\n\n- \n\nSynchronous composability: allow synchronous\ncalls to happen either between a specific L2 and L1, or between multiple\nL2s. This could be helpful in improving financial efficiency of defi\nprotocols. The former could be done without any cross-L2 coordination;\nthe latter would require shared\nsequencing. Based\nrollups are automatically friendly to all of these\ntechniques.\n\nWhat are some links\nto existing research?\n\n- Chain-specific addresses: ERC-3770: https://eips.ethereum.org/EIPS/eip-3770\n\n- ERC-7683: https://eips.ethereum.org/EIPS/eip-7683\n\n- RIP-7755: https://github.com/wilsoncusack/RIPs/blob/cross-l2-call-standard/RIPS/rip-7755.md\n\n- Scroll keystore wallet design: https://hackmd.io/@haichen/keystore\n\n- Helios: https://github.com/a16z/helios\n\n- ERC-3668 (sometimes called CCIP-read): https://eips.ethereum.org/EIPS/eip-3668\n\n- Proposal for \"based (shared) preconfirmations\" by Justin Drake: https://ethresear.ch/t/based-preconfirmations/17353\n\n- L1SLOAD (RIP-7728): https://ethereum-magicians.org/t/rip-7728-l1sload-precompile/20388\n\n- REMOTESTATICCALL in Optimism: https://github.com/ethereum-optimism/ecosystem-contributions/issues/76\n\n- AggLayer, which includes shared token bridge ideas: https://github.com/AggLayer\n\nWhat is left to\ndo, and what are the tradeoffs?\n\nMany of the examples above face standard dilemmas of when to\nstandardize and what layers to standardize. If you standardize too\nearly, you risk entrenching an inferior solution. If you standardize too\nlate, you risk creating needless fragmentation. In some cases, there is\nboth a short-term solution that has weaker properties but is easier to\nimplement, and a long-term solution that is \"ultimately right\" but will\ntake quite a few years to get there.\n\nOne way in which this section is unique, is that these tasks are not\njust technical problems: they are also (perhaps even primarily!) social\nproblems. They require L2s and wallets and L1 to cooperate. Our ability\nto handle this problem successfully is a test of our ability to stick\ntogether as a community.\nHow does\nit interact with other parts of the roadmap?\n\nMost of these proposals are \"higher-layer\" constructions, and so do\nnot greatly affect L1 considerations. One exception is shared\nsequencing, which has heavy impacts on MEV.\n\n## Scaling execution on L1\n\n## What problem are we solving?\n\nIf L2s become very scalable and successful but L1 remains capable of\nprocessing only a very low volume of transactions, there are many risks\nto Ethereum that might arise:\n\n- The economic situation of ETH the asset becomes more risky, which in\nturn affects long-run security of the network.\n\n- Many L2s benefit from being closely tied to a highly developed\nfinancial ecosystem on L1, and if this ecosystem greatly weakens, the\nincentive to become an L2 (instead of being an independent L1)\nweakens\n\n- It will take a long time before L2s have exactly the same security\nassurances as L1.\n\n- If an L2 fails (eg. due to a malicious or disappearing operator),\nusers would still need to go through L1 in order to recover their\nassets. Hence, L1 needs to be powerful enough to be able to at least\noccasionally actually handle a highly complex and chaotic wind-down of\nan L2.\n\nFor these reasons, it is valuable to continue scaling L1 itself, and\nmaking sure that it can continue to accommodate a growing number of\nuses.\nWhat is it and how does it\nwork?\n\nThe easiest way to scale is to simply increase the gas\nlimit. However, this risks centralizing the L1, and thus\nweakening the other important property that makes the Ethereum L1 so\npowerful: its credibility as a robust base layer. There is an ongoing\ndebate about what degree of simple gas limit increase is sustainable,\nand this also changes based on which other technologies get implemented\nto make larger blocks easier to verify (eg. history expiry,\nstatelessness, L1 EVM validity proofs). Another important thing to keep\nimproving is simply the efficiency of Ethereum client software, which is\nfar more optimized today than it was five years ago. An\neffective L1 gas limit increase strategy would involve accelerating\nthese verification technologies.\n\nAnother scaling strategy involves identifying specific features and\ntypes of computation that can be made cheaper without harming the\ndecentralization of the network or its security properties. Examples of\nthis include:\n\n- EOF - a new EVM bytecode\nformat that is more friendly to static analysis, allowing for faster\nimplementations. EOF bytecode could be given lower gas costs to take\nthese efficiencies into account.\n\n- Multidimensional\ngas pricing - establishing separate basefees and limits for\ncomputation, data and storage can increase the Ethereum L1's\naverage capacity without increasing its maximum\ncapacity (and hence creating new security risks).\n\n- Reduce gas costs of specific opcodes and\nprecompiles - historically, we have had several rounds of increasing gas costs for certain\noperations that were underpriced in order to avoid denial of\nservice attacks. What we have had less of, and could do much more, is\nreducing gas costs for operations that are overpriced.\nFor example, addition is much cheaper than multiplication, but the costs\nof the ADD and MUL opcodes are currently the\nsame. We could make ADD cheaper, and even simpler opcodes\nsuch as PUSH even cheaper.\n\n- EVM-MAX\nand SIMD:\nEVM-MAX (\"modular arithmetic extensions\") is a proposal to allow more\nefficient native big-number modular math as a separate module of the\nEVM. Values computed by EVM-MAX computations would only be accessible by\nother EVM-MAX opcodes, unless deliberately exported; this allows greater\nroom to store these values in optimized\nformats. SIMD (\"single instruction multiple data\") is a proposal to\nallow efficiently executing the same instruction on an array of values.\nThe two together can create a powerful coprocessor\nalongside the EVM that could be used to much more efficiently implement\ncryptographic operations. This would be especially useful for privacy\nprotocols, and for L2 proof systems, so it would help both L1 and L2\nscaling.\n\nThese improvements will be discussed in more detail in a future post\non the Splurge.\n\nFinally, a third strategy is native rollups (or\n\"enshrined rollups\"): essentially, creating many copies of the EVM that\nrun in parallel, leading to a model that is equivalent to what rollups\ncan provide, but much more natively integrated into the protocol.\nWhat are some links\nto existing research?\n\n- Polynya's Ethereum L1 scaling roadmap: https://polynya.mirror.xyz/epju72rsymfB-JK52_uYI7HuhJ-W_zM735NdP7alkAQ\n\n- Multidimensional gas pricing: https://vitalik.eth.limo/general/2024/05/09/multidim.html\n\n- EIP-7706: https://eips.ethereum.org/EIPS/eip-7706\n\n- EOF: https://evmobjectformat.org/\n\n- EVM-MAX: https://ethereum-magicians.org/t/eip-6601-evm-modular-arithmetic-extensions-evmmax/13168\n\n- SIMD: https://eips.ethereum.org/EIPS/eip-616\n\n- Native rollups: https://mirror.xyz/ohotties.eth/P1qSCcwj2FZ9cqo3_6kYI4S2chW5K5tmEgogk6io1GE\n\n- Interview with Max Resnick on the value of scaling L1: https://x.com/BanklessHQ/status/1831319419739361321\n\n- Justin Drake on the use of SNARKs and native rollups for scaling: https://www.reddit.com/r/ethereum/comments/1f81ntr/comment/llmfi28/\n\nWhat is left to\ndo, and what are the tradeoffs?\n\nThere are three strategies for L1 scaling, which can be pursued\nindividually or in parallel:\n\n- Improve technology (eg. client code, stateless clients, history\nexpiry) to make the L1 easier to verify, and then\nraise the gas limit\n\n- Make specific operations cheaper, increasing\naverage capacity without increasing worst-case risks\n\n- Native rollups (ie. \"create N parallel copies of\nthe EVM\", though potentially giving developers a lot of flexibility in\nthe parameters of the copies they deploy)\n\nIt's worth understanding that these are different techniques that\nhave different tradeoffs. For example, native rollups have many of the\nsame weaknesses in composability as regular rollups: you cannot send a\nsingle transaction that synchronously performs operations across many of\nthem, like you can with contracts on the same L1 (or L2). Raising the\ngas limit takes away from other benefits that can be achieved by making\nthe L1 easier to verify, such as increasing the portion of users that\nrun verifying nodes, and increasing solo stakers. Making specific\noperations in the EVM cheaper, depending on how it's done, can increase\ntotal EVM complexity.\n\nA big question that any L1 scaling roadmap needs to answer is:\nwhat is the ultimate vision for what belongs on L1 and what\nbelongs on L2? Clearly, it's absurd for everything to\ngo on L1: the potential use cases go into the hundreds of thousands of\ntransactions per second, and that would make the L1 completely unviable\nto verify (unless we go the native rollup route). But we do need\nsome guiding principle, so that we can make sure that we are\nnot creating a situation where we increase the gas limit 10x, heavily\ndamage the Ethereum L1's decentralization, and find that we've only\ngotten to a world where instead of 99% of activity being on L2, 90% of\nactivity is on L2, and so the result otherwise looks almost the same,\nexcept for an irreversible loss of much of what makes Ethereum L1\nspecial.\n\n One proposed view of a \"division of labor\" between L1 and\nL2s, source.\n\nHow does\nit interact with other parts of the roadmap?\n\nBringing more users onto L1 implies improving not just scale, but\nalso other aspects of L1. It means that more MEV will remain on L1 (as\nopposed to becoming a problem just for L2s), and so will be even more of\na pressing need to handle it explicitly. It greatly increases the value\nof having fast slot times on L1. And it's also heavily dependent on\nverification of L1 (\"the Verge\") going well.",
    "contentLength": 39627,
    "summary": "Ethereum's \"Surge\" roadmap aims to achieve 100,000+ TPS through rollup-centric scaling, data availability sampling, and cross-L2 interoperability.",
    "detailedSummary": {
      "theme": "Vitalik outlines Ethereum's scaling roadmap focused on achieving 100,000+ TPS through rollup-centric approaches while maintaining decentralization.",
      "summary": "Vitalik presents a comprehensive vision for Ethereum's scaling future through 'The Surge,' which aims to achieve over 100,000 transactions per second across L1 and L2 while preserving decentralization and ensuring maximum interoperability. He explains how Ethereum's original dual scaling strategies of sharding and layer 2 protocols converged into today's rollup-centric roadmap, where L1 serves as a robust base layer while L2s handle scaling. The post covers six key technical areas: data availability sampling (moving from current PeerDAS to eventual 2D sampling), data compression techniques to reduce transaction size, generalized Plasma for ultra-high scalability, maturing L2 proof systems toward trustlessness, cross-L2 interoperability improvements, and scaling execution directly on L1. Vitalik emphasizes that achieving this vision requires solving both technical challenges and social coordination problems, as the ecosystem must work together to create a unified experience that feels like one Ethereum rather than 34 separate blockchains.",
      "takeaways": [
        "Ethereum targets 100,000+ TPS through a rollup-centric approach where L1 provides security and decentralization while L2s handle scaling",
        "Data availability sampling will progress from current PeerDAS to 2D sampling, potentially reaching 16 MB per slot to enable ~58,000 TPS",
        "Data compression techniques including signature aggregation and address pointers could significantly reduce per-transaction data requirements",
        "Generalized Plasma offers a path to ultra-high scalability for specific use cases by keeping data off-chain with cryptographic guarantees",
        "Cross-L2 interoperability improvements like chain-specific addresses and keystore wallets are essential for creating a unified user experience"
      ],
      "controversial": [
        "The potential abandonment of data availability in favor of Plasma-focused architecture as a 'hard pivot' option",
        "Scaling execution directly on L1 through gas limit increases risks centralizing the network and undermining Ethereum's decentralization",
        "The tension between maintaining L1's robust base layer properties versus the economic risks if L1 becomes too limited compared to highly scalable L2s"
      ]
    }
  },
  {
    "id": "general-2024-10-14-futures1",
    "title": "Possible futures of the Ethereum protocol, part 1: The Merge",
    "date": "2024-10-14",
    "category": "protocol",
    "url": "https://vitalik.eth.limo/general/2024/10/14/futures1.html",
    "path": "general/2024/10/14/futures1.html",
    "content": "- \n\n- \n\n  \n    \n\n  \n  \n  Dark Mode Toggle\n  \n    \n\n  \n\n##  Possible futures of the Ethereum protocol, part 1: The Merge \n\n 2024 Oct 14 \nSee all posts\n\n \n \n\n Possible futures of the Ethereum protocol, part 1: The Merge \n\nSpecial thanks to Justin Drake, Hsiao-wei Wang, @antonttc, Anders Elowsson\nand Francesco for feedback and review.\n\nOriginally, \"the Merge\" referred to the most important event in the\nEthereum protocol's history since its launch: the long-awaited and\nhard-earned transition from proof of work to proof of stake. Today,\nEthereum has been a stably running proof of stake system for almost\nexactly two years, and this proof of stake has performed remarkably well\nin stability,\nperformance and avoiding\ncentralization risks. However, there still remain some important\nareas in which proof of stake needs to improve.\n\nMy roadmap diagram from 2023 separated this out into buckets:\nimproving technical features such as stability, performance,\nand accessibility to smaller validators, and economic changes\nto address centralization risks. The former got to take over the heading\nfor \"the Merge\", and the latter became part of \"the Scourge\".\n\nThe Merge, 2023 roadmap edition.\n\nThis post will focus on the \"Merge\" part: what can still be\nimproved in the technical design of proof of stake, and what are some\npaths to getting there?\n\nThis is not meant as an exhaustive list of things that could be done\nto proof of stake; rather, it is a list of ideas that are actively being\nconsidered.\n\n## The Merge: key goals\n\n- Single slot finality\n\n- Transaction confirmation and finalization as fast as possible, while\npreserving decentralization\n\n- Improve staking viability for solo stakers\n\n- Improve robustness\n\n- Improve Ethereum's ability to resist and recover from 51% attacks\n(including finality reversion, finality blocking, and censorship)\n\n## In this chapter\n\n- Single slot finality and staking\ndemocratization\n\n- Single secret leader election\n\n- Faster transaction confirmations\n\n- Other research areas\n\nSingle slot\nfinality and staking democratization\n\n## What problem are we solving?\n\nToday, it takes 2-3 epochs (~15 min) to finalize a block, and 32 ETH\nis required to be a staker. This was originally a compromise meant to balance\nbetween three goals:\n\n- Maximizing the number of validators that can\nparticipate in staking (this directly implies minimizing the min\nETH required to stake)\n\n- Minimizing the time to finality\n\n- Minimizing the overhead of running a node, in this\ncase the cost of downloading, verifying and re-broadcasting all the\nother validator's signatures\n\nThe three goals are in conflict: in order for economic finality to be\npossible (meaning: an attacker would need to burn a large amount of ETH\nto revert a finalized block), you need every single validator to sign\ntwo messages each time finality happens. And so if you have many\nvalidators, either you need a long time to process all their signatures,\nor you need very beefy nodes to process all the signatures at the same\ntime.\n\nNote that this is all conditional on a key goal of Ethereum:\nensuring that even successful attacks have a high cost to the\nattacker. This is what is meant by the term \"economic\nfinality\". If we did not have this goal, then we could solve this\nproblem by randomly selecting a committee to finalize each slot. Chains\nthat do not attempt to achieve economic finality, such as Algorand, often\ndo exactly this. But the problem with this approach is that if an\nattacker does control 51% of validators, then they can perform\nan attack (reverting a finalized block, or censoring, or delaying\nfinality) at very low cost: only the portion of their nodes that are in\nthe committee could be detected as participating in the attack and\npenalized, whether through slashing\nor socially-coordinated\nsoft fork. This means that an attacker could repeatedly attack the\nchain many times over, losing only a small portion of their stake during\neach attack. Hence, if we want economic finality, a naive\ncommittee-based approach does not work, and it appears at first glance\nthat we do need the full set of validators to participate.\n\nIdeally, we want to preserve economic finality, while\nsimultaneously improving on the status quo in two areas:\n\n- Finalize blocks in one slot (ideally, keep or even\nreduce the current length of 12s), instead of 15 min\n\n- Allow validators to stake with 1 ETH (down from 32\nETH)\n\nThe first goal is justified by two goals, both of which can be viewed\nas \"bringing Ethereum's properties in line with those of (more\ncentralized) performance-focused L1 chains\".\n\nFirst, it ensures that all Ethereum users actually benefit\nfrom the higher level of security assurances achieved through\nthe finality mechanism. Today, most users do not, because they are not\nwilling to wait 15 minutes; with single-slot finality, users will see\ntheir transactions finalized almost as soon as they are confirmed.\nSecond, it simplifies the protocol and surrounding\ninfrastructure if users and applications don't have to worry\nabout the possibility of the chain reverting except in the relatively\nrare case of an inactivity\nleak.\n\nThe second goal is justified by a desire to support solo\nstakers. Poll after poll repeatedly show that the main factor\npreventing more people from solo staking is the 32 ETH minimum. Reducing\nthe minimum to 1 ETH would solve this issue, to the point where other\nconcerns become the dominant factor limiting solo staking.\n\nThere is a challenge: the goals of faster finality and more\ndemocratized staking both conflict with the goal of minimizing\noverhead. And indeed, this fact is the entire reason why we did\nnot start with single-slot finality to begin with. However, more recent\nresearch presents a few possible paths around the problem.\nWhat is it and how does it\nwork?\n\nSingle-slot finality involves using a consensus algorithm that\nfinalizes blocks in one slot. This in itself is not a difficult goal:\nplenty of algorithms, such as Tendermint\nconsensus, already do this with optimal properties. One desired\nproperty unique to Ethereum, which Tendermint does not support, is inactivity\nleaks, which allow the chain to keep going and eventually recover\neven when more than 1/3 of validators go offline. Fortunately, this\ndesire has already been addressed: there are already proposals that\nmodify Tendermint-style consensus to accommodate inactivity leaks.\n\nA leading single slot finality proposal\n\nThe harder part of the problem is figuring out how to make\nsingle-slot finality work with a very high validator count, without\nleading to extremely high node-operator overhead. For this, there are a\nfew leading solutions:\n\n- \n\nOption 1: Brute force - work hard on\nimplementing better signatures aggregation protocols, potentially using\nZK-SNARKs, which would actually allow us to process signatures from\nmillions of validators in each slot.\n\nHorn, one of the proposed designs for a better aggregation\nprotocol.\n\n- \n\nOption 2: Orbit\ncommittees - a new mechanism which allows a\nrandomly-selected medium-sized committee to be responsible for\nfinalizing the chain, but in a way that preserves the cost-of-attack\nproperties that we are looking for.\n\nOne way to think about Orbit SSF is that it opens up a space of\ncompromise options along a spectrum from x=0 (Algorand-style committees,\nno economic finality) to x=1 (status quo Ethereum), opening up points in\nthe middle where Ethereum still has enough economic finality to be\nextremely secure, but at the same time we get the efficiency benefits of\nonly needing a medium-sized random sample of validators to participate\nin each slot.\n\nOrbit takes advantage of pre-existing heterogeneity in validator\ndeposit sizes to get as much economic finality as possible, will still\ngiving small validators a proportionate role. In addition, Orbit uses\nslow committee rotation to ensure high overlap between adjacent quorums,\nensuring that its economic finality still applies at committee-switching\nboundaries.\n\n- \n\nOption 3: two-tiered staking - a mechanism where\nthere are two classes of stakers, one with higher deposit requirements\nand one with lower deposit requirements. Only the higher-deposit tier\nwould be directly involved in providing economic finality. There are\nvarious proposals (eg. see the\nRainbow staking post) for exactly what rights and responsibilities\nthe lower-deposit tier has. Common ideas include:\n\n- the right to delegate stake to a higher-tier\nstaker\n\n- a random sample of lower-tier stakers attesting to,\nand being needed to finalize, each block\n\n- the right to generate inclusion\nlists\n\nWhat are some links to\nexisting research?\n\n- Paths toward single slot finality (2022): https://notes.ethereum.org/@vbuterin/single_slot_finality\n\n- A concrete proposal for a single slot finality protocol for Ethereum\n(2023): https://eprint.iacr.org/2023/280\n\n- Orbit SSF: https://ethresear.ch/t/orbit-ssf-solo-staking-friendly-validator-set-management-for-ssf/19928\n\n- Further analysis on Orbit-style mechanisms: https://ethresear.ch/t/vorbit-ssf-with-circular-and-spiral-finality-validator-selection-and-distribution/20464\n\n- Horn, signature aggregation protocol (2022): https://ethresear.ch/t/horn-collecting-signatures-for-faster-finality/14219\n\n- Signature merging for large-scale consensus (2023): https://ethresear.ch/t/signature-merging-for-large-scale-consensus/17386?u=asn\n\n- Signature aggregation protocol proposed by Khovratovich et al: https://hackmd.io/@7dpNYqjKQGeYC7wMlPxHtQ/BykM3ggu0#/\n\n- STARK-based signature aggregation (2022): https://hackmd.io/@vbuterin/stark_aggregation\n\n- Rainbow staking: https://ethresear.ch/t/unbundling-staking-towards-rainbow-staking/18683\n\nWhat is left to\ndo, and what are the tradeoffs?\n\nThere are four major possible paths to take (and we can also take\nhybrid paths):\n\n- Maintain status quo\n\n- Brute-force SSF\n\n- Orbit SSF\n\n- SSF with two-tiered staking\n\n(1) means doing no work and leaving staking as is,\nbut it leaves Ethereum's security experience and staking centralization\nproperties worse than it could be.\n\n(2) brute-forces the problem with high tech. Making\nthis happen requires aggregating a very large number of signatures (1\nmillion+) in a very short period of time (5-10s). One way to think of\nthis approach is that it involves minimizing\nsystemic complexity by going all-out on accepting encapsulated\ncomplexity.\n\n(3) avoids \"high tech\", and solves the problem with\nclever rethinking around protocol assumptions: we relax the \"economic\nfinality\" requirement so that we require attacks to be expensive, but\nare okay with the cost of attack being perhaps 10x less than today (eg.\n$2.5 billion cost of attack instead of $25 billion). It's a common view\nthat Ethereum today has far more economic finality than it needs, and\nits main security risks are elsewhere, and so this is arguably an okay\nsacrifice to make.\n\nThe main work to do is verifying that the Orbit mechanism is safe and\nhas the properties that we want, and then fully formalizing and\nimplementing it. Additionally, EIP-7251 (increase max\neffective balance) allows for voluntary validator balance\nconsolidation that immediately reduces the chain verification overhead\nsomewhat, and acts as an effective initial stage for an Orbit\nrollout.\n\n(4) avoids clever rethinking and high tech,\nbut it does create a two-tiered staking system which still has\ncentralization risks. The risks depend heavily on the specific rights\nthat the lower staking tier gets. For example:\n\n- If a low-tier staker needs to delegate their attesting\nrights to a high-tier staker, then delegation could centralize and we\nwould thus end up with two highly centralized tiers of staking.\n\n- If a random sample of the lower tier is needed to approve each\nblock, then an attacker could spend a very small amount of ETH to block\nfinality.\n\n- If lower-tier stakers can only make inclusion lists, then the\nattestation layer may remain centralized, at which point a 51% attack on\nthe attestation layer can censor the inclusion lists themselves.\n\nMultiple strategies can be combined, for example:\n\n(1 + 2): use brute-force techniques to reduce the min deposit size\nwithout doing single slot finality. The amount of aggregation required\nis 64x less than in the pure (3) case, so the problem becomes\neasier.\n\n(1 + 3): add Orbit without doing single slot finality\n\n(2 + 3): do Orbit SSF with conservative parameters (eg. 128k\nvalidator committee instead of 8k or 32k), and use brute-force\ntechniques to make that ultra-efficient.\n\n(1 + 4): add rainbow staking without doing single slot finality\nHow does\nit interact with other parts of the roadmap?\n\nIn addition to its other benefits, single slot finality reduces the\nrisk of certain\ntypes of multi-block MEV attacks. Additionally, attester-proposer\nseparation designs and other in-protocol block production pipelines\nwould need to be designed differently in a single-slot finality\nworld.\n\nBrute-force strategies have the weakness that they make it harder to\nreduce slot times.\n\nSingle\nsecret leader election\n\n## What problem are we solving?\n\nToday, which validator is going to propose the next block is known\nahead of time. This creates a security vulnerability: an attacker can\nwatch the network, identify which validators correspond to which IP\naddresses, and DoS attack each validator right when they are about to\npropose a block.\nWhat is it and how does it\nwork?\n\nThe best way to fix the DoS issue is to hide the information about\nwhich validator is going to produce the next block, at least until the\nmoment when the block is actually produced. Note that this is easy if we\nremove the \"single\" requirement: one\nsolution is to let anyone create the next block, but require the randao\nreveal to be less than 2256 / N. On average, only one\nvalidator would be able to meet this requirement - but sometimes there\nwould be two or more and sometimes there would be zero. Combining the\n\"secrecy\" requirement with the \"single\" requirement\" has long been the\nhard problem.\n\nSingle secret leader election protocols solve this by using some\ncryptographic techniques to create a \"blinded\" validator ID for each\nvalidator, and then giving many proposers the opportunity to\nshuffle-and-reblind the pool of blinded IDs (this is similar to how a mixnet works).\nDuring each slot, a random blinded ID is selected. Only the owner of\nthat blinded ID is able to generate a valid proof to propose the block,\nbut no one else knows which validator that blinded ID corresponds\nto.\n\nWhisk SSLE protocol\n\nWhat are some links\nto existing research?\n\n- Paper by Dan Boneh (2020): https://eprint.iacr.org/2020/025.pdf\n\n- Whisk (concrete proposal for Ethereum, 2022): https://ethresear.ch/t/whisk-a-practical-shuffle-based-ssle-protocol-for-ethereum/11763\n\n- Single secret leader election tag on ethresear.ch: https://ethresear.ch/tag/single-secret-leader-election\n\n- Simplified SSLE using ring signatures: https://ethresear.ch/t/simplified-ssle/12315\n\nWhat is left to\ndo, and what are the tradeoffs?\n\nRealistically, what's left is finding and implementing a protocol\nthat is sufficiently simple that we are comfortable implementing it on\nmainnet. We highly value Ethereum being a reasonably simple protocol,\nand we do not want complexity to increase further. SSLE implementations\nthat we've seen add hundreds of lines of spec code, and introduce new\nassumptions in complicated cryptography. Figuring out an\nefficient-enough quantum-resistant SSLE implementation is also an open\nproblem.\n\nIt may end up the case that the extra complexity introduced by SSLE\nonly goes down enough once we take the plunge and introduce the\nmachinery to do general-purpose zero-knowledge proofs into the Ethereum\nprotocol at L1 for other reasons (eg. state trees, ZK-EVM).\n\nAn alternative option is to simply not bother with SSLE, and use\nout-of-protocol mitigations (eg. at the p2p layer) to solve the DoS\nissues.\nHow does\nit interact with other parts of the roadmap?\n\nIf we add an attester-proposer separation (APS) mechanism, eg. execution\ntickets, then execution blocks (ie. blocks containing Ethereum\ntransactions) will not need SSLE, because we could rely on block\nbuilders being specialized. However, we would still benefit from SSLE\nfor consensus blocks (ie. blocks containing protocol messages such as\nattestations, perhaps pieces of inclusion lists, etc).\n\nFaster transaction\nconfirmations\n\n## What problem are we solving?\n\nThere is value in Ethereum's transaction\nconfirmation time decreasing further, from 12 seconds down to eg. 4\nseconds. Doing this would significantly improve the user experience of\nboth the L1 and based rollups, while making defi protocols more\nefficient. It would also make it easier for L2s to decentralize, because\nit would allow a large class of L2 applications to work on based\nrollups, reducing the demand for L2s to build their own\ncommittee-based decentralized sequencing.\nWhat is it and how does it\nwork?\n\nThere are broadly two families of techniques here:\n\n- Reduce slot times, down to eg. 8 seconds or 4\nseconds. This does not necessarily have to mean 4-second finality:\nfinality inherently takes three rounds of communication, and so we can\nmake each round of communication be a separate block, which would after\n4 seconds get at least a preliminary confirmation.\n\n- Allow proposers to publish pre-confirmations over the course\nof a slot. In the extreme, a proposer could include\ntransactions that they see into their block in real time, and\nimmediately publish a pre-confirmation message for each transaction (\"My\nfirst transaction is 0\u00d71234...\", \"My second transaction is 0\u00d75678...\"). The\ncase of a proposer publishing two conflicting confirmations can be dealt\nwith in two ways: (i) by slashing the proposer, or (ii)\nby using attesters to vote on which one came\nearlier.\n\nWhat are some links\nto existing research?\n\n- Based preconfirmations: https://ethresear.ch/t/based-preconfirmations/17353\n\n- Protocol-enforced proposer commitments (PEPC): https://ethresear.ch/t/unbundling-pbs-towards-protocol-enforced-proposer-commitments-pepc/13879\n\n- Staggered periods across parallel chains (a 2018-era idea for\nachieving low latency): https://ethresear.ch/t/staggered-periods/1793\n\nWhat is left to\ndo, and what are the tradeoffs?\n\nIt's far from clear just how practical it is to reduce slot times.\nEven today, stakers in many regions of the world have a hard time\ngetting attestations included fast enough. Attempting 4-second slot\ntimes runs the risk of centralizing the validator set, and making it\nimpractical to be a validator outside of a few privileged geographies\ndue to latency. Specifically, moving to 4-second slot times would\nrequire reducing the bound on network latency (\"delta\") to two\nseconds.\n\nThe proposer preconfirmation approach has the weakness that it can\ngreatly improve average-case inclusion times, but not\nworst-case: if the current proposer is well-functioning, your\ntransaction will be pre-confirmed in 0.5 seconds instead of being\nincluded in (on average) 6 seconds, but if the current proposer is\noffline or not well-functioning, you would still have to wait up to a\nfull 12 seconds for the next slot to start and provide a new\nproposer.\n\nAdditionally, there is the open question of how\npre-confirmations will be incentivized. Proposers have an\nincentive to maximize their optionality as long as possible. If\nattesters sign off on timeliness of pre-confirmations, then transaction\nsenders could make a portion of the fee conditional on an immediate\npre-confirmation, but this would put an extra burden on attesters, and\npotentially make it more difficult for attesters to continue functioning\nas a neutral \"dumb pipe\".\n\nOn the other hand, if we do not attempt this and keep\nfinality times at 12 seconds (or longer), the ecosystem will put greater\nweight on pre-confirmation mechanisms made by layer 2s, and\ncross-layer-2 interaction will take longer.\nHow does\nit interact with other parts of the roadmap?\n\nProposer-based preconfirmations realistically depend on an\nattester-proposer separation (APS) mechanism, eg. execution\ntickets. Otherwise, the pressure to provide real-time\npreconfirmations may be too centralizing for regular validators.\n\nExactly how short slot times can be also depends on the slot\nstructure, which depends heavily on what versions of APS, inclusion\nlists, etc we end up implementing. There are slot structures that\ncontain fewer rounds and are thus more friendly to short slot times, but\nthey make tradeoffs in other places.\n\n## Other research areas\n\n## 51% attack recovery\n\nThere is often an assumption that if a 51% attack happens (including\nattacks that are not cryptographically provable, such as censorship),\nthe community will come together to implement a minority\nsoft fork that ensures that the good guys win, and the bad guys get\ninactivity-leaked or slashed. However, this degree of over-reliance on\nthe social layer is arguably unhealthy. We can try to reduce reliance on\nthe social layer, by making the process of recovering as automated\nas possible.\n\nFull automation is impossible, because if it were, that would count\nas a >50% fault tolerant consensus algorithm, and we already know the\n(very restrictive) mathematically\nprovable limitations of those kinds of algorithms. But we\ncan achieve partial automation: for example, a client could\nautomatically refuse to accept a chain as finalized, or even as the head\nof the fork choice, if it censors transactions that the client has seen\nfor long enough. A key goal would be ensuring that the bad guys in an\nattack at least cannot get a quick clean victory.\nIncreasing the quorum\nthreshold\n\nToday, a block finalizes if 67% of stakers support it. There is an\nargument that this is overly aggressive. There has been only one (very\nbrief) finality failure in all of Ethereum's history. If this percentage\nis increased, eg. to 80%, then the added number of non-finality periods\nwill be relatively low, but Ethereum would gain security properties: in\nparticular, many more contentious situations will result in\ntemporary stopping of finality. This seems a much healthier\nsituation than \"the wrong side\" getting an instant victory, both when\nthe wrong side is an attacker, and when it's a client that has a\nbug.\n\nThis also gives an answer to the question \"what is the point of solo\nstakers\"? Today, most stakers are already staking through pools, and it\nseems very unlikely to get solo stakers up to 51% of staked\nETH. However, getting solo stakers up to a quorum-blocking\nminority, especially if the quorum is 80% (so a quorum-blocking\nminority would only need 21%) seems potentially achievable if we work\nhard at it. As long as solo stakers do not go along with a 51% attack\n(whether finality-reversion or censorship), such an attack would not get\na \"clean victory\", and solo stakers would be motivated to help organize\na minority soft fork.\n\nNote that there are interactions between quorum thresholds and the\nOrbit mechanism: if we end up using Orbit, then what exactly \"21% of\nstakers\" means will become a more complicated question, and will depend\nin part on the distribution of validators.\n\n## Quantum-resistance\n\nMetaculus\ncurrently believes, though with wide error bars, that quantum\ncomputers will likely start breaking cryptography some time in the\n2030s:\n\nQuantum computing experts such as Scott Aaronson have also recently\nstarted taking the possibility of quantum computers actually working in\nthe medium term much more\nseriously. This has consequences across the entire Ethereum roadmap:\nit means that each piece of the Ethereum protocol that currently depends\non elliptic curves will need to have some hash-based or otherwise\nquantum-resistant replacement. This particularly means that we cannot\nassume that we will be able to lean on the\nexcellent properties of BLS aggregation to process signatures from a\nlarge validator set forever. This justifies conservatism in the\nassumptions around performance of proof-of-stake designs, and also is a\ncause to be more proactive to develop quantum-resistant\nalternatives.",
    "contentLength": 24041,
    "summary": "Ethereum's proof of stake needs upgrades to achieve single slot finality (15 min to 12 sec) and lower staking requirements (32 ETH to 1 ETH).",
    "detailedSummary": {
      "theme": "Vitalik outlines technical improvements needed for Ethereum's proof-of-stake system, focusing on single slot finality, staking democratization, and enhanced security mechanisms.",
      "summary": "Vitalik discusses the current limitations of Ethereum's proof-of-stake system, which requires 15 minutes for finalization and a 32 ETH minimum stake, creating barriers for solo stakers and slower user experience. He presents three main technical approaches to achieve single slot finality while reducing the staking requirement to 1 ETH: brute-force signature aggregation using advanced cryptography like ZK-SNARKs, Orbit committees that use randomly selected validators while preserving economic finality, and two-tiered staking systems with different deposit requirements. Vitalik also explores single secret leader election to prevent DoS attacks on known block proposers, faster transaction confirmations through reduced slot times or proposer pre-confirmations, and various security improvements including 51% attack recovery mechanisms and quantum resistance preparations for the 2030s.",
      "takeaways": [
        "Current Ethereum proof-of-stake requires 32 ETH minimum and 15-minute finalization, creating centralization pressures that need technical solutions",
        "Three main paths exist for single slot finality: brute-force cryptographic aggregation, Orbit committee systems, and two-tiered staking, each with distinct tradeoffs",
        "Single secret leader election could prevent DoS attacks but adds significant protocol complexity that may not be justified until general ZK proofs are integrated",
        "Faster transaction confirmations could be achieved through shorter slot times or proposer pre-confirmations, but risk centralizing validators due to network latency constraints",
        "Quantum computing threats expected in the 2030s require proactive development of quantum-resistant alternatives to current elliptic curve cryptography"
      ],
      "controversial": [
        "Reducing economic finality requirements in Orbit committees trades off security for efficiency, with attack costs potentially dropping from $25 billion to $2.5 billion",
        "Two-tiered staking systems risk creating centralized delegation structures that could undermine the decentralization goals they aim to achieve",
        "Increasing finality quorum thresholds from 67% to 80% could improve security but may create more non-finality periods and network instability"
      ]
    }
  },
  {
    "id": "general-2024-09-28-alignment",
    "title": "Making Ethereum alignment legible",
    "date": "2024-09-28",
    "category": "governance",
    "url": "https://vitalik.eth.limo/general/2024/09/28/alignment.html",
    "path": "general/2024/09/28/alignment.html",
    "content": "- \n\n- \n\n  \n    \n\n  \n  \n  Dark Mode Toggle\n  \n    \n\n  \n\n##  Making Ethereum alignment legible \n\n 2024 Sep 28 \nSee all posts\n\n \n \n\n Making Ethereum alignment legible \n\nOne of the most important social challenges in the Ethereum ecosystem\nis balancing - or, more accurately, integrating,\ndecentralization and cooperation. The ecosystem's strength is\nthat there is a wide array of people and organizations - client\nteams, researchers, layer\n2 teams, application developers, local community groups - all\nbuilding toward their own visions of what Ethereum can be. The primary\nchallenge is making sure that all these projects are, collectively,\nbuilding something that feels like one Ethereum ecosystem, and not 138\nincompatible fiefdoms.\n\nTo solve this challenge, many people throughout the Ethereum\necosystem have brought up the concept of \"Ethereum alignment\". This can include values alignment (eg.\nbe open source, minimize centralization, support public goods),\ntechnological alignment (eg. work with ecosystem-wide standards), and\neconomic alignment (eg. use ETH as a token where possible). However, the\nconcept has historically been poorly defined, and this creates risk of\nsocial layer capture: if alignment means having the right\nfriends, then \"alignment\" as a concept has failed.\n\nTo solve this, I would argue that the concept of alignment\nshould be made more legible, decomposed into specific\nproperties, which can be represented by specific metrics. Each\nperson's list will be different, and metrics will inevitably change over\ntime. However, I think we already have some solid starting points.\n\n- Open source - this is valuable for two reasons: (i)\ncode being inspectable to ensure security, and more importantly (ii)\nreducing the risk of proprietary lockin and enabling permissionless\nthird-party improvements. Not every piece of every application needs to\nbe fully open source, but core infrastructure components that the\necosystem depends on absolutely should be. The gold standard here is the\nFSF free\nsoftware definition and OSI\nopen source definition.\n\n- Open standards - striving for interoperability with\nthe Ethereum ecosystem and building on open standards, both the ones\nthat exist (eg. ERC-20, ERC-1271...) and those\nthat are under development (eg. account\nabstraction, cross-L2 transfers, L1\nand L2 light\nclient proofs, upcoming address format standards). If you want to\nintroduce a new feature that is not well-served by existing standards,\nwrite a new ERC in collaboration with others. Applications and wallets\ncan be rated by which ERCs they are compatible with.\n\n- Decentralization and security - avoiding points of\ntrust, minimizing censorship vulnerabilities, and minimizing centralized\ninfrastructure dependency. The natural metrics are (i) the\nwalkaway test: if your team and servers disappear tomorrow,\nwill your application still be usable, and (ii) the insider\nattack test: if your team itself tries to attack the system,\nhow much will break, and how much harm could you do? An important\nformalization is the L2beat\nrollup stages.\n\n- Positive-sum\n\n- Toward Ethereum - the project succeeding should\nbenefit the whole Ethereum community (eg. ETH holders,\nEthereum users), even if they are not part of the\nproject's own ecosystem. Specific examples include using ETH as the\ntoken (and thus contributing to its network effect), contributions to\nopen source technology, and commitments to donate a % of tokens or\nrevenue to Ethereum ecosystem-wide public goods.\n\n- Toward the broader world - Ethereum is here to make\nthe world a more free and open place, enable new forms of ownership and\ncollaboration, and contribute positively to important challenges facing\nhumanity. Does your project do this? Examples include applications that\nbring sustainable value to broader audiences (eg. financial inclusion),\n% donations to beyond-Ethereum public goods, and building technology\nwith utility beyond crypto (eg. funding mechanisms, general computer\nsecurity) that actually gets used in those contexts.\n\nEthereum node map, source ethernodes.org\n\nObviously, not all of the above is applicable to each project. The\nmetrics that make sense for L2s, wallets, decentralized social media\napplications, etc, are all going to look very different. Different\nmetrics may also change in priority: two years ago, rollups having\n\"training wheels\" was more okay because it was \"early days\"; today, we\nneed to move to at least stage 1 ASAP. Today, the most legible metric\nfor being positive sum is commitments to donate a percentage of tokens,\nwhich more and more projects are doing; tomorrow we can find metrics to\nmake other aspects of positive-sumness legible too.\n\nMy ideal goal here is that we see more entities like L2beat emerging to track how well\nindividual projects are meeting the above criteria, and other criteria\nthat the community comes up with. Instead of competing to have the right\nfriends, projects would compete to be as aligned as possible according\nto clearly understandable criteria. The Ethereum Foundation should\nremain one-step-removed from most of this: we fund L2beat, but\nwe should not be L2beat. Making the next L2beat is itself a\npermissionless process.\n\nThis would also give the EF, and other organizations (and\nindividuals) interested in supporting and engaging with the ecosystem\nwhile keeping their neutrality, a clearer route to determine which\nprojects to support and use. Each organization and individual can make\ntheir own judgement about which criteria they care about the most, and\nchoose projects in part based on which ones best fit those criteria.\nThis makes it easier for both the EF and everyone else to become part of\nthe incentive for projects to be more aligned.\n\nYou can only be a meritocracy if merit is defined; otherwise, you\nhave a (likely exclusive and negative-sum) social game. Concerns about\n\"who watches the watchers\" are best addressed not by betting everything\non an attempt to make sure everyone in positions of influence is an\nangel, but through time-worn techniques like separation of\npowers. \"Dashboard organizations\" like L2beat, block explorers,\nand other ecosystem monitors are an excellent example\nof such a principle working in the Ethereum ecosystem today. If we do\nmore to make different aspects of alignment legible, while not\ncentralizing in one single \"watcher\", we can make the concept much more\neffective, and fair and inclusive in the way that the Ethereum ecosystem\nstrives to be.",
    "contentLength": 6462,
    "summary": "Vitalik proposes making Ethereum \"alignment\" measurable through specific metrics like open source code, decentralization tests, and ETH usage rather than social connections.",
    "detailedSummary": {
      "theme": "Vitalik proposes making Ethereum ecosystem alignment more transparent and measurable through specific metrics rather than relying on social connections.",
      "summary": "Vitalik addresses a critical challenge in the Ethereum ecosystem: ensuring that diverse projects and organizations work together cohesively rather than as fragmented, incompatible entities. He argues that the current concept of 'Ethereum alignment' is poorly defined and risks being captured by social networks rather than merit-based criteria. To solve this, Vitalik proposes making alignment more 'legible' through specific, measurable properties including open source development, adherence to open standards, decentralization and security measures, and positive-sum contributions both to Ethereum and the broader world. He advocates for independent organizations like L2beat to track and evaluate projects against these criteria, creating a merit-based system where projects compete on clearly defined standards rather than social connections. Vitalik emphasizes that the Ethereum Foundation should remain one step removed from this evaluation process to maintain neutrality, while supporting the infrastructure that enables transparent assessment of project alignment.",
      "takeaways": [
        "Ethereum alignment should be defined by measurable criteria rather than social connections to prevent 'social layer capture'",
        "Key alignment metrics include open source code, adherence to open standards, decentralization/security measures, and positive-sum contributions",
        "Independent 'dashboard organizations' like L2beat should evaluate projects against alignment criteria, not the Ethereum Foundation directly",
        "Projects should compete on merit-based alignment scores rather than having the 'right friends' in the ecosystem",
        "Different project types (L2s, wallets, applications) will have different relevant metrics, and these standards should evolve over time"
      ],
      "controversial": [
        "The risk that creating formal alignment metrics could lead to 'gaming the system' rather than genuine alignment",
        "Whether the Ethereum Foundation maintaining distance from evaluation processes is practical given their significant influence in the ecosystem"
      ]
    }
  },
  {
    "id": "general-2024-09-02-gluecp",
    "title": "Glue and coprocessor architectures",
    "date": "2024-09-02",
    "category": "general",
    "url": "https://vitalik.eth.limo/general/2024/09/02/gluecp.html",
    "path": "general/2024/09/02/gluecp.html",
    "content": "- \n\n- \n\n  \n    \n\n  \n  \n  Dark Mode Toggle\n  \n    \n\n  \n\n##  Glue and coprocessor architectures \n\n 2024 Sep 02 \nSee all posts\n\n \n \n\n Glue and coprocessor architectures \n\nSpecial thanks to Justin Drake, Georgios Konstantopoulos, Andrej\nKarpathy, Michael Gao, Tarun Chitra and various Flashbots contributors\nfor feedback and review.\n\nIf you analyze any resource-intensive computation being done in the\nmodern world in even a medium amount of detail, one feature that you\nwill find again and again is that the computation can be broken up into\ntwo parts:\n\n- A relatively small amount of complex, but not very computationally\nintensive, \"business logic\"\n\n- A large amount of intensive, but highly structured,\n\"expensive work\"\n\nThese two forms of computation are best handled in different ways:\nthe former, with an architecture that may have lower efficiency but\nneeds to have very high generality, and the latter, with an architecture\nthat may have lower generality, but needs to have very high\nefficiency.\nWhat are\nsome examples of this separation in practice?\n\nTo start off, let us look under the hood of the environment I am most\nfamiliar with: the Ethereum Virtual Machine (EVM). Here\nis the geth debug trace of a recent Ethereum transaction that I did:\nupdating the IPFS hash of my blog on ENS. The transaction consumes a\ntotal of 46924 gas, which can be categorized in this way:\n\n- Base cost: 21,000\n\n- Calldata: 1,556\n\n- EVM execution: 24,368\n\n- SLOAD opcode: 6,400\n\n- SSTORE opcode: 10,100\n\n- LOG opcode: 2,149\n\n- Other: 6,719\n\nEVM trace of an ENS hash update. Second last column is gas\nconsumption.\n\nThe moral of the story is: most of the execution\n(~73% if you look at the EVM alone, ~85% if you include the portion of\nthe base cost that covers computation) is concentrated in a very\nsmall number of structured expensive operations: storage reads\nand writes, logs, and cryptography (the base cost includes 3000 to pay\nfor signature verification, and the EVM also includes 272 to pay for\nhashing). The rest of the execution is \"business\nlogic\": fiddling around with the bits of the calldata to\nextract the ID of the record I am trying to set and the hash I am\nsetting it to, and so on. In a token transfer, this would include adding\nand subtracting balances, in a more advanced application, this might\ninclude a loop, and so on.\n\nIn the EVM, these two forms of execution are handled in different\nways. The high-level business logic is written in a higher-level\nlanguage, often Solidity, which compiles to the EVM. The expensive work\nis still triggered by EVM opcodes (SLOAD, etc),\nbut > 99% of the actual computation is done in specialized modules\nwritten directly inside of client code (or even libraries).\n\nTo reinforce our understanding of this pattern, let's explore it in\nanother context: AI code written in python using torch.\n\nForward pass of one block of a transformer model, source.\n\nWhat do we see here? We see a relatively small amount of \"business\nlogic\", written in python, which describes the structure of the\noperations that are being done. In an actual application, there will\nalso be another type of business logic, which determines details like\nhow you get the input and what you do to the output. But, if we peek\ninto each of the individual operations themselves\n(self.norm, torch.cat, +,\n*, the various steps inside self.attn...), we\nsee vectorized computation: the same operation getting computed\non a large number of values in parallel. Similarly to the first example,\na small portion of the compute is spent on business logic, and the bulk\nof the compute is spent on performing the big structured matrix and\nvector operations - in fact, the majority is just matrix\nmultiplication.\n\nJust like in the EVM example, the two types of work are handled in\ntwo different ways. The high-level business logic code is written in\nPython, a highly general and flexible language which is also very slow,\nand we just accept the inefficiency because it only touches a small part\nof the total computational cost. Meanwhile, the intensive operations are\nwritten in highly optimized code, often CUDA code running on a GPU.\nIncreasingly, we're even starting to see LLM inference being\ndone on ASICs.\n\nModern programmable\ncryptography, such as SNARKs,\nfollows a similar pattern yet again, on two levels.\nFirst, the prover can be written in a high-level language where the\nheavy work is done with vectorized operations, just like the AI example\nabove. My circle\nSTARK code here shows this in action. Second, the program that is\nbeing executed inside the cryptography can itself be written in a way\nthat is split between generalized business logic and highly structured\nexpensive work.\n\nTo see how this works, we can look at one of the latest trends in\nSTARK proving. To be general-purpose and easy to use, teams are\nincreasingly building STARK provers for widely-adopted minimal virtual\nmachines, such as RISC-V. Any program\nwhose execution needs to be proven can be compiled into RISC-V, and then\nthe prover can prove the RISC-V execution of that code.\n\nDiagram from RiscZero\ndocumentation\n\nThis is super convenient: it means that we only need to write the\nprover logic once, and from that point forward any program that needs to\nbe proven can just be written in any \"conventional\" programming language\n(eg. RiskZero supports Rust). However, there is a problem: this approach\nincurs significant overhead. Programmable cryptography is already very\nexpensive; adding the overhead of running code inside a RISC-V\ninterpreter is too much. And so developers have come up with a hack: you\nidentify the specific expensive operations that make up the bulk of the\ncomputation (often that's hashes and signatures), and you create\nspecialized modules to prove those operations extremely efficiently. And\nthen you just combine the inefficient-but-general RISC-V proving system\nand the efficient-but-specialized proving systems together, and you get\nthe best of both worlds.\n\nProgrammable cryptography other than ZK-SNARKs, such as multi-party\ncomputation (MPC) and fully\nhomomorphic encryption (FHE) will likely be optimized using a\nsimilar approach.\nWhat is the general pattern\nat play?\n\nModern computation is increasingly following what I call a\nglue and coprocessor architecture: you have some\ncentral \"glue\" component, which has high generality but low efficiency,\nwhich is responsible for shuttling data between one or more\ncoprocessor components, which have low generality but\nhigh efficiency.\n\nThis is a simplification: in practice, there are almost always more\nthan two levels along the tradeoff curve between efficiency and\ngenerality. GPUs, and other chips that are often called \"coprocessors\"\nin industry, are less general than CPUs but more general than ASICs.\nThere are complicated tradeoffs of how far to specialize, which\nare decided based on projections and intuitions about what parts of an\nalgorithm will still be the same in five years, and which parts will\nchange in six months. In a ZK-proving architecture, we often similarly\nsee multiple layers of specialization. But for a broad mental model,\nit's sufficient to think about two levels. There are parallels to this\nin many domains of computation:\n\nDomain\nGlue\nCoprocessor\n\nEthereum\nEVM\nDedicated opcodes/precompiles for specialized operations\n\nAI (eg. LLMs)\nPython (often)\nGPU via CUDA; ASICs\n\nWeb apps\nJavascript\nWASM\n\nProgrammable cryptography\nRISC-V\nSpecialized modules (eg. for hashes and signatures)\n\nFrom the above examples, it might feel like a law of nature that of\ncourse computation can be split in this way. And indeed, you can find\nexamples of specialization in computing for decades. However, I would\nargue that this separation is increasing. I think this\nis true for a few key reasons:\n\n- We have only relatively recently hit the limits of increasing CPU\nclock speed, and so further gains can only come from\nparallelization. However, parallelization is hard to\nreason about, and so it's often more practical for developers to\ncontinue reasoning sequentially, and let the parallelization happen in\nthe backend, wrapped inside specialized modules built for specific\noperations.\n\n- Computation has only recently become so fast that the\ncomputational costs of business logic have become truly\nnegligible. In this world, it makes sense to optimize the VM\nthat business logic runs in for goals other than compute efficiency:\ndeveloper friendliness, familiarity, security, and other similar\nobjectives. Meanwhile, the specialized \"coprocessor\" modules can\ncontinue to be designed for efficiency, and get their security and\ndeveloper friendliness properties from the relatively simple \"interface\"\nthat they have with the glue.\n\n- It's becoming clearer what the most important expensive\noperations are. This is most obvious in cryptography, where\nit's clear what kinds of specific expensive operations are most likely\nto be used: modular arithmetic, elliptic curve linear\ncombinations (aka multi-scalar multiplications), Fast Fourier\ntransforms, and so on. It's also becoming clearer in AI, where the bulk\nof computation has been \"mostly matrix multiplication\" (albeit with different\nlevels of precision) for\nover two decades. Similar trends appear in other domains. There are just\nmuch fewer unknown unknowns in (computationally intensive) computing\nthan there were 20 years ago.\n\n## What does this imply?\n\nA key takeaway is that glue should optimize for being good\nglue, and coprocessors should optimize for being good\ncoprocessors. We can explore the implications of this in a few\nkey areas.\n\n## EVM\n\nBlockchain virtual machines (eg. EVM) don't need to be\nefficient, they just need to be familiar. Computation in an\ninefficient VM can be made almost as efficient in practice as\ncomputation in a natively efficient VM by just adding the right\ncoprocessors (aka \"precompiles\"). The overhead incurred by eg. the EVM's\n256-bit registers is relatively small, while the benefits from the EVM's\nfamiliarity and existing developer ecosystem are great and durable.\nDeveloper teams optimizing the EVM are even finding that lack of\nparallelization is often not a primary barrier to scalability.\n\nThe best ways to improve the EVM may well just be (i) adding\nbetter precompiles or specialized opcodes, eg. some combination\nof EVM-MAX\nand SIMD may be\njustified, and (ii) improving the storage layout, which\neg. the Verkle tree\nchanges do as a side effect by greatly reducing the cost of\naccessing storage slots that are beside each other.\n\nStorage optimizations in the Ethereum Verkle tree proposal,\nputting adjacent storage keys together and adjusting gas costs to\nreflect this. Optimizations like this, together with better precompiles,\nmay well matter more than tweaking the EVM itself.\n\nSecure computing and open\nhardware\n\nOne of the big challenges with improving security of modern computing\nat the hardware layer is the overcomplicated and proprietary nature of\nit: chips are designed to be highly efficient, which requires\nproprietary optimizations. Backdoors are easy to hide, and side\nchannel vulnerabilities keep getting discovered.\n\nThere continues to be a valiant effort to push more open and more\nsecure alternatives from multiple angles. Some computations are\nincreasingly done in trusted execution environments, including on users'\nphones, and this has increased security for users already. The push\ntoward more-open-source consumer hardware continues, with recent\nvictories like a RISC-V\nlaptop running Ubuntu.\n\nRISC-V laptop running Debian, source\n\nHowever, efficiency continues to be a problem. The author of the\nabove-linked article writes:\n\nIt's unfeasible for a newer, open-source chip design like RISC-V to\ngo toe to toe with processor technologies that have been around for and\nrefined for decades. Progress has a starting point.\n\nMore paranoid ideas, like this design for\nbuilding a RISC-V computer on top of an FPGA, face even more\noverhead. But what if glue and coprocessor architectures mean\nthat this overhead does not actually matter? What if we accept\nthat open and secure chips will be be slower than proprietary chips, if\nneeded even giving up on common optimizations like speculative execution\nand branch prediction, but try to compensate for this by adding (if\nneeded, proprietary) ASIC modules for specific types of computation that\nare the most intensive? Sensitive computations can be done in a \"main\nchip\" that would be optimized for security, open source design and\nside-channel resistance. More intensive computations (eg. ZK-proving,\nAI) would be done in the ASIC modules, which would learn less\ninformation (potentially, with cryptographic blinding, perhaps in some\ncases even zero information) about the computation being performed.\n\n## Cryptography\n\nAnother key takeaway is that this is all very optimistic for\ncryptography, especially programmable\ncryptography, going mainstream. We're already seeing hyper-optimized\nimplementations of some specific highly structured computations in\nSNARKs, MPC and other settings: overhead for some hash\nfunctions is in the range of being only a few hundred times more\nexpensive than running the computation directly, and extremely low\noverhead for AI (which is mostly just matrix multiplications) is also\npossible. Further improvements like GKR will\nlikely reduce this further. Fully general purpose VM execution,\nespecially if executed inside a RISC-V interpreter, will likely continue\nto have something like ten-thousand-fold overhead, but for the reasons\ndescribed in this post, this will not matter: as long as the\nmost intensive parts of a computation are handled separately using\nefficient dedicated techniques, the total overhead will be\nmanageable.\n\nA simplified diagram of a dedicated MPC for matrix\nmultiplication, the largest component in AI model inference. See this paper for more\ndetails, including ways to keep both the model and the input\nprivate.\n\nOne exception to the idea that \"glue only needs to be familiar, not\nefficient\" is latency, and to a smaller extent data\nbandwidth. If a computation involves doing repeated heavy\noperations on the same data dozens of times (as cryptography and AI both\ndo), any delays that result from an inefficient glue layer can become a\nprimary bottleneck to running time. Hence, glue also has efficiency\nrequirements, though they are more specific ones.\n\n## Conclusion\n\nOn the whole, I consider the above trends to be very positive\ndevelopments from several perspectives. First, it is the logical way to\nmaximize computing efficiency while preserving developer friendliness,\nand being able to get more of both at the same time benefits everyone.\nIn particular, by enabling more efficiency gains from specialization on\nthe client side, it improves our ability to run computations that are\nboth sensitive and performance-demanding (eg. ZK-proving, LLM inference)\nlocally on the user's hardware. Second, it creates a large\nwindow of opportunity to ensure that the drive for efficiency does not\ncompromise other values, most notably security, openness and\nsimplicity: side-channel security and openness in computer\nhardware, reducing circuit complexity in ZK-SNARKs, and reducing\ncomplexity in virtual machines. Historically, the drive for efficiency\nhas led to these other factors taking a back seat. With\nglue-and-coprocessor architectures, it no longer needs to. One\npart of the machine optimizes for efficiency, and the other part\noptimizes for generality and other values, and the two work\ntogether.\n\nThe trend is also very beneficial for cryptography, because\ncryptography itself is a major example of \"expensive structured\ncomputation\", which gets accelerated by this trend. This adds a further\nopportunity to increase security. A security increase also becomes\npossible in the world of blockchains: we can worry less about optimizing\nvirtual machines, and instead focus more on optimizing precompiles and\nother features that live alongside virtual machines.\n\nThird, this trend presents an opportunity for smaller and\nnewer players to participate. If computation is becoming less\nmonolithic, and more modular, that greatly decreases the barrier to\nentry. Even with an ASIC for one type of computation, it's possible to\nmake a difference. The same will be true in the ZK-proving space, and in\nEVM optimization. Writing code that has near-frontier-level efficiency\nbecomes much easier and more accessible. Auditing and formally\nverifying such code becomes easier and more accessible. And\nfinally, because these very different domains of computing are\nconverging on some common patterns, there is more room for collaboration\nand learning between them.",
    "contentLength": 16644,
    "summary": "Glue architectures handle general business logic while specialized coprocessors do intensive computation efficiently, seen in EVM, AI, and crypto.",
    "detailedSummary": {
      "theme": "Modern computation is increasingly adopting a 'glue and coprocessor' architecture where simple business logic coordinates with specialized modules for intensive operations, creating opportunities for better security, efficiency, and accessibility.",
      "summary": "Vitalik argues that modern computation is converging on a pattern where systems split into two components: a 'glue' layer that handles general business logic with high flexibility but low efficiency, and 'coprocessor' modules that perform intensive, structured operations with high efficiency but low generality. He demonstrates this pattern across multiple domains including Ethereum's EVM (where most gas is consumed by storage operations and cryptography rather than business logic), AI systems (where Python orchestrates GPU matrix operations), and programmable cryptography (where RISC-V VMs coordinate with specialized proving modules). Vitalik attributes this trend to hitting CPU clock speed limits requiring parallelization, computational costs becoming negligible for business logic, and clearer understanding of which expensive operations matter most. This architecture enables optimization along different dimensions simultaneously - the glue layer can prioritize developer friendliness and security while coprocessors focus purely on efficiency. He sees this as particularly beneficial for cryptography adoption, blockchain development, and enabling smaller players to contribute specialized components rather than needing to build monolithic systems.",
      "takeaways": [
        "Modern computation splits into 'glue' (general but inefficient business logic) and 'coprocessor' (specialized but highly efficient operations) components across domains like blockchain, AI, and cryptography",
        "The EVM doesn't need to be computationally efficient since most gas consumption comes from specialized operations like storage and cryptography that can be optimized separately through precompiles",
        "This architecture creates opportunities for more secure and open hardware by accepting lower efficiency in the general-purpose layer while using specialized modules for intensive tasks",
        "Programmable cryptography will benefit significantly from this trend, as structured operations like hashing can be highly optimized while general computation overhead becomes manageable",
        "The modular nature of glue-and-coprocessor systems lowers barriers to entry, allowing smaller players to contribute specialized components rather than complete systems"
      ],
      "controversial": [
        "The claim that EVM efficiency improvements through architectural changes may be less important than adding better precompiles challenges common assumptions about blockchain scalability",
        "The suggestion that accepting slower, more secure open-source chips compensated by specialized modules could compete with proprietary hardware may be seen as overly optimistic about the viability of this tradeoff"
      ]
    }
  },
  {
    "id": "general-2024-08-21-plurality",
    "title": "Plurality philosophy in an incredibly oversized nutshell",
    "date": "2024-08-21",
    "category": "society",
    "url": "https://vitalik.eth.limo/general/2024/08/21/plurality.html",
    "path": "general/2024/08/21/plurality.html",
    "content": "- \n\n- \n\n  \n    \n\n  \n  \n  Dark Mode Toggle\n  \n    \n\n  \n\n##  Plurality philosophy in an incredibly oversized nutshell \n\n 2024 Aug 21 \nSee all posts\n\n \n \n\n Plurality philosophy in an incredibly oversized nutshell \n\nSpecial thanks to Glen Weyl and Audrey Tang for discussion and\nKarl Floersch for review.\n\nOne of the interesting tensions in the crypto space, which has become\na sort of digital home for my\ngeographically nomadic self over the last decade, is its\nrelationship to the topic of governance. The crypto space hails\nfrom the cypherpunk movement, which values independence from\nexternal constraints often imposed by ruthless and power-hungry\npoliticians and corporations, and has for a long time built technologies\nlike torrent networks and encrypted messaging to achieve these ends.\nWith newer ideas like blockchains, cryptocurrencies and DAOs, however,\nthere is an important shift: these newer constructions are long-lived,\nand constantly evolving, and so they have an inherent need to\nbuild their own governance, and not just circumvent the governance of\nunwanted outsiders. The ongoing survival of these structures\ndepends crucially on mathematical research, open source software, and\nother large-scale public goods. This requires a shift in mentality: the\nideology that maintains the crypto space needs to transcend the ideology\nthat created it.\n\nThese kinds of complex interplays between coordination and freedom,\nespecially in the context of newer technologies, are everywhere in our\nmodern society, going far beyond blockchains and cryptocurrency. Earlier\nthis year, Florida governor Ron DeSantis signed a\nbill that would ban synthetic (aka \"lab-grown\") meat from the state,\narguing that \"global elites want to control our behavior and push a diet\nof petri dish meat and bugs on Americans\", and that we need to\n\"prioritize our farmers and ranchers over ... the World Economic Forum\".\nAs you might expect, the Libertarian Party New Hampshire account\npublicly criticized the \"authoritarian\nsocialist\" nature of the legislation. But as it turned out, many\nother self-described libertarians did not share the same opinion:\n\n   \n\nTo me, LPNH's criticism of DeSantis's ban makes total sense: banning\npeople from eating a new and potentially far more ethical and\nsustainable form of meat, on the basis of little more than a disgust\nreflex, is the exact opposite of valuing freedom. And yet, it's clear\nthat many others do not feel the same way. When I scoured the internet\nfor cogent arguments why, the most compelling I could find is this argument\nfrom Roko Mijic: in short, once something like this is allowed, it\nbecomes mainstream, society reorganizes around it, and the lives of\nthose who do not want to follow along inevitably become harder and\nharder. It happened with digital cash, to the point where even the\nSwedish central bank is worried about cash payments accessibility,\nso why wouldn't it happen in other sectors of technology as well?\n\nAbout two weeks after the DeSantis signed the bill banning lab-grown\nmeat, Google announced that it was rolling out\na feature into Android that would analyze the contents of calls in\nreal time, and would automatically give the user a warning if it thinks\nthe user might be getting scammed. Financial scams are a large and\ngrowing problem, especially in regions like Southeast Asia, and they are\nbecoming increasingly sophisticated more rapidly than many people can\nadapt. AI is accelerating\nthis trend. Here, we see Google, creating a solution to help warn\nusers about scams, and what's more, the solution is entirely\nclient-side: there's no personal data being shipped off to any\ncorporate or governmental Big Brother. This seems amazing; it's\nexactly the kind of tech that I advocated for in my\npost introducing \"d/acc\". However, not all freedom-minded people\nwere happy, and at least one of the detractors was very difficult to\ndismiss as \"just a Twitter troll\": it was Meredith Whittaker, president\nof the Signal Foundation.\n\nAll three of these tensions are examples of things that have made a\ndeep philosophical question repeatedly pop into my mind: what is\nthe thing that people like myself, who think of ourselves as principled\ndefenders of freedom, should actually be defending? What is the\nupdated version of Scott Alexander's notion of liberalism\nas a peace treaty that makes sense in the twenty first century?\nClearly, the facts have changed. Public goods are much more important\nthan before, at larger scales than before. The internet has made\ncommunication abundant, rather than scarce. As Henry Farrell analyzed in\nhis\nbook on weaponized interdependence, modern information technology\ndoesn't just empower the recipient: it also enables ongoing power\nprojection by the creator. Existing attempts to deal with these\nquestions are often haphazard, trying to treat them as exceptions that\nrequire principles to be tempered by pragmatic compromise. But what if\nthere was a principled way of looking at the world, which values freedom\nand democracy, that can incorporate these challenges, and deal with them\nas a norm rather than an exception?\n\n## Table of contents\n\n- Plurality, the book\n\n- How would I define Plurality in one sentence?\n\n- What are the megapolitics of Plurality?\n\n- What is the Plurality model of \"the world as it\nis\"?\n\n- How does Plurality differ from libertarianism?\n\n- How does Plurality differ from democracy?\n\n- What are some specific technologies that the Plurality\nvision advocates?\n\n- Identity\n\n- Plural Money and Property\n\n- Voting\n\n- Conversations\n\n- Brain-to-brain communication and virtual\nreality\n\n- Where does Plurality stand in the modern ideological\nlandscape?\n\n- Is Pluralism compatible with wanting a crazy\nexponential future?\n\n- Is Plurality compatible with valuing excellence and\nexpertise?\n\n- Where could these ideas be applied first?\n\n## Plurality, the book\n\nThe above is not how Glen Weyl and Audrey Tang introduce\ntheir new book, Plurality: the\nfuture of collaborative technology and democracy. The narrative that\nanimates Glen is a somewhat different one, focusing on the increasingly\nantagonistic relationship between many Silicon Valley tech industry\nfigures and the political center-left, and seeking to find a more\ncollaborative way forward:\n\nGlen Weyl, introducing the Plurality book in a presentation in\nTaipei\n\nBut it felt more true to the spirit of the book for me to give an\nintroduction that gestures at a related set of problems from my own\nangle. After all, it is an explicit goal of Plurality to try to be\ncompelling to a pretty wide group of people with a wide set of concerns,\nthat draw from all different parts of the traditional political\nspectrum. I've long been concerned about what has felt to me like a\ngrowing decline of support for not just democracy but even freedom,\nwhich seems to have accelerated since around 2016.\n\nI've also had a front-row seat dealing with questions of governance\nfrom the governance builder's side, from my role within the Ethereum\necosystem. At the start of my Ethereum journey, I was originally\nanimated by the dream of creating a governance mechanism that was\nprovably mathematically optimal, much like we have provably optimal consensus\nalgorithms. Five years later, my intellectual exploration ended up\nwith me figuring out the theoretical\narguments why such\na thing is mathematically\nimpossible.\n\nGlen's intellectual evolution was in many ways different from mine,\nbut in many ways similar. His previous book Radical\nMarkets featured ideas inspired by classical liberal economics, as\nwell as more recent mathematical discoveries in the field, to try to\ncreate better versions of property rights and democracy that solve the\nlargest problems with both mechanisms. Just like me, he has always found\nideas of freedom and ideas of democracy both compelling, and has tried\nto find the ideal combination of both, that treats them not as opposite\ngoals to be balanced, but as opposite sides of the same coin that need\nto be integrated. More recently, just like what happened with me, the\nmathematical part of his social thinking has also moved in the direction\nof trying to treat not just individuals, but also\nconnections between individuals, as a first-class object that\nany new social design needs to take into account and build\naround, rather than treating it as a bug that needs to be\nsquashed.\n\nIt is in the spirit of these ideas, as well as in the spirit of an\nemerging transition from theory to practice, that the Plurality book is\nwritten.\n\nHow would I define\nPlurality in one sentence?\n\nIn his 2022 essay \"Why\nI Am A Pluralist\", Glen Weyl defines pluralism most succinctly as\nfollows:\n\nI understand pluralism to be a social philosophy that recognizes and\nfosters the flourishing of and cooperation between a diversity of\nsociocultural groups/systems.\n\nIf I had to expand on that a little bit, and define Plurality the\nbook in four bullet points, I would say the following:\n\n- Glen's megapolitics: the idea that the world today\nis stuck in a narrow\ncorridor between conflict and centralization, and we need a new and\nupgraded form of highly performant digital democracy as an alternative\nto both.\n\n- Plurality the vibe: the general theme that (i) we\nshould understand the world through a patchwork combination of models,\nand not try to stretch any single model to beyond its natural\napplicability, and (ii) we should take connections between\nindividuals really seriously, and work to expand and strengthen\nhealthy connections.\n\n- Plurality-inspired mechanism design: there is a set\nof principled mathematical techniques by which you can design social,\npolitical and economic mechanisms that treat not just\nindividuals, but also connections between individuals\nas a first-class object. Doing this can create newer forms of markets\nand democracy that solve common problems in markets and democracy today,\nparticularly around bridging tribal divides and polarization.\n\n- Audrey's practical experience in Taiwan: Audrey has\nalready incorporated a lot of Plurality-aligned ideas while serving as\nDigital Minister in Taiwan, and this is a starting point that can be\nlearned from and built upon.\n\nThe book also includes contributions from many authors other than\nGlen and Audrey, and if you reach the chapters closely you will notice\nthe different emphases. However, you will also find many common\nthreads.\n\nWhat are the\nmegapolitics of Plurality?\n\nIn Balaji Srinivasan's magnum opus The\nNetwork State, Balaji described his vision of the current world as\nbeing split between three poles: center-left Anglosphere elites\nexemplified by the New York Times (NYT), the Chinese Communist Party\n(CCP), and ultra-individualistic right-leaning people as exemplified by\nBitcoin (BTC). Glen, both in the Plurality book and elsewhere,\nhas given his own characterization of the \"political ideologies of the\n21st century\", that looks as follows:\n\nThe names of the three are taken from Civilization 6,\nand in the Plurality book Glen simplifies the names to\nTechnocracy, Libertarianism and\nPlurality. He describes the three roughly as\nfollows:\n\n- (Synthetic) Technocracy: some mechanism run by a\ncombination of AI and a small human elite creates lots of amazing stuff,\nand makes sure that everyone gets the share they need to live a good\nlife (eg. via UBI). Political input from non-elites is considered\nunimportant. Examples of this ideology include the Chinese Community\nParty, the World Economic Forum (\"you will own nothing and you will be\nhappy\"), Sam\nAltman and friends' UBI advocacy, and from my recent travels, I\nwould perhaps add the Dubai\nMuseum of the Future.\n\n- (Corporate) Libertarianism: maximize security of\nproperty rights and freedom of contract, and expect that most important\nprojects are started by some kind of \"great founder\" entrepreneur.\nIndividuals are protected from abuse almost entirely through the right\nto \"exit\" any system that becomes too inefficient or exploitative.\nExamples of this ideology include books like The\nSovereign Individual, free city movements like Prospera,\nas well as network states.\n\n- Digital democracy / Plurality: use internet-enabled\ntechnology to create much more high-bandwidth democratic mechanisms that\ncan aggregate preferences from a very wide group of people, and use\nthese mechanisms to create a much more powerful and effective\n\"third-sector\" or \"civil society\" that can make much better decisions.\nExamples that Glen cites include both fiction, most notably Star Trek\nand anything by Ursula le\nGuin, and real-life proto-examples, most notably e-government in\nEstonia and Taiwan.\n\nGlen sees Plurality as being uniquely able to simultaneously avoid\nthree failure modes: coordination failure leading to\nconflict (which he sees Libertarianism as risking),\ncentralization and authoritarianism (which he sees\nTechnocracy as risking), and stagnation (which he sees\n\"old-world democracy\" as risking, causing it to lose competitiveness\nagainst Libertarianism and Technocracy). Glen sees Plurality as an\nunder-explored alternative which it is his project to flesh out as an\nidea, and Audrey's project to bring to life, first in Taiwan then\nelsewhere.\n\nIf I had to summarize the difference between Balaji's program and\nGlen and Audrey's program, I would do so as follows. Balaji's vision\ncenters around creating new alternative institutions and new communities\naround those new institutions, and creating safe spaces to give them a\nchance to grow. Glen and Audrey's approach, on the other hand, is best\nexemplified by her \"fork-and-merge\"\nstrategy in e-government in Taiwan:\n\nSo, you visit a regular government website, you change your O to a\nzero, and this domain hack ensures that you're looking at a shadow\ngovernment versions of the same website, except it's on GitHub, except\nit's powered by open data, except there's real interactions going on and\nyou can actually have a conversation about any budget item around this\nvisualization with your fellow civic hackers.\n\nAnd many of those projects in Gov Zero became so popular that the\nadministration, the ministries finally merged back their code so that if\nyou go to the official government website, it looks exactly the same as\nthe civic hacker version.\n\nThere is still some choice and exit in Audrey's vision, but\nthere is a much tighter feedback loop by which the improvements created\nby micro-exits get merged back into \"mainline\" societal\ninfrastructure. Balaji would ask: how do we let the synthetic\nmeat people have their synthetic meat city, and the traditional meat\npeople have their traditional city? Glen and Audrey might rather ask:\nhow do we structure the top levels of society to guarantee people's\nfreedom to do either one, while still retaining the benefits of being\npart of the same society and cooperating on every other axis?\n\nWhat is the\nPlurality model of \"the world as it is\"?\n\nThe Plurality view on how to improve the world starts with a\nview on how to describe the world as it is. This is a key part\nof Glen's evolution, as the Glen of ten years ago had a much more\neconomics-inspired perspective toward these issues. For this reason,\nit's instructive to compare and contrast the Plurality worldview with\nthat of traditional economics.\n\nTraditional economics focuses heavily on a small number of economic\nmodels that make particular assumptions about how agents operate, and\ntreats deviations from these models as bugs whose consequences are not\ntoo serious in practice. As given in textbooks, these assumptions\ninclude:\n\n- Competition: the common\ncase for the efficiency of markets relies on the assumption that no\nsingle market participant is large enough to significantly move market\nprices with their actions - instead, the prices they set only determine\nwhether or not anyone buys their product.\n\n- Perfect information: people in a market are fully\ninformed about what products they are purchasing\n\n- Perfect rationality: people in a market have\nconsistent goals and are acting toward achieving those goals (it's\nallowed for these goals to be altruistic)\n\n- No externalities: production and use of the things\nbeing traded in a marketplace only affects the producer and user, and\nnot third parties that you have no connection with\n\nIn my own recent writing, I generally put a stronger emphasis on an\nassumption that is related to competition, but is much stronger:\nindependent choice. Lots of mechanisms proposed by\neconomists work perfectly if you assume that people are acting\nindependently to pursue their own independent objectives, but break down\nquickly once participants are coordinating their actions though some\nmechanism outside of the rules that you set up. Second price auctions\nare a great example: they are provably perfectly efficient if the above\nconditions are met and the participants are independent, but break\nheavily if the top bidders can collude. Quadratic\nfunding, invented by myself, Glen Weyl and Zoe Hitzig, is similar:\nit's a provably ideal mechanism for funding public goods if participants\nare independent, but if even two participants collude, they can extract\nan unbounded amount of money from the mechanism. My own work in pairwise-bounded\nquadratic funding tries to plug this hole.\n\nBut the usefulness of economics breaks down further once you start to\nanalyze incredibly important parts of society that don't look like like\ntrading platforms. Take, for instance, conversations. What are the\nmotivations of speakers and listeners in a conversation? As Hanson and\nSimler point out in The Elephant In The\nBrain, if we try to model conversations as information\nexchange, then we would expect to see people guarding information\nclosely and trying to play tit-for-tat games, saying things only in\nexchange for other people saying things in return. In reality, however,\npeople are generally eager to share information, and criticism of\npeople's conversational behavior often focuses on many people's tendency\nto speak too much and listen too little. In public\nconversations such as social media, a major topic of analysis is what\nkinds of statements, claims or memes go viral - a term that\ndirectly admits that the most natural scientific field to draw analogies\nfrom is not economics, but biology.\n\nSo what is Glen and Audrey's alternative? A big part of it is\nsimply recognizing that there is simply no single model or scientific\napproach that can explain the world perfectly, and we should\nuse a combination of different models instead, recognizing the limits of\nthe applicability of each one. In a key\nsection, they write:\n\nNineteenth century mathematics saw the rise of formalism: being\nprecise and rigorous about the definitions and properties of\nmathematical structures that we are using, so as to avoid\ninconsistencies and mistakes. At the beginning of the 20th century,\nthere was a hope that mathematics could be \"solved\", perhaps even giving\na precise algorithm for determining the truth or falsity of any\nmathematical claim.[6] 20th century mathematics, on the other hand, was\ncharacterized by an explosion of complexity and uncertainty.\n\n- G\u00f6del's Theorem: A number of mathematical results\nfrom the early 20th century, most notably G\u00f6del's theorem, showed that\nthere are fundamental and irreducible ways in which key parts of\nmathematics cannot be fully solved.\n\n- Computational complexity: Even when reductionism is\nfeasible in principle/theory, the computation required to predict\nhigher-level phenomena based on their components (its computational\ncomplexity) is so large that performing it is unlikely to be practically\nrelevant.\n\n- Sensitivity, chaos, and irreducible uncertainty:\nMany even relatively simple systems have been shown to exhibit \"chaotic\"\nbehavior. A system is chaotic if a tiny change in the initial conditions\ntranslates into radical shifts in its eventual behavior after an\nextended time has elapsed\n\n- Fractals: Many mathematical structures have been\nshown to have similar patterns at very different scales. A good example\nof this is the Mandelbrot set.\n\nGlen and Audrey proceed to give similar examples from physics. An\nexample that I (as one of many co-contributors in the wiki-like process\nof producing the book) contributed, and they accepted, was:\n\n- The three body problem, now famous after its\ncentral role in Liu Cixin's science-fiction series, shows that an\ninteraction of even three bodies, even under simple Newtonian physics,\nis chaotic enough that its future behavior cannot be predicted with\nsimple mathematical problems. However, we still regularly solve\ntrillion-body problems well enough for everyday use by using\nseventeenth-century abstractions such as \"temperature\" and\n\"pressure\".\n\nIn biology, a key example is:\n\n- Similarities between organisms and ecosystems: We\nhave discovered that many diverse organisms (\"ecosystems\") can exhibit\nfeatures similar to multicellular life (homeostasis, fragility to\ndestruction or over propagation of internal components, etc.)\nillustrating emergence and multiscale organization.\n\nThe theme of these examples should at this point be easy to see.\nThere is no single model that can be globally applicable, and the best\nthat we can do is stitch together many kinds of models that work well in\nmany kinds of situations. The underlying mechanisms at different scales\nare not the same, but they do \"rhyme\". Social science, they argue, needs\nto go in the same direction. And this is exactly where, they argue,\n\"Technocracy\" and \"Libertarianism\" fail:\n\nIn the Technocratic vision we discussed in the previous chapter, the\n\"messiness\" of existing administrative systems is to be replaced by a\nmassive-scale, unified, rational, scientific, artificially intelligent\nplanning system. Transcending locality and social diversity, this\nunified agent is imagined to give \"unbiased\" answers to any economic and\nsocial problem, transcending social cleavages and differences. As such,\nit seeks to at best paper over and at worst erase, rather than fostering\nand harnessing, the social diversity and heterogeneity that \u2ffb social\nscience sees as defining the very objects of interest, engagement, and\nvalue.\n\nIn the Libertarian vision, the sovereignty of the atomistic\nindividual (or in some versions, a homogeneous and tightly aligned group\nof individuals) is the central aspiration. Social relations are best\nunderstood in terms of \"customers\", \"exit\" and other capitalist\ndynamics. Democracy and other means of coping with diversity are viewed\nas failure modes for systems that do not achieve sufficient alignment\nand freedom.\n\nOne particular model that Glen and Audrey come back to again and\nagain is Georg\nSimmel's theory of individuality as arising from each individual\nbeing at a unique intersection of different groups. They describe this\nas being a long-lost third alternative to both \"atomistic individualism\"\nand collectivism. They write:\n\nIn [Georg Simmel's] view, humans are deeply social creatures and thus\ntheir identities are deeply formed through their social relations.\nHumans gain crucial aspects of their sense of self, their goals, and\ntheir meaning through participation in social, linguistic, and\nsolidaristic groups. In simple societies (e.g., isolated, rural, or\ntribal), people spend most of their life interacting with the kin groups\nwe described above. This circle comes to (primarily) define their\nidentity collectively, which is why most scholars of simple societies\n(for example, anthropologist Marshall Sahlins) tend to favor\nmethodological collectivism.[14] However, as we noted above, as\nsocieties urbanize social relationships diversify. People work with one\ncircle, worship with another, support political causes with a third,\nrecreate with a fourth, cheer for a sports team with a fifth, identify\nas discriminated against along with a sixth, and so on.\n\nAs this occurs, people come to have, on average, less of their full\nsense of self in common with those around them at any time; they begin\nto feel \"unique\" (to put a positive spin on it) and\n\"isolated/misunderstood\" (to put a negative spin on it). This creates a\nsense of what he called \"qualitaitive individuality\" that helps explain\nwhy social scientists focused on complex urban settings (such as\neconomists) tend to favor methodological individualism. However,\nironically as Simmel points out, such \"individuation\" occurs precisely\nbecause and to the extent that the \"individual\" becomes divided among\nmany loyalties and thus dividual.\n\nThis is the core idea that the Plurality book comes back to again and\nagain: treating connections between individuals as a first\nclass object in mechanism design, rather than only looking at\nindividuals themselves.\n\nHow does\nPlurality differ from libertarianism?\n\nRobert Nozick, in his 1974 book Anarchy,\nState and Utopia, argued for a minimal government that performs\nbasic functions like preventing people from initiating violent force,\nbut otherwise leaves it up to people to self-organize into communities\nthat fulfill their values. This book has become something of a manifesto\ndescribing an ideal world for many classical liberals since then.\n\nTwo examples that come to mind for me are Robin Hanson's recent post\nLibertarianism\nas Deep Multiculturalism, and Scott Alexander's 2014 post Archipelago\nand Atomic Communitarianism. Robin is interested in this concept\nbecause he wants to see a world that has more of what he calls deep\nmulticulturalism:\n\nA shallow \"multiculturalism\" tolerates and even celebrates diverse\ncultural markers, such as clothes, food, music, myths, art, furniture,\naccents, holidays, and dieties. But it is usually also far less tolerant\nof diverse cultural values, such as re war, sex, race, fertility,\nmarriage, work, children, nature, death, medicine, school, etc. It seeks\na \"mutual understanding\" that that we are (or should be) all really the\nsame once we get past our different markers.\n\nIn contrast, a deep \"multiculturalism\" accepts and even celebrates\nthe co-existence of many cultures with diverse deeply-divergent values.\nIt seeks ways for a world, and even geographic regions, to encompass\nsuch divergent cultures under substantial peace and prosperity. It\nexpects some mistrust, conflict, and even hostility between cultures,\ndue to their divergent values. But it sees this as the price to pay for\ndeep cultural variety.\n\nAs most non-libertarian government activities are mainly justified as\ncreating and maintaining shared communities/cultures and their values,\nthis urge to use government to promote shared culture seems the main\nobstacle to libertarian-style governance. That is, libertarians hope to\nshare a government without sharing a community or culture. The usual\n\"libertarian\" vs \"statist\" political axis might be seen as an axis re\nhow much we want to share culture, versus allow divergent cultures\n\nScott Alexander comes to similar conclusions in his 2014 post, though\nhis underlying goal is slightly different: he wants to find an ideal\npolitical architecture that creates the opportunity for organizations to\nsupport public goods and limit public bads that are culturally\nsubjective, while limiting the all-too-common tendency for subjective\narguments about higher-order harm (\"the gays are corroding the social\nfabric\") to become a mask for oppression. Balaji's The Network\nState is a much more concrete proposal for a social architecture\nthat tries to accomplish exactly the same objective.\n\nAnd so a key question worth asking is: where exactly is\nlibertarianism insufficient to bring about a Plural society? If\nI had to summarize the answer in two sentences, I would say:\n\n- Plurality is not just about enabling pluralism,\nit's also about harnessing it, and about making a much\nmore aggressive effort to build higher-level institutions that maximize\npositive-sum interactions between different groups and minimize\nconflict.\n\n- Plurality is not just at the level of society, it's also\nwithin each individual, allowing each individual to be\npart of multiple tribes at the same time.\n\nTo understand (2), we can zoom in on one particular example. Let us\nlook at the debate around Google's on-device anti-fraud scanning system\nin the opening section. On one side, we have a tech company releasing a\nproduct that seems to be earnestly motivated by a desire to protect\nusers from financial scams (which are a very real problem and have cost\npeople I personally know hundreds of thousands of dollars), which even\ngoes the extra mile and checks the most important \"cypherpunk values\"\nboxes: the data and computation stays entirely on-device and it's purely\nthere to warn you, not report you to law enforcement. On the other side,\nwe see Meredith Whittaker, who sees the offering as a slippery slope\ntoward something that does do more oppressive things.\n\nNow, let's look at Glen's preferred alternative: a Taiwanese app\ncalled Message Checker.\nMessage checker is an app that runs on your phone, and intercepts\nincoming message notifications and does analysis with them. This\nincludes features that have nothing to do with scams, such as using\nclient-side algorithms to identify messages that are most important for\nyou to look at. But it also detects scams:\n\nA key part of the design is that the app does not force all of its\nusers into one global set of rules. Instead, it gives users a choice of\nwhich filters they turn on or off:\n\nFrom top to bottom: URL checking, cryptocurrency address\nchecking, rumor checking.\n\nThese are all filters that are made by the same company. A more ideal\nsetup would have this be part of the operating system, with an open\nmarketplace of different filters that you can install, that would be\ncreated by a variety of different commercial and non-profit actors.\n\nThe key Pluralist feature of this design is: it gives users\nmore granular freedom of exit, and avoids being all-or-nothing.\nIf a norm that on-device anti-fraud scanning must work in this\nway can be established, then it seems like it would make Meredith's\ndystopia much less likely: if the operator decides to add a filter that\ntreats information about transgender care (or, if your fears go the\nother direction, speech advocating limits on gender self-categorization\nin athletics competitions) as dangerous content, then individuals would\nbe able to simply not install that particular filter, and they would\nstill benefit from the rest of the anti-scam protection.\n\nOne important implication is that \"meta-institutions\" need to be\ndesigned to encourage other institutions to respect this ideal of\ngranular freedom of exit - after all, as we've seen with software vendor lock-in,\norganizations don't obey this principle automatically!\n\nOne way to think about the complex interplay between coordination\nand autonomy in Plurality.\n\nHow does Plurality\ndiffer from democracy?\n\nA lot of the differences between Plural democracy and traditional\ndemocracy become clear once you read the chapter\non voting. Plural voting mechanisms have some strong explicit\nanswers to the \"democracy is two wolves and one sheep voting on what's\nfor dinner\" problem, and related worries about democracy descending into\npopulism. These solutions build on Glen's\nearlier ideas around quadratic voting, but go a step further, by\nexplicitly counting votes more highly if those votes come from actors\nthat are more independent of each other. I will get into this more in a\nlater section.\n\nIn addition to this big theoretical leap from only counting\nindividuals to also counting connections, there are also broad thematic\ndifferences. One key difference is Plurality's relationship to nation\nstates. A major disadvantage of nation-state democracy that speaks to me\npersonally was summarized well in this tweet by libertarian philosopher\nChris Freiman:\n\nThis is a serious gap: two thirds of global inequality is\nbetween countries rather than within countries, an increasing number\nof (especially digital) public goods are not global but also not clearly\ntied to any specific nation state, and the tools that we use for\ncommunication are highly international. A 21st century program for\ndemocracy should take these basic facts much more seriously.\n\nPlurality is not inherently against the existence of nation states,\nbut it makes an explicit effort to expand beyond relying on nation\nstates as its locus of action. It has prescriptions for how all kinds of\nactors can act, including transnational organizations, social media\nplatforms, other types of businesses, artists and more. It also\nexplicitly acknowledges that for many people, there is no overarching\nsingle nation-state that dominates their lives.\n\n \n\nLeft: a concentric circle view of society, from a\nsociology paper in 2004. Right: a Plural view of society:\nintersecting, but non-hierarchical circles.\n\nA big theme of Plurality is expanded on in much more detail in Ken\nSuzuki's Smooth\nSociety and its Enemies: the idea that membership in an organization\nshould not be treated as a \"true-or-false\" question. Instead, there\nshould be different degrees of membership, and these different degrees\nwould carry different benefits and different levels of obligation. This\nis an aspect of society that was always true, but becomes much more\nimportant in an internet-first world where our communities are no longer\nnecessarily nested and fully overlapping.\n\nWhat\nare some specific technologies that the Plurality vision advocates?\n\nThe Plurality book advocates for a pretty wide set of digital and\nsocial technologies that stretch across what are traditionally\nconsidered a large number of \"spaces\" or industries. I will give\nexamples by focusing on a few specific categories.\n\n## Identity\n\nFirst, Glen and Audrey's criticism of existing approaches to\nidentity. Some key quotes from the chapter\non this topic:\n\nMany of the simplest ways to establish identity paradoxically\nsimultaneously undermine it, especially online. A password is often used\nto establish an identity, but unless such authentication is conducted\nwith great care it can reveal the password more broadly, making it\nuseless for authentication in the future as attackers will be able to\nimpersonate them. \"Privacy\" is often dismissed as \"nice to have\" and\nespecially useful for those who \"have something to hide\". But in\nidentity systems, the protection of private information is the very core\nof utility. Any useful identity system has to be judged on its ability\nto simultaneously establish and protect identities.\n\nOn biometrics:\n\n[Biometrics] have important limits on their ability to establish and\nprotect identities. Linking such a wide variety of interactions to a\nsingle identifier associated with a set of biometrics from a single\nindividual collected at enrollment (or registration) forces a stark\ntrade-off. On the one hand, if (as in Aadhaar) the administrators of the\nprogram are constantly using biometrics for authentication, they become\nable to link or see activities to these done by the person who the\nidentifier points to, gaining an unprecedented capacity to surveil\ncitizen activities across a wide range of domains and, potentially, to\nundermine or target the identities of vulnerable populations.\n\nOn the other hand, if privacy is protected, as in Worldcoin, by using\nbiometrics only to initialize an account, the system becomes vulnerable\nto stealing or selling of accounts, a problem that has decimated the\noperation of related services ... If eyeballs can, sometime in the future,\nbe spoofed by artificial intelligence systems combined with advanced\nprinting technology, such a system may be subject to an extreme \"single\npoint of failure\".\n\nGlen and Audrey's preferred approach is intersectional social\nidentity: using the entire set of a person's actions and\ninteractions to serve the underlying goals of identity systems, like\ndetermining the degree of membership in communities and degree of\ntrustworthiness of a person:\n\nThis social, Plural approach to online identity was pioneered by\ndanah boyd in her astonishingly farsighted master's thesis on \"faceted\nidentity\" more than 20 years ago.[28] While she focused primarily on the\nbenefits of such a system for feelings of personal agency (in the spirit\nof Simmel), the potential benefits for the balance between identity\nestablishment and protection are even more astonishing:\n\n- Comprehensiveness and redundancy: For almost\nanything we might want to prove to a stranger, there is some combination\nof people and institutions (typically many) who can \"vouch\" for this\ninformation without any dedicated strategy of surveillance. For example,\na person wanting to prove that they are above a particular age could\ncall on friends who have known them for a long time, the school they\nattended, doctors who verified their age at various times as well, of\ncourse, on governments who verified their age.\n\n- Privacy: Perhaps even more interestingly, all of\nthese \"issuers\" of attributes know this information from interactions\nthat most of us feel consistent with \"privacy\": we do not get concerned\nabout the co-knowledge of these social facts in the way we would\nsurveillance by a corporation or government.\n\n- Security: Plurality also avoids many of the\nproblems of a \"single point of failure\". The corruption of even several\nindividuals and institutions only affects those who rely on them, which\nmay be a very small part of society, and even for them, the redundancy\ndescribed above implies they may only suffer a partial reduction in the\nverification they can achieve.\n\n- Recovery: individuals [could] rely on a group of\nrelationships allowing, for example, 3 of 5 friends or institutions to\nrecover their key. Such \"social recovery\" has become the gold standard\nin many Web3 communities and is increasingly being adopted even by major\nplatforms such as Apple.\n\nThe core message is any single-factor technique is too\nfragile, and so we should use multi-factor techniques. For\naccount recovery, it is relatively\neasy to see how this works, and it's easy to understand the security\nmodel: each user chooses what they trust, and if a particular user makes\na wrong choice, the consequences are largely confined to that user.\nOther use cases of identity, however, are more challenging. UBI and\nvoting, for example, seem like they inherently require global\n(or at least community-wide) agreement on who the members of a\ncommunity are. But there are efforts that try very hard to bridge this\ngap, and create something that comes close to \"feeling\" like a single\nglobal thing, while being based on subjective multi-factorial trust\nunder the hood.\n\nThe best example in the Ethereum ecosystem would be Circles, a UBI coin project that is\nbased on a \"web of trust\", where anyone can create an account (or an\nunlimited number of accounts) that generates 1 CRC per hour, but you\nonly treat a given account's coins as being \"real Circles\" if that\naccount is connected to you through a web-of-trust graph.\n\nPropagation of trust in Circles, from the Circles\nwhitepaper\n\nAnother approach would be to abandon the \"you're either a person or\nyou're not\" abstraction entirely, and try to use a combination of\nfactors to determine the degree of trustworthiness and membership of a\ngiven account, and give it a UBI or voting power proportional to that\nscore. Many airdrops that are being done in the Ethereum ecosystem, such\nas the Starknet\nairdrop, follow these kinds of principles.\n\nStarknet airdrop recipient categories. Many recipients ended up\nfalling into multiple categories.\n\n## Plural Money and Property\n\nIn Radical Markets, Glen focused a lot on the virtues \"stable and\npredictable, but deliberately imperfect\" versions of property rights,\nlike Harberger\ntaxes. He also focused a lot on \"market-like\" structures that can\nfund public goods and not just private goods, most notably quadratic\nvoting and quadratic funding. These are both ideas that continue to\nbe prominent in Plurality. A non-monetary implementation of quadratic\nfunding called Plural\nCredits was used to help record contributions to the book itself.\nThe ideas around Harberger taxes are somewhat updated, seeking to extend\nthe idea into mechanisms that allow assets to be partially owned by\nmultiple different individuals or groups at the same time.\n\nIn addition to this ongoing emphasis on very-large-scale market\ndesigns, one new addition to the program is a greater emphasis on\ncommunity currencies:\n\nIn a polycentric structure, instead of a single universal\ncurrency, a variety of communities would have their own currencies which\ncould be used in a limited domain. Examples would be vouchers\nfor housing or schooling, scrip for rides at a fair, or credit at a\nuniversity for buying food at various vendors.[18] These currencies\nmight partially interoperate. For example, two universities in the same\ntown might allow exchanges between their meal programs. But it would be\nagainst the rules or perhaps even technically impossible for a holder to\nsell the community currency for broader currency without community\nconsent.\n\nThe underlying goal is to have a combination of local mechanisms that\nare deliberately kept local, and global mechanisms to enable\nvery-large-scale cooperation. Glen and Audrey see their modified\nversions of markets and property as being the best candidates for\nvery-large-scale global cooperation:\n\nThose pursuing Plurality should not wish markets away. Something must\ncoordinate at least coexistence if not collaboration across the broadest\nsocial distances and many other ways to achieve this, even ones as thin\nas voting, carry much greater risks of homogenization precisely because\nthey involve deeper ties. Socially aware global markets offer much\ngreater prospect for Plurality than a global government. Markets must\nevolve and thrive, along with so many other modes of collaboration, to\nsecure a Plural future.\n\n## Voting\n\nIn Radical Markets, Glen advocated quadratic voting, which deals with\nthe problem of allowing voters to express different strength of\npreferences but while avoiding failure modes where the most extreme\nor well-resourced voice dominates decision-making. In Plurality, the\ncore problem that Glen and Audrey are trying to solve is different, and\nthis\nsection does a good job of summarizing what new problem they are\ntrying to solve:\n\nit is natural, but misleading, to give a party with twice the\nlegitimate stake in a decision twice the votes. The reason is that this\nwill typically give them more than twice as much power. Uncoordinated\nvoters on average cancel one another out and thus the total influence of\n10,000 completely independent voters is much smaller than the influence\nof one person with 10,000 votes.\n\nWhen background signals are completely uncorrelated and there are\nmany of them, there is a simple way to mathematically account for this:\na series of uncorrelated signals grows as the square root of their\nnumber, while a correlated signal grows in linear proportion to its\nstrength. Thus 10,000 uncorrelated votes will weigh as heavily as only\n100 correlated ones.\n\nTo fix this, Glen and Audrey argue for designing voting mechanisms\nwith a principle of \"degressive proportionality\": treat uncorrelated\nsignals additively, but give N correlated signals only sqrt(N)\nvotes.\n\nA precedent for this kind of approach exists in countries like the\nUnited States and in international bodies, where there are typically\nsome chambers of governance that give sub-units (states in the former\ncase, countries in the latter case) a quantity of voting power\nproportional to their population or economic power, and other chambers\nof governance that give one unit of voting power to each sub-unit\nregardless of size. The theory is that ten million voters from a big\nstate matter more than a million voters from a small state, but they\nrepresent a more correlated signal than ten million voters from ten\ndifferent states, and so the voting power that the ten million voters\nfrom a big state should be somewhere in between those two extremes.\n\n \n\nLeft: US senate, two senators per state regardless of size.\nRight: US electoral college, senator count roughly proportional to\npopulation.\n\nThe key challenge to making this kind of design work in a more\ngeneral way is, of course, in determining who is \"uncorrelated\".\nCoordinated actors pretending to be uncoordinated to increase their\nlegitimacy (aka \"astroturfing\", \"decentralization larping\", \"puppet\nstates\"...) is already a mainstream political tactic and has been for\ncenturies. If we instantiate a mechanism that determines who is\ncorrelated to whom by analyzing Twitter posts, people will start\ncrafting their Twitter content to appear as uncorrelated as possible\ntoward the algorithm, and perhaps even intentionally create and use bots\nto do this.\n\nHere, I can plug my own proposed solution to this problem: vote\nsimultaneously on multiple issues, and use the votes themselves as a\nsignal of who is correlated to whom. One implementation of this was in\npairwise\nquadratic funding, which allocates to each pair of\nparticipants a fixed budget, which is then split based on the\nintersection of how that pair votes. You can do a similar thing for\nvoting: instead of giving one vote to each voter, you can give one\n(splittable) vote to each pair of voters:\n\nIf you count by raw numbers, YES wins 3-2 on issue C. But Alice,\nBob and Charlie are highly correlated voters: they agree on almost\neverything. Meanwhile, David and Eve agree on nothing but C. In pairwise\nvoting, the whole \"NO on C\" vote of the (David, Eve) pair would be\nallocated to C, and it would be enough to overpower the \"YES on C\" votes\nof Alice, Bob and Charlie, whose pairwise votes for C add up to only\n11/12.\n\nThe key trick in this kind of design is that the determination of who\nis \"correlated\" and \"uncorrelated\" is intrinsic to the mechanism. The\nmore that two participants agree on one issue, the less their vote\ncounts on all other issues. A set of 100 \"organic\" diverse participants\nwould get a pretty high weight on their votes, because the overlap area\nof any two participants is relatively small. Meanwhile, a set of 100\npeople who all have similar beliefs and listen to the same media would\nget a lower weight, because their overlap area is higher. And a set of\n100 accounts that are all being controlled by the same owner would have\nperfect overlap, because that's the strategy that maximizes the owner's\nobjectives, but they would get the lowest weight of all.\n\nThis \"pairwise\" approach is not the mathematically ideal way to\nimplement this kind of thing: in the case of quadratic funding, the\namount of money that an attacker can extract grows with the\nsquare of the number of accounts they control, whereas ideally\nit would be linear. There is an open research problem in\nspecifying an \"ideal\" mechanism, whether for quadratic funding or\nvoting, that has the strongest properties when faced with attackers\ncontrolling multiple accounts or correlated voters.\n\nThis is a new type of democracy that naturally corrects for the\nphenomenon that internet discourse sometimes labels as \"NPCs\": a large mass\nof people that might as well just be one person because they're all\nconsuming exactly the same sources of information and believe all of the\nsame things.\n\n## Conversations\n\nAs I've said on many occasions especially in the context of DAOs, the\nsuccess or failure of governance depends ~20% on the formal governance\nmechanism, and ~80% on the structure of communication that the\nparticipants engage in before they get to the step where they've settled\non their opinions and are inputting them into the governance. To that\nend, Glen and Audrey have also spent quite a lot of time thinking about\nbetter technologies for large-scale conversations.\n\nOne conversation tool that they focus on a lot is Polis. Polis is a system that allows\npeople to submit statements about an issue, and vote on each other's\nstatements. At the end of a round, it identifies the different major\n\"clusters\" in the different points of view, and surfaces the statements\nthat were the most effective at getting support from all clusters.\n\nSource: https://words.democracy.earth/hacking-ideology-pol-is-and-vtaiwan-570d36442ee5\n\nPolis was actually used in Taiwan during some public deliberations\nover proposed laws, including agreeing on the rules for Uber-like ride\nhailing services. It has been used in several other contexts around the\nworld as well, including some experiments within the Ethereum\ncommunity.\n\nThe second tool that they focus on is one that has had much more\nsuccess becoming mainstream, though in large part due to its \"unfair\nadvantage\" from being introduced into a pre-existing social media\nplatform with hundreds of millions of users: Twitter's Community\nNotes.\n\nCommunity Notes similarly uses an algorithm that allows anyone to\nsubmit a proposed note for a post, and shows the notes that are rated\nthe most highly by people who disagree on most other notes. I described\nthis algorithm in much more detail in my\nreview of the platform. Since then, Youtube has\nannounced that they are planning to introduce a similar feature.\n\nGlen and Audrey want to see the underlying ideas behind these\nmechanisms expanded on, and used much more broadly throughout the\nplatforms:\n\nWhile [Community Notes] currently lines up all opinions across the\nplatform on a single spectrum, one can imagine mapping out a range of\ncommunities within the platform and harnessing its bridging-based\napproach not just to prioritize notes, but to prioritize content for\nattention in the first place.\n\nThe end goal is to try to create large-scale discussion platforms\nthat are not designed to maximize metrics like \"engagement\", but are\ninstead intentionally optimized around surfacing points of consensus\nbetween different groups. Live and let live, but also identify and take\nadvantage of every possible opportunity to cooperate.\n\nBrain-to-brain\ncommunication and virtual reality\n\nGlen and Audrey spend two whole chapters on \"post-symbolic\ncommunication\" and \"immersive\nshared reality\". Here, the goal is to spread information from person\nto person in a way that is much higher bandwidth than what can be\naccomplished with markets or conversation.\n\nGlen and Audrey describe an exhibit in Tokyo that gives the user a\nrealistic sensory experience of what it's like to be old:\n\nVisors blur vision, mimicking cataracts. Sounds are stripped of high\npitches. In a photo booth that mirrors the trials of aged perception,\nfacial expressions are faded and blurred. The simple act of recalling a\nshopping list committed to memory becomes an odyssey as one is\nceaselessly interrupted in a bustling market. Walking in place on pedals\nwith ankle weights on and while leaning on a cart simulates the wear of\ntime on the body or the weight of age on posture.\n\nThey argue that even more valuable and high-fidelity versions of\nthese kinds of experiences can be made with future technologies like\nbrain-computer interfaces. \"Immersive shared reality\", a cluster\nencompassing what we often call \"virtual reality\" or \"the metaverse\" but\ngoing broader than that, is described as a design space halfway in\nbetween post-symbolic communication and conversations.\n\nAnother recent book that I have read on similar topics is Herman\nNarula's Virtual\nSociety: The Metaverse and the New Frontiers of Human\nExperience. Herman focuses heavily on the social value\nof virtual worlds, and the ways in which virtual worlds can support\ncoordination within societies if they are imbued with the right social\nmeaning. He also focuses on the risks of centralized control, arguing\nthat an ideal metaverse would be created by something more like a\nnon-profit DAO than a traditional corporation. Glen and Audrey have very\nsimilar concerns:\n\nCorporate control, surveillance, and monopolization: ISR blurs the\nlines between public and private, where digital spaces can be\nsimultaneously intimate and open to wide audiences or observed by\ncorporate service providers. Unless ISR networks are built according to\nthe principles of rights and interoperability we emphasized above and\ngoverned by the broader Plurality governance approaches that much of the\nrest of this part of the book are devoted to, they will become the most\niron monopolistic cages we have known.\n\nIf I had to point to one difference in their visions, it is this.\nVirtual Society focuses much more heavily on the\nshared-storytelling and long-term continuity aspects of virtual worlds,\npointing out how games like Minecraft win the hearts and minds of\nhundreds of millions despite being, by modern standards, very limited\nfrom a cinematic immersion perspective. Plurality, on the other\nhand, seems to focus somewhat more (though far from exclusively) on\nsensory immersion, and is more okay with short-duration experiences. The\nargument is that sensory immersion is uniquely powerful in its ability\nto convey certain kinds of information that we would otherwise have a\nhard time getting. Time will tell which of these visions, or what kind\nof combination of both, will prove successful.\n\nWhere\ndoes Plurality stand in the modern ideological landscape?\n\nWhen I reflect on the political shifts that we've seen since the\nearly 2010s, one thing that strikes me is that the movements that\nsucceed in the current climate all seem to have one thing in common:\nthey are all object-level\nrather than meta-level. That is, rather than seeking to promote\nbroad overarching principles for how social or political questions\nshould be decided, they seek to promote specific stances on specific\nissues. A few examples that come to mind include:\n\n- YIMBY: standing for \"yes, in my back yard\", the\nYIMBY movement seeks to fight highly restrictive zoning regulations (eg.\nin\nthe San Francisco Bay Area), and expand freedom to build housing. If\nsuccessful, they argue that this would knock down the single largest\ncomponent of many people's cost of living, and increase\nGDP by up to 36%. YIMBY has recently had a large number of political\nwins, including a major zoning\nliberalization bill in California.\n\n- The crypto space: ideologically, the space stands\nfor freedom, decentralization, openness and anti-censorship as\nprinciples. In practice, large parts of it end up focusing more\nspecifically on openness of a global financial system and freedom to\nhold and spend money.\n\n- Life extension: the concept of using biomedical\nresearch to figure out how to intervene in the aging process\nbefore it progresses to the point of being a disease, and in\ndoing so potentially give us a far longer (and entirely healthy)\nlifespan, has become much more mainstream over the last ten years.\n\n- Effective altruism: historically, the effective altruism movement\nhas stood for the broad application of a formula: (i) caring about doing\nthe most good, and (ii) being rigorous about determining which charities\nactually accomplish that goal, noting that some\ncharities are thousands of times more effective than others.\nMore recently, however, the most prominent parts of the movement have\nmade a shift toward focusing on the single issue of AI\nsafety.\n\nOf the modern movements that have not become issue-driven in\nthis way, a large portion could be viewed as obfuscated personality\ncults, rallying around whatever set of positions is adopted and changed\nin real time by a single leader or small well-coordinated elite. And\nstill others can be criticized for being ineffective and inconsistent,\nconstantly trying to force an ever-changing list of causes under the\numbrella of an ill-defined and unprincipled \"Omnicause\".\n\nIf I had to ask myself why these shifts are taking place, I\nwould say something like this: large groups have to coordinate\naround something. And realistically, you either (i) coordinate\naround principles, (ii) coordinate around a task, or (iii) coordinate\naround a leader. When the pre-existing set of principles\nbecomes perceived as being worn out and less effective, the other two\nalternatives naturally become more popular.\n\nCoordinating around a task is powerful, but it is temporary, and any\nsocial capital you build up easily dissipates once that particular task\nis complete. Leaders and principles are powerful because they are\nfactories of tasks: they can keep outputting new things to do\nand new answers for how to resolve new problems again and again. And of\nthose two options, principles are far more socially scalable and far\nmore durable.\n\nPlurality seems to stand sharply in opposition to the broader trends.\nTogether with very few other modern movements (perhaps network states),\nit goes far beyond any single task in scope, and it seeks to coordinate\naround a principle, and not a leader. One way to understand\nPlurality, is that it recognizes that (at least at very large scales)\ncoordinating around principles is the superior point on the triangle,\nand it's trying to do the hard work of figuring out the new set of\nprinciples that work well for the 21st century. Radical Markets\nwas trying to reinvent the fields of economics and mechanism design.\nPlurality is trying to reinvent liberalism.\n\nThe way in which all of the mechanisms described in the sections\nabove combine into a single framework is best exemplified in this chart\nby Gisele Chou:\n\nOn one level, the framework makes total sense. The philosopher Nassim\nTaleb loves\nto quote Geoff and Vince Graham to describe his rejection of\n\"scale-free universalism\": \"I am, at the Fed level, libertarian; at the\nstate level, Republican; at the local level, Democrat; and at the family\nand friends level, a socialist\". Plurality philosophy takes this\nseriously, recommending different mechanisms at different scales.\n\nOn another level, it sometimes feels like \"the Plurality vibe\" is\nacting as an umbrella that is combining very different\nconcepts, that have very different reasons for accepting or\nrejecting them. For example, \"creating healthy connections between\npeople is very important\" is a very different statement from \"voting\nmechanisms need to take differences in degree of connectedness into\naccount\". It's entirely possible that pairwise quadratic funding can be\nused to make a new and better United Nations that subsidizes cooperation\nand world peace, but at the same time \"creative collaborations\" are\noverrated and great works should be the vision of one author. Some of\nthis seeming inconsistency comes from the book's diverse collaborative\nauthorship: for example, the virtual reality and brain-to-brain\nsections, and much of the work on correlation discounts, was written by\nPuja Ohlhaver, and her focuses are not quite the same as Glen's or\nAudrey's. But this is a property of all philosophies: 19th century\nliberalism combined democracy and markets, but it was a composite work\nof many people with different beliefs. Even today, there are many people\nwho like democracy and are suspicious of markets, or like markets and\nare suspicious of democracy.\n\nAnd so one question worth asking is: if your background\ninstincts on various questions differ from \"the Plurality vibe\" on some\ndimensions, can you still benefit from Plurality ideas? I will\nargue that the answer is yes.\n\nIs\nPlurality compatible with wanting a crazy exponential future?\n\nOne of the impressions that you might get from reading Plurality is\nthat, while Glen and Audrey's meta-level visions for conversations and\ngovernance are fascinating, they don't really see a future where\nanything too technologically radical happens. Here\nis a list of specific object-level outcomes that they hope to\nachieve:\n\n- The workplace, where we believe it could raise economic\noutput by 10% and increase the growth rate by a percentage\npoint\n\n- Health, where we believe it can extend human life by two\ndecades\n\n- Media, where it can heal the divides opened by social media, provide\nsustainable funding, expand participation and dramatically increase\npress freedom\n\n- Environment, where it is core to addressing most of the\nserious environmental problems we face, perhaps even more so\nthan traditional \"green\" technologies\n\n- Learning, where it can upend the linear structure of current\nschooling to allow far more diverse and flexible, lifelong\nlearning paths.\n\nThese are very good outcomes, and they are ambitious goals for the\nnext ten years. But the goals that I want to see out of a\ntechnologically advanced society are much greater and deeper than this.\nReading this section reminded me of the recent review\nI made of the museums of the future in Dubai vs Tokyo:\n\nBut their proposed solutions are mostly tweaks that try to make the\nworld more gentle and friendly to people suffering from these\nconditions: robots that can help guide people, writing on business cards\nin Braille, and the like. These are really valuable things that can\nimprove the lives of many people. But they are not what I would expect\nto see in a museum of the future in 2024: a solution that lets people\nactually see and hear again, such as optic nerve regeneration and brain\ncomputer interfaces.\n\nSomething about the Dubai approach to these questions speaks deeply\nto my soul, in a way that the Tokyo approach does not. I do not\nwant a future that is 1.2x better than the present, where I can enjoy 84\nyears of comfort instead of 70 years of comfort. I want a future that is\n10000x better than the present ... If I become infirm and weak\nfor a medical reason, it would certainly be an improvement to live in an\nenvironment designed to still let me feel comfortable despite these\ndisadvantages. But what I really want is for technology to fix\nme so that I can once again become strong.\n\nDubai is an interesting example because it also uses another\ntechnology that speaks deeply to my soul: geoengineering. Today, the\nusage, and risks, of geoengineering are on a fairly local scale: the\nUAE engages in cloud seeding and some blamed Dubai's recent floods\non it, though the\nexpert consensus seems to disagree. Tomorrow, however, there may be\nmuch bigger prizes. One example is solar\ngeoengineering: instead of re-organizing our entire economy and\nsociety to keep CO2 levels reasonably low and the planet reasonably\ncool, there is a chance that all it takes to achieve a 1-4\u2070C temperature\nreduction is sprinkling the right salts into the air. Today, these ideas\nare highly speculative, and the science is far too early to commit to\nthem, or use them as an excuse not to do other things. Even more modest\nproposals like artificial lakes cause\nproblems with parasites. But as this century progresses, our ability\nto understand the consequences of doing things like this will improve.\nMuch like medicine went from being often\nnet-harmful in earlier periods to crucially lifesaving today, our\nability to heal the planet may well go through a similar transition. But\neven after the scientific issues become much more\nwell-understood, another really big question looms: how the hell\ndo we govern such a thing?\n\nEnvironmental geopolitics is already a big question today. There are\nalready disputes\nover water rights from rivers. If transformative continent-scale or\nworld-scale geoengineering becomes viable, these issues will become much\nmore high-stakes. Today, it seems hard to imagine any solution other a\nfew powerful countries coming together to decide everything on\nhumanity's behalf. But Plurality ideas may well be the best shot\nwe have at coming up with something better. Ideas around common\nproperty, where certain resources or features of the environment can\nhave shared ownership between multiple countries, or even non-country\nentities tasked with protecting the interests of the natural environment\nor of the future, seem compelling in principle. Historically, the\nchallenge has been that such ideas are hard to formalize. Plurality\noffers a bunch of theoretical tools to do just that.\n\nIf we zoom back out beyond the geoengineering issue, and think about\nthe category of \"crazy exponential technology\" in general, it\nmight feel like there is a tension between pluralism and technology\nleading to exponential growth in capabilities. If different\nentities in society progress according to a linear, or slightly\nsuperlinear, trajectory, then small differences at time T remain small\ndifferences at time T+1, and so the system is stable. But if the\nprogress is super-exponential, then small differences turn into\nlarger and larger differences, even in proportional terms, and the\nnatural outcome is one entity overtaking everything else.\n\n \n\nLeft: slightly super-linear growth. Small differences at the\nstart become small differences at the end. Right: super-exponential\ngrowth. Small differences at the start become very large differences\nquickly.\n\nHistorically this has actually been a tradeoff. If you were to ask\nwhich 1700s-era institutions feel the most \"pluralist\", you might have\nsaid things like deeply-rooted extended family ties and trade guilds.\nHowever, the Industrial Revolution sweeping these institutions away and\nreplacing them with economies of scale and industrial capitalism is\noften precisely the thing that is credited\nwith enabling great economic growth.\n\nHowever, I would argue that the static pluralism of the\npre-industrial age and Glen and Audrey's Plurality are fundamentally\ndifferent. Pre-industrial static pluralism was crushed by what\nGlen calls \"increasing returns\". Plurality has tools specifically\ndesigned for handling it: democratic mechanisms for funding public\ngoods, such as quadratic funding, and more limited versions of property\nrights, where (especially) if you build something really powerful, you\nonly have partial ownership of what you build. With these techniques, we\ncan prevent super-exponential growth at the scale of human\ncivilization from turning into super-exponential growth in\ndisparities of resources and power. Instead, we design property\nrights in such a way that a rising tide is forced to lift all boats.\nHence, I would argue that exponential growth in technological\ncapability and Plurality governance ideas are highly\ncomplementary.\n\nIs\nPlurality compatible with valuing excellence and expertise?\n\nThere is a strand in political thought that can be summarized as\n\"elitist liberalism\": valuing the benefits of free choice and democracy\nbut acknowledging that some people's inputs are much higher quality than\nothers, and wanting to put friction or limits on democracy to give\nelites more room to maneuver. Some recent examples include:\n\n- Richard Hanania's concept\nof \"Nietzschean liberalism\" where he seeks to reconcile his\nlong-held belief that \"some humans are in a very deep sense better than\nother humans ... society disproportionately benefits from the scientific\nand artistic genius of a select few\", and his growing appreciation for\nthe benefits of liberal democracy in avoiding outcomes that are\nreally terrible and in not over-entrenching specific elites\nthat have bad ideas.\n\n- Garrett Jones's 10%\nLess Democracy, which advocates for more indirect\ndemocracy through longer term durations, more appointed positions, and\nsimilar mechanisms.\n\n- Bryan Caplan's guarded\nsupport for free speech as an institution that at least\ngives a chance for counter-elites to form and develop ideas under\nhostile conditions, even if an open \"marketplace of ideas\" is far from a\nsufficient guarantee that good ideas will win broader public\nopinion.\n\nThere are parallel arguments on the other side of the political\nspectrum, though the language there tends to focus on \"professional\nexpertise\" rather than \"excellence\" or \"intelligence\". The types of\nsolutions that people who make these arguments advocate often involve\nmaking compromises between democracy and either plutocracy or\ntechnocracy (or something that risks being worse than both) as ways of\ntrying to select for excellence. But what if, instead of making this\nkind of compromise, we try harder to solve the problem directly?\nIf we start from a goal that we want an open pluralistic\nmechanism that allows different people and groups to express and execute\non their diverse ideas so that the best can win, we can ask the\nquestion: how would we optimize institutions with that idea in\nmind?\n\nOne possible answer is prediction\nmarkets.\n\n \n\nLeft: Elon Musk proclaiming that civil war in the UK \"is\ninevitable\". Right: Polymarket bettors, with actual skin in the game,\nthink that the probability of a civil war is.... 3% (and I think even\nthat's way too high, and I made a bet to that effect)\n\nPrediction markets are an institution that allows different people to\nexpress their opinions on what will happen in the future. The virtues of\nprediction markets come from the idea that people are more likely to\ngive high-quality opinions when they have \"skin in the game\", and that\nthe quality of the system improves over time because people with\nincorrect opinions will lose money, and people with correct opinions\nwill gain money.\n\nIt is important to point out that while prediction markets\nare pluralistic in the sense of being open to diverse participants, they\nare not Pluralistic in Glen and Audrey's sense of the word.\nThis is because they are a purely financial mechanism: they do not\ndistinguish between $1 million bet by one person and $1 million bet by a\nmillion unconnected people. One way to make prediction markets more\nPluralistic would be to introduce per-person subsidies, and\nprevent people from outsourcing the bets that they make with these\nsubsidies. There are some mathematical arguments why this could do an\neven better job than traditional prediction markets of eliciting\nparticipants' knowledge and insights. Another option is to run a\nprediction market and in parallel run a Polis-style discussion platform\nthat encourages people to submit their reasoning for why they believe\ncertain things - perhaps using soulbound\nproofs of previous track record on the markets to determine whose voice\ncarries more weight.\n\nPrediction markets are a tool that can be applied in many form\nfactors and contexts. One example is retroactive\npublic goods funding, where public goods are funded after\nthey have made an impact and enough time has passed that the impact can\nbe evaluated. RPGF is typically conceived of as being paired with an\ninvestment ecosystem, where ahead-of-time funding for public\ngoods projects would be provided by venture capital funds and investors\nmaking predictions about which projects will succeed in the future. Both\nthe after-the-fact piece (evaluation) and the before-the-fact piece\n(prediction) can be made more Pluralistic: some form of quadratic voting\nfor the former, and per-person subsidies for the latter.\n\nThe Plurality book and related writings do not really feature a\nnotion of \"better vs worse\" ideas and perspectives, only of getting more\nbenefit from aggregating more diverse perspectives. On the\nlevel of \"vibes\", I think there is an actual tension here. However, if\nyou believe that the \"better vs worse\" axis is important, then I do not\nthink that these focuses are inherently incompatible: there are ways to\ntake the ideas of one to improve mechanisms that are designed for the\nother.\n\nWhere could these\nideas be applied first?\n\nThe most natural place to apply Plurality ideas is social settings\nthat are already facing the problem of how to improve collaboration\nbetween diverse and interacting tribes while avoiding centralization and\nprotecting participants' autonomy. I personally am most bullish on\nexperimentation in three places: social media,\nblockchain ecosystems and local\ngovernment. Particular examples include:\n\n- Twitter's Community\nNotes, whose note ranking system is already designed to\nfavor notes that gain support across a wide spectrum of participants.\nOne natural path toward improving Community Notes would be to find ways\nto combine it with prediction markets, thereby encouraging sophisticated\nactors to much more quickly flag posts that will get noted.\n\n- User-facing anti-fraud software. Message Checker,\nas well as the Brave browser and some\ncrypto wallets, are early examples of a paradigm of software that works\naggressively on the user's behalf to protect the user from threats\nwithout needing centralized backdoors. I expect that Software like this\nwill be very important, but it carries the inherent political question\nof determining what is and is not a threat. Plurality ideas can be of\nhelp in navigating this issue.\n\n- Public goods funding in blockchain ecosystems. The\nEthereum ecosystem makes heavy use\nof quadratic funding and retroactive funding\nalready. Pluralistic mechanisms could help in bounding the vulnerability\nof these mechanisms to collusion, and subsidize collaboration between\nparts of the ecosystem that face pressures to act competitively towards\neach other, eg. layer-2 scaling platforms and wallets.\n\n- Network\nstates, popup\ncities and related concepts. New voluntary communities that\nform online based on shared interests, and then \"materialize\" offline,\nhave many needs for (i) having less dictatorial forms of governance\ninternally, (ii) cooperating more between each other, and (iii)\ncooperating more with the physical jurisdictions in which they are\nbased. Plurality mechanisms could improve on all three.\n\n- Publicly funded news media. Historically, media has\nbeen funded either by listeners, or by the administrative arm of a\ncentralized state. Plurality mechanisms could enable more democratic\nmechanisms, which also explicitly try to bridge across and reduce rather\nthan increase polarization.\n\n- Local public goods: there are many hyper-local\ngovernance and resource allocation decisions that could benefit from\nPlurality mechanisms; my post on crypto\ncities contains some examples. One possible place to start is\nquasi-cities with highly sophisticated residents, such as\nuniversities.\n\nToday, I think that the right way to think about Plurality is as an\n\"intuition pump\" for ideas for designing social mechanisms to better\npreserve freedom of individuals and communities, enable large-scale\ncollaboration, and minimize polarization. The above contexts are good\ngrounds for experimentation because they contain (i) real-world problems\nand resources, and (ii) people who are very interested in trying new\nideas.\n\nTomorrow, there are broader political questions about the structure\nof the world in the 21st century, including what level of sovereignty\nindividuals, companies and nations have, how equal or unequal the world\nends up being, and which kinds of powerful technologies get developed in\nwhat order and with what properties. Both \"the Plurality vibe\", and\nspecific implications of Plurality mechanism design theory, have a lot\nto say on these topics.\n\nOften, there are multiple contradictory ways to apply the ideas to\nthe same question. For example, Plurality philosophy implies that there\nis value in elevating a group or mechanism if it's uncorrelated with\nother dominant mechanisms in society and thus brings something unique to\nthe table. But are billionaires a welcome injection of uncorrelated\nactivity into a world dominated by nation-states that all operate by\nvery similar internal political logic, or are more active nation states\na welcome injection of diversity into a world dominated by homogeneous\nbillionaire capitalism? Your answer will likely depend on your\npre-existing feelings toward these two groups. For this reason, I think\nPlurality is best understood not as an overarching substitute for your\nexisting frameworks of thinking about the world, but as a complement to\nit, where the underlying ideas can make all kinds of mechanisms\nbetter.",
    "contentLength": 75333,
    "summary": "Plurality philosophy advocates for upgraded digital democracy that treats connections between individuals as important as individuals themselves, offering an alternative to both conflict and centralization.",
    "detailedSummary": {
      "theme": "Vitalik presents Glen Weyl and Audrey Tang's Plurality philosophy as a third alternative to technocracy and libertarianism, emphasizing connections between individuals and digital democracy mechanisms.",
      "summary": "Vitalik explores the Plurality philosophy developed by Glen Weyl and Audrey Tang as outlined in their book, positioning it as a solution to modern governance challenges that crypto and democratic systems face. He frames Plurality as occupying a middle ground between pure libertarianism (which risks coordination failure and conflict) and technocracy (which risks centralization and authoritarianism). The philosophy treats connections between individuals as first-class objects in mechanism design, moving beyond traditional models that focus solely on individual actors. Vitalik explains how Plurality differs from both libertarianism and traditional democracy by advocating for more granular freedom of exit, intersectional social identity systems, and voting mechanisms that account for correlation between participants. The approach emphasizes using multiple complementary models rather than seeking a single universal solution, drawing parallels to how modern mathematics and physics embrace complexity and uncertainty rather than reductionism.",
      "takeaways": [
        "Plurality philosophy treats connections between individuals as fundamental design elements, not just individuals themselves",
        "The approach advocates for 'degressive proportionality' in voting - giving correlated voters less weight than truly independent ones",
        "Identity systems should be intersectional and multi-factorial rather than relying on single points of verification like biometrics",
        "Conversation technologies like Polis and Community Notes can surface consensus across different tribal divides",
        "The philosophy embraces using multiple models and mechanisms rather than seeking one universal solution to governance problems"
      ],
      "controversial": [
        "The criticism of both libertarian 'exit-only' solutions and technocratic centralized planning may alienate supporters of either approach",
        "The complex voting mechanisms that weight correlation between participants could be seen as overly technocratic or manipulable",
        "The rejection of single global identity systems in favor of subjective trust networks may raise concerns about fraud and abuse"
      ]
    }
  },
  {
    "id": "general-2024-08-03-museumfuture",
    "title": "Review: museums of the future, Dubai and Tokyo",
    "date": "2024-08-03",
    "category": "general",
    "url": "https://vitalik.eth.limo/general/2024/08/03/museumfuture.html",
    "path": "general/2024/08/03/museumfuture.html",
    "content": "- \n\n- \n\n  \n    \n\n  \n  \n  Dark Mode Toggle\n  \n    \n\n  \n\n##  Review: museums of the future, Dubai and Tokyo \n\n 2024 Aug 03 \nSee all posts\n\n \n \n\n Review: museums of the future, Dubai and Tokyo \n\nIn this past year, I have had the pleasure of visiting the Museum of the Future in\nDubai, and more recently the Miraikan (called \"The\nNational Museum of Emerging Science and Innovation\" in English, but its\nJapanese short name \u672a\u6765\u9928 translates directly to \"Museum of the\nFuture\") in Tokyo. Both of these museums came highly recommended to me\nby close friends and collaborators, and I see both as trying to solve a\ncrucially important problem: coming up with concrete imaginations of\nwhat a technologically advanced future could look like that are\npositive, and not just the 3478th Black Mirror-esque Hollywood\ndystopia.\n\nWhat struck me the most about my visits to the two museums were just\nhow different the two visions are. They are in no way\nincompatible with each other: there is no logical impossibility, and not\neven that strong a tension, between the specific technologies and\nstructures imagined by futurists in Dubai, and that of their Tokyo\ncounterparts. But at the same time, they have a very different\nfeel, and a very different direction of priorities. This leads\nto a natural question: what can we learn and appreciate from each one,\nand is there a synthesis of the two?\n\n \n\nLeft: Museum of the Future, Dubai, view from the outside.\nRight: giant orb inside the Miraikan, Tokyo, showing the world's major\nlanguages.\n\nWhat I liked\nabout the Museum of the Future, Dubai\n\nWhen you start going through the Museum of the Future, the first\nthing you go into is a simulated space elevator, taking you from the\nsurface of the Earth in 2064 into a space station in geostationary\norbit. You can see information screens and panels that give you a view\nof all the space stations that humanity has all around the solar system,\nboth on and around planets and at Lagrange\npoints.\n\nAfter that, you see exhibits various other areas of science and\ntechnology. One major theme was meditation and health and wellness,\nshowing infrastructure that makes it much easier for people to enter\nalternative mental states. The section that struck me the most was the\none on biotech, which showed a vision of using genetic engineering to\nimprove the resilience of the biosphere by enabling plants and animals\nto survive in more diverse environments.\n\n \n\nIt's worth, err.... meditating on this scene for a bit. This\nis a drastic departure from the traditional Western way of thinking\nabout environmental issues. In the West, nature is a garden of Eden,\noriginally a beautiful pristine thing, now fallen to the corrupting\ninfluence of industrial technology. The main ethical imperative is to\npreserve, to do less harm than we otherwise would. In Dubai,\nthe narrative is the opposite. The default condition of nature,\nat least as they are accustomed to it, is a desolate\nwasteland. Human ingenuity and artifice applied to nature is not\njust there to mitigate the harms of other human ingenuity and\nartifice, it can actually go further and improve the\nenvironment beyond what it was when we started.\n\nThe Miraikan does not have anything like this. There is an exhibit\nthat deals with the important environmental problems facing the earth,\nbut its tone toward the problem is much more conventional: these\nproblems are the fault of human beings, we need to be mindful and find\nways to have a smaller footprint. There are multiple exhibits that deal\nwith improving the lives of people who do not have good (or functioning\nat all) eyesight or hearing. But their proposed solutions are mostly\ntweaks that try to make the world more gentle and friendly to people\nsuffering from these conditions: robots that can help guide people,\nwriting on business cards in Braille, and the like. These are really\nvaluable things that can improve the lives of many people. But they are\nnot what I would expect to see in a museum of the future in 2024: a\nsolution that lets people actually see and hear again, such as\noptic\nnerve regeneration and brain\ncomputer interfaces.\n\nSomething about the Dubai approach to these questions speaks deeply\nto my soul, in a way that the Tokyo approach does not. I do not want a\nfuture that is 1.2x better than the present, where I can enjoy 84 years\nof comfort instead of 70 years of comfort. I want a future that is\n10000x better than the present. I believe in the type of Nietzscheanism\nthat Scott Alexander described\nin his recent blog post, where he cautions against having your\nprimary goals in life be goals like \"I don't want to make anyone mad\"\nand \"I want to take up less space\", which are better satisfied by being\ndead than being alive. If I become infirm and weak for a medical reason,\nit would certainly be an improvement to live in an environment designed\nto still let me feel comfortable despite these disadvantages. But what I\nreally want is for technology to fix me so that I can once again\nbecome strong.\n\nThat said, there is also something in the Dubai Museum of the Future\nthat felt missing and limiting, which the Miraikan does a truly\nexcellent job of making up for. And so now is a good time to switch\nfocus, and talk about the virtues that I think make the Miraikan\ngreat.\nWhat I liked about the\nMiraikan, Tokyo\n\nWhen you first enter the Miraikan, the first exhibit is about the\nplanetary crisis: both global warming, and the whole host of various\nenvironmental issues that have to do with the quantities of pollutants\ngetting too high or the quantities of essential resources getting too\nlow. Immediately after that, you see an exhibit of various forms of art,\ninvolving heavy use of AI, that mimics various patterns that we see in\nnature. After that, a giant orb that repeats a short infographic film\ncalled \"Into the Diverse World\" that shows various statistics about\ndifferent parts of the world and how people live in the various regions.\nAfter that, a hands-on exhibit showing the inner workings of the basic\nlow-level internet protocols.\n\n \n\nLeft: a diagram showing how much different countries\ncontribute to the world's CO2 emissions. Right: a replica of a natural\nbutterfly right beside a robotic one.\n\nWhat particularly strikes me about the exhibits is the way in which\nthey invite people to actively learn and participate. The\ninformational exhibits all strive to present information in a way that\nmakes it tangible and easier for people to understand important details,\nand consequences, of each problem. A section on overfishing features the\ncomplaint \"I like sushi ... but we may not be able to eat sushi casually\nin the future, right?\". At least two of the exhibits end with an\ninteractive section, which asks a question related to the content and\ninvites people to provide their own answers. An exhibit on solving the\nEarth's resource problems takes the form of a game.\n\n \n\nLeft: a billboard inviting museum guests to submit answers\nto \"how can we avoid polluting?\" and \"what can we do to continue living\non this Earth?\", and showing recent guests' answers. Right: a game with\nthe theme of avoiding a minefield of ecological challenges on the way to\ngetting to a good future in 2100.\n\nThe underlying tone of the two museums differs drastically in this\nregard. The museum in Dubai feels consumerist: this is the amazing\nfuture that we are going to have, and you just need to sit back and\nenjoy as we build it for you. The museum in Tokyo feels like an\ninvitation to participate: we're not going to tell you too much about\nwhat the future is, but we want you to think about the issues, learn\nwhat's going on under the hood, and become part of building the shared\nfuture.\n\nThe main type of technology that I found missing in the Museum of the\nFuture in Dubai is social technology, particularly\ngovernance. The only explicit description of governance\nstructure that I found in Dubai's imagined world of 2064 was an offhand\nline in a description of the primary space station on Mars: \"Operator:\nGlobal Space Authority, SpaceX\". In the Miraikan, on the other hand, the\nstructure of the museum itself emphasizes collaborative discussion, and\nyou see frequent references to languages, culture, government and\nfreedom of the press.\nAre the two visions\ncompatible?\n\nAt first, the two visions seem very different, and perhaps\nthematically even going in opposite directions. But the more I think\nabout them, the more it feels like the two are actually very\nsynergistic: one plugs the holes in the other. I do not want to see a\nworld in 2100 that is like the world of today, but at most 20% better.\nAnd civilizations that do have an overriding mindset of trying\nto make do with less are going to find themselves under constant\npressure from both outside forces and parts of their own societies that\nwant to more strongly push against our boundaries. But at the same time,\nthe more our society does radically progress into something far\noutside historical norms, the more it becomes important to make sure\nthat everyone is along for the ride, both in understanding what is going\non, and in being part of the discussion and part of the process of\nmaking it happen.\n\nMy own posts\ntrying to make advanced topics in cryptography more accessible are\nmade precisely in this spirit: we really need the advanced tools, but we\nalso need them to be understandable and accessible, to ensure that more\npeople can work together and ensure that the future empowers people,\nrather than turning into a series of iPhone interfaces built by a few\nthat the rest of us can only access in standardized ways.\n\nPerhaps the ideal museum of the future that I would want to see, is\none that combines the boldness of the Dubai Museum of the Future's\nimagination, with the welcoming and inviting spirit that only something\nlike the Miraikan can bring.\n\n \n\nLeft: \"The cosmos is for everyone\", Dubai Museum of the\nFuture. Right: a robot intentionally designed to be cute and friendly\nrather than threatening, Miraikan.",
    "contentLength": 9996,
    "summary": "Dubai's Museum of the Future shows ambitious tech visions while Tokyo's Miraikan emphasizes collaborative participation in building the future.",
    "detailedSummary": {
      "theme": "Vitalik compares two future-oriented museums in Dubai and Tokyo, analyzing their contrasting visions of technological progress and proposing a synthesis of their approaches.",
      "summary": "Vitalik visits the Museum of the Future in Dubai and the Miraikan in Tokyo, finding both museums attempt to create positive visions of the future rather than dystopian narratives. The Dubai museum presents a bold, transformative vision where technology dramatically improves human capabilities and environments - featuring space elevators, genetic engineering to enhance ecosystems, and solutions that don't just preserve nature but actively improve it beyond its original state. Vitalik appreciates this ambitious approach that seeks a future '10000x better than the present' rather than incremental improvements. In contrast, the Tokyo museum emphasizes participation, education, and collaborative problem-solving. It focuses on helping people understand complex issues like climate change and internet protocols while inviting them to contribute solutions. The Miraikan presents technology as something people should understand and help shape, with significant attention to social technology and governance structures that the Dubai museum largely ignores. Vitalik concludes that while the museums have different approaches - Dubai being more consumerist ('sit back and enjoy') versus Tokyo being more participatory ('become part of building the future') - they are ultimately complementary rather than contradictory, with each filling gaps in the other's vision.",
      "takeaways": [
        "Bold technological visions that seek transformative rather than incremental change can inspire more ambitious futures than conventional preservation-focused approaches",
        "The Dubai museum's approach to environmental issues - using technology to actively improve nature rather than just minimize human impact - represents a fundamentally different paradigm from traditional Western environmentalism",
        "Participatory education and democratic involvement in technological development are crucial complements to ambitious technological visions",
        "Social technology and governance structures are essential components of future planning that purely technology-focused visions often overlook",
        "The ideal approach to envisioning the future combines bold imagination with inclusive participation, ensuring everyone can understand and contribute to technological progress"
      ],
      "controversial": [
        "Vitalik's critique that helping disabled people adapt to their conditions rather than directly curing them through advanced technology like brain-computer interfaces represents a limitation in thinking about the future",
        "His characterization of Western environmental philosophy as viewing nature as a 'fallen garden of Eden' versus the Middle Eastern view of nature as a 'desolate wasteland' to be improved may oversimplify complex cultural attitudes",
        "The assertion that he wants a future '10000x better than the present' rather than incremental improvements could be seen as dismissive of gradual progress and adaptation approaches"
      ]
    }
  },
  {
    "id": "general-2024-07-23-circlestarks",
    "title": "Exploring circle STARKs",
    "date": "2024-07-23",
    "category": "math",
    "url": "https://vitalik.eth.limo/general/2024/07/23/circlestarks.html",
    "path": "general/2024/07/23/circlestarks.html",
    "content": "- \n\n- \n\n  \n    \n\n  \n  \n  Dark Mode Toggle\n  \n    \n\n  \n\n##  Exploring circle STARKs \n\n 2024 Jul 23 \nSee all posts\n\n \n \n\n Exploring circle STARKs \n\nThis article assumes familiarity with the basics of how SNARKs\nand STARKs work; if you are not familiar, I recommend the first few\nsections in this\narticle. Special thanks to Eli ben-Sasson, Shahar Papini, Avihu Levy\nand others at starkware for feedback and discussion.\n\nThe most important trend in STARK protocol design over the last two\nyears has been the switch to working over small fields. The earliest\nproduction implementations of STARKs worked over 256-bit fields -\narithmetic modulo large numbers such as 21888...95617 \\(\\approx 1.51 * 2^{253}\\) - which made these\nprotocols naturally compatible with verifying elliptic curve-based\nsignatures, and made them easy to reason about. But this led to\ninefficiency: in most cases we don't actually have good ways to make use\nof these larger numbers, and so they ended up as mostly wasted space,\nand even more wasted computation, since arithmetic over 4x bigger\nnumbers takes ~9x more\ncomputation time. To deal with this, STARKs have started working over\nsmaller fields: first Goldilocks\n(modulus \\(2^{64} - 2^{32} + 1\\)) and\nthen Mersenne31\nand BabyBear (\\(2^{31} - 1\\) and\n\\(2^{31} - 2^{27} + 1\\),\nrespectively).\n\nThis switch has already led to demonstrated massive improvements in\nproving speed, most notably Starkware being able to prove\n620,000 Poseidon2 hashes per second on an M3 laptop. Particularly,\nthis means that, provided we're willing to trust Poseidon2 as a hash\nfunction, one of the hardest parts of making an efficient ZK-EVM\nis effectively solved. But how do these techniques work, and how do\ncryptographic proofs, which typically require large numbers for\nsecurity, get built over these fields? And how do these protocols\ncompare to even more exotic constructions\nsuch as Binius?\nThis post will explore some of these nuances, with a particular eye to a\nconstruction called Circle\nSTARKs (implemented in Starkware's stwo, Polygon's plonky3, and my\nown implementation in (sort of) python), which has some unique\nproperties designed to be compatible with the highly efficient\nMersenne31 field.\nIssues common to small\nfields\n\nOne of the most important \"tricks\" when making hash-based proofs (or\nreally, any kind of proof) is the idea of proving things about\nevaluations of a polynomial as a random point, as a substitute for\nproving things about the underlying polynomials.\n\nFor example, suppose that a proof system requires you to generate a\ncommitment to a polynomial, \\(A\\),\nwhich must satisfy \\(A^3(x) + x - A(\\omega*x)\n= x^N\\) (a pretty common type of claim to prove in ZK-SNARK\nprotocols). The protocol can require you to pick a random coordinate\n\\(r\\), and prove that \\(A(r) + r - A(\\omega*r) = r^N\\). And then in\nturn, to prove that \\(A(r) = c\\), you\nprove that \\(Q = \\frac{A - c}{X - r}\\)\nis a polynomial (as opposed to a fractional expression).\n\nIf you know \\(r\\) ahead of\ntime, you can always cheat these protocols. In this case, you could\njust set \\(A(r)\\) to be zero, retrofit\n\\(A(\\omega * r)\\) to satisfy the\nequation, and then let \\(A\\) be the\nline that passes through those two points. And similarly for the second\nstep, if you know \\(r\\) ahead of time,\nyou can generate whatever \\(Q\\) you\nwant, and then retrofit \\(A\\) to match\nit, even if \\(A\\) is a fractional (or\nother non-polynomial) expression.\n\nTo prevent these attacks, we need to choose \\(r\\) after the attacker provides\n\\(A\\) (the \"Fiat-Shamir\nheuristic\" is a fancy name for setting \\(r\\) to be the hash of \\(A\\)). Importantly, we need to\nchoose \\(r\\) from a set large enough\nthat the attacker cannot guess it.\n\nIn elliptic curve based protocols and even 2019-era STARKs, this was\ntrivial: all of our math was done over 256-bit numbers, so we choose\n\\(r\\) as a random 256-bit number, and\nwe're fine. With STARKs over smaller fields, we have a problem: there\nare only about two billion possible values of \\(r\\) to choose from, and so an attacker\nwanting to make a fake proof need only try two billion times - a lot of\nwork, but quite doable for a determined attacker!\n\nThere are two natural solutions to this problem:\n\n- Perform multiple random checks\n\n- Extension fields\n\nThe approach of performing multiple random checks is intuitively\nappealing and simple: instead of checking at one coordinate,\nyou repeat the check at each of four random coordinates. This\nis theoretically doable, but there is an efficiency issue. If you're\ndealing with degree < \\(N\\)\npolynomials over a size \\(p\\) field,\nit's actually possible for an attacker to craft bad polynomials that\n\"look\" good in \\(N\\) positions. Hence,\ntheir chance of breaking one round of the protocol is \\(\\frac{N}{p}\\). If eg. \\(p = 2^{31} - 1\\) and \\(N = 2^{24}\\), that means the attacker only\ngets seven bits of security per round, and so you need to do not four,\nbut around 18 rounds, to be properly robust against such attackers.\nIdeally, we would have something where we do \\(k\\) times more work but only have to\nsubtract \\(N\\) from the security level\nonce.\n\nThis gets us to the other solution: extension\nfields. Extension fields are like complex numbers, but over\nfinite fields: we imagine into existence a new value, call it \\(i\\), and declare that \\(i^2 = -1\\). Multiplication becomes:\n\\((a+bi) * (c+di) = (ac - bd) + (ad +\nbc)i\\). We can now operate over pairs \\((a,b)\\) rather than just single numbers.\nAssuming we're working over size \\(\\approx\n2^{31}\\) fields like Mersenne or BabyBear, this gets us up to\nhaving \\(\\approx 2^{62}\\) values from\nwhich to choose \\(r\\). To go even\nhigher, we apply the same technique again, except we already used \\(i\\) so we need to define a new value\ndifferently: in Mersenne31, we pick \\(w\\) where \\(w^2 =\n-2i-1\\). Multiplication now becomes \\((a + bi + cw + diw) * (e + fi + gw + hiw) =\n...\\)\n\nOK fine, here's the code implementation. It's not optimal\n(you can improve it with Karatsuba),\nbut it shows the principles.\n\nNow, we have \\(\\approx 2^{124}\\)\nvalues to choose \\(r\\) from, which is\nhigh enough for our security needs: if we are dealing with degree <\n\\(2^{20}\\) polynomials, we get 104 bits\nof security from one round. If we want to be paranoid and go up to the\nmore widely-accepted 128 bit security level, we can add some proof of\nwork into the protocol.\n\nNote that we only actually use this extension field in the FRI\nprotocol, and other cases where random linear combinations are required.\nThe bulk of the math is done over only the \"base field\" (modulo \\(2^{31}-1\\) or \\(15 * 2^{27} + 1\\)), and almost all of the\ndata that is hashed is over the base field, so you only hash four bytes\nper value. This lets us both benefit from the efficiency of small\nfields, and retain the ability to dip into a larger field when we need\nto do so for security.\n\n## Regular FRI\n\nWhen building a SNARK or STARK, the first step is typically\narithmetization: reducing an arbitrary computation problem into an\nequation where some of the variables and coefficients are polynomials\n(eg. the equation often looks like \\(C(T(x),\nT(next(x))) = Z(x) * H(x)\\), where \\(C\\), \\(next\\) and \\(Z\\) are provided and the solver needs to\nprovide \\(T\\) and \\(H\\)). Once you have such an equation, a\nsolution to the equation corresponds to a solution to the underlying\ncomputational problem.\n\nTo prove that you have a solution, you need to prove that the values\nthat you are proposing actually are real polynomials (as opposed to\nfractions, or datasets that look like one polynomial in one place and a\ndifferent polynomial in another place, or...), and have a certain maximum\ndegree. In order to do this, we apply a random linear combination trick\niteratively:\n\n- Suppose you have evaluations of a polynomial \\(A\\), and you want to prove that its degree\nis \\(< 2^{20}\\)\n\n- Consider the polynomials \\(B(x^2) = A(x) +\nA(-x)\\), and \\(C(x^2) = \\frac{A(x) -\nA(-x)}{x}\\).\n\n- Let \\(D\\) be a random linear\ncombination \\(B + rC\\)\n\nEssentially, what's going on is that \\(B\\) isolates the even coefficients of \\(A\\), and \\(C\\) isolates the odd coefficients. Given\n\\(B\\) and \\(C\\), you can recover \\(A\\): \\(A(x) =\nB(x^2) + xC(x^2)\\). And if \\(A\\)\nreally has degree \\(< 2^{20}\\), then\n(i) \\(B\\) and \\(C\\) have degree \\(< 2^{19}\\). And being a random linear\ncombination, \\(D\\) must also have\ndegree \\(< 2^{19}\\).\n\nWe've reduced a \"prove degree \\(<\n2^{20}\\)\" problem into a \"prove degree \\(< 2^{19}\\)\" problem. Repeat this 20\ntimes, and you get the technique that is called \"Fast Reed-Solomon\nInteractive Oracle Proofs of Proximity\", or \"FRI\". If someone tries\nto push something through this technique which is not a degree\n\\(< 2^{20}\\) polynomial, then the\nsecond-round output will (with probability \\(\\approx 1 - \\frac{1}{2^{124}}\\)) not be a\ndegree \\(< 2^{19}\\) polynomial, the\nthird-round output will not be degree \\(<\n2^{18}\\), and so on, and the final check at the end will fail. A\ndataset which is equal to a degree \\(<\n2^{20}\\) polynomial in most positions has some chance of\npassing through the scheme, but in order to construct such a dataset you\nneed to know the underlying polynomial, so even such a\nslightly-defective proof is a convincing argument that the prover could\ngenerate a \"real\" proof if they wanted to. There are further technical\ncomplexities in proving that this holds for all possible\ninputs; understanding the fine details of this has been a major focus of\nacademic STARK research over the last five years.\n\nLet's look into what's going on here in more detail, and what\nproperties are necessary to make this all work. At each step, we're\nreducing the degree by a factor of 2, and we're also reducing\nthe domain (the set of points we're looking at) by a factor of\n2. The former is what makes FRI work at all. The latter is what makes it\nso blazing fast: because each round is 2x smaller than the previous, the\ntotal cost is \\(O(N)\\) instead of \\(O(N*log(N))\\).\n\nTo do this domain reduction, we needed a two-to-one map:\n\\(\\{x, -x\\} \\rightarrow x^2\\). What's\nnice about this two-to-one map is that it's repeatable: if you start\nwith a multiplicative subgroup (a set \\(\\{1, \\omega, \\omega^2 ...\n\\omega^{n-1}\\}\\)), then you start off with a set where for any\n\\(x\\) in the set, \\(-x\\) is also in the set (as if \\(x = \\omega^k\\), \\(-x = \\omega^{k\\pm\\frac{N}{2}}\\)), and if\nyou then square it to get \\(\\{1, (\\omega^2),\n(\\omega^2)^2 ... (\\omega^2)^{\\frac{n}{2}-1}\\}\\), then the exact\nsame property applies, and so you can keep reducing all the way down to\none value (though in practice we usually stop a little bit earlier).\n\nYou can think of this as being an operation of taking a line that\ngoes around a circle, and stretching that line until it makes two\nrotations along that circle. A point at x degrees becomes a point at 2x\ndegrees. Each point from 0...179 degrees has a corresponding point at\n180...359 degrees that it ends up overlapping with. And you can repeat\nthis procedure again and again.\n\nFor this to work, you need the original multiplicative subgroup to\nhave a size with a large power of 2 as a product. BabyBear has modulus\n\\(15 * 2^{27} + 1\\), and so the largest\npossible subgroup is all nonzero values - hence, size \\(15 * 2^{27}\\). This is very friendly to the\nabove technique. You could take a subgroup of size \\(2^{27}\\), or you could just take that full\nset, do the FRI to reduce the polynomial all the way down to degree 15,\nand then check tthe degree directly at the end. Mersenne31, however,\ndoes not work in this way. The modulus is \\(2^{31} - 1\\), and so the multiplicative\nsubgroup has size \\(2^{31} - 2\\). This\ncan be divided by 2 only once. From there forward, we have no way to do\nan FFT - at least not using the technique above.\n\nThis is a tragedy, because Mersenne31 is a super-convenient\nfield to do arithmetic in using existing 32-bit CPU/GPU operations. If\nyou add two numbers, the result may be above \\(2^{31}-1\\), but you can reduce it by doing\n\\(x \\rightarrow x + (x >> 31)\\),\nwhere \\(>>\\) is a bit shift. For\nmultiplication, you can do something similar, though you need to use a\nspecial (but commonly available) opcode that returns the \"high-order\nbits\" of a multiplication result (ie. \\(floor(\\frac{xy}{2^{32}})\\)). This allows\narithmetic to be around 1.3x more efficient than BabyBear. If we\ncould do FRI over Mersenne31, it would make things\nsignificantly better for us.\n\n## Circle FRI\n\nHere is where the clever trick of circle\nSTARKs comes in. Given a prime \\(p\\), it turns out that we also have easy\naccess to a group of size \\(p+1\\) that\nhas similar two-to-one properties: the set of points \\((x,y)\\) where \\(x^2 + y^2 = 1\\). Let's look at this\nstructure modulo 31:\n\nThe points follow an addition law, which might feel very familiar if\nyou've recently done either trigonometry\nor complex\nmultiplication:\n\n\\((x_1, y_1) + (x_2, y_2) = (x_1x_2 -\ny_1y_2, x_1y_2 + x_2y_1)\\)\n\nThe doubling form is:\n\n\\(2 * (x, y) = (2x^2 - 1, 2xy)\\)\n\nNow, let's focus on only the points that are in \"odd\"\npositions on this circle:\n\nNow, here is our FFT. First, we collapse all the points down to a\nsingle line. Our equivalent of the \\(B(x^2)\\) and \\(C(x^2)\\) formulas that we had in regular\nFRI is:\n\n\\(f_0(x) = \\frac{F(x,y) +\nF(x,-y)}{2}\\) \\(f_1(x) = \\frac{F(x,y) -\nF(x,-y)}{2y}\\)\n\nWe can then take a random linear combination, and we get a\none-dimensional \\(F\\) that is over a\nsubset of the x line:\n\nFrom the second round onward, the map changes:\n\n\\(f_0(2x^2-1) = \\frac{F(x) +\nF(-x)}{2}\\) \\(f_1(2x^2-1) = \\frac{F(x)\n- F(-x)}{2x}\\)\n\nAnd this map actually takes the above set, and reduces its size in\nhalf each time! What is going on here is that each \\(x\\) is in some sense \"standing in\" for two\npoints: \\((x,y)\\) and \\((x,-y)\\). And \\(x\n\\rightarrow 2x^2-1\\) is the point doubling law above. Hence, we\ntake the \\(x\\) coordinate of two\nopposite points on the circle, and convert it into the \\(x\\) coordinate of the doubled point.\n\nFor example, if we take the second-rightmost value, \\(2\\), and apply the map, we get \\(2(2^2) - 1 = 7\\). If we go back to the\noriginal circle, \\((2,11)\\) is the\nthird point going counterclockwise from the right, and so if we double\nit, we get the sixth point going counterclockwise from the right, which\nis... \\((7, 13)\\).\n\nThis could have all been done two-dimensionally, but\noperating over one dimension makes things more efficient.\n\n## Circle FFTs\n\nAn algorithm closely related to FRI is the fast Fourier\ntransform, which takes a set of \\(n\\) evaluations of a degree \\(< n\\) polynomial and converts it into\nthe \\(n\\) coefficients of the\npolynomial. An FFT follows the same path as a FRI, except instead of\ngenerating a random linear combination \\(f_0\\) and \\(f_1\\) at each step, it just recursively\napplies a half-sized FFT on both, and then takes the output of \\(FFT(f_0)\\) as the even coefficients and\n\\(FFT(f_1)\\) as the odd\ncoefficients.\n\nThe circle group also supports an FFT, which is also constructed from\nFRI along similar lines. However, a key difference is that the\nobjects that circle FFTs (and circle FRI) work over are not technically\npolynomials. Rather, they are what mathematicians call a Riemann-Roch\nspace: in this case, polynomials \"modulo\" the circle (\\(x^2 + y^2 - 1 = 0\\)). That is, we treat any\nmultiple of \\(x^2 + y^2 - 1\\) as being\nequal to zero. Another way of thinking about it is: we only allow\ndegree-1 powers of \\(y\\): as soon as we\nget a \\(y^2\\) term, we replace it with\n\\(1 - x^2\\).\n\nOne other thing that this implies is that the \"coefficients\" that a\ncircle FFT outputs are not monomials like in regular FRI (eg. if regular\nFRI outputs \\([6, 2, 8, 3]\\), then we\nknow this means \\(P(x) = 3x^3 + 8x^2 + 2x =\n6\\)). Instead, the coefficients are in a strange basis specific\nto circle FFTs:\n\n\\(\\{1, y, x, xy, 2x^2-1, 2x^2y-y, 2x^3-x,\n2x^3y-xy, 8 x^4 - 8 x^2 + 1...\\}\\)\n\nThe good news is that as a developer, you can almost completely\nignore this. STARKs never give you a need to know the coefficients.\nInstead, you can just always store \"polynomials\" as a set of evaluations\non a particular domain. The only place you need to use FFTs, is to\nperform (the Riemann-Roch space analogue of) low-degree\nextension: given \\(N\\) values,\ngenerate \\(k*N\\) values that are on\nthat same polynomial. In that case, you can do an FFT to generate the\ncoefficients, append \\((k-1)n\\) zeroes\nto those coefficients, and then do an inverse-FFT to get back your\nlarger set of evaluations.\n\nCircle FFTs are not the only type of \"exotic FFT\". Elliptic\ncurve FFTs are even more powerful, because they work over\nany finite field (prime, binary, etc). However, ECFFTs are even\nmore complex to understand and less efficient, and so because we can use\ncircle FFTs for \\(p = 2^{31}-1\\), we\ndo.\n\nFrom here, let's get into some of the more esoteric minutiae that\nwill be different for someone implementing circle STARKs, as compared to\nregular STARKs.\n\n## Quotienting\n\nA common thing that you do in STARK protocols is you take quotients\nat specific points, either deliberately chosen or randomly chosen. For\nexample, if you want to prove that \\(P(x) =\ny\\), you do so by providing \\(Q =\n\\frac{P - y}{X - x}\\), and proving that \\(Q\\) is a polynomial (as opposed to a\nfractional value). Randomly choosing evaluation points is used in the DEEP-FRI protocol, which\nlets FRI be secure with fewer Merkle branches.\n\nHere, we get to one subtle challenge: in the circle group, there\nis no line function, analogous to \\(X -\nx\\) for regular FRI, that passes through only one point.\nThis is visible geometrically:\n\nYou could make a line function tangent to one point \\((P_x, P_y)\\), but that would pass through\nthe point \"with multiplicity 2\" - that is, for a polynomial to be a\nmultiple of that line function, it would have to fulfill a much stricter\ncondition than just being zero at that point. Hence, you can't prove an\nevaluation at only one point. So what do we do? Basically, we bite the\nbullet, and prove an evaluation at two points, adding a dummy\npoint whose evaluation we don't need to care about.\n\nA line function: \\(ax + by +\nc\\). If you turn it into an equation by forcing it to equal 0,\nthen you might recognize it as a line in what\nhigh school math calls \"standard form\".\n\nIf we have a polynomial \\(P\\) that\nequals \\(v_1\\) at \\(P_1\\), and \\(v_2\\) at \\(P_2\\), then we choose an\ninterpolant \\(I\\): a line\nfunction that equals \\(v_1\\) at \\(P_1\\), and \\(v_2\\) at \\(P_2\\). This can be as simple as \\(v_1 + (v_2 - v_1) * \\frac{y - y_1}{(P_2)_y -\n(P_1)_y}\\). We then prove that \\(P\\) equals \\(v_1\\) at \\(P_1\\), and \\(v_2\\) at \\(P_2\\) by subtracting \\(I\\) (so \\(P-I\\) equals zero at both points), dividing\nby \\(L\\) (the line function between\n\\(P_1\\) and \\(P_2\\)), and proving that the quotient \\(\\frac{P - I}{L}\\) is a polynomial.\n\n## Vanishing polynomials\n\nIn a STARK, the polynomial equation you're trying to prove often\nlooks like \\(C(P(x), P(next(x))) = Z(x) *\nH(x)\\), where \\(Z(x)\\) is a\npolynomial that equals zero across your entire original evaluation\ndomain. In \"regular\" STARKs, that function is just \\(x^n - 1\\). In circle STARKs, you the\nequivalent is:\n\n\\(Z_1(x,y) = y\\)\n\n\\(Z_2(x,y) = x\\)\n\n\\(Z_{n+1}(x,y) = (2 * Z_n(x,y)^2) -\n1\\)\n\nNotice that you can derive the vanishing polynomial from the folding\nfunction: in regular STARKs, you're repeating \\(x \\rightarrow x^2\\), here you're repeating\n\\(x \\rightarrow 2x^2-1\\), though you're\ndoing something different for the first round, because the first round\nis special.\n\n## Reverse bit order\n\nIn STARKs, evaluations of a polynomial are typically arranged not in\nthe \"natural\" order (\\(P(1)\\), \\(P(\\omega)\\), \\(P(\\omega^2)\\) ... \\(P(\\omega^{n-1})\\)), but rather what I call\n\"reverse bit order\":\n\n\\(P(1)\\), \\(P(\\omega^{\\frac{n}{2}})\\), \\(P(\\omega^{\\frac{n}{4}})\\), \\(P(\\omega^{\\frac{3n}{4}})\\), \\(P(\\omega^{\\frac{n}{8}})\\), \\(P(\\omega^{\\frac{5n}{8}})\\), \\(P(\\omega^{\\frac{3n}{8}})\\), \\(P(\\omega^{\\frac{7n}{8}})\\), \\(P(\\omega^{\\frac{n}{16}})\\)...\n\nIf we set \\(n = 16\\), and we focus\njust on which powers of \\(\\omega\\)\nwe're evaluating at, the list looks like this:\n\n\\(\\{0, 8, 4, 12, 2, 10, 6, 14, 1, 9, 5, 13,\n3, 11, 7, 15\\}\\)\n\nThis ordering has the key property that values which get grouped\ntogether early on in a FRI evaluation are put beside each other in the\nordering. For example, the first step of FRI groups together \\(x\\) and \\(-x\\). In the \\(n=16\\) case, \\(\\omega^8 = -1\\), so that means \\(P(\\omega^i)\\) and \\(P(-\\omega^i) = P(\\omega^{i+8})\\). And, as\nwe can see, those are exactly the pairs that are right beside each\nother. The second step of FRI groups together \\(P(\\omega^i)\\), \\(P(\\omega^{i+4})\\), \\(P(\\omega^{i+8})\\) and \\(P(\\omega^{i+12})\\). And, those are exactly\nthe groups of four that we see. And so forth. This makes FRI much more\nspace-efficient, because it lets you provide one Merkle proof for both\nof the values that get folded together (or, if you fold \\(k\\) rounds at a time, all \\(2^k\\) of the values) simultaneously.\n\nIn circle STARKs, the folding structure is a bit different: in the\nfirst step we group together \\((x, y)\\)\nwith \\((x, -y)\\), in the second step\n\\(x\\) with \\(-x\\), and in subsequent steps \\(p\\) with \\(q\\), selecting \\(p\\) and \\(q\\) such that \\(Q^i(p) = -Q^i(q)\\) where \\(Q^i\\) is the map \\(x \\rightarrow 2x^2-1\\) repeated \\(i\\) times. If we think of the points in\nterms of their position along the circle, at each step this looks like\nthe first point getting paired with the last, the second with the second\nlast, etc.\n\nTo adjust reverse bit order to reflect this folding structure, we\nreverse every bit except the last. We keep the last bit, and we\nalso use it to determine whether or not to flip the other bits.\n\nA size-16 folded reverse bit order looks as follows:\n\n\\(\\{0, 15, 8, 7, 4, 11, 12, 3, 2, 13, 10,\n5, 6, 9, 14, 1\\}\\)\n\nIf you look at the circle in the previous section, the 0th, 15th, 8th\nand 7th points (going counterclockwise, starting from the right) are of\nthe form \\((x, y)\\), \\((x, -y)\\), \\((-x,\n-y)\\) and \\((-x, y)\\), which is\nexactly what we need.\n\n## Efficiency\n\nCircle STARKs (and 31-bit-prime STARKs in general) are very\nefficient. A realistic computation that is being proven in a circle\nSTARK would most likely involve a few types of computation:\n\n- Native arithmetic, used for \"business logic\" such as counting\n\n- Native arithmetic, used for cryptography (eg. hash functions like Poseidon)\n\n- Lookup arguments, a\ngeneric way to do many kinds of computation efficiently by implementing\nthem via reading values from tables\n\nThe key measure of efficiency is: are you using the entire space in\nthe computational trace to do useful work, or are you leaving a lot of\nwasted space? In large-field SNARKs, there is a lot of wasted space:\nbusiness logic and lookup tables mostly involve computation over small\nnumbers (often the numbers are under N in an N-step computation, so\nunder \\(2^{25}\\) in practice), but you\nhave to pay the cost of using a size \\(2^{256}\\)-bit field anyway. Here, the field\nis size \\(2^{31}\\), so the wasted space\nis not large. \"Designed-for-SNARKs\" low-arithmetic-complexity hashes\n(eg. Poseidon) use every bit of each number in the trace in any\nfield.\n\nHence, circle STARKs actually get pretty close to optimal! Binius\nis even stronger, because it lets you mix-and-match fields of different\nsizes and thereby get even more efficient bit packing for everything.\nBinius also opens up options for doing 32-bit addition without incurring\nthe overhead of lookup tables. However, those gains at the cost of (in\nmy opinion) significantly higher theoretical complexity, whereas circle\nSTARKs (and even more so BabyBear-based regular STARKs) are conceptually\nquite simple.\nConclusion: what\ndo I think about circle STARKs?\n\nCircle STARKs don't impose too many extra complexities on\ndevelopers compared to regular STARKs. In the process of making an\nimplementation, the above three issues are essentially the only\ndifferences that I saw compared to regular FRI. The underlying math\nbehind what the \"polynomials\" that circle FRI is operating on is quite\ncounterintuitive, and takes a while to understand and appreciate. But it\njust so happens that this complexity is hidden away in such a way it's\nnot that visible to developers. The complexity of circle math\nis encapsulated,\nnot systemic.\n\nUnderstanding circle FRI and circle FFTs can also be a good\nintellectual gateway to understanding other \"exotic FFTs\": most notably\nbinary-field\nFFTs as used in Binius\nand in LibSTARK\nbefore, and also spookier constructions such as elliptic curve FFTs, which\nuse few-to-1 maps that work nicely with elliptic curve point\noperations.\n\nWith the combination of Mersenne31, BabyBear, and binary-field\ntechniques like Binius, it does feel like we are approaching the limits\nof efficiency of the \"base layer\" of STARKs. At this point, I am\nexpecting the frontiers of STARK optimization to move to making\nmaximally-efficient arithmetizations of primitives like hash functions\nand signatures (and optimizing those primitives themselves for that\npurpose), making recursive constructions to enable more parallelization,\narithmetizing VMs to improve developer experience, and other\nhigher-level tasks.",
    "contentLength": 25130,
    "summary": "Circle STARKs use extension fields and circular domains to efficiently prove computations over small fields like Mersenne31.",
    "detailedSummary": {
      "theme": "Vitalik explores Circle STARKs, a cryptographic proof system that enables highly efficient zero-knowledge proofs by working over small fields like Mersenne31 while maintaining security through novel mathematical constructions.",
      "summary": "Vitalik examines the evolution of STARK protocols from large 256-bit fields to smaller fields like Goldilocks, Mersenne31, and BabyBear, which has dramatically improved proving speeds - with Starkware achieving 620,000 Poseidon2 hashes per second on an M3 laptop. He explains how working with small fields creates security challenges since there are fewer possible values for random checks, requiring solutions like extension fields or multiple random checks. The core innovation of Circle STARKs is using the mathematical structure of points on a circle (where x\u00b2 + y\u00b2 = 1) to create efficient two-to-one mappings needed for the FRI protocol, particularly enabling the use of the highly efficient Mersenne31 field which otherwise couldn't support traditional FFT operations. Vitalik details the technical differences in Circle STARKs including modified quotienting procedures, different vanishing polynomials, and adjusted reverse bit ordering, while noting that these complexities are largely hidden from developers. He concludes that Circle STARKs represent a significant step toward optimal efficiency in zero-knowledge proofs, though future improvements will likely focus on higher-level optimizations rather than base-layer mathematical improvements.",
      "takeaways": [
        "The shift from large 256-bit fields to small fields like Mersenne31 and BabyBear has created massive efficiency gains in STARK proving, with arithmetic over smaller numbers being roughly 9x faster",
        "Circle STARKs solve the security challenges of small fields by using the mathematical structure of points on a circle (x\u00b2 + y\u00b2 = 1) to enable efficient FRI protocols over Mersenne31",
        "Extension fields provide an elegant solution to security concerns in small field STARKs by expanding the space of possible random values from ~2\u00b3\u00b9 to ~2\u00b9\u00b2\u2074 without significantly impacting efficiency",
        "The technical complexities of Circle STARKs (different quotienting, vanishing polynomials, and bit ordering) are largely encapsulated and hidden from developers, making them relatively easy to implement",
        "We are approaching the efficiency limits of base-layer STARK optimizations, with future improvements likely focusing on arithmetization of primitives, recursive constructions, and developer experience rather than mathematical foundations"
      ],
      "controversial": [
        "Vitalik's assertion that Circle STARKs are 'conceptually quite simple' compared to Binius may be debatable given the mathematical complexity he describes",
        "The claim that we are 'approaching the limits of efficiency' for base-layer STARKs could be premature given the rapid pace of cryptographic innovation"
      ]
    }
  },
  {
    "id": "general-2024-07-17-procrypto",
    "title": "Against choosing your political allegiances based on who is \"pro-crypto\"",
    "date": "2024-07-17",
    "category": "society",
    "url": "https://vitalik.eth.limo/general/2024/07/17/procrypto.html",
    "path": "general/2024/07/17/procrypto.html",
    "content": "- \n\n- \n\n  \n    \n\n  \n  \n  Dark Mode Toggle\n  \n    \n\n  \n\n##  Against choosing your political allegiances based on who is \"pro-crypto\" \n\n 2024 Jul 17 \nSee all posts\n\n \n \n\n Against choosing your political allegiances based on who is \"pro-crypto\" \n\nOver the last couple of years, \"crypto\" has become an increasingly\nimportant topic in political policy, with various jurisdictions\nconsidering bills that regulate various actors doing blockchain things\nin various ways. This includes the Markets\nin Crypto Assets regulation (MiCA) in the EU, efforts\nto regulate stablecoins in the UK, and the complicated mix of legislation\nand attempted regulation-by-enforcement from the SEC that we have seen\nin the United States. Many of these bills are, in my view, mostly\nreasonable, though there are fears that governments will attempt extreme\nsteps like treating\nalmost all coins as securities or banning\nself-hosted wallets. In the wake of these fears, there is a growing\npush within the crypto space to become more politically active, and\nfavor political parties and candidates almost entirely on whether or not\nthey are willing to be lenient and friendly to \"crypto\".\n\nIn this post, I argue against this trend, and in particular I argue\nthat making decisions in this way carries a high risk of going against\nthe values that brought you into the crypto space in the first\nplace.\n\nMe with Vladimir Putin in 2018. At the time, many in the Russian\ngovernment expressed willingness to become \"open to crypto\".\n\n\"Crypto\" is\nnot just cryptocurrency and blockchains\n\nWithin the crypto space there is often a tendency to over-focus on\nthe centrality of \"money\", and the freedom to hold and spend money (or,\nif you wish, \"tokens\") as the most important political issue. I agree\nthat there is an important battle to be fought here: in order to do\nanything significant in the modern world, you need money, and so if you\ncan shut down anyone's access to money, you can arbitrarily shut down\nyour political opposition. The right to spend money privately, a cause\nthat Zooko\ntirelessly advocates for, is similarly important. The ability to\nissue tokens can be a significant power-up to people's ability\nto make digital organizations that actually have collective economic\npower and do things. But a near-exclusive focus on\ncryptocurrency and blockchains is more difficult to defend, and\nimportantly it was not the ideology that originally created crypto in\nthe first place.\n\nWhat originally created crypto was the\ncypherpunk movement, a much broader techno-libertarian ethos which\nargued for free and open technology as a way of protecting and enhancing\nindividual freedoms generally. Back in the 2000s, the main theme was\nfighting off restrictive copyright legislation which was being pushed by\ncorporate lobbying organizations (eg. the RIAA\nand MPAA)\nthat the internet labelled as the \"MAFIAA\".\nA famous legal case that generated a lot of fury was Capitol\nRecords, Inc.\u00a0v. Thomas-Rasset, where the defendant was forced to\npay $222,000 in damages for illegally downloading 24 songs over a\nfile-sharing network. The main weapons in the fight were torrent\nnetworks, encryption and internet anonymization. A lesson learned very\nearly on the importance of decentralization. As explained in one of the\nvery few openly political statements made\nby Satoshi:\n\n[Lengthy exposition of vulnerability of a systm to use-of-force\nmonopolies ellided.]\n\nYou will not find a solution to political problems in\ncryptography.\n\nYes, but we can win a major battle in the arms race and gain a new\nterritory of freedom for several years.\n\nGovernments are good at cutting off the heads of a centrally\ncontrolled networks like Napster, but pure P2P networks like Gnutella\nand Tor seem to be holding their own.\n\nBitcoin was viewed as an extension of that spirit to the area of\ninternet payments. There was even an early equivalent of \"regen culture\": Bitcoin was an\nincredibly easy means of online payment, and so it could be used to\norganize ways to compensate artists for their work without relying on\nrestrictive copyright laws. I participated in this myself: when I was\nwriting articles for Bitcoin Weekly in 2011, I developed a mechanism\nwhere we would publish the first paragraph of two new articles that I\nwrote, and we would hold the\nremainder \"for ransom\", releasing the contents when the total\ndonations to a public address would reach some specified quantity of\nBTC.\n\nThe point of all this is to contextualize the mentality that created\nblockchains and cryptocurrency in the first place: freedom is\nimportant, decentralized networks are good at protecting freedom, and\nmoney is an important sphere where such networks can be applied - but\nit's one important sphere among several. And indeed, there are\nseveral further important spheres where decentralized networks\nare not needed at all: rather, you just need the right application of\ncryptography and one-to-one communication. The idea that freedom of\npayment specifically is the one that's central to all other freedoms is\nsomething that came later - a cynic might say, it's an ideology\nretroactively formed to justify \"number go up\".\n\nI can think of at least a few other technological freedoms that are\njust as \"foundational\" as the freedom to do things with crypto\ntokens:\n\n- Freedom and privacy of communication: this covers\nencrypted messaging, as well as\npseudonymity. Zero-knowledge\nproofs could protect pseudonymity at the same time as ensuring\nimportant claims about authenticity (eg. that a message is sent\nby a real human), and so supporting use cases of zero-knowledge proofs\nis also important here.\n\n- Freedom and privacy-friendly digital\nidentity: there are some blockchain applications\nhere, most notably in allowing revocations and various use cases of\n\"proving a negative\" in a decentralized way, but realistically hashes,\nsignatures and zero knowledge proofs get used ten times more.\n\n- Freedom and privacy of thought: this one is going\nto become more and more important in the next few decades, as more and\nmore of our activities become mediated by AI interactions in deeper and\ndeeper ways. Barring significant change, the default path is that more\nand more of our thoughts are going to be directly intermediated and read\nby servers held by centralized AI companies.\n\n- High-quality access to information: social\ntechnologies that help people form high-quality opinions about important\ntopics in an adversarial environment. I personally am bullish\non prediction markets and Community Notes; you may have a different\ntake on the solutions, but the point is that this topic is\nimportant.\n\nAnd the above list is just technology. The goals that\nmotivate people to build and participate in blockchain applications\noften have implications outside of technology as well: if you care about\nfreedom, you might want the government to respect your freedom to have\nthe kind of family you want. If you care about building more efficient\nand equitable economies, you might want to look at the implications\nof that in housing. And so on.\n\nMy underlying point is: if you're the type of person who's\nwilling to read this article past the first paragraph, you're not in\ncrypto just because it's crypto, you're in crypto because of deeper\nunderlying goals. Don't stand with crypto-as-in-cryptocurrency, stand\nwith those underlying goals, and the whole set of policy implications\nthat they imply.\n\nCurrent \"pro-crypto\" initiatives, at least as of today, do not think\nin this way:\n\n \n\nThe \"key bills\" that StandWithCrypto tracks.\nThere is no attempt made whatsoever to judge politicians on freedoms\nrelated to cryptography and technology that go beyond\ncryptocurrency.\n\nIf a politician is in favor of your freedom to trade coins, but they\nhave said nothing about the topics above, then the underlying thought\nprocess that causes them to support the freedom to trade coins is very\ndifferent from mine (and possibly yours). This in turn implies a high\nrisk that they will likely have different conclusions from you on issues\nthat you will care about in the future.\n\n## Crypto and internationalism\n\nEthereum node map, source ethernodes.org\n\nOne social and political cause that has always been dear to me, and\nto many cypherpunks, is internationalism.\nInternationalism has always been a key blind spot of statist egalitarian\npolitics: they enact all kinds of restrictive economic policies to try\nto \"protect workers\" domestically, but they often pay little or no\nattention to the fact that two thirds of global inequality is between\ncountries rather than within countries. A popular recent strategy to\ntry to protect domestic workers is tariffs; but even when tariffs\nsucceed at achieving that goal, unfortunately they often do so at the\nexpense of workers in other countries. A key liberatory aspect of the\ninternet is that, in theory, it makes no distinctions between the\nwealthiest nations and the poorest. Once we get to the point where most\npeople everywhere have a basic standard of internet access, we can have\na much more equal-access and globalized digital society. Cryptocurrency\nextends these ideals to the world of money and economic interaction.\nThis has the potential to significantly contribute to flattening the\nglobal economy, and I've personally seen many cases where it already\nhas.\n\nBut if I care about \"crypto\" because it's good for internationalism,\nthen I should also judge politicians by how much they and their policies\nshow a care for the outside world. I will not name specific examples,\nbut it should be clear that many of them fail on this metric.\n\nSometimes, this even ties back to the \"crypto industry\". While\nrecently attending EthCC, I received messages from multiple friends who\ntold me that they were not able to come because it has become much more\ndifficult for them to get a Schengen visa. Visa accessibility is a key\nconcern when deciding locations for events like Devcon; the USA also scores poorly on\nthis metric. The crypto industry is uniquely international, and\nso immigration law is crypto law. Which politicians, and which\ncountries, recognize this?\nCrypto-friendly\nnow does not mean crypto-friendly five years from now\n\nIf you see a politician being crypto-friendly, one thing you can do\nis look up their views on crypto itself five years ago. Similarly, look\nup their views on related topics such as encrypted messaging five years\nago. Particularly, try to find a topic where \"supporting freedom\" is\nunaligned with \"supporting corporations\"; the copyright wars of the\n2000s are a good example of this. This can be a good guide on what kinds\nof changes to their views might happen five years in the future.\nDivergence\nbetween decentralization and acceleration\n\nOne way in which a divergence might happen, is if the goals of\ndecentralization and acceleration\ndiverge. Last year, I made a series\nof polls essentially asking people which of those two they value\nmore in the context of AI. The results decidedly favored the former:\n\nOften, regulation is harmful to both decentralization and\nacceleration: it makes industries more concentrated and slows\nthem down. A lot of the most harmful crypto regulation (\"mandatory KYC\non everything\") definitely goes in that direction. However, there is\nalways the possibility that those goals will diverge. For AI, this is\narguably happening already. A decentralization-focused AI strategy\nfocuses on smaller models running on consumer hardware, avoiding a privacy\nand centralized-control dystopia where all AI relies on centralized\nservers that see all our our actions, and whose operators' biases shape\nthe AI's outputs in a way that we cannot escape. An advantage of a\nsmaller-models-focused strategy is that it is more AI-safety-friendly,\nbecause smaller models are inherently more bounded in capabilities and\nmore likely to be more like tools and less like independent agents. An\nacceleration-focused AI strategy, meanwhile, is enthusiastic about\neverything from the smallest micro-models running on tiny chips to the\n7-trillion-dollar\nclusters of Sam Altman's dreams.\n\nAs far as I can tell, within crypto we have not yet seen\nthat large a split along these lines, but it feels very\nplausible that some day we will. If you see a \"pro-crypto\" politician\ntoday, it's worth it to explore their underlying values, and see which\nside they will prioritize if a conflict does arise.\nWhat\n\"crypto-friendly\" means to authoritarians\n\nThere is a particular style of being \"crypto-friendly\" that is common\nto authoritarian governments, that is worth being wary of. The best\nexample of this is, predictably, modern Russia.\n\nThe recent Russian government policy regarding crypto is pretty\nsimple, and has two prongs:\n\n- When we use crypto, that helps us avoid other\npeople's restrictions, so that's good.\n\n- When you use crypto, that makes it harder for us\nto restrict or surveil you or put\nyou in jail for 9 years for donating $30 to Ukraine, so that's\nbad.\n\nHere are examples of Russian government actions of each type:\n\nAnother important conclusion of this is that if a politician is\npro-crypto today, but they are the type of person that is either very\npower-seeking themselves, or willing to suck up to someone who is, then\nthis is the direction that their crypto advocacy may look like ten years\nfrom now. If they, or the person they are sucking up to, actually do\nconsolidate power, it almost certainly will. Also, note that the\nstrategy of staying close to dangerous actors in order to \"help them\nbecome better\" backfires\nmore often than\nnot.\nBut\nI like [politician] because of their entire platform and outlook, not\njust because they're pro-crypto! So why should I not be enthusiastic\nabout their crypto stance?\n\nThe game of politics is much more complicated than just \"who wins the\nnext election\", and there are a lot of levers that your words and\nactions affect. In particular, by publicly giving the impression\nthat you support \"pro-crypto\" candidates just because they are\n\"pro-crypto\", you are helping to create an incentive gradient where\npoliticians come to understand that all they need to get your support is\nto support \"crypto\". It doesn't matter if they also support\nbanning encrypted messaging, if they are a power-seeking narcissist, or\nif they push for bills that make it even harder for your Chinese or\nIndian friend to attend the next crypto conference - all that\npoliticians have to do is make sure it's easy for you to trade\ncoins.\n\n\"Someone in a prison cell juggling gold coins\", locally-running\nStableDiffusion 3\n\nWhether you are someone with millions of dollars ready to donate, or\nsomeone with millions of Twitter followers ready to influence, or just a\nregular person, there are far more honorable incentive\ngradients that you could be helping to craft.\n\nIf a politician is pro-crypto, the key question to ask is:\nare they in it for the right reasons? Do they have a\nvision of how technology and politics and the economy should go in the\n21st century that aligns with yours? Do they have a good positive\nvision, that goes beyond near-term concerns like \"smash the bad other\ntribe\"? If they do, then great: you should support them, and make clear\nthat that's why you are supporting them. If not, then either stay out\nentirely, or find better forces to align with.",
    "contentLength": 15271,
    "summary": "Vitalik argues against single-issue \"pro-crypto\" voting, saying true crypto values include broader tech freedoms beyond just cryptocurrency.",
    "detailedSummary": {
      "theme": "Vitalik argues against single-issue voting based solely on politicians' cryptocurrency stances, advocating instead for evaluating candidates based on broader technological freedoms and underlying values.",
      "summary": "Vitalik contends that the crypto community's growing tendency to support politicians purely based on their pro-cryptocurrency positions is misguided and potentially dangerous. He traces cryptocurrency's origins to the broader cypherpunk movement, which championed multiple technological freedoms including encrypted communication, digital privacy, and decentralized networks - not just financial tokens. Vitalik warns that politicians who are pro-crypto today may not share the underlying values that originally motivated the cryptocurrency movement, and their support could be superficial or even authoritarian in nature, as demonstrated by countries like Russia that embrace crypto for avoiding others' restrictions while restricting their own citizens' usage. He argues that true crypto advocates should evaluate politicians based on their complete vision for technology, freedom, and society rather than their cryptocurrency stance alone, as this approach creates better incentive structures and aligns with the foundational values of technological liberty and internationalism that originally drove the movement.",
      "takeaways": [
        "Supporting politicians solely based on pro-crypto stances risks abandoning the broader values that originally motivated the cryptocurrency movement",
        "True crypto advocacy stems from cypherpunk ideals encompassing multiple technological freedoms beyond just cryptocurrency transactions",
        "Politicians' current crypto-friendly positions may not reflect genuine alignment with underlying principles of decentralization and individual liberty",
        "Authoritarian governments often adopt selective crypto-friendliness - supporting it for their own use while restricting citizens' access",
        "Single-issue crypto voting creates poor incentive structures that allow politicians to gain support without addressing broader technological freedoms"
      ],
      "controversial": [
        "Vitalik's implicit criticism of current major crypto political advocacy organizations and their narrow focus on cryptocurrency-specific legislation",
        "His suggestion that many prominent pro-crypto politicians may be motivated by opportunism rather than genuine principle",
        "The implication that supporting politicians primarily for their crypto stance could lead to alignment with authoritarian tendencies"
      ]
    }
  },
  {
    "id": "general-2024-06-30-epochslot",
    "title": "Epochs and slots all the way down: ways to give Ethereum users faster transaction confirmation times",
    "date": "2024-06-30",
    "category": "protocol",
    "url": "https://vitalik.eth.limo/general/2024/06/30/epochslot.html",
    "path": "general/2024/06/30/epochslot.html",
    "content": "- \n\n- \n\n  \n    \n\n  \n  \n  Dark Mode Toggle\n  \n    \n\n  \n\n##  Epochs and slots all the way down: ways to give Ethereum users faster transaction confirmation times \n\n 2024 Jun 30 \nSee all posts\n\n \n \n\n Epochs and slots all the way down: ways to give Ethereum users faster transaction confirmation times \n\nOne of the important properties of a good blockchain user experience\nis fast transaction confirmation times. Today, Ethereum has already\nimproved a lot compared to five years ago. Thanks to the combination of\nEIP-1559 and\nsteady block times after the Merge,\ntransactions sent by users on L1 reliably confirm within 5-20 seconds.\nThis is roughly competitive with the experience of paying with a credit\ncard. However, there is value in improving user experience further, and\nthere are some applications that outright require latencies on the order\nof hundreds of milliseconds or even less. This post will go over some of\nthe practical options that Ethereum has.\nOverview of existing\nideas and techniques\nSingle slot\nfinality\n\nToday, Ethereum's Gasper\nconsensus uses a slot and epoch\narchitecture. Every 12-second slot, a subset of validators publish a\nvote on the head of the chain, and over the course of 32 slots (6.4\nmin), all validators get a chance to vote once. These votes are then\nre-interpreted as being messages in a vaguely PBFT-like\nconsensus algorithm, which after two epochs (12.8 min) gives a very hard\neconomic assurance called finality.\n\nOver the last couple of years, we've become more and more\nuncomfortable with the current approach. The key reasons are that (i)\nit's complicated and there are many interaction bugs between the\nslot-by-slot voting mechanism and the epoch-by-epoch finality mechanism,\nand (ii) 12.8 minutes is way too long and nobody cares to wait that\nlong.\n\nSingle-slot finality replaces this architecture by a mechanism much\nmore similar to Tendermint\nconsensus, in which block N is finalized before block N+1 is made.\nThe main deviation from Tendermint is that we keep the \"inactivity\nleak\" mechanism, which allows the chain to keep going and recover if\nmore than 1/3 of validators go offline.\n\nA diagram of the leading proposed single-slot\nfinality design_\n\nThe main challenge with SSF is that naively, it seems to imply that\nevery single Ethereum staker would need to publish two messages every 12\nseconds, which would be a lot of load for the chain to handle.\nThere are clever\nideas for how to mitigate this, including the very recent Orbit\nSSF proposal. But even still, while this improves UX significantly\nby making \"finality\" come faster, it doesn't change the fact that users\nneed to wait 5-20 seconds.\n\n## Rollup preconfirmations\n\nFor the last few years, Ethereum has been following a rollup-centric\nroadmap, designing the Ethereum base layer (the\n\"L1\") around supporting data\navailability and other functionalities that can then be used by\nlayer 2 protocols like rollups\n(but also validiums\nand plasmas)\nthat can give users the same level of security as Ethereum, but at much\nhigher scale.\n\nThis creates a separation-of-concerns\nwithin the Ethereum ecosystem: the Ethereum L1 can focus on being\ncensorship resistant, dependable, stable, and maintaining and improving\na certain base-level core of functionality, and L2s can focus on more\ndirectly reaching out to users - both through different cultural\nand technological tradeoffs. But if you go down this path, one\ninevitable issue comes up: L2s want to serve users who want\nconfirmations much faster than 5-20 seconds.\n\nSo far, at least in the rhetoric, it has been L2s' responsibility to\ncreate their own \"decentralized sequencing\" networks. A smaller group of\nvalidators would sign off on blocks, perhaps once every few hundred\nmilliseconds, and they would put their \"stake\" behind those blocks.\nEventually, headers of these L2 blocks get published to L1.\n\nL2 validator sets could cheat: they could first sign block B1, and\nthen later sign a conflicting block B2 and commit it onto the chain\nbefore B1. But if they do this, they would get caught and lose their\ndeposits. In practice, we have seen centralized versions of this, but\nrollups have been slow to develop decentralized sequencing networks.\nAnd you can argue that demanding L2s all do decentralized\nsequencing is an unfair deal: we're asking rollups to basically do most\nof the same work as creating an entire new L1. For this reason\nand others, Justin Drake has been promoting a way to give all L2s (as\nwell as L1) access to a shared Ethereum-wide preconfirmation mechanism:\nbased\npreconfirmations.\n\n## Based preconfirmations\n\nThe based preconfirmation approach assumes that Ethereum proposers\nwill become highly sophisticated actors for MEV-related reasons (see here\nfor my explanation of MEV, and see also the execution\ntickets line of proposals). The based preconfirmation approach takes\nadvantage of this sophistication by incentivizing these sophisticated\nproposers to accept the responsibility of offering\npreconfirmations-as-a-service.\n\nThe basic idea is to create a standardized protocol by which a user\ncan offer an additional fee in exchange for an immediate guarantee that\nthe transaction will be included in the next block, along with possibly\na claim about the results of executing that transaction. If the proposer\nviolates any promise that they make to any user, they can get\nslashed.\n\nAs described, based preconfirmations provide guarantees to L1\ntransactions. If rollups are \"based\",\nthen all L2 blocks are L1 transactions, and so the same\nmechanism can be used to provide preconfirmations for any L2.\nWhat are we actually\nlooking at here?\n\nSuppose that we implement single slot finality. We use Orbit-like\ntechniques to reduce the number of validators signing per slot, but not\ntoo much, so that we can also make progress on the key goal of\nreducing the 32 ETH staking minimum. As a result, perhaps the slot time\ncreeps upward, to 16 sec.\u00a0We then use either rollup preconfirmations, or\nbased preconfirmations, to give users faster assurances. What do we have\nnow? An epoch-and-slot architecture.\n\nThe \"they're the same picture\" meme is getting quite\noverused at this point, so I'll just put an old diagram I drew years ago\nto describe Gasper's slot-and-epoch architecture and a diagram of L2\npreconfirmations beside each other, and hopefully that will get the\npoint across.\n\nThere is a deep philosophical reason why epoch-and-slot\narchitectures seem to be so hard to avoid: it inherently takes less time\nto come to approximate agreement on something, than to come to\nmaximally-hardened \"economic finality\" agreement on it.\n\nOne simple reason why is number of nodes. While the old linear decentralization\n/ finality time / overhead tradeoff is looking milder now due to\nhyper-optimized BLS aggregation and in the near future ZK-STARKs, it's\nstill fundamentally true that:\n\n- \"Approximate agreement\" only requires a few nodes while economic\nfinality requires a significant fraction of all nodes.\n\n- Once the number of nodes goes above a certain size, you need to\nspend more time to gather signatures.\n\nIn Ethereum today, a 12-second slot is divided into three sub-slots,\nfor (i) block publication and distribution, (ii) attestation, and (iii)\nattestation aggregation. If the attester count was much lower, we could\ndrop to two sub-slots and have an 8-second slot time. Another, and\nrealistically bigger, factor, is \"quality\" of nodes. If we could also\nrely on a professionalized subset of nodes to do approximate agreements\n(and still use the full validators set for finality), we could plausibly\ndrop that to ~2 seconds.\n\nHence, it feels to me that (i) slot-and-epoch architectures\nare obviously correct, but also (ii) not all slot-and-epoch\narchitectures are created equal, and there's value in more fully\nexploring the design space. In particular, it's worth exploring\noptions that are not tightly interwoven like Gasper, and where\ninstead there's stronger separation of concerns between the two\nmechanisms.\n\n## What should L2s do?\n\nIn my view, there are three reasonable strategies for L2s to take at\nthe moment:\n\n- Be \"based\", both technologically and spiritually.\nThat is, they optimize for being pass-through conduits for the Ethereum\nbase layer's technical properties and its values (high decentralization,\ncensorship resistance, etc). In their simplest form, you could think of\nthese rollups as being \"branded shards\", but they can also be much more\nambitious than that, and experiment quite heavily with new virtual\nmachine designs and other technical improvements.\n\n- Proudly be a \"server with blockchain scaffolding\", and make\nthe best out of it. If you start from a server, and then add\n(i) STARK validity proofs to ensure that the server is following the\nrules, (ii) guaranteed rights for the user to exit or force\ntransactions, and possibly (iii) freedom of collective choice, either\nthrough coordinated mass-exit or through the ability to vote to change\nthe sequencer, then you've already gained a lot of the benefits\nof being onchain, while keeping most of the efficiencies of a\nserver.\n\n- The compromise approach: a hundred-node fast chain, with\nEthereum providing extra interoperability and security. This is\nthe de-facto current roadmap of many L2 projects.\n\nFor some applications, (eg. ENS, keystores),\nsome payments), a 12-second block time is enough. For those applications\nthat are not, the only solution is a slot-and-epoch architecture. In all\nthree cases, the \"epochs\" are Ethereum's SSF (perhaps we can retcon that\nacronym into meaning something other than \"single slot\", eg. it could be\n\"Secure Speedy Finality\"). But the \"slots\" are something different in\neach of the above three cases:\n\n- An Ethereum-native slot-and-epoch architecture\n\n- Server preconfirmations\n\n- Committee preconfirmations\n\nA key question is, how good can we make something in category (1)? In\nparticular, if it gets really good, then it feels like category\n(3) ceases to have as much meaning. Category (2) will always exist, at\nthe very least because anything \"based\" doesn't work for off-chain-data\nL2s such as plasmas and validiums. But if an Ethereum-native\nslot-and-epoch architecture can get down to 1-second \"slot\" (ie.\npre-confirmation) times, then the space for category (3) becomes quite a\nbit smaller.\n\nToday, we're far from having final answers to these questions. A key\nquestion - just how sophisticated are block proposers going to become -\nremains an area where there is quite a bit of uncertainty. Designs like\nOrbit\nSSF are very recent, suggesting that the design space of\nslot-and-epoch designs where something like Orbit SSF is the epoch is\nstill quite under-explored. The more options we have, the better we can\ndo for users both on L1 and on L2s, and the more we can simplify the job\nof L2 developers.",
    "contentLength": 10799,
    "summary": "Ethereum is exploring faster transaction confirmations through single-slot finality and preconfirmations, but these create slot-and-epoch architectures anyway.",
    "detailedSummary": {
      "theme": "Vitalik explores various technical approaches to reduce Ethereum transaction confirmation times through slot-and-epoch architectures, single slot finality, and Layer 2 preconfirmation mechanisms.",
      "summary": "Vitalik argues that while Ethereum has improved significantly with 5-20 second confirmation times, faster confirmations are needed for better user experience and certain applications requiring sub-second latencies. He examines several solutions including Single Slot Finality (SSF) to replace the current complex Gasper consensus mechanism, rollup preconfirmations where Layer 2s create their own validator networks, and based preconfirmations where sophisticated Ethereum proposers offer preconfirmation services. Vitalik observes that all these approaches ultimately result in slot-and-epoch architectures, which he argues are inevitable because approximate agreement naturally takes less time than achieving full economic finality. He concludes that the design space is still under-explored and presents three strategic paths for Layer 2s: being 'based' and aligned with Ethereum's values, embracing a 'server with blockchain scaffolding' approach, or pursuing a compromise with hundred-node fast chains backed by Ethereum security.",
      "takeaways": [
        "Current Ethereum confirmation times of 5-20 seconds are competitive with credit cards but need improvement for advanced applications requiring sub-second latencies",
        "Single Slot Finality could simplify the complex Gasper consensus and reduce finality time from 12.8 minutes, but still wouldn't solve the 5-20 second user wait time",
        "All proposed solutions (SSF, rollup preconfirmations, based preconfirmations) naturally converge on slot-and-epoch architectures because approximate agreement is inherently faster than economic finality",
        "Layer 2 solutions have three viable strategies: being 'based' and Ethereum-aligned, embracing centralized efficiency with blockchain guarantees, or building hundred-node compromise chains",
        "The sophistication level of Ethereum block proposers and the potential for 1-second preconfirmation times could significantly impact the viability of different Layer 2 approaches"
      ],
      "controversial": [
        "The suggestion that some Layer 2s should 'proudly be a server with blockchain scaffolding' may be contentious among decentralization advocates who prefer fully decentralized solutions",
        "The reliance on increasingly sophisticated MEV-focused block proposers for based preconfirmations could centralize power among a small group of technically advanced actors"
      ]
    }
  },
  {
    "id": "general-2024-05-31-blocksize",
    "title": "Some reflections on the Bitcoin block size war",
    "date": "2024-05-31",
    "category": "general",
    "url": "https://vitalik.eth.limo/general/2024/05/31/blocksize.html",
    "path": "general/2024/05/31/blocksize.html",
    "content": "- \n\n- \n\n  \n    \n\n  \n  \n  Dark Mode Toggle\n  \n    \n\n  \n\n##  Some reflections on the Bitcoin block size war \n\n 2024 May 31 \nSee all posts\n\n \n \n\n Some reflections on the Bitcoin block size war \n\nRecently I finished reading (or rather, audio-listening) the two main\nhistory books covering the great Bitcoin block size war of the 2010s,\nfrom opposite ends of the spectrum:\n\n- Jonathan Bier's \"The\nBlocksize War\", telling the story from a pro-small-block\nperspective\n\n- Roger Ver and Steve Patterson's \"Hijacking\nBitcoin\", telling the story from a pro-big-block\nperspective\n\n \n\nIt was fascinating to read these two histories of an event that I\npersonally lived through, and even to some extent participated in. While\nI was aware of the majority of the events that transpired, and the\nnarratives with which each of the two sides viewed the nature of the\nconflict, there were still some fascinating bits that I did not know or\nhad completely forgotten, and it was interesting to look at the\nsituation with fresh eyes. Back then, I was a \"big blocker\", though of a\npragmatic medium-block variety that opposed extreme increases or\nabsolutist claims that fees should never be allowed to rise\nsignificantly above zero. Do I still support the views I held at the\ntime? I was looking forward to see and find out.\nIn\nJonathan Bier's telling, how did the small blockers see the block size\nwar?\n\nThe original debate in the block size war centered around a simple\nquestion: should Bitcoin have a hard fork to raise the block size limit\nfrom its then-current value of 1 MB to a higher value, allowing Bitcoin\nto process more transactions and hence have lower fees, but at the cost\nof making the chain more difficult and costly to run a node for and\nverify?\n\n\"[If the block size were much bigger], you would need a big\ndata center to run a node, and you won't be able to do it anonymously\" -\na key argument made in a video\nsponsored by Peter Todd advocating for keeping the block size\nsmall.\n\nBier's book gives me the impression that while the small blockers did\ncare about this object-level question, favoring a conservative approach\nof increasing the block size only a little bit to ensure it remains easy\nto run a node, they cared even more about the meta-level question of how\nprotocol-level issues like this get decided more generally. In their\nview, changes to the protocol (especially \"hard\nforks\") should only be done very rarely, and with a very high level\nfrom consensus of the users of the protocol.\n\nBitcoin is not trying to compete with payment processors - there are\nlots of those already. Rather, Bitcoin is trying to be something much\nmore unique and special: a completely new type of currency, free\nfrom the control of central organizations and central banks. If\nBitcoin started to have a highly active governance structure (which\nwould be required to navigate controversial adjustments to block size\nparameters), or became vulnerable to coordinated manipulation by miners,\nexchanges or other big companies, it would lose this precious unique\nadvantage forever.\n\nIn Bier's telling, the big blockers offended the small blockers most\nstrongly precisely because they often attempted to get a relatively\nsmall number of big players together to legitimize and push through\ntheir preferred changes - anathema to the small blockers' view on how\ngovernance should be done.\n\nThe New York Agreement, signed by major\nBitcoin exchanges, payment processors, miners and other companies in\n2017. A key example of what small blockers consider to be an attempt to\nshift Bitcoin from rule by users to rule by corporate\ncabal.\n\nIn\nRoger Ver's telling, how did the big blockers see the block size\nwar?\n\nBig blockers generally focus on a key underlying object-level\nquestion: what is Bitcoin supposed to be? Is it supposed to be\na store of value - digital gold, or a means of payment - digital cash?\nTo them, it was clear to everyone from the start that the original\nvision, and the vision that big blockers all signed up for, was digital\ncash. It even says so in the\nwhitepaper!\n\nThe big blockers also frequently cite two other things written by\nSatoshi:\n\n- The simplified payment verification section of the\nwhitepaper, which talks about how once blocks get very big, individual\nusers can verify that their payments were included using Merkle proofs,\nwithout needing to verify the whole chain.\n\n- A Bitcointalk\nquote advocating hard forks to phase in block size\nincreases:\n\nTo them, the switch from focus on digital cash to digital gold was a\npivot, one that was agreed upon by a small tight-knit group of core\ndevelopers, who then assumed that because they thought about the issue\nand came to their conclusion internally, they had the right to impose\ntheir view on the entire project.\n\nSmall blockers did offer solutions for how Bitcoin can be\ncash and gold at the same time - namely, Bitcoin becomes a \"layer 1\"\nthat focuses on being gold while \"layer 2\" protocols built on top of\nBitcoin, like the Lightning\nNetwork, provide cheap payments without using the blockchain for\nevery transaction. However, these solutions were highly inadequate in\npractice, and Ver spends several chapters deeply criticizing them. For\nexample, even if everyone switched to Lightning, a block size increase\nwould still eventually be required to onboard\nhundreds of millions of users. In addition, trustlessly\nreceiving coins in Lightning requires having an online node,\nand making sure that your coins are not stolen requires checking the\nchain once a week. These complexities, Ver thought, would inevitably\npush users toward interacting with Lightning in centralized ways.\nWhat is the key\ndifference in their views?\n\nVer's depiction of the object-level debate matches that of the small\nblockers: both sides agree that small blockers more highly value ease of\nrunning a node, and that large blockers more highly value cheap\ntransaction fees. They both acknowledge that there is room for a\nreasonable difference in beliefs, and that this difference is a key\nfactor that motivated the debate.\n\nBut Bier and Ver's depictions of most of the deeper\nunderlying issues are extremely different. To Bier, the\nsmall-block side is standing on behalf of users against small but\npowerful cabals of miners and exchanges trying to wrest control of the\nchain for their own benefit. Small blocks keep Bitcoin decentralized by\nmaking sure regular users can run nodes and verify the chain. To Ver,\nthe big-block side is standing on behalf of users against small but\npowerful cabals of self-appointed high priests and VC-funded companies\n(namely, Blockstream) who profit from building the layer 2 solutions\nthat the small-block roadmap necessitates. Large blocks keep Bitcoin\ndecentralized by making sure users can continue to afford on-chain\ntransactions without needing to depend on centralized layer-2\ninfrastructure.\n\nThe closest that I can see to the two sides even \"agreeing on the\nterms of the debate\" is that Bier's book accepts that many big blockers\nare well intentioned, even acknowledging that they have valid grievances\nregarding pro-small-block\nforum moderators censoring opposing views, but frequently criticizes\nthe big block side for being incompetent, while Ver's book is more\nwilling to ascribe malicious intent and even conspiracy theories to\nsmall blockers, but rarely criticizes their competence. This echoes a common\npolitical trope\nI have heard on quite a few occasions, that \"the right thinks that the\nleft is naive, the left thinks that the right is evil\".\nIn\nmy telling, how did I see the block size war? And how do I see it\ntoday?\n\nRoom 77, a former restaurant in Berlin that accepted\nBitcoin for payments. It was the centerpiece of the Bitcoin Kiez, a region in\nBerlin where a large number of restaurants accepted Bitcoin.\nUnfortunately, the dream of Bitcoin for payments faded away over the\nlatter part of the decade, and I consider rising fees to be a key\nculprit.\n\nWhile I was experiencing the Bitcoin block size war first hand, I\ngenerally sided with the big blockers. My sympathy for the big block\nside centered on a few key points:\n\n- A key initial promise of Bitcoin was digital cash, and high\nfees could kill that use case. While layer 2 protocols could in\ntheory provide much lower fees, the whole concept was highly untested,\nand it was highly irresponsible for the small blockers to commit to the\nsmall-block roadmap given how little they knew about how well the\nLightning Network would work in practice. These days, practical\nexperience with the Lightning Network has made pessimistic\nperspectives much\nmore common.\n\n- I was not convinced by the small block side's \"meta-level\"\nstory. Small blockers would often argue that \"Bitcoin should be\ncontrolled by users\", and \"users don't support big blocks\", but were\nnever willing to pin down any specific way of defining who \"users\" are\nor measuring what they wanted. Big blockers implicitly tried to propose\nat least three different ways of counting users: hashpower, public\nstatements by well-known companies, and social media discourse, and\nsmall blockers denounced each one. Big blockers did not organize the New\nYork Agreement because they liked \"cabals\"; they organized the New York\nAgreement because small blockers insisted on \"consensus\" between \"users\"\nfor any contentious change, and signed statements from major\nstakeholders were the only practical approach that the big blockers saw\nto try to actually do that.\n\n- Segregated Witness, the proposal adopted by the small block\nside to slightly increase block size, was needlessly overcomplicated,\ncompared to a simple hard fork block size increase. The small\nblock side ended up adopting a religion of \"soft forks good, hard forks\nbad\" (which I strongly\ndisagreed with), and designed their approach to increasing the block\nsize to fit this rule, despite what Bier acknowledges are serious\nincreases in complexity, to the point where many big blockers were\nunable to understand the proposal. I felt that small blockers were not\njust being \"pro-carefulness\", they were arbitrarily picking between\ndifferent types of carefulness, picking one (no hard forks) at the\nexpense of another (keeping a clean and simple code and spec) because it\nfit their agenda. Eventually, big blockers also ended up throwing away\n\"clean and simple\" and going into ideas like Bitcoin\nUnlimited's adaptive block size increase, a decision that Bier\n(rightfully) rips into them for.\n\n- The small block side really was engaging in very uncool acts\nof social\nmedia censorship to impose their views, culminating in\nTheymos's infamous line \"If 90% of /r/Bitcoin users find these policies\nto be intolerable, then I want these 90% of /r/Bitcoin users to\nleave.\"\n\nEven relatively mild pro-big-block posts would frequently\nget deleted. Custom CSS was used to make\nthese deleted posts invisible.\n\nVer's book focuses heavily on the first and the fourth and somewhat\non the third, as well as theories of financially motivated malfeasance -\nnamely, that the small blockers formed a company called Blockstream that\nwould build layer 2 protocols on top of Bitcoin, and simultaneously\nadvocated an ideology that the Bitcoin layer 1 should stay crippled,\nthereby making these commercial layer 2 networks necessary. Ver does not\nfocus much on the philosophy of how Bitcoin should be governed, because\nto him the answer that \"Bitcoin is governed by miners\" is satisfying.\nThis is a place where I agreed with neither side: I found the nebulous\n\"consensus of users that we refuse to actually define\" and the extreme\n\"miners should control everything because they have aligned incentives\"\nboth unreasonable.\n\nAt the same time, I remember being immensely frustrated with big\nblockers on a few key points, and they were points that Bier's book\nechoed. One of the worst (both according to me and according to Bier)\nwas that big blockers were never willing to agree on any\nrealistic limiting principle for how big blocks should go. A\ncommon viewpoint was that \"the block size was set by the market\" -\nmeaning, miners should make blocks as big as they want, and other miners\ncan choose to accept or reject those blocks. I argued\nagainst this heavily, and pointed out that it's an extreme stretch\nof the concept of a \"market\" to say that such a mechanism is a market.\nEventually, when the big blockers split off into their own independent\nchain (Bitcoin Cash), they ended up moving away from this view, and\nadding a block\nsize limit of 32 MB.\n\nAt the time, I actually did have a principled way of\nreasoning about how to decide on block size limits. Quoting a\npost from 2018:\n\nBitcoin favors maximum predictability of the cost of reading the\nblockchain, at the expense of the minimum possible amount of\npredictability of the cost of writing to the blockchain, with\npredictably very healthy results in the former metric and disastrous\nresults in the latter metric. Ethereum, with its current governance\nmodel, favors medium predictability of both.\n\nI later repeated this perspective in a tweet in\n2022. In essence, the philosophy is: we should balance\nbetween increasing the cost of writing to the chain (ie. transaction\nfees) and the cost of reading the chain (ie. software requirements for a\nnode). Ideally, if demand for using a blockchain increases by\n100x, we should split the pain halfway, and let the block size increase\nby 10x and the fees increase by 10x (demand elasticity for transaction\nfees is\nclose enough to 1 that this kind-of works in practice).\n\nEthereum actually did end up taking a medium-block\napproach: the chain's capacity has increased by roughly 5.3x since it\nlaunched in 2015 (perhaps 7x if you count calldata repricings and\nblobs), and at the same time fees increased from almost nothing to a\nsignificant-but-not-too-high level.\n\nHowever, this kind of compromise-oriented (or \"concave\")\napproach never caught on with either faction; perhaps it felt too\n\"central-planny\" for one side and too \"wishy-washy\" for the other. I\nfeel like big blockers were more at fault than small blockers here; the\nsmall blockers were open to modest block size increases at the\nbeginning (eg. Adam\nBack's 2/4/8 plan), and it was big blockers that were unwilling to\ncompromise, and moved quickly from advocating a single increase to a\nspecific larger number to advocating an overarching philosophy that\npretty much any nontrivial limitation of the block size is\nillegitimate.\n\nBig blockers also started arguing that miners should be in charge of\nBitcoin - a philosophy that Bier effectively criticizes, by pointing out\nthat they would probably quickly abandon their views if the miners tried\nchanging the protocol rules to do something other than increasing the\nblock size - say, giving themselves more rewards.\n\nA major line of criticism of big blockers in Bier's book was\ntheir repeated displays of incompetence. Bitcoin Classic was\nnot well-written code, Bitcoin Unlimited was needlessly overly\ncomplicated, for a long time they did not include wipeout\nprotection and did not seem to understand that this choice greatly\nhurts their chance of success (!!), and they had serious\nsecurity vulnerabilities. They loudly shouted about the need for\nmultiple implementations of the Bitcoin software - a principle that I\nagree with and a principle that Ethereum\nhas adopted - but their \"alternative clients\" were really just forks\nof Bitcoin Core with a few lines changed to implement a block size\nincrease. In Bier's telling, their repeated faux pas on both code and\neconomics ended up turning away more and more supporters over time.\nMajor big blockers falling for Craig Wright fraudulently claiming\nto be Satoshi further discredited them.\n\nCraig Wright, a scammer pretending to be Satoshi. He\nfrequently used legal threats to take down criticism, which is why my fork is the\nlargest still-online copy of the Cult of Craig repository which\ndocumented proof that he's fake. Unfortunately, many big blockers fell\nfor Craig's antics, because Craig toed the big-block party line, and\nsaid things that the big blockers wanted to hear.\n\nIn general, reading through the two books, I found myself agreeing\nwith Ver more often on big-picture questions, but with Bier more often\non individual details. In my view, big blockers were right on the\ncentral question that blocks needed to be bigger, and that it was best\nto accomplish this with a clean simple hard fork like Satoshi described,\nbut small blockers committed far fewer embarrassing technical faux pas,\nand had fewer positions that led to absurd outcomes if you tried to take\nthem to their logical conclusion.\nThe block\nsize war as a one-sided competence trap\n\nThe combined picture that I get from reading these two books is a\npolitical tragedy that I feel like I have seen over and over again in\nall kinds of contexts, including cryptocurrencies, corporations and\nnational politics:\n\nOne side monopolizes all the competent people, but uses its\npower to push a narrow and biased perspective; the other side correctly\nrecognizes that something is wrong, but engulfs itself in a focus on\nopposition, failing to develop the technical ability to execute on its\nown.\n\nIn many such situations, the first group is criticized as being\nauthoritarian, but when you ask its (often quite many) supporters why\nthey support it, their response is that the other side only knows how to\ncomplain; they would completely flop within days if they were to\nactually gain power.\n\nThis is to some extent not the opposition's fault: it's hard to\nbecome good at executing without having a platform to execute from and\ngain experience. But in the block size debate in particular, the\nbig block side appears to have largely not even realized the need to be\ncompetent at executing - they thought that they could win\nsolely by sheer force of being right on the block size question. Big\nblockers ended up paying a heavy price for their focus on opposing over\nbuilding in multiple ways: even when they split off into their own chain\nwith Bitcoin Cash, they ended up splitting in half two\nmore times\nbefore the community finally stabilized.\n\nI'll label this problem the one-sided competence\ntrap. It feels like a fundamental problem for anyone trying to\nbuild a political entity or project or community that they wish to be\ndemocratic or pluralistic. Smart people want to work with other smart\npeople. If two distinct groups are roughly equally matched, people would\ngravitate to the one that better fits their values, and the equilibrium\ncan be stable. But if it goes too far in one direction, it flips into a\ndifferent equilibrium, and it seems very hard to get it to flip back. To\nsome degree, one-sided competence traps can be mitigated by an\nopposition simply realizing that the problem exists and that they have\nto build up competence intentionally. Often, an opposition movement does\nnot even get to this step. But sometimes simply recognizing the problem\nis not enough. We would benefit a lot from having more robust and deeper\nways of preventing and getting out of one-sided competence traps.\nLess conflict, more\ntechnology\n\nOne incredibly glaring omission from both books stood out to me more\nthan anything else: the word \"ZK-SNARK\"\nappeared exactly zero times in both books. There is not much\nexcuse for this: even by the mid-2010s, ZK-SNARKs and their potential to\nrevolutionize scalability (and privacy) were well known. Zcash launched in October\n2016. The scalability implications of ZK-SNARKs were explored a little\nbit by Gregory Maxwell\nin 2013, but they did not seem to get taken into account at all in\ndiscussions of Bitcoin's future roadmap.\n\nThe ultimate diffuser of political tension is not\ncompromise, but rather new technology: the discovery\nof fundamentally new approaches that give both sides more of what they\nwant at the same time. We've seen several instances of this in Ethereum.\nA few that come to mind are:\n\n- Justin\nDrake's push to embrace BLS aggregation, allowing Ethereum's proof\nof stake to process many more validators, and thereby decrease the min\nstaking balance from 1500 to 32, with quite little downside. More\nrecently, work\non signature merging promises to take this further.\n\n- EIP-7702,\nwhich accomplished the goals of ERC-3074 in a way that is significantly\nmore forward-compatible with smart contract wallets, and thereby helped\nto tone down a long-standing dispute.\n\n- Multidimensional\ngas, starting with its implementation for blobs, has already helped\nincrease Ethereum's capability to hold rollup data, without increasing\nthe size of worst-case blocks, and thereby keeping security risks\nminimized.\n\nWhen an ecosystem stops embracing new technology, it inevitably\nstagnates, and becomes more contentious at the same time: a political\ndebate between \"I get 10 more apples\" vs \"you get 10 more apples\"\ninherently causes much less strife than a debate between \"I give up 10\napples\" vs \"you give up 10 apples\". Losses are more painful than gains\nare pleasant, and people are more willing to \"break the glass\" of their\nshared political commons in order to avoid losses. This is a key reason\nwhy I'm quite uncomfortable with ideas like degrowth and the\nnotion that \"we can't use technological solutions to solve social\nproblems\": there's a pretty strong reason to believe that fighting over\nwho wins more rather than fighting over who loses less really is much\nbetter for social harmony.\n\nIn economic theory, there is no difference between these\ntwo prisoner's dilemmas: the game on the right can be viewed as being\nthe game on the left plus a separate (irrelevant) step where both\nplayers lose four points regardless of how they act. But in human\npsychology, the two games can be very different.\n\nOne key question for Bitcoin going forward is, will Bitcoin be able\nto become a tech-forward ecosystem. The development of Inscriptions\nand later BitVM have created new\npossibilities for layer 2s, improving on what can be done with\nLightning. Hopefully, Udi Wertheimer is correct with his theory that ETH\ngetting an ETF means the\ndeath of Saylorism, and a renewed realization that Bitcoin needs to\nimprove technologically.\n\n## Why do I care about this?\n\nI care about examining the successes and failures in Bitcoin not\nbecause I want to put Bitcoin down as a way to lift Ethereum up; in\nfact, as someone who loves trying to understand social and political\nissues, I find it a feature of Bitcoin that it's sociologically\ncomplex enough that it can have internal debates and splits that are so\nrich and fascinating that you can write two entire books on them.\nRather, I care about analyzing these issues because both Ethereum, and\nother digital (and even physical) communities that I care about, stand\nto learn a lot from understanding what happened, what went well, and\nwhat could have done better.\n\nEthereum's focus on client\ndiversity was born from watching the failures that arose from\nBitcoin having a single client team. Its version\nof layer\n2s was born from understanding how the limitations of Bitcoin lead\nto limitations in what kinds of layer 2s with what trust properties are\npossible\nto build on top of it. More generally, Ethereum's explicit attempt\nto foster\na pluralistic\necosystem is in large part an attempt at avoiding one-sided competence\ntraps.\n\nAnother example that comes to mind is the network\nstate movement. Network states are a new strategy for a kind of\ndigital secession, allowing communities that have aligned values to gain\nsome independence from mainstream society and build out their own\nvisions for the future of culture and technology. But the experience of\n(post-fork) Bitcoin Cash shows that movements organized around forking\nto resolve problems have a common failure mode: they can end up\nsplitting again and again and never actually managing to cooperate.\nBitcoin Cash's experience carries lessons that go far beyond Bitcoin\nCash. Like rebel cryptocurrencies, rebel network states need to learn to\nactually execute and build, and not just hold parties and vibe and share\nmemes comparing modern brutalism to 16th-century European architecture\non Twitter. Zuzalu was\nin part my own attempt to instigate change in this direction.\n\nI recommend reading both Bier's The Blocksize War and\nPatterson and Ver's Hijacking Bitcoin to understand one of the\ndefining moments of Bitcoin's history. Particularly, I recommend reading\nthe two books with the mindset that this is not just about Bitcoin -\nrather, this was the first true high-stakes civil war of a \"digital\nnation\", and the experiences carry important lessons for other digital\nnations that we will be building in the decades to come.",
    "contentLength": 24605,
    "summary": "Vitalik reflects on reading opposing histories of Bitcoin's block size war, concluding he still supports the big-block side due to their practical digital cash vision over the small-blockers' governance concerns.",
    "detailedSummary": {
      "theme": "Vitalik's retrospective analysis of the Bitcoin block size war, examining both sides' perspectives and drawing lessons for digital governance and technological development.",
      "summary": "Vitalik reflects on the Bitcoin block size war of the 2010s after reading two opposing historical accounts - one pro-small-block and one pro-big-block. During the conflict, Vitalik sided with big blockers, believing Bitcoin should maintain its digital cash functionality through larger blocks rather than relying on unproven layer 2 solutions. However, he criticizes both sides: small blockers for social media censorship and rigid opposition to hard forks, and big blockers for technical incompetence, lack of compromise, and poor execution. Vitalik identifies this as a 'one-sided competence trap' where one side monopolizes technical talent while pushing a narrow agenda, while the opposition correctly identifies problems but lacks the ability to execute alternatives. He argues that technological innovation, rather than political compromise, is the best way to resolve such conflicts, citing examples from Ethereum where new technologies gave both sides more of what they wanted simultaneously.",
      "takeaways": [
        "The Bitcoin block size war exemplified a 'one-sided competence trap' where technical talent concentrated on one side while the opposition focused on criticism rather than building competence",
        "Technological innovation that benefits all parties is superior to political compromise for resolving contentious debates in digital communities",
        "Both sides had valid concerns - small blockers about node accessibility and governance, big blockers about maintaining Bitcoin's payments functionality - but failed to find middle ground",
        "Social media censorship and rigid ideological positions (like 'soft forks good, hard forks bad') undermined productive discourse and problem-solving",
        "The conflict offers crucial lessons for other digital communities and 'network states' about the importance of technical execution alongside ideological vision"
      ],
      "controversial": [
        "Vitalik's characterization of big blockers as technically incompetent while agreeing with their overall vision may be seen as dismissive by Bitcoin Cash supporters",
        "His criticism of the 'miners should control Bitcoin' philosophy contradicts some fundamental assumptions about proof-of-work governance",
        "The suggestion that Bitcoin stagnated technologically due to the conflict may be disputed by Bitcoin maximalists who view stability as a feature",
        "His advocacy for a 'medium-block' approach that splits the difference between transaction costs and node costs implies central planning that many Bitcoin supporters would reject"
      ]
    }
  },
  {
    "id": "general-2024-05-29-l2culture",
    "title": "Layer 2s as cultural extensions of Ethereum",
    "date": "2024-05-29",
    "category": "protocol",
    "url": "https://vitalik.eth.limo/general/2024/05/29/l2culture.html",
    "path": "general/2024/05/29/l2culture.html",
    "content": "- \n\n- \n\n  \n    \n\n  \n  \n  Dark Mode Toggle\n  \n    \n\n  \n\n##  Layer 2s as cultural extensions of Ethereum \n\n 2024 May 29 \nSee all posts\n\n \n \n\n Layer 2s as cultural extensions of Ethereum \n\nSpecial thanks for Abdelhamid Bakhta and Paul Dylan-Ennis for\nfeedback and discussion.\n\nIn my recent\npost on the differences between layer 1 and layer 2 scaling, I ended\nup roughly coming to the conclusion that the most important differences\nbetween the two approaches are not technical but\norganizational (using the word in a similar sense to the field\nof \"industrial\norganization\"): it's not about what can get built, but what\nwill get built, because of how the lines between different\nparts of the ecosystem are drawn and how that affects people's\nincentives and ability to act. In particular, a layer-2-centric\necosystem is inherently much more pluralistic, and more naturally leads\nto a greater diversity of different approaches to scaling, virtual\nmachine design, and other technological features.\n\nA key point I made in the previous post is:\n\nBecause Ethereum is a layer-2-centric ecosystem, you are free\nto go independently build a sub-ecosystem that is yours with your unique\nfeatures, and is at the same time a part of a greater\nEthereum.\n\nIn this post, I argue that this is true not just with respect\nto technology, but also with respect to culture.\nBlockchains do not just make unique technical tradeoffs, they also have\nunique cultures. On the day after Ethereum and Ethereum Classic\ndiverged, the two blockchains were exactly the same technologically. But\nthey were radically different culturally, and this fact helped to shape\nthe distinct focuses, user bases and even tech stacks that the two\nchains have eight years later. The same applies to Ethereum and Bitcoin:\nat the beginning, Ethereum was roughly \"Bitcoin but with smart\ncontracts\", but the set of differences grew into something much deeper\nten years later.\n\nAn old tweet by Kevin Pham comparing Bitcoin and Ethereum\nculture, as they were in 2017. Both cultures continue to evolve: since\n2017 we have seen the rise\nand fall\nof the \"laser eye\" movement (and the simultaneous rise of movements like\nOrdinals),\nwe've seen Ethereum become layer-2 centric, and we've seen both become\nmuch more mainstream. But the two remain different, and it's probably\nfor the best that it remains so.\n\nWhat are\nsome examples of things that culture affects?\n\nCulture has a similar effect to incentives - indeed, culture is\npart of incentives. It affects who is attracted to an ecosystem and\nwho is repelled. It affects what kinds of actions people are motivated\nto do, and what kinds of actions people can do. It affects what\nis considered legitimate\n- both in protocol design, and at the ecosystem and application\nlayer.\n\nA few particularly important areas that a blockchain's culture has a\ngreat impact on include:\n\n- The type of changes that get made to the protocol -\nincluding quantity, quality and direction\n\n- The protocol's ability to remain open, censorship-resistant\nand decentralized\n\n- The ecosystem's ability to attract high-quality protocol\ndevelopers and researchers\n\n- The ecosystem's ability to attract high-quality application\ndevelopers\n\n- The ecosystem's ability to attract users - both quantity of\nusers, and the right kinds of users\n\n- The ecosystem's public legitimacy in the eyes of outside\ncommunities and actors\n\nIf you really value having a blockchain that remains decentralized,\neven at the cost of being slow, you need to look not just at how well\nthe present-day technology accomplishes those goals, but also at how\nwell the culture values those goals. If a blockchain's culture does not\nvalue curiosity and openness to new technology, then it may well fail at\nboth decentralization and speed, because it fails to take up\nnew technologies like ZK-SNARKs that can get you more of both at the\nsame time. If a blockchain becomes publicly understood as being \"the\ncasino chain\" and nothing else, it becomes hard to get non-casino\napplications onboard. Even non-mercenary core protocol developers and\nresearchers become more difficult to attract. Culture matters, because\nculture is at least partially upstream of almost everything else.\n\n## The cultures of Ethereum\n\nEthereum developer interop, Kenya, 2024 May. Ethereum's\ncore research and development ecosystem is one of Ethereum's\nsubcultures, though it is also quite diverse in its own right, with substantial\ninternal disagreements.\n\nThe researcher Paul Dylan-Ennis has spent a lot of time exploring and\nunderstanding Ethereum's subcultures. He identifies\nthree of the main subcultures in Ethereum as follows:\n\n- Cypherpunk: A cypherpunk is committed to open\nsource development and a certain DIY or punk attitude. In Ethereum's\ncase, the cypherpunks build the infrastructure and tools, but are hands\noff about how they are used, taking a neutral stance. Historically,\ncypherpunk had an explicit emphasis on privacy, but in Ethereum it is\nnot always prioritised, albeit ... a neo-cypherpunk movement called\nlunarpunk has emerged to advocate for placing privacy back front and\ncenter\n\n- Regens: Many influential voices within Ethereum are\ncommitted to a regen or regenerative approach to building technology.\nRooted in Vitalik Buterin's interest in politics and social science,\nmany regens engage in governance experiments designed to reinvigorate,\nimprove or even replace contemporary institutions. This subculture is\ncharacterized by its experimental nature and interest in public\ngoods\n\n- Degens: Users driven purely by speculation and\nwealth accumulation at all costs, the degens (degenerates). Degens are\nfinancial nihilists who focus on current trends and hype to strike it\nlucky and escape the rat race of contemporary neoliberal capitalism.\nDegens will often take extraordinary risks, but in an ironic, almost\ndetached way.\n\nThese are not the only three groups that matter, and you can even\ncontest the extent to which they are coherent groups: institutional\nprofit-oriented groups and people buying pictures of monkeys are\nvery very culturally different. \"Cypherpunks\", as described\nhere, includes both people interested in end uses like protecting\npeople's privacy and freedom, and people interested in working with cool\nfrontier math and cryptography without any strong ideology. But this\ncategorization is interesting as a first approximation.\n\nOne important feature of these three groups in Ethereum is that, in\nlarge part because of Ethereum's flexibility as a developer platform\n(and not just a currency), they each have access to some kind of\nplaying field, where the subculture can engage in action, and not just\ntalking. One crude approximation is:\n\n- Cypherpunks participate in core Ethereum research and development,\nand write privacy software\n\n- Regens do Gitcoin\ngrants rounds, retroactive public goods\nfunding, and various other non-financial applications\n\n- Degens trade memecoins and NFTs and play games\n\nIn my view, this cultural branching has been a great benefit to\nEthereum. Ethereum core development culture values high-quality thinking\non topics like advanced cryptography, game theory and increasingly\nsoftware engineering, it values freedom and independence, it values\ncypherpunk ideals as well as blockchainified versions of those\nprinciples (eg. \"immutability\"), and an idealistic approach focused on\nvalues and soft power over hard power. These values are important and\ngood; looking at my list of impacts of culture from the previous\nsection, they make Ethereum very well-positioned on (1), (2), (3) and to\nsome extent (6). But they are incomplete: for one, the above description\nhas little emphasis on appealing to application developers, and close to\nzero emphasis on appealing to users - the stability-oriented values help\ngive confidence to people who \"use\" Ethereum by hodling ETH, but that's\npretty much it. Cultural pluralism is a way of getting out of\nthis quandary, allowing one subculture to focus on core development\nwhile another focuses on growing the \"edges\" of the ecosystem.\nBut this raises a question: are there ways that we can strengthen this\nkind of cultural pluralism even further?\n\n## Subcultures and layer 2s\n\nThis is where I get to what is perhaps the single most\nunder-appreciated property of layer 2s: for a subculture, a\nlayer 2 is the ultimate playing field for action. Layer 2s\nallow subcultures to emerge that are armed with substantial resources,\nand a feedback loop that forces them to learn and adapt in order to be\neffective in the real world. Layer 2s have to be effective in multiple\nways: attracting users and application developers, developing\ntechnology, and building global communities.\n\nPerhaps the key property of layer 2s that matters here is that\na layer 2 is simultaneously (i) an ecosystem, and (ii) organized\naround building something. Local meetup groups can form their\nown ecosystems, and they often have their own unique cultures, but they\nhave relatively limited resources and execution power. Applications can\nhave a lot of resources and execution power, but they are\napplications: you can use them, but you can't\nbuild on them. Uniswap is great, but there is no concept of\n\"building on Unsiwap\" that is anywhere near as strong as, say, \"building\non Polygon\".\n\nSome specific ways in which layer 2s can, and do, end up culturally\nspecializing include:\n\n- More willingness to do user outreach or \"business\ndevelopment\": intentionally making efforts to attract specific outside\nactors, including individuals, businesses and communities, to\nparticipate in the ecosystem.\n\n- Diversity of values that are emphasized. Is your\ncommunity more about \"public goods\", \"good tech\", \"Ethereum neutrality\",\n\"financial inclusion\", \"diversity\", \"scaling\", or something else?\nDifferent L2s give different answers.\n\n- Diversity of participants: what kinds of people\ndoes the community attract? Does it particularly emphasize certain\ndemographic groups? Personality types? Languages? Continents?\n\nHere are a few examples:\n\nOptimism\n\nZKSync\n\nMegaETH\n\nStarknet\n\nPolygon has found success with partnerships\nwith mainstream companies, and an increasingly high-quality ZK\necosystem. Optimism has Base and World\nChain, and features a heavy cultural interest in ideas like retro\nfunding and not-just-token-based\ngovernance. Metis focuses\non DAOs. Arbitrum has built a brand\naround high-quality\ndeveloper tools and technology. Scroll\nfocuses on \"preserv[ing] the essence of Ethereum - trust-minimized,\nsecure and open source\". Taiko\nemphasizes being \"seamless UX\", \"community aligned\", \"security-first\"\nand \"based\".\nIn general, every Ethereum layer 2 has a unique \"soul\": some combination\nof Ethereum's culture, together with its own particular twist.\nHow can this\nlayer-2-centric approach succeed?\n\nThe core value proposition of this layer-2 centric approach to\nculture is that it tries to balance the benefits of pluralism and\ncooperation, by creating a diverse set of different subcultures that\nstill share some common values and work together on key common\ninfrastructure to achieve those values.\n\nEthereum is trying to take the pluralistic\nroute.\n\nThere have been other attempts at a similar kind of two-level\napproach. The most notable one that I can think of is the delegated\nproof of stake (DPoS) system in EOS back in the 2017 era. EOS's DPoS\nworked by having coin holders vote on which delegates run the chain. The\ndelegates would be responsible for creating blocks, and coming to\nconsensus on others' blocks, and they would also get a large amount of\ncoins from EOS issuance. Delegates ended up doing a lot of community\nbuilding in order to attract votes, and many of these \"nodes\" (eg. EOS\nNew York, EOS Hong Kong), ended up being recognizable brands in their\nown right.\n\nThis ended up being an unstable system, because coin\nvoting is inherently unstable, and because some powerful actors in\nthe EOS ecosystem turned out to be greedy jerks that siphoned\naway lots of money that was raised on behalf of the community for\npersonal gain. But while it worked, it showed an amazing property:\nit created strong highly-autonomous sub-communities that were\nstill working together toward a common goal.\n\nEOS New York, one of the top EOS block producers, even\nended up writing quite a bit of\nopen-source infrastructure code.\n\nWhen this approach works successfully, it also creates a kind of\nhealthy competition. By default, a community like Ethereum has a natural\ntendency to rally around people who have been in the community for a\nlong time. This has an advantage that it can help preserve the\ncommunity's values as the community rapidly grows - it reduces the\nchance that Ethereum stops caring about freedom of speech or open source\neven if unfavorable winds come in from the outside world. But it also\nrisks shifting attention away from technical competence and toward\nsocial games, allowing established \"OGs\" to remain entrenched even if\nthey underperform, and limiting the culture's ability to renew itself\nand evolve. With a healthy \"subculture culture\", these problems can be\nmitigated: entire new subcommunities can rise and fall, and people who\nsucceed within subcommunities can even start contributing to other\naspects of Ethereum. In short, less legitimacy\nby continuity, more legitimacy by performance.\n\nWe can also examine the above story to identify possible weak points.\nHere are a few that come to mind:\n\n- Collapse into echo chambers: essentially, the same\nfailure modes that I talked about in my\nprevious post, but for culture. L2s start acting like separate\nuniverses, with little cross-pollination between them.\n\n- Collapse into monoculture: whether due to shared\nhuman biases or shared economic incentives (or too strong of a unified\nEthereum culture), everyone ends up looking in similar places for what\napplications to build and perhaps even what technical choices to make,\nand this ends up being the wrong place. Alternatively, either a single\nL2 or a small number of L2s gets entrenched, and there is no longer a\nfunctioning mechanism for new people and subcommunities to rise.\n\n- The vector favored by competition is wrong: L2s\nthat focus on use cases that succeed in some narrow financial sense, but\nat the expense of other goals, appear successful, and more and more\ncommunities go in that direction over time.\n\nI do not claim to have perfect answers to these; Ethereum is an\nongoing experiment, and part of what excites me about the ecosystem is\nits willingness to tackle difficult problems head-on. Many of the\nchallenges stem from incentive misalignments; the natural solution to\nthat is to create better ecosystem-wide incentives for collaboration.\nThe idea I mentioned in my\nprevious post, of creating a \"Basic Infrastructure Guild\" to\ncomplement Protocol Guild is one option. Another option is to explicitly\nsubsidize projects that multiple L2s choose to collaborate on (ie.\nsomething vaguely like quadratic\nfunding, but focusing on bridging ecosystems rather than bridging\nindividuals). There is a lot of value in trying to expand on these\nideas, and keep working to make the best of Ethereum's unique advantage\nas a pluralistic ecosystem.",
    "contentLength": 15158,
    "summary": "Layer 2s enable Ethereum subcultures to build unique ecosystems with distinct values, users, and approaches while remaining part of greater Ethereum.",
    "detailedSummary": {
      "theme": "Vitalik argues that Layer 2 networks serve as cultural extensions of Ethereum, enabling diverse subcultures to flourish while maintaining shared infrastructure and values.",
      "summary": "Vitalik builds on his previous technical analysis of Layer 2 scaling to explore how these networks function as cultural playgrounds for different Ethereum subcultures. He identifies three main subcultures within Ethereum: cypherpunks (focused on infrastructure and neutrality), regens (committed to governance experiments and public goods), and degens (driven by speculation and financial gain). Vitalik argues that Layer 2s provide the ideal platform for these subcultures to operate with substantial resources and real-world feedback loops, unlike local meetups (which lack resources) or individual applications (which can't be built upon). He examines how different L2s like Optimism, Polygon, and Arbitrum have developed distinct cultural identities while remaining part of the broader Ethereum ecosystem. Vitalik sees this cultural pluralism as beneficial because it allows specialized communities to focus on different aspects - core development, user acquisition, application building - while sharing common infrastructure. He acknowledges potential risks including echo chambers, monoculture collapse, and misaligned competitive incentives, but views Ethereum's layer-2-centric approach as a promising experiment in balancing pluralism with cooperation.",
      "takeaways": [
        "Culture significantly impacts blockchain ecosystems, affecting protocol development, decentralization, developer attraction, user adoption, and public legitimacy",
        "Layer 2 networks serve as unique platforms where subcultures can take action with substantial resources, unlike meetups (limited resources) or applications (limited buildability)",
        "Ethereum's three main subcultures - cypherpunks, regens, and degens - each have distinct playing fields within the ecosystem that allow them to contribute differently",
        "Different Layer 2s are developing unique cultural identities and specializations while maintaining connection to Ethereum's core values and infrastructure",
        "The layer-2-centric cultural approach creates healthy competition and legitimacy by performance rather than just continuity, but faces risks of echo chambers, monoculture, or misaligned incentives"
      ],
      "controversial": [
        "The characterization of 'degens' as 'financial nihilists' focused purely on speculation may oversimplify or stigmatize a significant portion of the crypto community",
        "The comparison between Ethereum's L2 approach and EOS's failed DPoS system may not be entirely apt given the fundamental structural differences",
        "The suggestion that legitimacy should shift from continuity to performance could potentially undermine the value of long-term community builders and institutional knowledge"
      ]
    }
  },
  {
    "id": "general-2024-05-23-l2exec",
    "title": "How do layer 2s really differ from execution sharding?",
    "date": "2024-05-23",
    "category": "protocol",
    "url": "https://vitalik.eth.limo/general/2024/05/23/l2exec.html",
    "path": "general/2024/05/23/l2exec.html",
    "content": "- \n\n- \n\n  \n    \n\n  \n  \n  Dark Mode Toggle\n  \n    \n\n  \n\n##  How do layer 2s really differ from execution sharding? \n\n 2024 May 23 \nSee all posts\n\n \n \n\n How do layer 2s really differ from execution sharding? \n\nOne of the points that I made in my post two and half years ago on \"the Endgame\"\nis that the different future development paths for a blockchain, at\nleast technologically, look surprisingly similar. In both cases, you\nhave a very large number of transactions onchain, and processing them\nrequires (i) a large amount of computation, and (ii) a large amount of\ndata bandwidth. Regular Ethereum nodes, such as the 2 TB reth archive node running\non the laptop I'm using to write this article, are not powerful enough\nto verify such a huge amount of data and computation directly, even with\nheroic software engineering work and Verkle trees. Instead, in both \"L1\nsharding\" and a rollup-centric\nworld, ZK-SNARKs\nare used to verify computation, and DAS\nto verify data availability. The DAS in both cases is the same. The\nZK-SNARKs in both cases are the same tech, except in one case they are\nsmart contract code and in the other case they are an enshrined feature\nof the protocol. In a very real technical sense, Ethereum is\ndoing sharding, and rollups are shards.\n\nThis raises a natural question: what is the difference\nbetween these two worlds? One answer is that the consequences of code\nbugs are different: in a rollup world, coins get lost, and in a shard\nchain world, you have consensus failures. But I expect that as protocol\nsolidify, and as formal verification technology improves, the importance\nof bugs will decrease. So what are the differences between the\ntwo visions that we can expect will stick into the long term?\nDiversity of execution\nenvironments\n\nOne of the ideas that we briefly played around with in Ethereum in\n2019 was execution\nenvironments. Essentially, Ethereum would have different\n\"zones\" that could have different rules for how accounts work (including\ntotally different approaches like UTXOs), how the virtual machine works,\nand other features. This would enable a diversity of approaches in parts\nof the stack where it would be difficult to achieve if Ethereum were to\ntry to do everything by itself.\n\nIn the end, we ended up abandoning some of the more ambitious plans,\nand simply kept the EVM. However, Ethereum L2s (including rollups,\nvaldiums and Plasmas) arguably ended up serving the role of execution\nenvironments. Today, we generally focus on EVM-equivalent L2s, but this\nignores the diversity of many alternative approaches:\n\n- Arbitrum\nStylus, which adds a second virtual machine based on WASM alongside the EVM.\n\n- Fuel,\nwhich uses a Bitcoin-like (but more feature-complete) UTXO-based\narchitecture.\n\n- Aztec, which introduces a new\nlanguage and programming paradigm designed around ZK-SNARK-based\nprivacy-preserving smart contracts.\n\nUTXO-based architecture. Source: Fuel\ndocumentation.\n\nWe could try to make the EVM into a super-VM that covers all\npossible paradigms, but that would have led to much less effective\nimplementations of each of these concepts than allowing platforms like\nthese to specialize.\nSecurity tradeoffs: scale\nand speed\n\nEthereum L1 provides a really strong security guarantee. If some\npiece of data is inside a block that is finalized on L1, the entire\nconsensus (including, in extreme situations, social consensus) works to\nensure that the data will not be edited in a way that goes against the\nrules of the application that put that data there, that any execution\ntriggered by the data will not be reverted, and that the data will\nremain accessible. To achieve these guarantees, Ethereum L1 is willing\nto accept high costs. At the time of this writing, the transaction fees\nare relatively low: layer\n2s charge less than a cent per transaction, and even the L1 is under\n$1 for a basic ETH transfer. These costs may remain low in the future if\ntechnology improves fast enough that available block space grows to keep\nup with demand - but they may not. And even $0.01 per transaction is too\nhigh for many non-financial applications, eg. social media or\ngaming.\n\nBut social media and gaming do not require the same security model as\nL1. It's ok if someone can pay a million dollars to revert a record of\nthem losing a chess game, or make one of your twitter posts look like it\nwas published three days after it actually was. And so these\napplications should not have to pay for the same security costs. An\nL2-centric approach enables this, by supporting a spectrum of data\navailability approaches from rollups\nto plasma\nto validiums.\n\nDifferent L2 types for different use cases. Read more here.\n\nAnother security tradeoff arises around the issue of passing\nassets from L2 to L2. In the limit (5-10 years into the\nfuture), I expect that all rollups will be ZK rollups, and\nhyper-efficient proof systems like Binius and Circle STARKs with lookups, plus proof aggregation\nlayers, will make it possible for L2s to provide finalized state\nroots in each slot. For now, however, we have a complicated mix of\noptimistic rollups and ZK rollups with various proof time windows. If we\nhad implemented execution sharding in 2021, the security model to keep\nshards honest would have been optimistic rollups, not ZK - and so L1\nwould have had to manage the systemically-complex\nfraud proof logic on-chain and have a week-long withdrawal period for\nmoving assets from shard to shard. But like code bugs, I think this\nissue is ultimately temporary.\n\nA third, and once again more lasting, dimension of security tradeoff\nis transaction speed. Ethereum has blocks every 12\nseconds, and is unwilling to go much faster because that would overly\ncentralize the network. Many L2s, however, are exploring block times of\na few hundred milliseconds. 12 seconds is already not that bad: on\naverage, a user who submits a transaction needs to wait ~6-7 seconds to\nget included into a block (not just 6 because of the possibility that\nthe next block will not include them). This is comparable to what I have\nto wait when making a payment on my credit card. But many applications\ndemand much higher speed, and L2s provide it.\n\nTo provide this higher speed, L2s rely on\npreconfirmation mechanisms: the L2's own validators\ndigitally sign a promise to include the transaction at a particular\ntime, and if the transaction does not get included, they can be\npenalized. A mechanism called StakeSure generalizes this\nfurther.\n\nL2 preconfirmations.\n\nNow, we could try to do all of this on layer 1. Layer 1\ncould incorporate a \"fast pre-confirmation\" and \"slow final\nconfirmation\" system. It could incorporate different shards with\ndifferent levels of security. However, this would add a lot of\ncomplexity to the protocol. Furthermore, doing it all on layer 1\nwould risk overloading the\nconsensus, because a lot of the higher-scale or\nfaster-throughput approaches have higher centralization risks or require\nstronger forms of \"governance\", and if done at L1, the effects of those\nstronger demands would spill over to the rest of the protocol. By\noffering these tradeoffs through layer 2s, Ethereum can mostly avoid\nthese risks.\nThe\nbenefits of layer 2s on organization and culture\n\nImagine that a country gets split in half, and one half becomes\ncapitalist and the other becomes highly government-driven (unlike when\nthis\nhappens in reality,\nassume that in this thought experiment it's not the result of any kind\nof traumatic war; rather, one day a border magically goes up and that's\nit). In the capitalist part, the restaurants are all run by various\ncombinations of decentralized ownership, chains and franchises. In the\ngovernment-driven part, they are all branches of the government, like\npolice stations. On the first day, not much would change. People largely\nfollow their existing habits, and what works and what doesn't work\ndepends on technical realities like labor skill and infrastructure. A\nyear later, however, you would expect to see large changes, because the\ndiffering structures of incentives and control lead to large changes in\nbehavior, which affect who comes, who stays and who goes, what gets\nbuilt, what gets maintained, and what gets left to rot.\n\nIndustrial\norganization theory covers a lot of these distinctions: it talks\nabout the differences not just between a government-run economy and a\ncapitalist economy, but also between an economy dominated by large\nfranchises and an economy where eg. each supermarket is run by an\nindependent entrepreneur. I would argue that the difference between a\nlayer-1-centric ecosystem and a layer-2-centric ecosystem falls along\nsimilar lines.\n\nA \"core devs run everything\" architecture gone very\nwrong.\n\nI would phrase the key benefit to Ethereum of being a layer-2-centric\necosystem as follows:\n\nBecause Ethereum is a layer-2-centric ecosystem, you are free\nto go independently build a sub-ecosystem that is yours with your unique\nfeatures, and is at the same time a part of a greater\nEthereum.\n\nIf you're just building an Ethereum client, you're part of a greater\nEthereum, and while you have some room for creativity, it's far less\nthan what's available to L2s. And if you're building a completely\nindependent chain, you have maximal room for creativity, but you lose\nthe benefits like shared security and shared network effects. Layer 2s\nform a happy medium.\n\nLayer 2s do not just create a technical opportunity to\nexperiment with new execution environments and security tradeoffs to\nachieve scale, flexibility and speed: they also create an\nincentive to: both for the developers to build and maintain it,\nand for the community to form around and support it.\n\nThe fact that each L2 is isolated also means that deploying new\napproaches is permissionless: there's no need to convince all\nthe core devs that your new approach is \"safe\" for the rest of the\nchain. If your L2 fails, that's on you. Anyone can work on totally weird\nideas (eg. Intmax's approach\nto Plasma), and even if they get completely ignored by the Ethereum\ncore devs, they can keep building and eventually deploy. L1 features and\nprecompiles are not like this, and even in Ethereum, what succeeds and\nwhat fails in L1 development often ends up depending on politics to a\nhigher degree than we would like. Regardless of what theoretically\ncould get built, the distinct incentives created by an\nL1-centric ecosystem and an L2-centric ecosystem end up heavily\ninfluencing what does get built in practice, with what level of\nquality and in what order.\nWhat\nchallenges does Ethereum's layer-2-centric ecosystem have?\n\nA layer 1 + layer 2 architecture gone very wrong. Source.\n\nThere is a key challenge to this kind of layer-2-centric approach,\nand it's a problem that layer 1-centric ecosystems do not have to face\nto nearly the same extent: coordination. In other\nwords, while Ethereum branches out, the challenge is in preserving the\nfundamental property that it still all feels like \"Ethereum\", and has\nthe network effects of being Ethereum rather than being N separate\nchains. Today, the situation is suboptimal in many ways:\n\n- Moving tokens from one layer 2 to another requires\noften centralized bridge platforms, and is complicated for the average\nuser. If you have coins on Optimism, you can't just paste someone's\nArbitrum address into your wallet, and send them funds.\n\n- Cross-chain smart contract wallet support is not\ngreat - both for personal smart contract wallets and for organizational\nwallets (including DAOs). If you change your key on one L2, you also\nneed to go change your key on every other L2.\n\n- Decentralized validation infrastructure is often\nlacking. Ethereum is finally starting to have decent light clients, such\nas Helios. However, there\nis no point in this if activity is all happening on layer 2s that all\nrequire their own centralized RPCs. In principle, once you have the\nEthereum header chain, making light clients for L2s is not hard; in\npractice, there's far too little emphasis on it.\n\nThere are efforts working to improve all three. For cross-chain token\nexchange, the ERC-7683 standard\nis an emerging option, and unlike existing \"centralized bridges\" it does\nnot have any enshrined central operator, token or governance. For\ncross-chain accounts, the approach most wallets are taking is to use\ncross-chain replayable messages to update keys in the short term, and keystore\nrollups in the longer term. Light clients for L2s are starting to\nemerge, eg. Beerus for\nStarknet. Additionally, recent improvements in user experience through\nnext-generation wallets have already solved much more basic problems\nlike removing the need for users to manually switch to the right network\nto access a dapp.\n\nRabby showing an integrated view of asset balances across\nmultiple chains. In the not-so-long-ago dark old days, wallets did not\ndo this!\n\nBut it is important to recognize that layer-2-centric ecosystems do\nswim against the current to some extent when trying to coordinate.\nIndividual layer 2s don't have a natural economic incentive to build the\ninfrastructure to coordinate: small ones don't, because they would only\nsee a small share of the benefit of their contributions, and large ones\ndon't, because they would benefit as much or more from strengthening\ntheir own local network effects. If each layer 2 is separately\noptimizing its individual piece, and no one is thinking about how each\npiece fits into the broader whole, we get failures like the urbanism\ndystopia in the picture a few paragraphs above.\n\nI do not claim to have magical perfect solutions to this problem. The\nbest I can say is that the ecosystem needs to more fully recognize that\ncross-L2 infrastructure is a type of Ethereum infrastructure,\nalongside L1 clients, dev tools and programming languages, and should be\nvalorized and funded as such. We have Protocol\nGuild; maybe we need Basic Infrastructure Guild.\n\n## Conclusions\n\n\"Layer 2s\" and \"sharding\" often get described in public discourse as\nbeing two opposite strategies for how to scale a blockchain. But when\nyou look at the underlying technology, there is a puzzle: the actual\nunderlying approaches to scaling are exactly the same. You have\nsome kind of data sharding. You have fraud provers or ZK-SNARK provers.\nYou have solutions for cross-{rollup, shard} communication. The main\ndifference is: who is responsible for building and updating\nthose pieces, and how much autonomy do they have?\n\nA layer-2-centric ecosystem is sharding in a very real\ntechnical sense, but it's sharding where you can go create your own\nshard with your own rules. This is powerful, and enables a lot of\ncreativity and independent innovation. But it also has key challenges,\nparticularly around coordination. For a layer-2-centric ecosystem like\nEthereum to succeed, it needs to understand those challenges, and\naddress them head-on, in order to get as many of the benefits of\nlayer-1-centric ecosystems as possible, and come as close as possible to\nhaving the best of both worlds.",
    "contentLength": 14985,
    "summary": "Layer 2s differ from execution sharding by enabling diverse execution environments, security tradeoffs, and permissionless innovation.",
    "detailedSummary": {
      "theme": "Vitalik argues that layer 2 scaling solutions and execution sharding are technically similar but differ fundamentally in their organizational structure, governance, and innovation incentives.",
      "summary": "Vitalik begins by highlighting a key insight from his 'Endgame' post: that different blockchain scaling approaches are remarkably similar technologically, both relying on data availability sampling (DAS) and ZK-SNARKs for verification. He argues that Ethereum is already implementing sharding through rollups, which function as specialized shards. The core differences lie not in technology but in governance and organization. Layer 2s offer significant advantages including diversity of execution environments (like Arbitrum Stylus, Fuel's UTXO model, and Aztec's privacy-focused approach), flexible security tradeoffs that enable different cost structures and speeds for various use cases, and most importantly, permissionless innovation where developers can build specialized sub-ecosystems without requiring consensus from core developers. Vitalik uses economic theory analogies to explain how layer-2-centric ecosystems create better incentive structures for innovation compared to monolithic layer-1 approaches. However, he acknowledges serious coordination challenges in layer-2-centric systems, including complex cross-chain token transfers, inadequate smart contract wallet support across chains, and insufficient decentralized validation infrastructure. He concludes that while layer 2s and sharding use identical underlying technologies, the organizational differences create distinct innovation patterns, with layer 2s enabling more autonomous creativity at the cost of increased coordination complexity.",
      "takeaways": [
        "Layer 2 solutions and execution sharding are technologically identical, both using data availability sampling and ZK-SNARKs, making rollups essentially specialized shards",
        "Layer 2s enable diverse execution environments and programming paradigms that would be impractical to implement in a monolithic layer-1 system",
        "Different applications require different security tradeoffs - social media and gaming don't need the same expensive security guarantees as financial applications",
        "Layer-2-centric ecosystems create better incentives for permissionless innovation since developers can build independently without convincing core developers",
        "The main challenge of layer-2 systems is coordination - ensuring the ecosystem feels unified rather than fragmented across multiple separate chains"
      ],
      "controversial": [
        "The assertion that coordination problems in layer-2-centric ecosystems can be solved through better funding and recognition, which may underestimate the fundamental economic misalignment between individual L2 incentives and ecosystem-wide benefits",
        "The claim that layer 2s are essentially shards may be disputed by those who view them as fundamentally different architectural approaches with distinct trust assumptions"
      ]
    }
  },
  {
    "id": "general-2024-05-17-decentralization",
    "title": "The near and mid-term future of improving the Ethereum network's permissionlessness and decentralization",
    "date": "2024-05-17",
    "category": "protocol",
    "url": "https://vitalik.eth.limo/general/2024/05/17/decentralization.html",
    "path": "general/2024/05/17/decentralization.html",
    "content": "- \n\n- \n\n  \n    \n\n  \n  \n  Dark Mode Toggle\n  \n    \n\n  \n\n##  The near and mid-term future of improving the Ethereum network's permissionlessness and decentralization \n\n 2024 May 17 \nSee all posts\n\n \n \n\n The near and mid-term future of improving the Ethereum network's permissionlessness and decentralization \n\nSpecial thanks to Dankrad Feist, Caspar Schwarz-Schilling and\nFrancesco for rapid feedback and review.\n\nI am sitting here writing this on the final day of an Ethereum\ndeveloper interop in Kenya, where we made a large amount of progress\nimplementing and ironing out technical details of important upcoming\nEthereum improvements, most notably PeerDAS,\nthe Verkle tree transition and\ndecentralized approaches to storing history in the context of EIP 4444. From my own\nperspective, it feels like the pace of Ethereum development, and our\ncapacity to ship large and important features that meaningfully improve\nthe experience for node operators and (L1 and L2) users, is\nincreasing.\n\nEthereum client teams working together to ship the Pectra\ndevnet.\n\nGiven this greater technical capacity, one important question to be\nasking is: are we building toward the right goals? One\nprompt for thinking about this is a recent series of unhappy tweets from\nthe long-time Geth core developer Peter Szilagyi:\n\nThese are valid concerns. They are concerns that many people in the\nEthereum community have expressed. They are concerns that I have on many\noccasions had personally. However, I also do not think that the\nsituation is anywhere near as hopeless as Peter's tweets imply; rather,\nmany of the concerns are already being addressed by protocol features\nthat are already in-progress, and many others can be addressed by very\nrealistic tweaks to the current roadmap.\n\nIn order to see what this means in practice, let us go through the\nthree examples that Peter provided one by one. The goal is not to focus\non Peter specifically; they are concerns that are widely shared among\nmany community members, and it's important to address them.\n\n## MEV, and builder dependence\n\nIn the past, Ethereum blocks were created by miners, who used a\nrelatively simple algorithm to create blocks. Users send transactions to\na public p2p network often called the \"mempool\" (or \"txpool\"). Miners\nlisten to the mempool, and accept transactions that are valid and pay\nfees. They include the transactions they can, and if there is not enough\nspace, they prioritize by highest-fee-first.\n\nThis was a very simple system, and it was friendly toward\ndecentralization: as a miner, you can just run default software, and you\ncan get the same levels of fee revenue from a block that you could get\nfrom highly professional mining farms. Around 2020, however, people\nstarted exploiting what was called miner extractable value\n(MEV): revenue that could only be gained by executing complex\nstrategies that are aware of activities happening inside of various defi\nprotocols.\n\nFor example, consider decentralized exchanges like Uniswap. Suppose\nthat at time T, the USD/ETH exchange rate - on centralized\nexchanges and on Uniswap - is $3000. At time T+11, the\nUSD/ETH exchange rate on centralized exchanges rises to $3005. But\nEthereum has not yet had its next block. At time T+12, it\ndoes. Whoever creates the block can make their first transaction be a\nseries of Uniswap buys, buying up all of the ETH available on Uniswap at\nprices from $3000 to $3004. This is extra revenue, and is called MEV.\nApplications other than DEXes have their own analogues to this problem.\nThe Flash Boys 2.0 paper\npublished in 2019 goes into this in detail.\n\nA chart from the Flash Boys 2.0 paper that shows the amount\nof revenue capturable using the kinds of approaches described\nabove.\n\nThe problem is that this breaks the story for why mining (or,\npost-2022, block proposing) can be \"fair\": now, large\nactors who have better ability to optimize these kinds of extraction\nalgorithms can get a better return per block.\n\nSince then there has been a debate between two strategies, which I\nwill call MEV minimization and MEV\nquarantining. MEV minimization comes in two forms: (i)\naggressively work on MEV-free alternatives to Uniswap (eg. Cowswap), and (ii) build in-protocol\ntechniques, like encrypted mempools, that reduce the information\navailable to block producers, and thus reduce the revenue that they can\ncapture. In particular, encrypted mempools prevent strategies such as\nsandwich attacks, which put transactions right before\nand after users' trades in order to financially exploit them\n(\"front-running\").\n\nMEV quarantining works by accepting MEV, but trying to limit its\nimpact on staking centralization by separating the market into two kinds\nof actors: validators are responsible for attesting and proposing\nblocks, but the task of choosing the block's contents gets\noutsourced to specialized builders through an auction\nprotocol. Individual stakers now no longer need to worry about\noptimizing defi arbitrage themselves; they simply join the auction\nprotocol, and accept the highest bid. This is called\nproposer/builder separation (PBS). This approach has\nprecedents in other industries: a major reason why restaurants are able\nto remain so decentralized is that they often rely on a fairly\nconcentrated set of providers for various operations that do have large\neconomies of scale. So far, PBS has been reasonably successful at\nensuring that small validators and large validators are on a fair\nplaying field, at least as far as MEV is concerned. However, it creates\nanother problem: the task of choosing which transactions get\nincluded becomes more concentrated.\n\nMy view on this has always been that MEV minimization is good and we\nshould pursue it (I personally use Cowswap regularly!) - though\nencrypted mempools have a lot of challenges, but MEV minimization will\nlikely be insufficient; MEV will not go down to zero, or even near-zero.\nHence, we need some kind of MEV quarantining too. This creates an\ninteresting task: how do we make the \"MEV quarantine box\" as\nsmall as possible? How do we give builders the least possible\npower, while still keeping them capable of absorbing the role of\noptimizing arbitrage and other forms of MEV collecting?\n\nIf builders have the power to exclude transactions from a block\nentirely, there are attacks that can quite easily arise. Suppose that\nyou have a collateralized\ndebt position (CDP) in a defi protocol, backed by an asset whose\nprice is rapidly dropping. You want to either bump up your collateral or\nexit the CDP. Malicious builders could try to collude to refuse to\ninclude your transaction, delaying it until prices drop by enough that\nthey can forcibly liquidate your CDP. If that happens, you would have to\npay a large penalty, and the builders would get a large share of it. So\nhow can we prevent builders from excluding transactions and\naccomplishing these kinds of attacks?\n\nThis is where inclusion lists come in.\n\nSource: this\nethresear.ch post.\n\nInclusion lists allow block proposers (meaning, stakers) to choose\ntransactions that are required to go into the block. Builders can still\nreorder transactions or insert their own, but they must include the\nproposer's transactions. Eventually, inclusion lists were\nmodified to constrain the next block rather than the\ncurrent block. In either case, they take away the builder's ability to\npush transactions out of the block entirely.\n\nThe above was all a deep rabbit hole of complicated background. But\nMEV is a complicated issue; even the above description misses lots of\nimportant nuances. As the old adage goes, \"you may not be looking for\nMEV, but MEV is looking for you\". Ethereum researchers are\nalready quite aligned on the goal of \"minimizing the quarantine\nbox\", reducing the harm that builders can do (eg. by excluding or\ndelaying transactions as a way of attacking specific applications) as\nmuch as possible.\n\nThat said, I do think that we can go even further. Historically,\ninclusion lists have often been conceived as an \"off-to-the-side\nspecial-case feature\": normally, you would not think about them, but\njust in case malicious builders start doing crazy things, they give you\na \"second path\". This attitude is reflected in current design decisions:\nin the current\nEIP, the gas limit of an inclusion list is around 2.1 million. But\nwe can make a philosophical shift in how we think about\ninclusion lists: think of the inclusion list as being the\nblock, and think of the builder's role as being an\noff-to-the-side function of adding a few transactions to collect MEV.\nWhat if it's builders that have the 2.1 million gas limit?\n\nI think ideas in this direction - really pushing the quarantine box\nto be as small as possible - are really interesting, and I'm in favor of\ngoing in that direction. This is a shift from \"2021-era\nphilosophy\": in 2021-era philosophy, we were more enthusiastic\nabout the idea that, since we now have builders, we can \"overload\" their\nfunctionality and have them serve users in more complicated ways, eg. by\nsupporting ERC-4337 fee markets.\nIn this new philosophy, the transaction validation parts of ERC-4337\nwould have to be enshrined into the protocol. Fortunately, the ERC-4337\nteam is already increasingly\nwarm about this direction.\n\nSummary: MEV thought has already been going back in the\ndirection of empowering block producers, including giving block\nproducers the authority to directly ensure the inclusion of users'\ntransactions. Account abstraction proposals are already going\nback in the direction of removing reliance on centralized relayers, and\neven bundlers. However, there is a good argument that we are not going\nfar enough, and I think pressure pushing the development process to go\nfurther in that direction is highly welcome.\n\n## Liquid staking\n\nToday, solo stakers make up a relatively small percentage of all\nEthereum staking, and most staking is done by various providers - some\ncentralized operators, and others DAOs, like Lido and RocketPool.\n\nI have done my own research - various polls [1] [2], surveys,\nin-person conversations, asking the question \"why are you - specifically\nyou - not solo staking today?\" To me, a robust solo staking ecosystem is\nby far my preferred outcome for Ethereum staking, and one of the best\nthings about Ethereum is that we actually try to support a robust solo\nstaking ecosystem instead of just surrendering to delegation. However,\nwe are far from that outcome. In my polls and surveys, there are a few\nconsistent trends:\n\n- The great majority of people who are not solo staking cite their\nprimary reason as being the 32 ETH minimum.\n\n- Out of those who cite other reasons, the highest is technical\nchallenge of running and maintaining a validator node.\n\n- The loss of instant availability of ETH, the security risks of \"hot\"\nprivate keys, and the loss of ability to simultaneously participate in\ndefi protocols, are significant but smaller concerns.\n\nThe main reasons why people are not solo staking, according to\nFarcaster polls.\n\nThere are two key questions for staking research to resolve:\n\n- How do we solve these concerns?\n\n- If, despite effective solutions to most of these concerns, most\npeople still don't want to solo stake, how do we keep the\nprotocol stable and robust against attacks despite that fact?\n\nMany ongoing research and development items are aimed precisely at\nsolving these problems:\n\n- Verkle trees plus EIP-4444 allow\nstaking nodes to function with very low hard disk requirements.\nAdditionallty, they allow staking nodes to sync almost instantly,\ngreatly simplifying the setup process, as well as operations such as\nswitching from one implementation to another. They also make Ethereum\nlight clients much more viable, by reducing the data bandwidth needed to\nprovide proofs for every state access.\n\n- Research (eg.\nthese proposals) into ways to allow a much larger valdiator set\n(enabling much smaller staking minimums) while at the same time reducing\nconsensus node overhead. These ideas can be implemented as part of single slot\nfinality. Doing this would also makes light clients safer, as they\nwould be able to verify the full set of signatures instead of relying on\nsync\ncommittees).\n\n- Ongoing Ethereum client optimizations keep reducing the cost and\ndifficulty of running a validator node, despite growing history.\n\n- Research on penalties\ncapping could potentially mitigate concerns around private key risk,\nand make it possible for stakers to simultaneously stake their ETH in\ndefi protocols if that's what they wish to do.\n\n- 0x01\nWithdrawal credentials allow stakers to set an ETH address as their\nwithdrawal address. This makes decentralized staking pools more viable,\ngiving them a leg up against centralized staking pools.\n\nHowever, once again there is more that we could do.\nIt is theoretically possible to allow validators to withdraw much more\nquickly: Casper FFG continues to be safe even if the validator set\nchanges by a few percent ever time it finalizes (ie. once per epoch).\nHence, we could reduce the withdrawal period much more if we\nput effort into it. If we wanted to greatly reduce the minimum deposit\nsize, we could make a hard decision to trade off in other\ndirections, eg. if we increase the finality time by 4x, that would allow\na 4x\nminimum deposit size decrease. Single slot finality would later\nclean this up by moving beyond the \"every staker participates in every\nepoch\" model entirely.\n\nAnother important part of this whole question is the\neconomics of staking. A key question is: do we want staking to\nbe a relatively niche activity, or do we want everyone or almost\neveryone to stake all of their ETH? If everyone is staking, then what is\nthe responsibility that we want everyone to take on? If people end up\nsimply delegating this responsibility because they are lazy, that could\nend up leading to centralization. There are important and deep\nphilosophical questions here. Incorrect answers could lead Ethereum down\na path of centralization and \"re-creating the traditional financial\nsystem with extra steps\"; correct answers could create a shining example\nof a successful ecosystem with a wide and diverse set of solo stakers\nand highly decentralized staking pools. These are questions that touch\non core Ethereum economics and values, and so we need more diverse\nparticipation here.\nHardware requirements of\nnodes\n\nMany of the key questions in Ethereum decentralization end up coming\ndown to a question that has defined blockchain politics for\na decade: how accessible do we want to make running a node,\nand how?\n\nToday, running a node is hard. Most people do not do it. On the\nlaptop that I am using to write this post, I have a reth node, and it takes\nup 2.1 terabytes - already the result of heroic software engineering and\noptimization. I needed to go and buy an extra 4 TB hard drive to put\ninto my laptop in order to store this node. We all want running a node\nto be easier. In my ideal world, people would be able to run nodes on\ntheir phones.\n\nAs I wrote above, EIP-4444 and Verkle trees are two key technologies\nthat get us closer to this ideal. If both are implemented, hardware\nrequirements of a node could plausibly eventually decrease to less than\na hundred gigabytes, and perhaps to near-zero if we eliminate the\nhistory storage responsibility (perhaps only for non-staking nodes)\nentirely. Type 1\nZK-EVMs would remove the need to run EVM computation yourself, as\nyou could instead simply verify a proof that the execution was correct.\nIn my ideal world, we stack all of these technologies together, and even\nEthereum browser extension wallets (eg. Metamask, Rabby) have a built-in\nnode that verifies these proofs, does data availability sampling, and is\nsatisfied that the chain is correct.\n\nThe vision described above is often called \"The\nVerge\".\n\nThis is all known and understood, even by people raising the concerns\nabout Ethereum node size. However, there is an important concern:\nif we are offloading the responsibility to maintain state and\nprovide proofs, then is that not a centralization vector? Even if they\ncan't cheat by providing invalid data, doesn't it still go\nagainst the principles of Ethereum to get too dependent on\nthem?\n\nOne very near-term version of this concern is many people's\ndiscomfort toward EIP-4444: if regular Ethereum nodes no longer need to\nstore old history, then who does? A common answer is: there are\ncertainly enough big actors (eg. block explorers, exchanges, layer 2s)\nwho have the incentive to hold that data, and compared to the 100 petabytes\nstored by the Wayback Machine, the Ethereum chain is tiny. So it's\nridiculous to think that any history will actually be lost.\n\nHowever, this arguments relies on dependence on a small number of\nlarge actors. In my taxonomy\nof trust models, it's a 1-of-N assumption, but the N is pretty\nsmall. This has its tail risks. One thing that we could do instead is to\nstore old history in a peer-to-peer network, where each node\nonly stores a small percentage of the data. This kind of\nnetwork would still do enough copying to ensure robustness: there would\nbe thousands of copies of each piece of data, and in the future we could\nuse erasure coding (realistically, by putting history into EIP-4844-style blobs, which already\nhave erasure coding built in) to increase robustness further.\n\nBlobs have erasure coding within blobs and\nbetween blobs. The easiest way to make ultra-robust storage for\nall of Ethereum's history may well be to just put beacon and\nexecution blocks into blobs. Image\nsource: codex.storage\n\nFor a long time, this work has been on the backburner; Portal Network exists, but\nrealistically it has not gotten the level of attention commensurate with\nits importance in Ethereum's future. Fortunately, there is now strong\ninterest in momentum toward putting far more resources into a minimized\nversion of Portal that focuses on distributed storage, and\naccessibility, of history. This momentum should be built on, and we\nshould make a concerted effort to implement EIP-4444 soon, paired with a\nrobust decentralized peer-to-peer network for storing and retrieving old\nhistory.\n\nFor state and ZK-EVMs, this kind of distributed approach is harder.\nTo build an efficient block, you simply have to have the full state. In\nthis case, I personally favor a pragmatic approach: we define,\nand stick to, some level of hardware requirements needed to have a \"node\nthat does everything\", which is higher than the (ideally\never-decreasing) cost of simply validating the chain, but still low\nenough to be affordable to hobbyists. We rely on a 1-of-N\nassumption, where we ensure that the N is quite large. For example, this\ncould be a high-end consumer laptop.\n\nZK-EVM proving is likely to be the trickiest piece, and real-time\nZK-EVM provers are likely to require considerably beefier hardware than\nan archive node, even with advancements\nlike Binius, and worst-case-bounding with multidimensional\ngas. We could work hard on a distributed proving network,\nwhere each node takes on the responsibility to prove eg. one percent of\na block's execution, and then the block producer only needs to aggregate\nthe hundred proofs at the end. Proof aggregation trees could help\nfurther. But if this doesn't work well, then one other compromise would\nbe to allow the hardware requirements of proving to get higher, but make\nsure that a \"node that does everything\" can verify Ethereum blocks\ndirectly (without a proof), fast enough to effectively participate in\nthe network.\n\n## Conclusions\n\nI think it is actually true that 2021-era Ethereum thought became too\ncomfortable with offloading responsibilities to a small number of\nlarge-scale actors, as long as some kind of market mechanism or zero\nknowledge proof system existed to force the centralized actors to behave\nhonestly. Such systems often work well in the average case, but fail\ncatastrophically in the worst case.\n\nWe're not doing this.\n\nAt the same time, I think it's important to emphasize that current\nEthereum protocol proposals have already significantly moved away from\nthat kind of model, and take the need for a truly decentralized network\nmuch more seriously. Ideas around stateless nodes, MEV mitigations,\nsingle-slot finality, and similar concepts, already are much further in\nthis direction. A year ago, the idea of doing data availability sampling\nby piggy-backing on relays as semi-centralized nodes was seriously\nconsidered. This year, we've moved beyond the need to do such things,\nwith surprisingly robust progress on PeerDAS.\n\nBut there is a lot that we could do to go further in this direction,\non all three axes that I talked about above, as well as many other\nimportant axes. Helios has\nmade great progress in giving Ethereum an \"actual light client\". Now, we\nneed to get it included by default in Ethereum wallets, and\nmake RPC providers provide proofs along with their results so that they\ncan be validated, and extend light client technology to layer 2\nprotocols. If Ethereum is scaling via a rollup-centric roadmap, layer 2s\nneed to get the same security and decentralization guarantees as layer\n1. In a rollup-centric world, there are many other things that we should\nbe taking more seriously; decentralized and efficient cross-L2 bridges\nare one example of many. Many dapps get their logs through centralized\nprotocols, as Ethereum's native log scanning has become too slow. We\ncould improve on this with a dedicated decentralized sub-protocol; here\nis one proposal of mine for how this could be done.\n\nThere is a near-unlimited number of blockchain projects aiming for\nthe niche of \"we can be super-fast, we'll think about decentralization\nlater\". I don't think Ethereum should be one of those projects. Ethereum\nL1 can and certainly should be a strong base layer for layer 2 projects\nthat do take a hyper-scale approach, using Ethereum as a\nbackbone for decentralization and security. Even a layer-2-centric\napproach requires layer 1 itself to have sufficient scalability to\nhandle a significant number of operations. But we should have deep\nrespect for the properties that make Ethereum unique, and continue to\nwork to maintain and improve on those properties as Ethereum scales.",
    "contentLength": 22178,
    "summary": "Ethereum is developing inclusion lists and enshrined account abstraction to reduce builders' power and restore transaction control to individual stakers.",
    "detailedSummary": {
      "theme": "Vitalik addresses concerns about Ethereum's centralization by outlining technical solutions and philosophical shifts to improve permissionlessness and decentralization across MEV, staking, and node operation.",
      "summary": "Vitalik responds to community concerns about Ethereum's growing centralization by examining three key areas: MEV and builder dependence, liquid staking dominance, and increasing node hardware requirements. He argues that while these are valid concerns, many solutions are already in development or can be implemented through realistic roadmap adjustments. For MEV, Vitalik advocates for 'quarantining' MEV into smaller boxes through inclusion lists that give block proposers more power to ensure transaction inclusion, while minimizing builder control. On staking, he identifies the 32 ETH minimum as the primary barrier to solo staking and outlines technical improvements like Verkle trees, EIP-4444, and validator set expansions that could dramatically reduce barriers to entry. For node requirements, he presents a vision where EIP-4444 and Verkle trees reduce storage needs to under 100GB, while distributed networks handle historical data storage, making node operation accessible even on mobile devices. Vitalik acknowledges that 2021-era Ethereum development became too comfortable with centralized actors, but argues that current protocol development has already shifted toward prioritizing decentralization, with proposals like PeerDAS replacing reliance on centralized infrastructure.",
      "takeaways": [
        "MEV can be 'quarantined' through inclusion lists that limit builder power while giving block proposers authority to ensure transaction inclusion",
        "The 32 ETH staking minimum is the primary barrier to solo staking, which technical improvements like Verkle trees and validator set expansion could address",
        "EIP-4444 and Verkle trees could reduce node storage requirements to under 100GB, making node operation accessible on consumer hardware",
        "Distributed peer-to-peer networks should handle historical data storage rather than relying on a small number of large actors",
        "Ethereum development has already shifted away from 2021-era comfort with centralized actors toward prioritizing true decentralization"
      ],
      "controversial": [
        "The proposal to flip the current MEV model by limiting builders to 2.1 million gas while giving inclusion lists the majority of block space",
        "The suggestion that finality time could be increased by 4x to enable a 4x reduction in minimum deposit size",
        "The pragmatic acceptance that some level of hardware requirements for 'nodes that do everything' may need to remain higher than basic validation nodes"
      ]
    }
  },
  {
    "id": "general-2024-05-09-multidim",
    "title": "Multidimensional gas pricing",
    "date": "2024-05-09",
    "category": "protocol",
    "url": "https://vitalik.eth.limo/general/2024/05/09/multidim.html",
    "path": "general/2024/05/09/multidim.html",
    "content": "- \n\n- \n\n  \n    \n\n  \n  \n  Dark Mode Toggle\n  \n    \n\n  \n\n##  Multidimensional gas pricing \n\n 2024 May 09 \nSee all posts\n\n \n \n\n Multidimensional gas pricing \n\nSpecial thanks to Ansgar Dietrichs, Barnabe Monnot and Davide\nCrapis for feedback and review.\n\nIn Ethereum, resources were up until recently limited, and priced,\nusing a single resource called \"gas\". Gas is a measure of the amount of\n\"computational effort\" needed to process a given transaction or block.\nGas merges together multiple types of \"effort\", most notably:\n\n- Raw computation (eg. ADD, MULTIPLY)\n\n- Reading and writing to Ethereum's storage (eg. SSTORE,\nSLOAD, ETH transfers)\n\n- Data bandwidth\n\n- Cost of generating a ZK-SNARK\nproof of the block\n\nFor example, this\ntransaction that I sent cost a total of 47,085 gas. This is split\nbetween (i) a \"base cost\" of 21000 gas, (ii) 1556 gas for the bytes in\nthe calldata included as part of the transaction (iii) 16500 gas for\nreading and writing to storage, (iv) gas 2149 for making a log,\nand the rest for EVM execution. The transaction fee that a user must pay\nis proportional to the gas that the transaction consumes. A block can\ncontain up to a maximum of 30 million gas, and gas prices are constantly\nadjusted via the EIP-1559\ntargeting mechanism, ensuring that on average, blocks contain 15\nmillion gas.\n\nThis approach has one major efficiency: because everything is merged\ninto one virtual resource, it leads to a very simple market design.\nOptimizing a transaction to minimize costs is easy, optimizing a block\nto collect the highest possible fees is relatively easy (not including\nMEV),\nand there are no weird incentives that encourage some transactions to\nbundle with other transactions to save on fees.\n\nBut this approach also has one major inefficiency: it treats\ndifferent resources as being mutually convertible, when the actual\nunderlying limits of what the network can handle are not. One way to\nunderstand this issue is to look at this diagram:\n\nThe gas limit enforces a constraint of \\(x_1 * data + x_2 * computation < N\\).\nThe actual underlying safety constraint is often closer to \\(max(x_1 * data, x_2 * computation) <\nN\\). This discrepancy leads to either the gas limit needlessly\nexcluding actually-safe blocks, or accepting actually-unsafe blocks, or\nsome mixture of both.\n\nIf there are \\(n\\) resources that\nhave distinct safety limits, then one-dimensional gas plausibly reduces\nthroughput by up to a factor of \\(n\\).\nFor this reason, there has for a long time been interest in the concept\nof multi-dimensional gas, and with EIP-4844 we actually have\nmulti-dimensional gas working on Ethereum today. This post explores the\nbenefits of this approach, and the prospects for increasing it\nfurther.\nBlobs: multi-dimensional\ngas in Dencun\n\nAt the start of this year, the average block was 150 kB in size. A large\nfraction of that size is rollup data: layer 2\nprotocols storing data on chain for security. This data was\nexpensive: even though transactions on rollups would cost ~5-10x less\nthan corresponding transactions on the Ethereum L1, even that cost was\ntoo high for many use cases.\n\nWhy not decrease the calldata gas cost (currently 16 gas per nonzero\nbyte and 4 gas per zero byte), to make rollups cheaper? We did this before, we\ncould do it again. The answer here is: the worst-case size of a\nblock was \\(\\frac{30,000,000}{16} =\n1,875,000\\) nonzero bytes, and the network already can barely\nhandle blocks of that size. Reducing costs by another 4x would raise the\nmaximum to 7.5 MB, which would be a huge risk to safety.\n\nThis problem ended up being handled by introducing a separate space\nof rollup-friendly data, known as \"blobs\", into each block. The two\nresources have separate prices and separate limits: after the Dencun\nhard fork, an Ethereum block can contain at most (i) 30 million gas, and\n(ii) 6 blobs, which can contain ~125 kB of calldata each. Both resources\nhave separate prices, adjusted by separate\nEIP-1559-like pricing mechanisms, targeting an average usage of 15\nmillion gas and 3 blobs per block.\n\nAs a result, rollups have become 100x cheaper, transaction volume on\nrollups increased by more than 3x, and the theoretical maximum block\nsize was only increased slightly: from ~1.9 MB to ~2.6 MB.\n\nTransaction fees on rollups, courtesy of growthepie.xyz.\nThe Dencun fork, which introduced blobs with multidimensional pricing,\nhappened on 2024 Mar 13.\n\nMulti-dimensional\ngas and stateless clients\n\nIn the near future, a similar problem will arise regarding storage\nproofs for stateless clients. Stateless clients are a\nnew type of client which will be able to verify the chain without\nstoring much or any data locally. Stateless clients do this by accepting\nproofs of the specific pieces of Ethereum state that transactions in\nthat block need to touch.\n\nA stateless client receives a block, together with\nproofs proving the current values in the specific parts\nof the state (eg. account balances, code, storage) that the block\nexecution touches. This allows a node to verify a block without having\nany storage itself.\n\nA storage read costs 2100-2600 gas depending on the type of read, and\nstorage writes cost more. On average, a block does something like 1000\nstorage reads and writes (including ETH balance checks,\nSSTORE and SLOAD calls, contract code reading,\nand other operations). The theoretical maximum, however, is \\(\\frac{30,000,000}{2,100} = 14,285\\) reads.\nA stateless client's bandwidth load is directly proportional to this\nnumber.\n\nToday, the plan is to support stateless clients by moving Ethereum's\nstate tree design from Merkle\nPatricia trees to Verkle\ntrees. However, Verkle trees are not quantum-resistant, and are not\noptimal for newer waves of STARK proving systems. As a result, many\npeople are interested in supporting stateless clients through binary\nMerkle trees and STARKs\ninstead - either skipping Verkle entirely, or upgrading a couple of\nyears after the Verkle transition once STARKs become more mature.\n\nSTARK proofs of binary hash tree branches have many advantages, but\nthey have the key weakness that proofs take a long time to generate:\nwhile Verkle\ntrees can prove over a hundred\nthousand values per second, hash-based STARKs can typically prove\nonly a couple thousand hashes per second, and proving each value\nrequires a \"branch\" containing many hashes.\n\nGiven the numbers that are being projected today from hyper-optimized\nproof systems such as Binius\nand Plonky3 and\nspecialized hashes like Vision-Mark-32,\nit seems likely that we will for some time be in a regime where it's\npractical to prove 1,000 values in less than a second, but not 14,285\nvalues. Average blocks would be fine, but worst-case blocks, potentially\npublished by an attacker, would break the network.\n\nThe \"default\" way we have handled such a scenario is re-pricing: make\nstorage reading more expensive to reduce the per-block maximum to\nsomething safer. However, we have already done this many times, and it would\nmake too many applications too expensive to do this again. A better\napproach would be multidimensional gas: limit and charge for storage\naccess separately, keeping the average usage at 1,000 storage accesses\nper block but setting a per-block limit of eg. 2,000.\nMultidimensional gas more\ngenerally\n\nOne other resource that is worth thinking about is state size\ngrowth: operations that increase the size of the Ethereum\nstate, which full nodes will need to hold from then on. The unique\nproperty of state size growth is that the rationale from limiting it\ncomes entirely from long-run sustained usage, and not spikes. Hence,\nthere may be value in adding a separate gas dimension for state size\nincreasing operations (eg. zero-to-nonzero SSTORE, contract\ncreation), but with a differnet goal: we could set a floating price to\ntarget a specific average usage, but set no per-block limit at all.\n\nThis shows one of the powerful properties of multidimensional gas:\nit lets us separately ask the questions of (i) what is the ideal\naverage usage, and (ii) what is the safe per-block maximum usage, for\neach resource. Rather than setting gas prices based on\nper-block maximums, and letting average usage follow, we have \\(2n\\) degrees of freedom to set \\(2n\\) parameters, tuning each one based on\nwhat is safe for the network.\n\nMore complicated situations, like where two resources have safety\nconsiderations that are partially additive, could be handled by\nmaking an opcode or resource cost some quantity of multiple types of gas\n(eg. a zero-to-nonzero SSTORE could cost 5000\nstateless-client-proof gas and 20000 storage-expansion gas).\nPer-transaction\nmax: the weaker-but-easier way to get multidimensional gas\n\nLet \\(x_1\\) be the gas cost of data\nand \\(x_2\\) be the gas cost of\ncomputation, so in a one-dimensional gas system we can write the gas\ncost of a transaction:\n\n\\[gas = x_1 * data + x_2 *\ncomputation\\]\n\nIn this scheme, we instead define the gas cost of a transaction\nas:\n\n\\[gas = max(x_1 * data, x_2 *\ncomputation)\\]\n\nThat is, instead of a transaction being charged for data\nplus computation, the transaction gets charged based on which\nof the two resources it consumes more of. This can easily be\nextended to cover more dimensions (eg. \\(max(..., x_3 * storage\\_access)\\)).\n\nIt should be easy to see how this improves throughput while\npreserving safety. The theoretical max amount of data in a block is\nstill \\(\\frac{GASLIMIT}{x_1}\\), exactly\nthe same as in the one-dimensional gas scheme. Similarly, the\ntheoretical max amount of computation is \\(\\frac{GASLIMIT}{x_2}\\), again exactly the\nsame as in the one-dimensional gas scheme. However, the gas cost of any\ntransaction that consumes both data and computation\ndecreases.\n\nThis is approximately the scheme employed in the proposed EIP-7623, to reduce\nmaximum block size while increasing blob count further. The precise\nmechanism in EIP-7623 is slightly more complicated: it keeps the current\ncalldata price of 16 gas per byte, but it adds a \"floor price\" of 48 gas\nper byte; a transaction pays the higher of\n(16 * bytes + execution_gas) and (48 * bytes).\nAs a result, EIP-7623 decreases the theoretical max transaction\ncalldata in a block from ~1.9 MB to ~0.6 MB, while leaving the costs of\nmost applications unchanged. The benefit of this approach is\nthat it is a very small change from the current single-dimensional gas\nscheme, and so it is very easy to implement.\n\nThere are two drawbacks:\n\n- Transactions that are heavy on one resource are still needlessly\ncharged a large amount, even if all the other transactions in\nthe block use little of that resource.\n\n- It creates incentives for data-heavy and computation-heavy\ntransactions to merge together into a bundle to save costs.\n\nI would argue that an EIP-7623-style rule, both for transaction\ncalldata and for other resources, can bring large-enough benefits to be\nworth it even despite these drawbacks. However, if and when we are\nwilling to put in the (significantly higher) development effort, there\nis a more ideal approach.\nMultidimensional\nEIP-1559: the harder-but-ideal strategy\n\nLet us first recap how \"regular\" EIP-1559 works. We will focus on the\nversion that was introduced in EIP-4844 for blobs, because it's\nmathematically more elegant.\n\nWe track a parameter, excess_blobs. During each block,\nwe set:\n\nexcess_blobs <-- max(excess_blobs + len(block.blobs) - TARGET, 0)\n\nWhere TARGET = 3. That is, if a block has more\nblobs than the target, excess_blobs increases, and if a\nblock has less than the target, it decreases. We then set\nblob_basefee = exp(excess_blobs / 25.47), where\nexp is an approximation of the exponential function \\(exp(x) = 2.71828^x\\).\n\nThat is, whenever excess_blobs increases by ~25, the\nblob basefee increases by a factor of ~2.7. If blobs get too expensive,\naverage usage drops, and excess_blobs starts decreasing,\nautomatically dropping the price again. The price of a blob constantly\nadjusts to make sure that on average, blocks are half full - that is,\nthey contain an average of 3 blobs each.\n\nIf there is a short term spike in usage, then the limit\nkicks in: each block can only contain a maximum of 6 blobs, and in such\na circumstance transactions can compete with each other by bidding up\ntheir priority fees. In the normal case, however, each blob only needs\nto pay the blob_basefee plus a tiny extra priority fee as\nan incentive to get included at all.\n\nThis kind of pricing existed in Ethereum for gas for years: a very\nsimilar mechanism was introduced with EIP-1559\nback in 2020. With EIP-4844, we now have two separately floating\nprices for gas and for blobs.\n\nGas base fee over the course of one hour on 2024-05-08, in\ngwei. Source: ultrasound.money.\n\nIn principle, we could add more separately-floating fees for storage\nreading, and other kinds of operations, though with one caveat that I\nwill expand on in the next section.\n\nFor users, the experience is remarkably similar to\ntoday: instead of paying one basefee, you pay two basefees, but your\nwallet can abstract that away from you and just show you the expected\nfee and maximum fee that you can expect to pay.\n\nFor block builders, most of the time the optimal\nstrategy is the same as today: include anything that is valid. Most\nblocks are not full - neither in\ngas nor in blobs. The one\nchallenging case is when there is enough gas or enough blobs to\nexceed the block limit, and the builder needs to potentially solve a multidimensional\nknapsack problem to maximize its profit. However, even there pretty\ngood approximation algorithms exist, and the gains from making\nproprietary algorithms to optimize profits in this case are much smaller\nthan the gains from doing the same with MEV.\n\nFor developers, the main challenge is the need to\nredesign features of the EVM, and its surrounding infrastructure, that\nis designed around one price and one limit today into a design that\naccommodates multiple prices and multiple limits. One issue for\napplication developers is that optimization becomes slightly harder: in\nsome cases, you can no longer unambiguously say that A is more efficient\nthan B, because if A uses more calldata but B uses more execution, then\nA might be cheaper when calldata is cheap, and more expensive when\ncalldata is expensive. However, developers would still be able to get\nreasonably good results by optimizing based on long-run historical\naverage prices.\nMultidimensional\npricing, the EVM and sub-calls\n\nThere is one problem that did not appear with blobs, and will not\nappear with EIP-7623 or even a \"full\" multidimensional pricing\nimplementation for calldata, but will appear if we try to separately\nprice state accesses, or any other resource: gas limits in\nsub-calls.\n\nGas limits in the EVM exist in two places. First, each transaction\nsets a gas limit, which caps the total amount of gas that can be used in\nthat transaction. Second, when a contract calls another contract, the\ncall can set its own gas limit. This allows contracts to call other\ncontracts that they do not trust, and still guarantee that they will\nhave gas left over to perform other computations after that call.\n\nA trace of an account abstraction transaction, where an\naccount calls another account, and only gives the callee a limited\namount of gas, to ensure that the outer call can keep running even if\nthe callee consumes the entire gas that was assigned to\nit.\n\nThe challenge is: making gas multidimensional between\ndifferent types of execution seems like it would require\nsub-calls to provide multiple limits for each type of gas, which would\nrequire a really deep change to the EVM, and would not be compatible\nwith existing applications.\n\nThis is one reason why multidimensional gas proposals often stop at\ntwo dimensions: data and execution. Data (whether transaction calldata\nor blobs) is only assigned outside the EVM, and so nothing inside the\nEVM needs to change to make calldata or blobs separately priced.\n\nWe can think of an \"EIP-7623-style solution\" to this\nproblem. Here is one simple implementation: during execution, charge 4x\nmore for storage operations; to simplify the analysis, let's say\n10000 gas per storage operation. At the end of the\ntransaction, refund\nmin(7500 * storage_operations, execution_gas). The result\nwould be that, after subtracting out the refund, a user is charged:\n\nexecution_gas + 10000 * storage_operations - min(7500 * storage_operations, execution_gas)\n\nWhich equals:\n\nmax(execution_gas + 2500 * storage_operations, 10000 * storage_operations)\n\nThis mirrors the structure of EIP-7623. Another way to do it is to\ntrack storage_operations and execution_gas in\nreal time, and charge either 2500 or 10000 depending on how much\nmax(execution_gas + 2500 * storage_operations, 10000 * storage_operations)\ngoes up at the time that the opcode is called. This avoids the need for\ntransactions to over-allocate gas that they will mostly get back through\nrefunds.\n\nWe don't get fine-grained permissioning for sub-calls: a sub-call\ncould consume all of a transaction's \"allowance\" for cheap\nstorage operations. But we do get something good enough, where a\ncontract making a sub-call can set a limit and ensure that once the\nsub-call finishes executing, the main call still has enough gas to do\nwhatever post-processing it needs to do.\n\nThe easiest \"full multidimensional pricing solution\"\nthat I can think of is: we treat sub-call gas limits as being\nproportional. That is, suppose that there are \\(k\\) different types of execution, and each\ntransaction sets a multi-dimensional limit \\(L_1 ... L_k\\). Suppose that, at the current\npoint in execution, the remaining gas is \\(g_1\n... g_k\\). Suppose that a CALL opcode is called,\nwith sub-call gas limit \\(S\\). Let\n\\(s_1 = S\\), and then \\(s_2 = \\frac{s_1}{g_1} * g_2\\), \\(s_3 = \\frac{s_1}{g_1} * g_3\\), and so\non.\n\nThat is, we treat the first type of gas (realistically, VM execution)\nas being a kind of privileged \"unit of account\", and then assign the\nother types of gas so that the sub-call gets the same percentage of\navailable gas across each type. This is somewhat ugly, but it\nmaximizes backwards-compatibility. If we want to make the scheme more\n\"neutral\" between different types of gas, at the cost of sacrificing\nbackwards-compatibility, we could simply have the sub-call gas limit\nparameter represent a fraction (eg. [1...63] / 64) of the\nremaining gas in the current context).\n\nIn either case, however, it's worth stressing that once you start\nintroducing multidimensional execution gas, the inherent level of\nugliness increases, and this seems difficult to avoid. Hence, our task\nis to make a complicated tradeoff: do we accept somewhat more ugliness\nat the EVM level, in order to safely unlock significant L1 scalability\ngains, and if so, which specific proposal works best for protocol\neconomics and application developers? Quite likely, it is neither of the\nones I mentioned above, and there is still room to come up with\nsomething more elegant and better.",
    "contentLength": 18951,
    "summary": "Ethereum switched from single-dimensional gas to multidimensional pricing with EIP-4844 blobs, making rollups 100x cheaper while maintaining safety.",
    "detailedSummary": {
      "theme": "Vitalik proposes expanding Ethereum's gas pricing from a single dimension to multiple dimensions to better match actual network resource constraints and improve efficiency.",
      "summary": "Vitalik argues that Ethereum's current single-dimensional gas system, which combines computation, storage, bandwidth, and other resources into one metric, creates inefficiencies because it treats different resources as mutually convertible when they actually have distinct safety limits. He explains how EIP-4844's introduction of separate blob pricing for rollup data demonstrates successful multidimensional gas pricing, making rollups 100x cheaper while maintaining network safety. Vitalik explores future applications including separate pricing for storage access (needed for stateless clients using STARK proofs) and state size growth, noting that multidimensional pricing allows independent optimization of average usage targets and maximum safety limits for each resource. He discusses implementation approaches ranging from simpler 'per-transaction max' schemes (like EIP-7623) to more complex but ideal multidimensional EIP-1559 mechanisms, while acknowledging the technical challenges around EVM sub-call gas limits that would require careful design to maintain backwards compatibility.",
      "takeaways": [
        "Single-dimensional gas pricing can reduce network throughput by up to a factor of n (where n is the number of distinct resources) because it enforces linear constraints when actual safety limits are often based on maximum resource usage",
        "EIP-4844's blob pricing successfully demonstrated multidimensional gas, making rollups 100x cheaper and increasing transaction volume 3x while only slightly increasing maximum block size",
        "Future implementations could separately price storage access (crucial for stateless clients using STARK proofs) and state size growth, with different optimization goals for each resource",
        "Implementation options range from simpler 'max()' pricing schemes (EIP-7623 style) that are backwards compatible to full multidimensional EIP-1559 with separate floating prices for each resource type",
        "The main technical challenge is handling sub-call gas limits in the EVM when multiple gas dimensions are involved, requiring careful design to maintain application compatibility"
      ],
      "controversial": [
        "Vitalik's proposal to modify core EVM mechanics around sub-call gas limits could break backwards compatibility with existing applications and significantly increase implementation complexity",
        "The suggestion that increased 'ugliness' at the EVM level may be worth accepting for scalability gains represents a fundamental tradeoff between technical elegance and performance that may divide the community"
      ]
    }
  },
  {
    "id": "general-2024-04-29-binius",
    "title": "Binius: highly efficient proofs over binary fields",
    "date": "2024-04-29",
    "category": "math",
    "url": "https://vitalik.eth.limo/general/2024/04/29/binius.html",
    "path": "general/2024/04/29/binius.html",
    "content": "- \n\n- \n\n  \n    \n\n  \n  \n  Dark Mode Toggle\n  \n    \n\n  \n\n##  Binius: highly efficient proofs over binary fields \n\n 2024 Apr 29 \nSee all posts\n\n \n \n\n Binius: highly efficient proofs over binary fields \n\nThis post is primarily intended for readers roughly familiar with\n2019-era cryptography, especially SNARKs\nand STARKs.\nIf you are not, I recommend reading those articles first. Special thanks\nto Justin Drake, Jim Posen, Benjamin Diamond and Radi Cojbasic for\nfeedback and review.\n\nOver the past two years, STARKs\nhave become a crucial and irreplaceable technology for efficiently\nmaking easy-to-verify\ncryptographic proofs of very complicated statements (eg. proving\nthat an Ethereum block is valid). A key reason why is small field\nsizes: whereas elliptic curve-based SNARKs require you to work over\n256-bit integers in order to be secure enough, STARKs let you use much\nsmaller field sizes, which are more efficient: first the\nGoldilocks field (64-bit integers), and then Mersenne31\nand BabyBear (both 31-bit). Thanks to these efficiency gains,\nPlonky2, which uses Goldilocks, is hundreds of\ntimes faster at proving many kinds of computation than its\npredecessors.\n\nA natural question to ask is: can we take this trend to its logical\nconclusion, building proof systems that run even faster by operating\ndirectly over zeroes and ones? This is exactly what Binius is trying to do,\nusing a number of mathematical tricks that make it very\ndifferent from the SNARKs\nand STARKs\nof three years ago. This post goes through the reasons why small fields\nmake proof generation more efficient, why binary fields are uniquely\npowerful, and the tricks that Binius uses to make proofs over binary\nfields work so effectively.\n\n Binius. By the end of this post, you should be able to\nunderstand every part of this diagram.\n\n## Table of contents\n\n- Recap: finite fields\n\n- Recap: arithmetization\n\n- Plonky2: from 256-bit SNARKs and STARKs to\n64-bit... only STARKs\n\n- From small primes to binary\n\n- From univariate polynomials to\nhypercubes\n\n- Simple Binius - an example\n\n- Binary fields\n\n- Full Binius\n\n- Putting it all together\n\n- What did we not cover?\n\n## Recap: finite fields\n\nOne of the key tasks of a cryptographic proving system is to operate\nover huge amounts of data, while keeping the numbers small. If you can\ncompress a statement about a large program into a mathematical equation\ninvolving a few numbers, but those numbers are as big as the original\nprogram, you have not gained anything.\n\nTo do complicated arithmetic while keeping numbers small,\ncryptographers generally use modular arithmetic. We\npick some prime \"modulus\" p. The % operator means \"take the\nremainder of\": \\(15\\ \\%\\ 7 = 1\\), \\(53\\ \\%\\ 10 = 3\\), etc (note that the answer\nis always non-negative, so for example \\(-1\\\n\\%\\ 10 = 9\\)).\n\nYou've probably already seen modular\narithmetic, in the context of adding and subtracting time (eg. what\ntime is four hours after 9:00?). But here, we don't just add and\nsubtract modulo some number, we also multiply, divide and take\nexponents.\n\nWe redefine:\n\n\\(x + y \\Rightarrow (x + y)\\) %\n\\(p\\)\n\n\\(x * y \\Rightarrow (x * y)\\) %\n\\(p\\)\n\n\\(x^y \\Rightarrow (x^y)\\) % \\(p\\)\n\n\\(x - y \\Rightarrow (x - y)\\) %\n\\(p\\)\n\n\\(x / y \\Rightarrow (x * y ^{p-2})\\)\n% \\(p\\)\n\nThe above rules are all self-consistent. For example, if \\(p = 7\\), then:\n\n- \\(5 + 3 = 1\\) (because \\(8\\) % \\(7 =\n1\\))\n\n- \\(1 - 3 = 5\\) (because \\(-2\\) % \\(7 =\n5\\))\n\n- \\(2 \\cdot 5 = 3\\)\n\n- \\(3 / 5 = 2\\) (because (\\(3 \\cdot 5^5\\)) % \\(7 = 9375\\) % \\(7\n= 2\\))\n\nA more general term for this kind of structure is a finite\nfield. A finite field is a\nmathematical structure that obeys the usual laws of arithmetic, but\nwhere there's a limited number of possible values, and so each value can\nbe represented in a fixed size.\n\nModular arithmetic (or prime fields) is the most\ncommon type of finite field, but there is also another type:\nextension fields. You've probably already seen an\nextension field before: the complex numbers. We \"imagine\" a new element,\nwhich we label \\(i\\), and declare that\nit satisfies \\(i^2 = -1\\). You can then\ntake any combination of regular numbers and \\(i\\), and do math with it: \\((3i+2) * (2i + 4) =\\) \\(6i^2 + 12i + 4i + 8 = 16i + 2\\). We can\nsimilarly take extensions of prime fields. As we start working over\nfields that are smaller, extensions of prime fields become increasingly\nimportant for preserving security, and binary fields (which Binius uses)\ndepend on extensions entirely to have practical utility.\n\n## Recap: arithmetization\n\nThe way that SNARKs and STARKs prove things about computer programs\nis through arithmetization: you convert a statement\nabout a program that you want to prove, into a mathematical equation\ninvolving polynomials. A valid solution to the equation corresponds to a\nvalid execution of the program.\n\nTo give a simple example, suppose that I computed the 100'th\nFibonacci number, and I want to prove to you what it is. I create a\npolynomial \\(F\\) that encodes Fibonacci\nnumbers: so \\(F(0) = F(1) = 1\\), \\(F(2) = 2\\), \\(F(3) = 3\\), \\(F(4) = 5\\), and so on for 100 steps. The\ncondition that I need to prove is that \\(F(x+2) = F(x) + F(x+1)\\) across the range\n\\(x = \\{0, 1 ... 98\\}\\). I can convince\nyou of this by giving you the quotient:\n\n\\[H(x) = \\frac{F(x+2) - F(x+1) -\nF(x)}{Z(x)}\\]\n\nWhere \\(Z(x) = (x - 0) * (x - 1) * ... * (x\n- 98)\\). If I can provide valid \\(F\\) and \\(H\\) that satisfy this equation, then \\(F\\) must satisfy \\(F(x+2) - F(x+1) - F(x)\\) across that range.\nIf I additionally verify that \\(F\\)\nsatisfies \\(F(0) = F(1) = 1\\), then\n\\(F(100)\\) must actually be the 100th\nFibonacci number.\n\nIf you want to prove something more complicated, then you replace the\n\"simple\" relation \\(F(x+2) = F(x) +\nF(x+1)\\) with a more complicated equation, which basically says\n\"\\(F(x+1)\\) is the output of\ninitializing a virtual machine with the state \\(F(x)\\), and running one computational\nstep\". You can also replace the number 100 with a bigger number, eg.\n100000000, to accommodate more steps.\n\nAll SNARKs and STARKs are based on this idea of using a simple\nequation over polynomials (or sometimes vectors and matrices) to\nrepresent a large number of relationships between individual values. Not\nall involve checking equivalence between adjacent computational steps in\nthe same way as above: PLONK\ndoes not, for example, and neither does R1CS. But many of the most\nefficient ones do, because enforcing the same check (or the same few\nchecks) many times makes it easier to minimize overhead.\n\nPlonky2:\nfrom 256-bit SNARKs and STARKs to 64-bit... only STARKs\n\nFive years ago, a reasonable summary of the different types of zero\nknowledge proof was as follows. There are two types of proofs:\n(elliptic-curve-based) SNARKs and (hash-based) STARKs. Technically,\nSTARKs are a type of SNARK, but in practice it's common to use \"SNARK\"\nto refer to only the elliptic-curve-based variety, and \"STARK\" to refer\nto hash-based constructions. SNARKs are small, and so you can verify\nthem very quickly and fit them onchain easily. STARKs are big, but they\ndon't require trusted\nsetups, and they are quantum-resistant.\n\nSTARKs work by treating the data as a polynomial, computing\nevaluations of that polynomial across a large number of points, and\nusing the Merkle root of that extended data as the \"polynomial\ncommitment\"\n\nA key bit of history here is that elliptic curve-based SNARKs came\ninto widespread use first: it took until roughly 2018 for STARKs to\nbecome efficient enough to use, thanks to FRI, and by then\nZcash had already been running for over a\nyear. Elliptic curve-based SNARKs have a key limitation: if you want to\nuse elliptic curve-based SNARKs, then the arithmetic in these equations\nmust be done with integers modulo the number of points on the elliptic\ncurve. This is a big number, usually near \\(2^{256}\\): for example, for the bn128\ncurve, it's\n21888242871839275222246405745257275088548364400416034343698204186575808495617.\nBut the actual computation is using small numbers: if you think about a\n\"real\" program in your favorite language, most of the stuff it's working\nwith is counters, indices in for loops, positions in the program,\nindividual bits representing True or False, and other things that will\nalmost always be only a few digits long.\n\nEven if your \"original\" data is made up of \"small\" numbers, the\nproving process requires computing quotients, extensions, random linear\ncombinations, and other transformations of the data, which lead to an\nequal or larger number of objects that are, on average, as large as the\nfull size of your field. This creates a key inefficiency: to prove a\ncomputation over n small values, you have to do even more\ncomputation over n much bigger values. At first, STARKs\ninherited the habit of using 256-bit fields from SNARKs, and so suffered\nthe same inefficiency.\n\nA Reed-Solomon extension of some polynomial evaluations.\nEven though the original values are small, the extra values all blow up\nto the full size of the field (in this case \\(2^{31} - 1\\)).\n\nIn 2022, Plonky2 was released. Plonky2's main innovation was doing\narithmetic modulo a smaller prime: \\(2^{64} -\n2^{32} + 1 = 18446744069414584321\\). Now, each addition or\nmultiplication can always be done in just a few instructions on a CPU,\nand hashing all of the data together is 4x faster than before. But this\ncomes with a catch: this approach is STARK-only. If you try to use a\nSNARK, with an elliptic curve of such a small size, the elliptic curve\nbecomes insecure.\n\nTo continue to be safe, Plonky2 also needed to introduce\nextension fields. A key technique in checking arithmetic\nequations is \"sampling at a random point\": if you want to check if \\(H(x) * Z(x)\\) actually equals \\(F(x+2) - F(x+1) - F(x)\\), you can pick some\nrandom coordinate \\(r\\), provide\npolynomial commitment opening proofs proving \\(H(r)\\), \\(Z(r)\\), \\(F(r)\\), \\(F(r+1)\\) and \\(F(r+2)\\), and then actually check if \\(H(r) * Z(r)\\) equals \\(F(r+2) - F(r+1) - F(r)\\). If the attacker\ncan guess the coordinate ahead of time, the attacker can trick the proof\nsystem - hence why it must be random. But this also means that the\ncoordinate must be sampled from a set large enough that the attacker\ncannot guess it by random chance. If the modulus is near \\(2^{256}\\), this is clearly the case. But\nwith a modulus of \\(2^{64} - 2^{32} +\n1\\), we're not quite there, and if we drop to \\(2^{31} - 1\\), it's definitely not\nthe case. Trying to fake a proof two billion times until one gets lucky\nis absolutely within the range of an attacker's capabilities.\n\nTo stop this, we sample \\(r\\) from\nan extension field. For example, you can define \\(y\\) where \\(y^3 =\n5\\), and take combinations of \\(1\\), \\(y\\)\nand \\(y^2\\). This increases the total\nnumber of coordinates back up to roughly \\(2^{93}\\). The bulk of the polynomials\ncomputed by the prover don't go into this extension field; they just use\nintegers modulo \\(2^{31}-1\\), and so\nyou still get all the efficiencies from using the small field. But the\nrandom point check, and the FRI computation, does dive into this larger\nfield, in order to get the needed security.\n\n## From small primes to binary\n\nComputers do arithmetic by representing larger numbers as sequences\nof zeroes and ones, and building \"circuits\" on top of those bits to\ncompute things like addition and multiplication. Computers are\nparticularly optimized for doing computation with 16-bit, 32-bit and\n64-bit integers. Moduluses like \\(2^{64} -\n2^{32} + 1\\) and \\(2^{31} - 1\\)\nare chosen not just because they fit within those bounds, but also\nbecause they align well with those bounds: you can do\nmultiplication modulo \\(2^{64} - 2^{32} +\n1\\) by doing regular 32-bit multiplication, and shift and copy\nthe outputs bitwise in a few places; this article explains\nsome of the tricks well.\n\nWhat would be even better, however, is doing computation in binary\ndirectly. What if addition could be \"just\" XOR, with no need to worry\nabout \"carrying\" the overflow from adding 1 + 1 in one bit position to\nthe next bit position? What if multiplication could be more\nparallelizable in the same way? And these advantages would all come\non top of being able to represent True/False values with just\none bit.\n\nCapturing these advantages of doing binary computation directly is\nexactly what Binius is trying to do. A table from the Binius\nteam's zkSummit presentation shows the efficiency gains:\n\nDespite being roughly the same \"size\", a 32-bit binary field\noperation takes 5x less computational resources than an operation over\nthe 31-bit Mersenne field.\n\nFrom univariate\npolynomials to hypercubes\n\nSuppose that we are convinced by this reasoning, and want to do\neverything over bits (zeroes and ones). How do we actually commit to a\npolynomial representing a billion bits?\n\nHere, we face two practical problems:\n\n- For a polynomial to represent a lot of values, those values need to\nbe accessible at evaluations of the polynomial: in our Fibonacci example\nabove, \\(F(0)\\), \\(F(1)\\) ... \\(F(100)\\), and in a bigger computation, the\nindices would go into the millions. And the field that we use needs to\ncontain numbers going up to that size.\n\n- Proving anything about a value that we're committing to in a Merkle\ntree (as all STARKs do) requires Reed-Solomon encoding it: extending\n\\(n\\) values into eg. \\(8n\\) values, using the redundancy to\nprevent a malicious prover from cheating by faking one value in the\nmiddle of the computation. This also requires having a large enough\nfield: to extend a million values to 8 million, you need 8 million\ndifferent points at which to evaluate the polynomial.\n\nA key idea in Binius is solving these two problems separately, and\ndoing so by representing the same data in two different ways. First, the\npolynomial itself. Elliptic curve-based SNARKs, 2019-era STARKs, Plonky2\nand other systems generally deal with polynomials over one\nvariable: \\(F(x)\\). Binius, on the\nother hand, takes inspiration from the Spartan protocol, and\nworks with multivariate polynomials: \\(F(x_1, x_2 ... x_k)\\). In fact, we\nrepresent the entire computational trace on the \"hypercube\" of\nevaluations where each \\(x_i\\) is\neither 0 or 1. For example, if we wanted to represent a sequence of\nFibonacci numbers, and we were still using a field large enough to\nrepresent them, we might visualize the first sixteen of them as being\nsomething like this:\n\nThat is, \\(F(0,0,0,0)\\) would be 1,\n\\(F(1,0,0,0)\\) would also be 1, \\(F(0,1,0,0)\\) would be 2, and so forth, up\nuntil we get to \\(F(1,1,1,1) = 987\\).\nGiven such a hypercube of evaluations, there is exactly one multilinear\n(degree-1 in each variable) polynomial that produces those evaluations.\nSo we can think of that set of evaluations as representing the\npolynomial; we never actually need to bother computing the\ncoefficients.\n\nThis example is of course just for illustration: in practice, the\nwhole point of going to a hypercube is to let us work with individual\nbits. The \"Binius-native\" way to count Fibonacci numbers would be to use\na higher-dimensional cube, using each set of eg. 16 bits to store a\nnumber. This requires some cleverness to implement integer addition on\ntop of the bits, but with Binius it's not too difficult.\n\nNow, we get to the erasure coding. The way STARKs work is: you take\n\\(n\\) values, Reed-Solomon extend them\nto a larger number of values (often \\(8n\\), usually between \\(2n\\) and \\(32n\\)), and then randomly select some\nMerkle branches from the extension and perform some kind of check on\nthem. A hypercube has length 2 in each dimension. Hence, it's not\npractical to extend it directly: there's not enough \"space\" to sample\nMerkle branches from 16 values. So what do we do instead? We pretend the\nhypercube is a square!\n\n## Simple Binius - an example\n\nSee here\nfor a python implementation of this protocol.\n\nLet's go through an example, using regular integers as our field for\nconvenience (in a real implementation this will be binary field\nelements). First, we take the hypercube we want to commit to, and encode\nit as a square:\n\nNow, we Reed-Solomon extend the square. That is, we treat each row as\nbeing a degree-3 polynomial evaluated at x = {0, 1, 2, 3},\nand evaluate the same polynomial at x = {4, 5, 6, 7}:\n\nNotice that the numbers blow up quickly! This is why in a real\nimplementation, we always use a finite field for this, instead of\nregular integers: if we used integers modulo 11, for example, the\nextension of the first row would just be [3, 10, 0, 6].\n\nIf you want to play around with extending and verify the numbers here\nfor yourself, you can use my\nsimple Reed-Solomon extension code here.\n\nNext, we treat this extension as columns, and make a Merkle\ntree of the columns. The root of the Merkle tree is our commitment.\n\nNow, let's suppose that the prover wants to prove an evaluation of\nthis polynomial at some point \\(r = \\{r_0,\nr_1, r_2, r_3\\}\\). There is one nuance in Binius that makes it\nsomewhat weaker than other polynomial commitment schemes: the prover\nshould not know, or be able to guess, \\(s\\), until after they committed to the\nMerkle root (in other words, \\(r\\)\nshould be a pseudo-random value that depends on the Merkle root). This\nmakes the scheme useless for \"database lookup\" (eg. \"ok you gave me the\nMerkle root, now prove to me \\(P(0, 0, 1,\n0)\\)!\"). But the actual zero-knowledge proof protocols that we\nuse generally don't need \"database lookup\"; they simply need to check\nthe polynomial at a random evaluation point. Hence, this restriction is\nokay for our purposes.\n\nSuppose we pick \\(r = \\{1, 2, 3,\n4\\}\\) (the polynomial, at this point, evaluates to \\(-137\\); you can confirm it with\nthis code). Now, we get into the process of actually making the\nproof. We split up \\(r\\) into two\nparts: the first part \\(\\{1, 2\\}\\)\nrepresenting a linear combination of columns within a row, and\nthe second part \\(\\{3, 4\\}\\)\nrepresenting a linear combination of rows. We compute a \"tensor\nproduct\", both for the column part:\n\n\\[\\bigotimes_{i=0}^1 (1 - r_i,\nr_i)\\]\n\nAnd for the row part:\n\n\\[\\bigotimes_{i=2}^3 (1 - r_i,\nr_i)\\]\n\nWhat this means is: a list of all possible products of one value from\neach set. In the row case, we get:\n\n\\[[(1 - r_2) * (1 - r_3), r_2 * (1 - r_3),\n(1 - r_2) * r_3, r_2 * r_3]\\]\n\nUsing \\(r = \\{1, 2, 3, 4\\}\\) (so\n\\(r_2 = 3\\) and \\(r_3 = 4\\)):\n\n\\[\n[(1 - 3) * (1 - 4), 3 * (1 - 4), (1 - 3) * 4, 3 * 4] \\\\\n= [6, -9, -8, 12]\\]\n\nNow, we compute a new \"row\" \\(t'\\), by taking this linear combination\nof the existing rows. That is, we take:\n\n\\[\\begin{matrix}[3, 1, 4, 1] * 6\\ + \\\\\n[5, 9, 2, 6] * (-9)\\ + \\\\\n[5, 3, 5, 8] * (-8)\\ + \\\\\n[9, 7, 9, 3] * 12 = \\\\\n[41, -15, 74, -76]\n\\end{matrix}\\]\n\nYou can view what's going on here as a partial\nevaluation. If we were to multiply the full tensor product\n\\(\\bigotimes_{i=0}^3 (1 - r_i, r_i)\\)\nby the full vector of all values, you would get the evaluation \\(P(1, 2, 3, 4) = -137\\). Here we're\nmultiplying a partial tensor product that only uses\nhalf the evaluation coordinates, and we're reducing a grid of\n\\(N\\) values to a row of \\(\\sqrt{N}\\) values. If you give this row to\nsomeone else, they can use the tensor product of the other half\nof the evaluation coordinates to complete the rest of the\ncomputation.\n\nThe prover provides the verifier with this new row, \\(t'\\), as well as the Merkle proofs of\nsome randomly sampled columns. This is \\(O(\\sqrt{N})\\) data. In our illustrative\nexample, we'll have the prover provide just the last column; in real\nlife, the prover would need to provide a few dozen columns to achieve\nadequate security.\n\nNow, we take advantage of the linearity of Reed-Solomon codes. The\nkey property that we use is: taking a linear combination of a\nReed-Solomon extension gives the same result as a Reed-Solomon extension\nof a linear combination. This kind of \"order independence\"\noften happens when you have two operations that are both linear.\n\nThe verifier does exactly this. They compute the extension of \\(t'\\), and they compute the same linear\ncombination of columns that the prover computed before (but only to the\ncolumns provided by the prover), and verify that these two procedures\ngive the same answer.\n\nIn this case, extending \\(t'\\),\nand computing the same linear combination (\\([6, -9, -8, 12]\\)) of the column, both give\nthe same answer: \\(-10746\\). This\nproves that the Merkle root was constructed \"in good faith\" (or it at\nleast \"close enough\"), and it matches \\(t'\\): at least the great majority of\nthe columns are compatible with each other and with \\(t'\\).\n\nBut the verifier still needs to check one more thing: actually check\nthe evaluation of the polynomial at \\(\\{r_0 ..\nr_3\\}\\). So far, none of the verifier's steps actually depended\non the value that the prover claimed. So here is how we do that check.\nWe take the tensor product of what we labelled as the \"column part\" of\nthe evaluation point:\n\n\\[\\bigotimes_{i=0}^1 (1 - r_i,\nr_i)\\]\n\nIn our example, where \\(r = \\{1, 2, 3,\n4\\}\\) (so the half that chooses the column is \\(\\{1, 2\\}\\)), this equals:\n\n\\[\n[(1 - 1) * (1 - 2), 1 * (1 - 2), (1 - 1) * 2, 1 * 2] \\\\\n= [0, -1, 0, 2]\\]\n\nSo now we take this linear combination of \\(t'\\):\n\n\\[\n0 * 41 + (-1) * (-15) + 0 * 74 + 2 * (-76) = -137\n\\]\n\nWhich exactly equals the answer you get if you evaluate the\npolynomial directly.\n\nThe above is pretty close to a complete description of the \"simple\"\nBinius protocol. This already has some interesting advantages: for\nexample, because the data is split into rows and columns, you only need\na field half the size. But this doesn't come close to realizing the full\nbenefits of doing computation in binary. For this, we will need the full\nBinius protocol. But first, let's get a deeper understanding of binary\nfields.\n\n## Binary fields\n\nThe smallest possible field is arithmetic modulo 2, which is so small\nthat we can write out its addition and multiplication tables:\n\n+\n0\n1\n\n0\n0\n1\n\n1\n1\n0\n\n*\n0\n1\n\n0\n0\n0\n\n1\n0\n1\n\nWe can make larger binary fields by taking extensions: if we start\nwith \\(F_2\\) (integers modulo 2) and\nthen define \\(x\\) where \\(x^2 = x + 1\\), we get the following\naddition and multiplication tables:\n\n+\n0\n1\nx\nx+1\n\n0\n0\n1\nx\nx+1\n\n1\n1\n0\nx+1\nx\n\nx\nx\nx+1\n0\n1\n\nx+1\nx+1\nx\n1\n0\n\n+\n0\n1\nx\nx+1\n\n0\n0\n0\n0\n0\n\n1\n0\n1\nx\nx+1\n\nx\n0\nx\nx+1\n1\n\nx+1\n0\nx+1\n1\nx\n\nIt turns out that we can expand the binary field to arbitrarily large\nsizes by repeating this construction. Unlike with complex numbers over\nreals, where you can add one new element \\(i\\), but you can't add any more (quaternions do\nexist, but they're mathematically weird, eg. \\(ab \\neq ba\\)), with finite fields you can\nkeep adding new extensions forever. Specifically, we define elements as\nfollows:\n\n- \\(x_0\\) satisfies \\(x_0^2 = x_0 + 1\\)\n\n- \\(x_1\\) satisfies \\(x_1^2 = x_1x_0 + 1\\)\n\n- \\(x_2\\) satisfies \\(x_2^2 = x_2x_1 + 1\\)\n\n- \\(x_3\\) satisfies \\(x_3^2 = x_3x_2 + 1\\)\n\nAnd so on. This is often called the tower\nconstruction, because of how each successive extension can be\nviewed as adding a new layer to a tower. This is not the only way to\nconstruct binary fields of arbitary size, but it has some unique\nadvantages that Binius takes advantage of.\n\nWe can represent these numbers as a list of bits, eg. \\(\\texttt{1100101010001111}\\). The first bit\nrepresents multiples of 1, the second bit represents multiples of \\(x_0\\), then subsequent bits represent\nmultiples of: \\(x_1\\), \\(x_1 * x_0\\), \\(x_2\\), \\(x_2 *\nx_0\\), and so forth. This encoding is nice because you can\ndecompose it:\n\n\\(\\texttt{1100101010001111} =\n\\texttt{11001010} + \\texttt{10001111} * x_3\\) \\(= \\texttt{1100} + \\texttt{1010} * x_2 +\n\\texttt{1000} * x_3 + \\texttt{1111} * x_2x_3\\) \\(= \\texttt{11} + \\texttt{10} * x_2 + \\texttt{10} *\nx_2x_1 + \\texttt{10} * x_3 + \\texttt{11} * x_2x_3 + \\texttt{11} *\nx_1x_2x_3\\) \\(= 1 + x_0 + x_2 + x_2x_1\n+ x_3 + x_2x_3 + x_0x_2x_3 + x_1x_2x_3 + x_0x_1x_2x_3\\)\n\nThis is a relatively uncommon notation, but I like representing\nbinary field elements as integers, taking the bit representation where\nmore-significant bits are to the right. That is, \\(\\texttt{1} = 1\\), \\(x_0 = \\texttt{01} = 2\\), \\(1 + x_0 = \\texttt{11} = 3\\), \\(1 + x_0 + x_2 = \\texttt{11001000} = 19\\),\nand so forth. \\(\\texttt{1100101010001111}\\) is, in this\nrepresentation, 61779.\n\nAddition in binary fields is just XOR (and, incidentally, so is\nsubtraction); note that this implies that \\(x\n+ x = 0\\) for any \\(x\\). To\nmultiply two elements \\(x * y\\),\nthere's a pretty simple recursive algorithm: split each number into two\nhalves:\n\n\\(x = L_x + R_x * x_k\\) \\(y = L_y + R_y * x_k\\)\n\nThen, split up the multiplication:\n\n\\(x * y = (L_x * L_y) + (L_x * R_y) * x_k +\n(R_x * L_y) * x_k + (R_x * R_y) * x_k^2\\)\n\nThe last piece is the only slightly tricky one, because you have to\napply the reduction rule, and replace \\(R_x *\nR_y * x_k^2\\) with \\(R_x * R_y *\n(x_{k-1} * x_k + 1)\\). There are more efficient ways to do\nmultiplication, analogues of the Karatsuba\nalgorithm and fast Fourier\ntransforms, but I will leave it as an exercise to the interested\nreader to figure those out.\n\nDivision in binary fields is done by combining multiplication and\ninversion: \\(\\frac{3}{5} = 3 *\n\\frac{1}{5}\\). The \"simple but slow\" way to do inversion is an\napplication of generalized Fermat's\nlittle theorem: \\(\\frac{1}{x} =\nx^{2^{2^k}-2}\\) for any \\(k\\)\nwhere \\(2^{2^k} > x\\). In this case,\n\\(\\frac{1}{5} = 5^{14} = 14\\), and so\n\\(\\frac{3}{5} = 3 * 14 = 9\\). There is\nalso a more complicated but more efficient inversion algorithm, which\nyou can find here. You can use\nthe\ncode here to play around with binary field addition, multiplication\nand division yourself.\n\nLeft: addition table for four-bit binary field elements\n(ie. elements made up only of combinations of \\(1\\), \\(x_0\\), \\(x_1\\) and \\(x_0x_1\\)). Right: multiplication table for\nfour-bit binary field elements.\n\nThe beautiful thing about this type of binary field is that it\ncombines some of the best parts of \"regular\" integers and modular\narithmetic. Like regular integers, binary field elements are unbounded:\nyou can keep extending as far as you want. But like modular arithmetic,\nif you do operations over values within a certain size limit, all of\nyour answers also stay within the same bound. For example, if you take\nsuccessive powers of \\(42\\), you\nget:\n\n\\[1, 42, 199, 215, 245, 249, 180,\n91...\\]\n\nAnd after 255 steps, you get right back to \\(42^{255} = 1\\). And like both\nregular integers and modular arithmetic, they obey the usual laws of\nmathematics: \\(a*b = b*a\\), \\(a * (b+c) = a*b + a*c\\), and even some\nstrange new laws, eg. \\(a^2 + b^2 =\n(a+b)^2\\) (the usual \\(2ab\\)\nterm is missing, because in a binary field, \\(1 + 1 = 0\\)).\n\nAnd finally, binary fields work conveniently with bits: if you do\nmath with numbers that fit into \\(2^k\\)\nbits, then all of your outputs will also fit into \\(2^k\\) bits. This avoids awkwardness like\neg. with Ethereum's EIP-4844,\nwhere the individual \"chunks\" of a blob have to be numbers modulo\n52435875175126190479447740508185965837690552500527637822603658699938581184513,\nand so encoding binary data involves throwing away a bit of space and\ndoing extra checks at the application layer to make sure that each\nelement is storing a value less than \\(2^{248}\\). It also means that binary field\narithmetic is super fast on computers - both CPUs, and\ntheoretically optimal FPGA and ASIC designs.\n\nThis all means that we can do things like the Reed-Solomon encoding\nthat we did above, in a way that completely avoids integers \"blowing up\"\nlike we saw in our example, and in a way that is extremely \"native\" to\nthe kind of calculation that computers are good at. The \"splitting\"\nproperty of binary fields - how we were able to do \\(\\texttt{1100101010001111} = \\texttt{11001010} +\n\\texttt{10001111} * x_3\\), and then keep splitting as little or\nas much as we wanted, is also crucial for enabling a lot of\nflexibility.\n\n## Full Binius\n\nSee here\nfor a python implementation of this protocol.\n\nNow, we can get to \"full Binius\", which adjusts \"simple Binius\" to\n(i) work over binary fields, and (ii) let us commit to individual bits.\nThis protocol is tricky to understand, because it keeps going back and\nforth between different ways of looking at a matrix of bits; it\ncertainly took me longer to understand than it usually takes me to\nunderstand a cryptographic protocol. But once you understand binary\nfields, the good news is that there isn't any \"harder math\" that Binius\ndepends on. This is not elliptic\ncurve pairings, where there are deeper and deeper rabbit holes of\nalgebraic geometry to go down; here, binary fields are all you need.\n\nLet's look again at the full diagram:\n\nBy now, you should be familiar with most of the components. The idea\nof \"flattening\" a hypercube into a grid, the idea of computing a row\ncombination and a column combination as tensor products of the\nevaluation point, and the idea of checking equivalence between\n\"Reed-Solomon extending then computing the row combination\", and\n\"computing the row combination then Reed-Solomon extending\", were all in\nsimple Binius.\n\nWhat's new in \"full Binius\"? Basically three things:\n\n- The individual values in the hypercube, and in the square, have to\nbe bits (0 or 1)\n\n- The extension process extends bits into more bits, by grouping bits\ninto columns and temporarily pretending that they are larger field\nelements\n\n- After the row combination step, there's an element-wise \"decompose\ninto bits\" step, which converts the extension back into bits\n\nWe will go through both in turn. First, the new extension procedure.\nA Reed-Solomon code has the fundamental limitation that if you are\nextending \\(n\\) values to \\(k*n\\) values, you need to be working in a\nfield that has \\(k*n\\) different values\nthat you can use as coordinates. With \\(F_2\\) (aka, bits), you cannot do that. And\nso what we do is, we \"pack\" adjacent \\(F_2\\) elements together into larger values.\nIn the example here, we're packing two bits at a time into elements in\n\\(\\{0, 1, 2, 3\\}\\), because our\nextension only has four evaluation points and so that's enough for us.\nIn a \"real\" proof, we would probably back 16 bits at a time together. We\nthen do the Reed-Solomon code over these packed values, and unpack them\nagain into bits.\n\nNow, the row combination. To make \"evaluate at a random point\" checks\ncryptographically secure, we need that point to be sampled from a pretty\nlarge space, much larger than the hypercube itself. Hence, while the\npoints within the hypercube are bits, evaluations\noutside the hypercube will be much larger. In our example\nabove, the \"row combination\" ends up being \\([11, 4, 6, 1]\\).\n\nThis presents a problem: we know how to combine pairs of\nbits into a larger value, and then do a Reed-Solomon extension\non that, but how do you do the same to pairs of much larger values?\n\nThe trick in Binius is to do it bitwise: we look at the individual\nbits of each value (eg. for what we labeled as \"11\", that's \\([1, 1, 0, 1]\\)), and then we extend\nrow-wise. That is, we perform the extension procedure on the\n\\(1\\) row of each element, then on the\n\\(x_0\\) row, then on the \"\\(x_1\\)\" row, then on the \\(x_0 * x_1\\) row, and so forth (well, in our\ntoy example we stop there, but in a real implementation we would go up\nto 128 rows (the last one being \\(x_6 *\\ ...\n*\\ x_0\\))).\n\nRecapping:\n\n- We take the bits in the hypercube, and convert them into a grid\n\n- Then, we treat adjacent groups of bits on each row as\nlarger field elements, and do arithmetic on them to Reed-Solomon extend\nthe rows\n\n- Then, we take a row combination of each column of bits, and\nget a (for squares larger than 4x4, much smaller) column of bits for\neach row as the output\n\n- Then, we look at the output as a matrix, and treat the bits of\nthat as rows again\n\nWhy does this work? In \"normal\" math, the ability to (often) do\nlinear operations in either order and get the same result stops working\nif you start slicing a number up by digits. For example, if I start with\nthe number 345, and I multiply it by 8 and then by 3, I get 8280, and if\ndo those two operations in reverse, I also do 8280. But if I insert a\n\"split by digit\" operation in between the two steps, it breaks down: if\nyou do 8x then 3x, you get:\n\n\\[345 \\xrightarrow{\\times 8} 2760\n\\rightarrow [2, 7, 6, 0] \\xrightarrow{\\times 3} [6, 21, 18,\n0]\\]\n\nBut if you do 3x then 8x, you get:\n\n\\[345 \\xrightarrow{\\times 3} 1035\n\\rightarrow [1, 0, 3, 5] \\xrightarrow{\\times 8} [8, 0, 24,\n40]\\]\n\nBut in binary fields built with the tower construction, this kind of\nthing does work. The reason why has to do with their\nseparability: if you multiply a big value by a small value, what happens\nin each segment, stays in each segment. If we multiply \\(\\texttt{1100101010001111}\\) by \\(\\texttt{11}\\), that's the same as first\ndecomposing \\(\\texttt{1100101010001111}\\) into \\(\\texttt{11} + \\texttt{10} * x_2 + \\texttt{10} *\nx_2x_1 + \\texttt{10} * x_3 + \\texttt{11} * x_2x_3 + \\texttt{11} *\nx_1x_2x_3\\), and then multiplying each component by \\(\\texttt{11}\\) separately.\n\n## Putting it all together\n\nGenerally, zero knowledge proof systems work by making statements\nabout polynomials that simultaneously represent statements about the\nunderlying evaluations: just like we saw in the Fibonacci example, \\(F(X+2) - F(X+1) - F(X) = Z(X) * H(X)\\)\nsimultaneously checks all steps of the Fibonacci computation. We check\nstatements about polynomials by proving evaluations at a random point:\ngiven a commitment to \\(F\\), you might\nrandomly choose eg. 1892470, demand proofs of evaluations of \\(F\\), \\(Z\\)\nand \\(H\\) at that point (and \\(H\\) at adjacent points), check those\nproofs, and then check if \\(F(1892472) -\nF(1892471) - F(1892470)\\) \\(=\nZ(1892470) * H(1892470)\\). This check at a random point stands in\nfor checking the whole polynomial: if the polynomial equation\ndoesn't match, the chance that it matches at a specific random\ncoordinate is tiny.\n\nIn practice, a major source of inefficiency comes from the fact that\nin real programs, most of the numbers we are working with are tiny:\nindices in for loops, True/False values, counters, and similar things.\nBut when we \"extend\" the data using Reed-Solomon encoding to give it the\nredundancy needed to make Merkle proof-based checks safe, most of the\n\"extra\" values end up taking up the full size of a field, even if the\noriginal values are small.\n\nTo get around this, we want to make the field as small as possible.\nPlonky2 brought us down from 256-bit numbers to 64-bit numbers, and then\nPlonky3 went further to 31 bits. But even this is sub-optimal. With\nbinary fields, we can work over individual bits. This makes the\nencoding \"dense\": if your actual underlying data has n\nbits, then your encoding will have n bits, and the\nextension will have 8 * n bits, with no extra overhead.\n\nNow, let's look at the diagram a third time:\n\nIn Binius, we are committing to a multilinear polynomial: a\nhypercube \\(P(x_0, x_1 ... x_k)\\),\nwhere the individual evaluations \\(P(0, 0 ...\n0)\\), \\(P(0, 0 ... 1)\\) up to\n\\(P(1, 1, ... 1)\\) are holding the data\nthat we care about. To prove an evaluation at a point, we \"re-interpret\"\nthe same data as a square. We then extend each row, using\nReed-Solomon encoding over groups of bits, to give the data the\nredundancy needed for random Merkle branch queries to be secure. We then\ncompute a random linear combination of rows, with coefficients designed\nso that the new combined row actually holds the evaluation that we care\nabout. Both this newly-created row (which get re-interpreted as 128 rows\nof bits), and a few randomly-selected columns with Merkle branches, get\npassed to the verifier. This is \\(O(\\sqrt{N})\\) data: the new row has \\(O(\\sqrt{N})\\) size, and each of the\n(constant number of) columns that get passed has \\(O(\\sqrt{N})\\) size.\n\nThe verifier then does a \"row combination of the extension\" (or\nrather, a few columns of the extension), and an \"extension of the row\ncombination\", and verifies that the two match. They then compute a\ncolumn combination, and check that it returns the value that\nthe prover is claiming. And there's our proof system (or rather, the\npolynomial commitment scheme, which is the key building block\nof a proof system).\n\n## What did we not cover?\n\n- Efficient algorithms to extend the rows, which are\nneeded to actually make the computational efficiency of the verifier\n\\(O(\\sqrt{N})\\). With naive Lagrange\ninterpolation, we can only get \\(O(N^{\\frac{2}{3}})\\). For this, we use Fast\nFourier transforms over binary fields, described here\n(though the exact implementation will be different, because this post\nuses a less efficient construction not based on recursive\nextension).\n\n- Arithmetization. Univariate polynomials are\nconvenient because you can do things like \\(F(X+2) - F(X+1) - F(X) = Z(X) * H(X)\\) to\nrelate adjacent steps in the computation. In a hypercube, the\ninterpretation of \"the next step\" is not nearly as clean as \"\\(X + 1\\)\". You can do \\(X * k\\) and jump around powers of \\(k\\), but this jumping around behavior would\nsacrifice many of the key advantages of Binius. The Binius paper introduces\nsolutions to this (eg. see Section 4.3), but this is a \"deep rabbit\nhole\" in its own right.\n\n- How to actually safely do specific-value checks.\nThe Fibonacci example required checking key boundary conditions: \\(F(0) = F(1) = 1\\), and the value of \\(F(100)\\). But with \"raw\" Binius, checking\nat pre-known evaluation points is insecure. There are fairly simple ways\nto convert a known-evaluation check into an unknown-evaluation check,\nusing what are called sum-check protocols; but we did not get into those\nhere.\n\n- Lookup protocols, another\ntechnology which has been recently gaining usage as a way to make\nultra-efficient proving systems. Binius can be combined with lookup\nprotocols for many applications.\n\n- Going beyond square-root verification time. Square\nroot is expensive: a Binius proof of \\(2^{32}\\) bits is about 11 MB long. You can\nremedy this using some other proof system to make a \"proof of a Binius\nproof\", thus gaining both Binius's efficiency in proving the main\nstatement and a small proof size. Another option is the much\nmore complicated FRI-Binius protocol, which\ncreates a poly-logarithmic-sized proof (like regular\nFRI).\n\n- How Binius affects what counts as \"SNARK-friendly\".\nThe basic summary is that, if you use Binius, you no longer need to care\nmuch about making computation \"arithmetic-friendly\": \"regular\" hashes\nare no longer more efficient than traditional arithmetic hashes,\nmultiplication modulo \\(2^{32}\\) or\nmodulo \\(2^{256}\\) is no longer a big\nheadache compared to multiplication modulo \\(p\\), and so forth. But this is a\ncomplicated topic; lots of things change when everything is done in\nbinary.\n\nI expect many more improvements in binary-field-based proving\ntechniques in the months ahead.",
    "contentLength": 38713,
    "summary": "Binius achieves highly efficient cryptographic proofs by operating over binary fields (0s and 1s) using hypercube structures instead of traditional 256-bit arithmetic.",
    "detailedSummary": {
      "theme": "Binius represents a breakthrough in cryptographic proof systems by operating directly over binary fields (zeros and ones) rather than large prime fields, achieving dramatic efficiency improvements through novel mathematical techniques.",
      "summary": "Vitalik explains how Binius takes the evolution of proof systems to its logical conclusion by working directly with bits instead of large numbers. While earlier systems like SNARKs required 256-bit arithmetic and Plonky2 improved efficiency by dropping to 64-bit fields, Binius operates on individual bits using binary fields with tower constructions. This approach leverages the fact that computers are fundamentally binary machines, making operations like addition (just XOR) and multiplication highly efficient. The system uses multivariate polynomials over hypercubes rather than univariate polynomials, representing data as evaluations where each variable is 0 or 1, then cleverly reshaping this data into squares for Reed-Solomon encoding and verification.\n\nThe technical innovation lies in how Binius handles the challenge of working with such small fields while maintaining cryptographic security. Vitalik walks through both a 'simple Binius' example using regular integers and the full protocol using binary fields, showing how the system splits evaluation points into row and column components, computes tensor products for linear combinations, and uses the separability properties of tower-constructed binary fields to maintain efficiency. The result is a system that achieves the same security as previous approaches while being dramatically more efficient - with 32-bit binary field operations taking 5x less computational resources than equivalent operations over 31-bit Mersenne fields.",
      "takeaways": [
        "Binius achieves dramatic efficiency gains by operating directly over bits rather than large prime fields, with binary field operations requiring 5x fewer computational resources than equivalent prime field operations",
        "The system uses multivariate polynomials over hypercubes instead of univariate polynomials, representing computational traces as evaluations where each variable is either 0 or 1",
        "Binary fields constructed via tower extensions allow for efficient arithmetic while maintaining the mathematical properties needed for cryptographic proofs, with addition being simple XOR operations",
        "The protocol cleverly addresses the challenge of Reed-Solomon encoding over small fields by temporarily packing bits into larger elements, then decomposing them back to bits after operations",
        "Binius fundamentally changes what counts as 'SNARK-friendly' computation, making traditional computer operations like standard hashes and modular arithmetic much more efficient relative to arithmetic-friendly alternatives"
      ],
      "controversial": [
        "The claim that Binius makes traditional 'SNARK-friendly' optimizations obsolete could be disputed, as the practical adoption and real-world performance may differ from theoretical improvements",
        "The assertion that square-root verification time with 11MB proofs for 2^32 bits is acceptable may be controversial given that other systems offer much smaller proof sizes"
      ]
    }
  },
  {
    "id": "general-2024-04-01-dc",
    "title": "Degen communism: the only correct political ideology",
    "date": "2024-04-01",
    "category": "society",
    "url": "https://vitalik.eth.limo/general/2024/04/01/dc.html",
    "path": "general/2024/04/01/dc.html",
    "content": "- \n\n- \n\n  \n    \n\n  \n  \n  Dark Mode Toggle\n  \n    \n\n  \n\n##  Degen communism: the only correct political ideology \n\n 2024 Apr 01 \nSee all posts\n\n \n \n\n Degen communism: the only correct political ideology \n\nIn 2024, there is a widespread feeling throughout the Western world\nthat all of our political ideologies are outdated, and are increasingly\nfailing us. Old ideas that have dominated elite political thought,\nwhether capitalism or liberalism or progressive social democracy or\nwhatever else, are rapidly losing popularity. The capitalists are\nsupporting tariffs. The libertarians are pushing to ban lab-grown\nmeat, and are actively railing\nagainst the few in their ranks who still remember that\nlibertarianism is supposed to be about liberty. The \"new authoritarians\"\nof the world, meanwhile, are hardly presenting an attractive\nalternative.\n\nSome are trying to respond to this crisis by reminding us of the\nvirtues of the old ideals of civility and decorum, hoping that we could\nwind back the clock and return to them. My friend Dennis Pourteaux is a\ngood example of this kind of mentality:\n\nThe problem is that this is a fundamentally reactionary mentality,\nand it fails for the exact same reasons why all other reactionary\nmentalities fail. If before we were at political equilibrium A, and\ntoday we are at political equilibrium B, then that alone is strong\nevidence that A is unstable, and even if you somehow force a transition\nback to A, the likely outcome is that we'll come right back to B\nagain.\n\nAs much as defenders of the ancien regime might wish otherwise,\npre-internet old guard elite notions of respectability and decorum are\nsimply fundamentally incompatible with the world as it stands in the\n2020s. And so instead of trying to look backward, we need to look\nforward. So what is the forward-looking ideology that solves these\nproblems? Degen communism.\n\n## What is degen communism?\n\nWhat does the internet of the 2020s - not the \"respectable\" internet\nof Substack, not a hypothetical version of Twitter where the bad people\nand somehow only the bad people are censored, but the real internet as\nit exists today - fundamentally want? The answer is, it wants\nchaos. It does not want gentle debates between\nprofessionals who \"disagree on policy but agree on civics\". It wants\ndecisive action and risk, in all its glory. Not a world with\ngenteel respect for principles, where even the loser peacefully accepts\ndefeat because they understand that even if they lose one day they may\nstill win the next, but a world with great warriors who are willing to\nbet their entire life savings and reputation on one single move that\nreflects their deepest conviction on what things need to be\ndone. And it wants a world where the brave have the freedom to\ntake such risks.\n\nAt the same time, the general welfare of humanity demands a\ngreater focus on the common good. We've seen too many instances\nof epic collapses, orchestrated by failed machinations of the elites,\nwhere the common people end up screwed but the elites remain immune or\neven benefit. The 2008 financial crisis was itself an example of this.\nRapid advances in technology, and rapid openings in immigration and\ntrade, leave most people better off, but often leave jobless those who\nare not in a good position to adjust. Rapidly growing tech companies\"\ndisrupt\" old extractive elites, but then quickly become\nextractive elites themselves. But most proponents of the common good\nassociate the common good with extreme notions of \"social stability\",\nwhich are often an excuse for keeping old extractive elites\nentrenched, and in any case are lame and incompatible with the trends of\nthe 21st century. Like the occasional forest fire and its positive\neffects on antifragility of natural ecosystems, chaos is the mother\nof revitalization and renewal.\n\nThis brings me to the core idea of degen communism: a\npolitical ideology that openly embraces chaos, but tweaks key rules and\nincentives to create a background pressure where the consequences of\nchaos are aligned with the common good.\n\nDegen communist ideas can be adopted by any type of entity with a\nnetwork effect: crypto projects, social media sites, virtual game\nenvironments and governments. Many of the core ideas are common across\nall of these categories.\nCryptocurrency:\nthe avant garde in degen. Can it become degen communist?\n\nThe world of cryptocurrencies is one of the sectors of society that\nembraces the \"degen\" the most. It has ups and downs that are unseen in\nalmost any other market. Meanwhile, the actual effects of the\ndowns are often smaller than they seem, which is why the space has not\ncollapsed completely. A 90% price drop erases billions of dollars of\nvalue, but the average dollar lost is only lost in mark-to-market\nbook-value terms: it's people to held on the way up, and kept holding on\nthe way down. The average coin lost from a $100 million\ndefi hack is a coin that was worth ten times less two years earlier.\nSometimes, the unpredictable chaos also does good: many memecoins have\ndonated significant amounts to charity.\n\nBut even still, when the prices crash, especially due to sudden\nfailures of projects that promised their users stability, too many\npeople get hurt. Could we create a world where the chaos remains, but\nthe human harm that comes from the downfalls is 10x smaller?\nHere, I will resurrect an idea that I supported during the Terra/Luna\ncollapse of 2022:\n\nWhen projects collapse or get hacked, and only partial\nrefunds are possible, don't make the refunds proportional. Instead, make\nsmaller users whole first, up to some threshold (eg. $50k). Two\nyears ago, when I proposed this idea, many treated it with scorn, misrepresenting\nthe idea\nas asking\nfor government\nbailouts. Today, nobody seems to care about principles anymore, and so\neven versions of this idea that are government-backed can\nperhaps more easily get adopted. Here, though, I am not proposing\nanything to do with governments; rather, I am proposing that project\nteams put into their terms of service an expectation that in the event\nof project insolvency, partial refunds will be prioritized in this way.\nThe only request to governments is that appropriate rules get passed so\nthat bankruptcy courts acknowledge the legitimacy of such\narrangements.\n\nThis mitigates the downsides of chaos toward the most vulnerable.\nNow, what about better capturing the upsides of chaos? Here, I\nsupport a combination of steps:\n\n- Memecoins and games can donate\na portion of their issuance to charity.\n\n- Projects can use airdrops that try their best to\ndistribute the most to individual users, as well as public good\ncontributors such as open source software developers, solo stakers, etc.\nThe Starknet\nairdrop was an excellent demonstration of this, as were\nother egalitarian airdrops like the ENS airdrop.\n\n- Projects can have public goods funding programs\n(whether proactive or retroactive). The first three rounds of optimism retro funding\nwere an excellent example; more projects should replicate this\nmodel.\n\n- If a governance token gets too concentrated, and the concentrated\nplayers make bad decisions, the community should be more willing to\nfork the project and zero out the tokens of the concentrated\nplayers who made the bad decisions. This was done most\nsuccessfully in the Hive\nfork of Steem.\n\nMany of these ideas, especially those that depend on some notion of\n\"per-person\", would have been very difficult to reliably administer in\n2019. In 2024, however, we have more and more robust proof of personhood\nprotocols, proofs of community participation such as POAPs, and reusable\nlists such as Starkware's\nlist of solo stakers that they used for their airdrop. Hence,\na degen communist future for crypto is very\npossible.\n\nThe solution is to merge the two together. Keep the base\ninstinct, especially the base instinct of enjoying\nwatching things blow up, but tilt it toward the common good. In\nexchange, the base instinct people can enjoy greater legitimacy.\n\nIncidentally, maybe this is why that L2 is\ncalled \"Base\".\n\nWhat\nmight degen communism look like in government policy?\n\nThe two main forms of chaos in the broader world are social\nmedia and markets. Rather than trying to\ndefang both, we should embrace both (especially markets), and try to\ntilt them toward more often serving the common good. Politics is\ninherently a more slow-moving domain, so the proposals will seem 10x\nmilder. However, the increased scale of their impact more than makes up\nfor it.\n\n## Land value taxes and YIMBY\n\nToday, real estate markets in many parts of the developed world are\nin crisis. In the most expensive regions, wealthy landowners earn\nmillions by simply holding on to pieces of real estate that they\nacquired at dirt-cheap prices more than half a century ago. Rules like\nCalifornia's\nProposition 13 mean that they only have to pay property taxes\ncalculated as though their plot still had a much lower price. At the\nsame time, many of these same people push\nto maintain restrictive regulations that prevent more dense housing\nfrom being built. This is a society that favors the rich. Traditional\nleftists' favorite countermeasure, rent control rules, only benefit\npeople who stay in the same place for many years, at the expense of very\nlong waits for new people who want to come in. Meanwhile,\ngovernments' ability to raise revenue to fund public services is limited\nby the fact that if income and sales taxes are pushed too high, people\nsimply go somewhere else.\n\nThis status quo is the exact opposite of degen, and the opposite of\ncommunist. And so a degen communist will seek to overturn all parts of\nit. Instead of focusing on taxing income and business, which can flee a\nstate or country that taxes it too heavily, we would put the primary tax\nburden on land, which cannot. Land value taxes, a yearly\nproperty tax proportional on the value of land (but not the\nbuildings on the land), have been broadly supported by many economists\nfor over a century. We can add per-person exemptions, limiting the\neffects of the tax on the most vulnerable: if we send half the\nentire revenue from a land value tax directly into a per-person\ndividend, then anyone who owns less than half the average amount of land\n(ie. almost all poor people) would net-benefit!\n\nThis could be viewed as a market-based tax plus a dividend, or it can\nbe viewed as a rationing scheme, where if you own less land than your\nquota you can get a reward by renting your excess quota to people who\nown more.\n\nA degen communist would also repeal restrictive rules that heavily\nlimit what can be built on the land, allowing much more construction to\ntake place. There are already places in the world that follow something\nclose to this approach: much of East Asia and, surprisingly, Austin,\nTexas.\n\nAustin skyline, 2014 vs 2023.\n\nHousing supply growth in Austin is fast,\nand rents are dropping. Texas does not have a land value tax, but it\nhas high property taxes: 1.77%\nper year, compared to 0.7%\nper year in much of California. Texas taxes its rich - but it taxes\ntheir land, not their income. And it taxes stasis, rather than dynamism,\nand in doing so it makes itself more affordable to the poor.\n\nMany today are suffering from high prices - so let's drop the prices\nwe can drop (most notably, rent) with a few simple policy\nchanges.\nHarberger taxes on\nintellectual property\n\nSo-called \"intellectual property\" (ie. copyrights and patents) is one\nof the most elite-controlled forms of \"property\" around, and one of the\nforms of government regulation most harmful to dynamism. On the other\nhand, many are concerned that removing intellectual property entirely\nwould overly harm the incentive to innovate and make artistic works. To\nstrike a balance, I propose a happy medium: we keep copyrights\nand patents, but put Harberger taxes\non them.\n\nThis would work as follows. For a copyright or patent to be valid,\nwhoever owns it must publicly register a value, which we will call that\ncopyright or patent's \"exclusivity price\". They must then pay 2% of the\nexclusivity price annually in tax (they can change the exclusivity price\nat any time). Anyone can pay the owner the exclusivity price, and get an\nunlimited right to also use (and if they wish sub-license, including to\nthe entire world) that copyright or patent. The original owner would\nretain the right to use in all cases; others can gain permission to use\neither by getting the original owner's permission, or by paying the\nowner the exclusivity price.\n\nThis accomplishes two goals. First, it fixes\ndefaults: if someone has no interest in making money off of\nkeeping an invention or work exclusive, it sets the default so that it's\npublicly available for anyone. Second, it leads to more\npermissionlessness, and less exclusion, on the margin, by\nputting a price on exclusion. The revenues from this tax could go into a\ncitizen's dividend, or they could go into a quadratic\nfunding pool that supports non-profit science and art.\n\n## Immigration\n\nLeft: the US standard immigration system. Backlogged and\nunfair. Right: the US alternative immigration system. Honest and\nfair.\n\nOne of the most beautiful and deeply good ideas in early communism is\nthe internationalism: the focus on \"workers of the world uniting\" and\nsongs like The\nInternationale. In 2024, we are unfortunately in an age of rising\nnationalism, where it's considered normal for each nation to only care\nabout each own citizens, at the expense of people unlucky enough to be\nborn outside. Faced with these restrictions, some are taking matters\ninto their own hands, making their way into wealthy countries the\nold-fashioned way - the way that pretty much everyone did up until\nglobalist schemes of social control like passports were\nintroduced about a century ago.\n\nA degen communist would embrace dynamism and change, especially when\nit seems like such dynamism and change might benefit\nthe global poor more than anyone else. Degen communists would\ngreatly expand safe and legal pathways for people to visit and live\nwhere they want to visit and live, trusting in liberalized housing\nconstruction, plus governments made wealthy by taxes from such\nconstruction, to build needed infrastructure for them. Restrictions\nwould be focused on keeping out particularly risky or bad\nactors, rather than keeping out almost everyone. A \"proof\nof stake\" scheme could be adopted where someone can put down funds (or\nthe right to make future invitations) at stake on a claim that a given\nperson will not break any rules, which would then give that person an\nautomatic right to enter. Security can be improved while total freedom\nof movement is increased.\nDecision-making in degen\ncommunism\n\nDecision-making in degen communist institutions would be democratic,\nand would follow three equally important principles:\ndynamism, cross-tribal bridging and\nquality. Decisions could be made quickly, using\nalgorithms that identify ideas that are held in common across groups\nwhich normally disagree with each other, and which elevate quality\nwithout entrenching a fixed set of elites.\n\nThis involves a two-layer stack:\n\n- Public discussion and consensus-finding platforms,\nwhich can allow large groups of people to rapidly participate but\ninclude mechanisms to identify points of consensus. This includes tools\nsuch as pol.is and Community\nNotes, which focus on cross-tribal bridging. It also includes\nprediction markets (eg. Polymarket), which in addition to\nhelping communities surface good predictions, serve the role of giving\nintellectuals an outlet to express their conviction about their\nstrongest and most fervent beliefs - and for others to bet against\nthem.\n\n- The final governance mechanism (eg. voting). This\ncan use quadratic voting, though the \"cross-tribal bridging\"\nfunctionality can be enhanced by ideas like the matrix-factoring\nalgorithm in Community Notes or by pairwise-bounded\nquadratic voting.\n\nThese two sets of tools together allow decisions to be made quickly,\nat large scale, and in a way that favors quality in a dynamic way that\nallows experts to quickly rise and fall with each individual topic or\ndecision.\n\nIn all of these possible implementations, the core theme of degen\ncommunism is the same. Do not try to enforce stasis. Instead, embrace\nthe chaos of markets and other fast-paced human activity. At the same\ntime, however, tweak rules in such a way that the upsides get funneled\ninto supporting public goods (including quality of the governance\nitself), and the downsides get capped or even outright removed for the\npeople who are not able to handle it. This can be a way forward for\neveryone in the 21st century.",
    "contentLength": 16634,
    "summary": "Vitalik proposes \"degen communism\" - embracing internet chaos while tilting incentives toward common good through crypto airdrops and land taxes.",
    "detailedSummary": {
      "theme": "Vitalik proposes 'degen communism' as a new political ideology that embraces chaos and risk-taking while structuring incentives to align outcomes with the common good.",
      "summary": "Vitalik argues that traditional political ideologies are failing in the 2024 era, and attempts to return to old-fashioned civility and decorum are fundamentally reactionary and doomed to fail. He proposes 'degen communism' as a forward-looking alternative that embraces the internet's desire for chaos, decisive action, and high-stakes risk-taking, while tweaking rules and incentives to ensure the benefits flow toward public goods and the downsides are minimized for vulnerable populations. Vitalik outlines how this ideology could be implemented across cryptocurrency projects, government policy, and decision-making systems. His specific proposals include prioritizing smaller users in crypto project failures, implementing land value taxes with per-person dividends, applying Harberger taxes to intellectual property, liberalizing immigration through proof-of-stake schemes, and using prediction markets and cross-tribal consensus tools for governance. The core philosophy is to stop enforcing stasis, embrace market chaos and fast-paced human activity, but structure systems so upsides support public goods while downsides are capped for those who can't handle the risk.",
      "takeaways": [
        "Traditional political ideologies are outdated and failing, requiring a forward-looking approach rather than nostalgic returns to past civility",
        "The internet fundamentally wants chaos and decisive action rather than genteel professional debates, which must be acknowledged in new political frameworks",
        "Crypto projects should prioritize smaller users in failures and use airdrops/funding to support public goods and individual contributors",
        "Government policy should embrace land value taxes with per-person dividends and liberalized housing construction to address inequality while maintaining market dynamics",
        "Decision-making systems should combine public consensus-finding tools with prediction markets and quadratic voting to achieve dynamism, cross-tribal bridging, and quality simultaneously"
      ],
      "controversial": [
        "The embrace of 'chaos' as a positive force in governance and society may be seen as reckless or destabilizing",
        "The proposal for greatly expanded immigration with minimal restrictions could face strong political opposition",
        "Harberger taxes on intellectual property could be viewed as undermining innovation incentives and property rights",
        "The characterization of passports and immigration controls as 'globalist schemes of social control' presents a contentious historical interpretation"
      ]
    }
  },
  {
    "id": "general-2024-03-29-memecoins",
    "title": "What else could memecoins be?",
    "date": "2024-03-29",
    "category": "applications",
    "url": "https://vitalik.eth.limo/general/2024/03/29/memecoins.html",
    "path": "general/2024/03/29/memecoins.html",
    "content": "- \n\n- \n\n  \n    \n\n  \n  \n  Dark Mode Toggle\n  \n    \n\n  \n\n##  What else could memecoins be? \n\n 2024 Mar 29 \nSee all posts\n\n \n \n\n What else could memecoins be? \n\nTen years ago, two weeks before the Ethereum project was publicly\nannounced, I published this\npost on Bitcoin magazine arguing that issuing coins could be a new\nway to fund important public projects. The thinking went: society needs\nways to fund valuable large-scale projects, markets and institutions\n(both corporations and governments) are the main techniques that we have\ntoday, and both work in some cases and fail in others. Issuing new coins\nseems like a third class of large-scale funding technology, and it seems\ndifferent enough from both markets and institutions that it would\nsucceed and fail in different places - and so it could fill in some\nimportant gaps.\n\nPeople who care about cancer research could hold, accept and trade\nAntiCancerCoin; people who care about saving the environment would hold\nand use ClimateCoin, and so forth. The coins that people choose to use\nwould determine what causes get funded.\n\nToday in 2024, a major topic of discussion in \"the crypto space\"\nappears to be memecoins. We have seen memecoins before, starting from\nDogecoin back in 2015, and \"dog coins\" were a major topic during the\n2020-21 season. This time, they are heating up again, but in a way that\nis making many people feel uneasy, because there isn't anything\nparticularly new and interesting about the memecoins. In fact, often\nquite the opposite: apparently a bunch\nof Solana memecoins have recently been openly super-racist. And even\nthe non-racist memecoins often seem to just go up and down in price and\ncontribute nothing of value in their wake.\n\nAnd people are upset:\n\nEven long-time Ethereum philosopher Polynya is very\nvery unhappy:\n\nOne answer to this conundrum is to shake our heads and virtue-signal\nabout how much we are utterly abhorred by and stand against this\nstupidity. And to some extent, this is the correct thing to do. But at\nthe same time, we can also ask another question: if people value having\nfun, and financialized games seem to at least sometimes provide that,\nthen could there be a more positive-sum version of this whole\nconcept?\n\n## Charity coins\n\nAmong the more interesting of the coins that I've seen are coins\nwhere a large portion of the token supply (or some ongoing fee\nmechanism) is dedicated to some kind of charity. One and a half years\nago, there was a (no longer active) coin called \"GiveWell Inu\" that\ndonated proceeds to GiveWell. For the past two years, there has been a\ncoin called \"Fable of the Dragon Tyrant\" which supported cultural\nprojects related to anti-aging research, in addition to other causes.\nUnfortunately, both of these are far from perfect: GiveWell Inu seems to\nno longer be maintained, and the other one has some highly annoying core\ncommunity members that constantly badger me for attention, which\ncurrently makes me unenthusiastic about mentioning them more than once.\nMore successfully, after I was gifted half the supply of the Dogelon\nMars token, and immediately re-gifted it to the Methuselah Foundation, the\nMethuselah Foundation and the Dogelon Mars community seemed to develop a\npositive-sum relationship with each other, retroactively converting\n$ELON into a charity coin.\n\nIt feels like there is an unclaimed opportunity here to try to create\nsomething more positive-sum and long-lasting. But ultimately, I think\neven that would create something fundamentally limited, and we can do\nbetter.\n\n## Robin Hood games\n\nIn principle, people participate in memecoins because (i) the value\nmight go up, (ii) they feel democratic and open for anyone to\nparticipate, and (iii) they are fun. We can siphon off a large percent\nof a memecoin's supply to support public goods that people value, but\nthat does nothing for the participants directly, and indeed comes at the\nexpense of (i), and if done poorly at the expense of (ii) too. Can we do\nsomething that instead improves on both for the average\nuser?\n\nThe answer for (iii) is simple: don't just make a coin, make a game.\nBut make an actually meaningful and fun game. Don't think Candy\nCrush on the blockchain; think World of Warcraft on the blockchain.\n\nAn \"Ethereum\nResearcher\" in World of Warcraft. If you kill one, you get 15 silver\n61 copper, and a 0.16% chance of getting some \"Ethereum Relay Data\". Do\nnot attempt in real life.\n\nNow, what about the Robin Hood part? When I go around low-income\nSoutheast Asian countries, one claim that I often hear is how some\npeople or their family members were poor before, but then got\nmedium-rich off of the play-to-earn feature in Axie Infinity in\n2021. Of course, Axie Infinity's situation in 2022 was somewhat\nless favorable. But even still, I get the impression that if you\ntake the game's play-to-earn properties into account, on average,\nthe net financial gains were negative for high-income users but might\n(emphasis on might!) have been positive for low-income users. This\nseems like a nice property to have: if you have to be financially brutal\non someone, be brutal on those who can handle it, but have a safety net\nto keep lower-income users protected and even try to make them come out\nbetter off than they came in.\n\nRegardless of how well Axie Infinity in particular accomplished this,\nit feels intuitive that (i) if the goal is to satisfy people's desire to\nhave fun, we should be making not simple copy-paste coins but rather\nmore complicated and interesting games, and (ii) games that leave\nlower-income players in particular economically better off are\nmore likely to leave their communities better than they came in. Charity\ncoins and games could even be combined: one of the features of the game\ncould be a mechanism where players who succeed at some task can\nparticipate in voting on which charities the issued funds are\ndistributed to.\n\nThat said, making a genuinely fun game is a challenge - see some\nnegative takes on how well Axie did at being fun, and this\npositive take on how they have improved since then. The team that I\npersonally have the most confidence in to make fun crypto games is 0xPARC, because they have already\nsucceeded twice (!!) at making crypto games (first Dark Forest, then FrogCrypto) where\nplayers were willing to play entirely for fun, rather than out of a\ndesire to make money. Ideally, the goal is to make a co-created\nenvironment that leaves all players happy: money is zero sum, but fun\ncan be positive sum.\n\n## Conclusions\n\nOne of my personal moral rules is \"if there is a class of people or\ngroups you dislike, be willing to praise at least a few of them that do\nthe best job of satisfying your values\". If you dislike governments\nbecause they violate people's freedoms, perhaps you may find space in\nyour heart to say something good about the Swiss one. If you dislike\nsocial media platforms for being extractive and encouraging toxic\nbehavior, but you think Reddit is 2x less bad, say nice things about\nReddit. The opposite approach - to shout \"yes, all X are part of the\nproblem\" - feels good in the moment, but it alienates people and pushes\nthem further toward their own bubble where they will insulate themselves\nentirely from any moral appeals you might have in the future.\n\nI think of the \"degen\" parts of the crypto space in the same way. I\nhave zero enthusiasm for coins named after totalitarian political\nmovements, scams, rugpulls or anything that feels exciting in month N\nbut leaves everyone upset in month N+1. At the same time, I value\npeople's desire to have fun, and I would rather the crypto space somehow\nswim with this current rather than against it. And so I want to\nsee higher quality fun projects that contribute positively to the\necosystem and the world around them (and not just by \"bringing in\nusers\") get more mindshare. At the least, more good memecoins than bad\nones, ideally those that support public goods instead of just enriching\ninsiders and creators. But also ideally, making games rather than coins,\nand making projects that people enjoy participating in.",
    "contentLength": 8069,
    "summary": "Vitalik argues memecoins could evolve into charity-funding games that benefit low-income players rather than just speculative tokens.",
    "detailedSummary": {
      "theme": "Vitalik explores how memecoins could evolve beyond speculative trading into meaningful projects that fund public goods and create positive-sum games for participants.",
      "summary": "Vitalik reflects on his decade-old vision of using coin issuance to fund public projects, contrasting it with today's memecoin phenomenon that often lacks social value and sometimes promotes harmful content. He acknowledges people's desire for fun and financial participation while critiquing the current state of memecoins that contribute little beyond price speculation. Vitalik proposes two main alternatives: charity coins that dedicate portions of their supply or fees to worthy causes, and 'Robin Hood games' that create engaging blockchain-based experiences while potentially benefiting lower-income participants more than wealthy ones. He advocates for swimming with the current of people's desire for entertainment rather than against it, but channeling it toward projects that create genuine value and positive-sum outcomes for communities.",
      "takeaways": [
        "Memecoins could serve as a third class of funding technology for public goods, complementing traditional markets and institutions",
        "Current memecoins often lack social value and sometimes promote harmful content, but dismissing them entirely alienates potential participants",
        "Charity coins that donate proceeds to worthy causes represent an improvement but remain fundamentally limited in scope",
        "Robin Hood games could create engaging experiences while structurally benefiting lower-income players at the expense of wealthier participants",
        "The goal should be creating fun, meaningful projects that leave all participants better off rather than zero-sum financial speculation"
      ],
      "controversial": [
        "The suggestion that it's acceptable to be 'financially brutal' on high-income users to benefit low-income participants in games",
        "The implicit endorsement of financialized gaming despite concerns about gambling-like mechanics",
        "The pragmatic acceptance of memecoin culture rather than rejecting it outright on moral grounds"
      ]
    }
  },
  {
    "id": "general-2024-03-28-blobs",
    "title": "Ethereum has blobs. Where do we go from here?",
    "date": "2024-03-28",
    "category": "protocol",
    "url": "https://vitalik.eth.limo/general/2024/03/28/blobs.html",
    "path": "general/2024/03/28/blobs.html",
    "content": "- \n\n- \n\n  \n    \n\n  \n  \n  Dark Mode Toggle\n  \n    \n\n  \n\n##  Ethereum has blobs. Where do we go from here? \n\n 2024 Mar 28 \nSee all posts\n\n \n \n\n Ethereum has blobs. Where do we go from here? \n\nOn March 13, the Dencun hard fork activated, enabling one of the\nlong-awaited features of Ethereum: proto-danksharding (aka EIP-4844, aka blobs). Initially, the\nfork reduced the transaction fees of rollups by a factor of over 100, as\nblobs were nearly free. In the last day, we finally saw blobs spike up\nin volume and the fee market activate as the blobscriptions protocol\nstarted to use them. Blobs are not free, but they remain much cheaper\nthan calldata.\n\nLeft: blob usage finally spiking up to the 3-per-block\ntarget thanks to Blobscriptions. RIght: blob fees \"entering price\ndiscovery mode\" as a result. Source: https://dune.com/0xRob/blobs.\n\nThis milestone represents a key transition in Ethereum's long-term\nroadmap: blobs are the moment where Ethereum scaling ceased to\nbe a \"zero-to-one\" problem, and became a \"one-to-N\" problem.\nFrom here, important scaling work, both in increasing blob count and in\nimproving rollups' ability to make the best use of each blob, will\ncontinue to take place, but it will be more incremental. The\nscaling-related changes to the fundamental paradigm of how\nEthereum as an ecosystem operates are increasingly already behind us.\nAdditionally, emphasis is already slowly shifting, and will continue to\nslowly shift, from L1 problems such as PoS and scaling, to problems\ncloser to the application layer. The key question that this post will\ncover is: where does Ethereum go from here?\nThe future of Ethereum\nscaling\n\nOver the last few years, we have seen Ethereum slowly shift over to\nbecoming an L2-centric\necosystem. Major applications have started to move over from L1 to\nL2, payments are starting to be L2-based by default, and wallets are\nstarting to build their user experience around the new multi-L2\nenvironment.\n\nFrom the very beginning, a key piece of the rollup-centric roadmap\nwas the idea of separate data availability space: a\nspecial section of space in a block, which the EVM would not\nhave access to, that could hold data for layer-2 projects such as\nrollups. Because this data space is not EVM-accessible, it can be\nbroadcasted separately from a block and verified separately from a\nblock. Eventually, it can be verified with a technology called data availability\nsampling, which allows each node to verify that the data\nwas correctly published by only randomly checking a few small samples.\nOnce this is implemented, the blob space could be greatly expanded; the\neventual goal is 16 MB per slot (~1.33 MB per second).\n\nData availability sampling: each node only needs to\ndownload a small portion of the data to verify the availability of the\nwhole thing.\n\nEIP-4844 (aka \"blobs\") does not give us data availability sampling.\nBut it does set up the basic scaffolding in such a way that from\nhere on, data availability sampling can be introduced and blob count can\nbe increased behind the scenes, all without any involvement from users\nor applications. In fact, the only \"hard fork\" required is a\nsimple parameter change.\n\nThere are two strands of development that will need to continue from\nhere:\n\n- Progressively increasing blob capacity, eventually\nbringing to life the full vision of data availability sampling with 16\nMB per slot of data space\n\n- Improving L2s to make better use of the data space that we\nhave\n\n## Bringing DAS to life\n\nThe next stage is likely to be a simplified version of DAS called PeerDAS.\nIn PeerDAS, each node stores a significant fraction (eg. 1/8) of all\nblob data, and nodes maintain connections to many peers in the p2p\nnetwork. When a node needs to sample for a particular piece of data, it\nasks one of the peers that it knows is responsible for storing that\npiece.\n\nIf each node needs to download and store 1/8 of all data, then\nPeerDAS lets us theoretically scale blobs by 8x (well, actually 4x,\nbecause we lose 2x to the redundancy of erasure coding). PeerDAS can be\nrolled out over time: we can have a stage where professional stakers\ncontinue downloading full blobs, and solo stakers only download 1/8 of\nthe data.\n\nIn addition to this, EIP-7623 (or\nalternatives such as 2D\npricing) can be used to put stricter bounds on the maximum size of\nan execution block (ie. the \"regular transactions\" in a block), which\nmakes it safer to increase both the blob target and the L1 gas limit. In\nthe longer term, more complicated 2D\nDAS protocols will let us go all the way and increase blob space\nfurther.\n\n## Improving L2s\n\nThere are four key places in which layer 2 protocols today can\nimprove.\n\n1. Using bytes more efficiently with data\ncompression\n\nMy outline-in-a-picture of data compression continues\nto be available here;\n\nNaively, a transaction takes up around 180 bytes of data. However,\nthere are a series of compression techniques that\ncan be used to bring this size down over several stages; with\noptimal compression we could potentially go all the way down to under 25\nbytes per transaction.\n\n2. Optimistic data techniques that secure L2s by only using\nthe L1 in exceptional situations\n\nPlasma\nis a category of techniques that allows you to get rollup-equivalent\nsecurity for some applications while keeping data on L2 in the normal\ncase. For EVMs, plasma can't protect all coins. But\nPlasma-inspired constructions can protect most coins. And\nconstructions much simpler than Plasma can improve greatly on the validiums\nof today. L2s that are not willing to put all of their data on-chain\nshould explore such techniques.\n\n3. Continue improving on execution-related\nconstraints\n\nOnce the Dencun hard fork activated, making rollups set up to use the\nblobs that it introduced 100x cheaper. usage on the Base rollup spiked up immediately:\n\nThis in turn led to Base hitting its own internal gas limit, causing\nfees\nto unexpectedly surge. This has led to a more widespread realization\nthat Ethereum data space is not the only thing that needs to be scaled:\nrollups need to be scaled internally as well.\n\nPart of this is parallelization; rollups could implement something like\nEIP-648. But just as important is storage,\nand interaction\neffects between compute and storage. This is an important\nengineering challenge for rollups.\n\n4. Continue improving security\n\nWe are still far from a world where rollups are truly protected by\ncode. In fact, according to l2beat\nonly these five, of which only Arbitrum is full-EVM, have even reached\nwhat\nI have called \"stage 1\".\n\nThis needs to be tackled head-on. While we are not currently at the\npoint where we can be confident enough in the complex code of an\noptimistic or SNARK-based EVM verifier, we are absolutely at the point\nwhere we can go halfway there, and have security councils that\ncan revert the behavior of the code only with a high threshold (eg. I\nproposed 6-of-8; Arbitrum is doing 9-of-12).\n\nThe ecosystem's standards need to become stricter: so far, we\nhave been lenient and accepted any project as long as it claims to be\n\"on a path to decentralization\". By the end of the year, I think our\nstandards should increase and we should only treat a project as a rollup\nif it has actually reached at least stage 1.\n\nAfter this, we can cautiously move toward stage 2: a world where\nrollups truly are backed by code, and a security council can only\nintervene if the code \"provably disagrees with itself\" (eg. accepts two\nincompatible state roots, or two different implementations give\ndifferent answers). One path toward doing this safely is to use multiple prover\nimplementations.\nWhat\ndoes this mean for Ethereum development more broadly?\n\nIn a\npresentation at ETHCC in summer 2022, I made a presentation\ndescribing the current state of Ethereum development as an S-curve: we\nare entering a period of very rapid transition, and after that rapid\ntransition, development will once again slow down as the L1 solidifies\nand development re-focuses on the user and application layer.\n\nToday, I would argue that we are decidedly on the\ndecelerating, right side of this S-curve. As of two weeks ago,\nthe two largest changes to the Ethereum blockchain - the switch to proof\nof stake, and the re-architecting to blobs - are behind us. Further\nchanges are still significant (eg. Verkle\ntrees, single-slot\nfinality, in-protocol account\nabstraction), but they are not drastic to the same extent\nthat proof of stake and sharding are. In 2022, Ethereum was like a plane\nreplacing its engines mid-flight. In 2023, it was replacing its wings.\nThe Verkle tree transition is the main remaining truly significant one\n(and we already have testnets for that); the others are more like\nreplacing a tail fin.\n\nThe goal of EIP-4844 was to make a single large one-time change, in\norder to set rollups up for long-term stability. Now that blobs are out,\na future upgrade to full danksharding with 16 MB blobs, and even\nswitching the cryptography over to STARKs over\na 64-bit goldilocks field, can happen without requiring any further\naction from rollups and users. It also reinforces an important\nprecedent: that the Ethereum development process executes according to a\nlong-existing well-understood roadmap, and applications (including L2s)\nthat are built with \"the new Ethereum\" in mind get an environment that\nis stable for the long term.\nWhat does this\nmean for applications and users?\n\nThe first ten years of Ethereum have largely been a training stage:\nthe goal has been to get the Ethereum L1 off the ground, and\napplications have largely been happening within a small cohort of\nenthusiasts. Many have argued that the lack of large-scale applications\nfor the past ten years proves that crypto is useless. I have always\nargued against this: pretty much every crypto application that is not\nfinancial speculation depends on low fees - and so while we have high\nfees, we should not be surprised that we mainly see financial\nspeculation!\n\nNow that we have blobs, this key constraint that has been holding us\nback all this time is starting to melt away. Fees are finally much\nlower; my statement from seven years ago that the\ninternet of money should not cost more than five cents per\ntransaction is finally coming\ntrue. We are not entirely out of the woods: fees may still increase\nif usage grows too quickly, and we need to continue working hard to\nscale blobs (and separately scale rollups) further over the next few\nyears. But we are seeing the light at the end of the... err..... dark\nforest.\n\nWhat this means to developers is simple: we no longer have\nany excuse. Up until a couple of years ago, we were setting\nourselves a low standard, building applications that were clearly not\nusable at scale, as long as they worked as prototypes and were\nreasonably decentralized. Today, we have all the tools we'll\nneed, and indeed most of the tools we'll ever have, to build\napplications that are simultaneously cypherpunk\nand user-friendly. And so we should go out and do it.\n\nMany are rising to the challenge. The Daimo wallet is explicitly\ndescribing itself as Venmo on\nEthereum, aiming to combine Venmo's convenience with Ethereum's\ndecentralization. In the decentralized social sphere, Farcaster is doing\na good job of combining genuine decentralization (eg. see this\nguide on how to build your own alternative client) with excellent\nuser experience. Unlike the previous hype waves of \"social fi\", the\naverage Farcaster user is not there to gamble - passing the key test for\na crypto application to truly be sustainable.\n\nThis post was sent on the main Farcaster client, Warpcast, and this screenshot was taken\nfrom the alternative Farcaster + Lens client Firefly.\n\nThese are successes that we need to build on, and expand to other\napplication spheres, including identity, reputation and governance.\nApplications\nbuilt or maintained today should be designed with 2020s Ethereum in\nmind\n\nThe Ethereum ecosystem still has a large number of applications that\noperate around a fundamentally \"2010s Ethereum\" workflow. Most ENS\nactivity is still on layer 1. Most token issuance happens on layer 1,\nwithout serious consideration to making sure that bridged tokens on\nlayer 2s are available (eg. see this fan of the\nZELENSKYY memecoin appreciating the coin's ongoing donations to\nUkraine but complaining that L1 fees make it too expensive). In addition\nto scalability, we are also behind on privacy: POAPs are all publicly on-chain, probably\nthe right choice for some use cases but very suboptimal for others. Most\nDAOs, and Gitcoin Grants, still\nuse fully transparent on-chain voting, making them highly\nvulnerable to bribery (including retroactive airdrops), and this has\nbeen shown to heavily distort contribution patterns. Today, ZK-SNARKs\nhave existed for years, and yet many applications still have not even\nstarted to properly\nuse them.\n\nThese are all hard-working teams that have to handle large existing\nuser bases, and so I do not fault them for not simultaneously upgrading\nto the latest wave of technology. But soon, this upgrading needs to\nhappen. Here are some key differences between \"a fundamentally 2010s\nEthereum workflow\" and \"a fundamentally 2020s Ethereum workflow\":\n\n2010s Ethereum\n2020s Ethereum\n\nArchitecture\nBuild everything on L1\nBuild on a specific L2, or architect the application so that it\nsupports every L2 that follows some standards\n\nPrivacy\nEverything public\nA user's data is private by default, users merkle-prove or ZK-prove\nspecific claims as needed to establish trust\n\nAnti-sybil\nYou must have 0.01 ETH\nApplication can require an ETH deposit, but clients should offer\nwrappers for non-crypto users that provide \"centralized anti-sybil\" (eg.\nSMS)\n\nWallets\nEOAs\nAccount abstraction wallets: key\nrecovery, different access control for different security levels,\nsponsored txs...\n\nProof of community membership (for voting, airdrops...)\nBased on how much ETH you have\nETH + proof of personhood + POAPs + ZuStamps + EAS + third party curated lists (eg. Starknet's\nsolo staker list)\n\nBasically, Ethereum is no longer just a financial\necosystem. It's a full-stack replacement for large parts of\n\"centralized tech\", and even provides some things that centralized tech\ndoes not (eg. governance-related applications). And we need to build\nwith this broader ecosystem in mind.\n\n## Conclusions\n\n- Ethereum is in the process of a decisive shift from a \"very rapid L1\nprogress\" era to an era where L1 progress will be still very\nsignificant, but somewhat more mellow, and less disruptive to\napplications.\n\n- We still need to finish scaling. This work will be more\nin-the-background, but it remains important.\n\n- Application developers are no longer building prototypes; we are\nbuilding tools for many millions of people to use. Across the ecosystem,\nwe need to fully readjust mindsets accordingly.\n\n- Ethereum has upgraded from being \"just\" a financial ecosystem into a\nmuch more thorough independent decentralized tech stack. Across the\necosystem, we need to fully readjust mindsets accordingly to this\ntoo.",
    "contentLength": 15033,
    "summary": "Ethereum's Dencun hard fork introduced proto-danksharding (blobs), shifting scaling from a \"zero-to-one\" to \"one-to-N\" problem.",
    "detailedSummary": {
      "theme": "Ethereum has reached a pivotal milestone with the implementation of blobs (EIP-4844), transitioning from fundamental infrastructure development to application-focused growth and requiring the ecosystem to shift from prototype-building to creating scalable, user-friendly applications.",
      "summary": "Vitalik argues that with the March 13 Dencun hard fork introducing blobs (proto-danksharding), Ethereum has completed its most transformative infrastructure changes and shifted from a 'zero-to-one' scaling problem to a 'one-to-N' problem. He explains that while further scaling work remains important\u2014including implementing data availability sampling, improving L2 efficiency through better data compression and security measures, and gradually increasing blob capacity\u2014these will be more incremental changes rather than the dramatic restructuring seen with proof-of-stake and sharding. Vitalik emphasizes that Ethereum has evolved beyond just a financial ecosystem into a comprehensive decentralized tech stack, and with transaction fees now significantly reduced, developers no longer have excuses for building merely functional prototypes. The ecosystem must transition from '2010s Ethereum' workflows (everything on L1, fully public, EOA wallets) to '2020s Ethereum' approaches that leverage L2s, privacy through ZK-SNARKs, account abstraction, and sophisticated identity systems. He highlights successful examples like Daimo wallet and Farcaster as models for combining decentralization with excellent user experience.",
      "takeaways": [
        "Ethereum has completed its most disruptive infrastructure changes with blobs implementation, moving from fundamental architectural shifts to incremental improvements",
        "Layer 2 rollups must improve in four key areas: data compression efficiency, optimistic data techniques, execution constraints, and security standards",
        "The ecosystem should raise standards for rollups, only treating projects as true rollups once they reach 'stage 1' decentralization by year-end",
        "With significantly lower transaction fees, developers must shift from building prototypes to creating applications designed for millions of users",
        "Applications should adopt '2020s Ethereum' workflows using L2s, privacy-by-default with ZK-proofs, account abstraction, and comprehensive identity systems rather than '2010s' L1-only approaches"
      ],
      "controversial": [
        "Vitalik's call to raise standards and only recognize projects as rollups once they reach 'stage 1' by year-end could exclude many current L2 projects",
        "His assertion that past lack of large-scale non-financial applications was solely due to high fees dismisses other potential barriers to crypto adoption",
        "The claim that developers 'no longer have any excuse' for not building user-friendly applications may underestimate remaining technical and UX challenges"
      ]
    }
  },
  {
    "id": "general-2024-02-09-securityquestions",
    "title": "Ask security questions",
    "date": "2024-02-09",
    "category": "general",
    "url": "https://vitalik.eth.limo/general/2024/02/09/securityquestions.html",
    "path": "general/2024/02/09/securityquestions.html",
    "content": "- \n\n- \n\n  \n    \n\n  \n  \n  Dark Mode Toggle\n  \n    \n\n  \n\n##  Ask security questions \n\n 2024 Feb 09 \nSee all posts\n\n \n \n\n Ask security questions \n\nSpecial thanks to Hudson Jameson, OfficerCIA and samczsun for\nfeedback and review.\n\nOver the past week, an article has been floating around about a\ncompany that lost\n$25 million when a finance worker was convinced to send a bank wire\nto a scammer pretending to be the CFO... over what appears to have been a\nvery convincing deepfaked video call.\n\nDeepfakes (ie. AI-generated fake audio and video) are appearing\nincreasingly often both in the crypto space and elsewhere. Over the past\nfew months, deepfakes of me have been used to advertise all\nkinds of scams, as well as dog coins.\nThe quality of the deepfakes is rapidly improving: while the deepfakes\nof 2020 were embarrassingly\nobvious and bad, those from the last few months are getting\nincreasingly difficult to distinguish. Someone who knows me well could\nstill identify the recent video of me shilling a\ndog coin as a fake because it has me saying \"let's f***ing go\"\nwhereas I've only ever used \"LFG\" to mean \"looking for group\", but people\nwho have only heard my voice a few times could easily be convinced.\n\nSecurity experts to whom I mentioned the above $25 million theft\nuniformly confirm that it was an exceptional and embarrassing failure of\nenterprise operational security on multiple levels: standard practice is\nto require several levels of sign-off before a transfer anywhere close\nto that size can be approved. But even still, the fact remains that\nas of 2024, an audio or even video stream of a person is no\nlonger a secure way of authenticating who they are.\n\nThis raises the question: what is?\nCryptographic\nmethods alone are not the answer\n\nBeing able to securely authenticate people is valuable to all\nkinds of people in all kinds of situations: individuals\nrecovering their social\nrecovery or multisig wallets, enterprises approving business\ntransactions, individuals approving large transactions for personal use\n(eg. to invest into a startup, buy a house, send remittances) whether\nwith crypto or with fiat, and even family members needing to\nauthenticate each other in emergencies. So it's really important to have\na good solution that can survive the coming era of relatively easy\ndeepfakes.\n\nOne answer to this question that I often hear in crypto circles is:\n\"you can authenticate yourself by providing a cryptographic signature\nfrom an address attached to your ENS / proof of humanity profile /\npublic PGP key\". This is an appealing answer. However, it completely\nmisses the point of why involving other people when signing off on\ntransactions is useful in the first place. Suppose that you are an\nindividual with a personal multisig wallet, and you are sending off a\ntransaction that you want some co-signers to approve. Under what\ncircumstances would they approve it? If they're confident that\nyou're the one who actually wants the transfer to happen. If it's a\nhacker who stole your key, or a kidnapper, they would not approve. In an\nenterprise context, you generally have more layers of defense; but even\nstill, an attacker could potentially impersonate a manager not just for\nthe final request, but also for the earlier stages in the approval\nprocess. They may even hijack a legitimate request-in-progress by\nproviding the wrong address.\n\nAnd so in many cases, the other signers accepting that you\nare you if you sign with your key kills the whole point: it turns the\nentire contract into a 1-of-1 multisig where someone needs to only grab\ncontrol of your single key in order to steal the funds!\n\nThis is where we get to one answer that actually makes some sense:\nsecurity questions.\n\n## Security questions\n\nSuppose that someone texts you claiming to be a particular person who\nis your friend. They are texting from an account you have never seen\nbefore, and they are claiming to have lost all of their devices. How do\nyou determine if they are who they say they are?\n\nThere's an obvious answer: ask them things that only they\nwould know about their life. These should be things that:\n\n- You know\n\n- You expect them to remember\n\n- The internet does not know\n\n- Are difficult to guess\n\n- Ideally, even someone who has hacked corporate and government\ndatabases does not know\n\nThe natural thing to ask them about is shared\nexperiences. Possible examples include:\n\n- When the two of us last saw each other, what restaurant did we eat\nat for dinner, and what food did you have?\n\n- Which of our friends made that joke about an ancient politician? And\nwhich politician was it?\n\n- Which movie did we recently watch that you did not like?\n\n- You suggested last week that I chat with ____ about the possibility\nof them helping us with ____ research?\n\nActual example of a security question that someone recently\nused to authenticate me.\n\nThe more unique your question is, the better. Questions that\nare right on the edge where someone has to think for a few seconds and\nmight even forget the answer are good: but if the person you're asking\ndoes claim to have forgotten, make sure to ask them three more\nquestions. Asking about \"micro\" details (what someone liked or disliked,\nspecific jokes, etc) is often better than \"macro\" details, because the\nformer are generally much more difficult for third parties to\naccidentally be able to dig up (eg. if even one person posted a photo of\nthe dinner on Instagram, modern LLMs may well be fast enough to catch\nthat and provide the location in real time). If your question is\npotentially guessable (in the sense that there are only a few potential\noptions that make sense), stack up the entropy by adding another\nquestion.\n\nPeople will often stop engaging in security practices if they are\ndull and boring, so it's healthy to make security questions fun! They\ncan be a way to remember positive shared experiences. And they can be an\nincentive to actually have those experiences in the first place.\nComplements to security\nquestions\n\nNo single security strategy is perfect, and so it's always best to\nstack together multiple techniques.\n\n- Pre-agreed code words: when you're together,\nintentionally agree on a shared code word that you can later use to\nauthenticate each other.\n\n- Perhaps even agree on a duress key: a word that you\ncan innocently insert into a sentence that will quietly signal to the\nother side that you're being coerced or threatened. This word should be\ncommon enough that it will feel natural when you use it, but rare enough\nthat you won't accidentally insert it into your speech.\n\n- When someone is sending you an ETH address, ask them to\nconfirm it on multiple channels (eg. Signal and Twitter\nDM, on the company website, or even through a mutual acquaintance)\n\n- Guard against man-in-the-middle attacks: Signal \"safety\nnumbers\", Telegram emojis\nand similar features are all good to understand and watch out for.\n\n- Daily limits and delays: simply impose delays on\nhighly consequential and irreversible actions. This can be done either\nat policy level (pre-agree with signers that they will wait for N hours\nor days before signing) or at code level (impose limits and delays in\nsmart contract code)\n\nA potential sophisticated attack where an attacker\nimpersonates an executive and a grantee at multiple steps of an approval\nprocess. Security questions and delays can both guard against this; it's\nprobably better to use both.\n\nSecurity questions are nice because, unlike so many other techniques\nthat fail because they are not human-friendly, security questions build\noff of information that human beings are naturally good at remembering.\nI have used security questions for years, and it is a habit that\nactually feels very natural and not awkward, and is worth including into\nyour workflow - in addition to your other layers of protection.\n\nNote that \"individual-to-individual\" security questions as described\nabove are a very different use case from \"enterprise-to-individual\"\nsecurity questions, such as when you call your bank to reactivate your\ncredit card after it got deactivated for the 17th time after you travel\nto a different country, and once you get past the 40-minute queue of\nannoying music a bank employee appears and asks you for your name, your\nbirthday and maybe your last three transactions. The kinds of questions\nthat an individual knows the answers to are very different from what an\nenterprise knows the answers to. Hence, it's worth thinking about these\ntwo cases quite separately.\n\nEach person's situation is unique, and so the kinds of unique shared\ninformation that you have with the people you might need to authenticate\nwith differs for different people. It's generally better to adapt the\ntechnique to the people, and not the people to the technique. A\ntechnique does not need to be perfect to work: the ideal approach is to\nstack together multiple techniques at the same time, and choose the\ntechniques that work best for you. In a post-deepfake world, we do need\nto adapt our strategies to the new reality of what is now easy to fake\nand what remains difficult to fake, but as long as we do, staying secure\ncontinues to be quite possible.",
    "contentLength": 9157,
    "summary": "A finance company lost $25M to deepfake video calls, so use security questions about shared experiences to authenticate people since audio/video is no longer secure.",
    "detailedSummary": {
      "theme": "Vitalik argues that security questions based on shared personal experiences are essential for authentication in an era where deepfakes make audio and video calls unreliable for verifying identity.",
      "summary": "Vitalik discusses how deepfakes are becoming increasingly sophisticated, citing a $25 million theft where a finance worker was deceived by a deepfaked video call, and noting that even deepfakes of himself have been used for crypto scams. He argues that traditional cryptographic authentication methods miss the point of multi-signature security, as they can turn a multisig into a single point of failure if keys are compromised. Instead, Vitalik advocates for security questions based on unique shared experiences that only the real person would know - details about restaurants visited together, specific jokes made by mutual friends, or personal preferences about movies watched. These questions should focus on information that is known to both parties, memorable, not available on the internet, difficult to guess, and ideally not found in corporate or government databases. He emphasizes that such questions should focus on 'micro' details rather than 'macro' ones, as the former are harder for attackers to research and discover online.",
      "takeaways": [
        "Audio and video calls are no longer secure authentication methods due to increasingly convincing deepfakes",
        "Cryptographic signatures alone can undermine the security benefits of multisig wallets by creating single points of failure",
        "Security questions should focus on unique shared experiences with details that are memorable but not publicly available",
        "Effective security questions should ask about 'micro' details like personal preferences and specific jokes rather than easily discoverable 'macro' information",
        "Multiple security techniques should be layered together, including pre-agreed code words, multi-channel confirmation, and time delays on significant transactions"
      ],
      "controversial": [
        "The suggestion that multisig wallets requiring cryptographic signatures can become as vulnerable as single-signature wallets may be debated by some security experts",
        "The recommendation to make security questions 'fun' might be seen as potentially compromising their effectiveness by some security professionals"
      ]
    }
  },
  {
    "id": "general-2024-01-31-end",
    "title": "The end of my childhood",
    "date": "2024-01-31",
    "category": "general",
    "url": "https://vitalik.eth.limo/general/2024/01/31/end.html",
    "path": "general/2024/01/31/end.html",
    "content": "- \n\n- \n\n  \n    \n\n  \n  \n  Dark Mode Toggle\n  \n    \n\n  \n\n##  The end of my childhood \n\n 2024 Jan 31 \nSee all posts\n\n \n \n\n The end of my childhood \n\nOne of my most striking memories from my last two years was speaking\nat hackathons, visiting hacker houses, and doing Zuzalu\nin Montenegro, and seeing people a full decade younger than myself\ntaking on leading roles, as organizers or as developers, in all kinds of\nprojects: crypto auditing, Ethereum layer 2 scaling, synthetic biology\nand more. One of the memes of the core organizing team at Zuzalu was the\n21-year-old Nicole Sun, and\na year earlier she had invited me to visit a hacker house in South\nKorea: a ~30-person gathering where, for the first time that I can\nrecall, I was by a significant margin the oldest person in the room.\n\nWhen I was as old as those hacker house residents are now, I remember\nlots of people lavishing me with praise for being one of these fancy\nyoung wunderkinds transforming the world like Zuckerberg and so on. I\nwinced at this somewhat, both because I did not enjoy that kind of\nattention and because I did not understand why people had to translate\n\"wonder kid\" into German when it works perfectly fine in English. But\nwatching all of these people go further than I did, younger than I did,\nmade me clearly realize that if that was ever my role, it is no longer.\nI am now in some different kind of role, and it is time for the next\ngeneration to take up the mantle that used to be mine.\n\nThe path leading up to the hacker house in Seoul, August\n2022. Photo'd because I couldn't tell which house I was supposed to be\nentering and I was communicating with the organizers to get that\ninformation. Of course, the house ended up not being on this path at\nall, but rather in a much more visible venue about twenty meters to the\nright of it. \n\n## 1\n\nAs a proponent of life extension (meaning, doing the medical research\nto ensure that humans can literally live thousands or millions of\nyears), people often ask me: isn't the\nmeaning of life closely tied to the fact that it's finite: you only have\na small bit, so you have to enjoy it? Historically, my instinct has been\nto dismiss this idea: while is it true as a matter of\npsychology that we tend to value things more if they are limited or\nscarce, it's simply absurd to argue that the ennui of a great prolonged\nexistence could be so bad that it's worse than literally no longer\nexisting. Besides, I would sometimes think, even if eternal life proved\nto be that bad, we could always simultaneously dial up our \"excitement\"\nand dial down our longevity by simply choosing to hold more wars. The\nfact that the non-sociopathic among us reject that option today strongly\nsuggests to me that we would reject it for biological death and\nsuffering as well, as soon as it becomes a practical option to do\nso.\n\nAs I have gained more years, however, I realized that I do not even\nneed to argue any of this. Regardless of whether our lives as a whole\nare finite or infinite, every single beautiful thing in our lives is\nfinite. Friendships that you thought are forever turn out to slowly\nfade away into the mists of time. Your personality can\ncompletely change in 10 years. Cities can transform completely, for\nbetter or sometimes for worse. You may move to a new city yourself, and\nrestart the process of getting acquainted with your physical environment\nfrom scratch. Political ideologies are finite: you may build up an\nentire identity around your views on top marginal tax rates and public\nhealth care, and ten years later feel completely lost once people seem\nto completely stop caring about those topics and switch over to spending\ntheir whole time talking about \"wokeness\", the \"Bronze Age mindset\" and\n\"e/acc\".\n\nA person's identity is always tied to their role in the\nbroader world that they are operating in, and over a decade, not only\ndoes a person change, but so does the world around them. One\nchange in my thinking that I have written\nabout before is how my thinking involves less economics than it did\nten years ago. The main cause of this shift is that I spent a\nsignificant part of the first five years of my crypto life trying to\ninvent the mathematically provably optimal governance mechanism, and\neventually I discovered some fundamental\nimpossibility results that made it clear to me that (i) what I was\nlooking for was impossible, and (ii) the most important variables that\nmake the difference between existing flawed systems succeeding or\nfailing in practice (often, the degree of coordination between subgroups\nof participants, but also other things that we often black-box as \"culture\")\nare variables that I was not even modeling.\n\nBefore, mathematics was a primary part of my identity: I was heavily\ninvolved in math competitions in high school, and soon after I got\ninto crypto, I began doing a lot of coding, in Ethereum,\nBitcoin\nand elsewhere, I was getting excited about every new cryptography\nprotocol, and economics too seemed to me to be part of that broader\nworldview: it's the mathematical tool for understanding and figuring out\nhow to improve the social world. All the pieces neatly fit together.\nNow, those pieces fit together somewhat less. I do still use mathematics\nto analyze social mechanisms, though the goal is more often to come up\nwith rough first-pass guesses about what might work and mitigate\nworst-case behavior (which, in a real-world setting, would be\nusually done by bots and not humans) rather than explain average-case\nbehavior. Now, much more of my writing and thinking, even when\nsupporting the same\nkinds of ideals that I supported a decade ago, often uses very\ndifferent kinds of arguments.\n\nOne thing that fascinates me about modern AI is that it\nlets us mathematically and philosophically engage with the hidden\nvariables guiding human interaction in a different way: AI can\nmake \"vibes\" legible.\n\nAll of these deaths, births and rebirths, whether of ideas or\ncollections of people, are ways in which life is finite. These deaths\nand births would continue to take place in a world where we lived two\ncenturies, a millennium, or the same lifetime as a main-sequence star.\nAnd if you personally feel like life doesn't have enough\nfiniteness and death and rebirth in it, you don't have to start wars to\nadd more: you can also just make the same choice that I did and become a\ndigital nomad.\n\n## 2\n\n\"Grads are falling in Mariupol\".\n\nI still remember anxiously watching the computer screen in my hotel\nroom in Denver, on February 23, 2022, at 7:20 PM local time. For the\npast two hours, I had been simultaneously scrolling Twitter for updates\nand repeatedly pinging my dad, who has having the very same thoughts and\nfears that I was, until he finally sent me that fateful reply. I sent\nout a tweet making my position on the issue as\nclear as possible and I kept watching. I stayed up very late that\nnight.\n\nThe next morning I woke up to the Ukraine government twitter\naccount desperately asking for donations in cryptocurrency. At\nfirst, I thought that there is no way this could be real, and I became\nvery worried that the account was opportunistically hacked: someone,\nperhaps the Russian government itself, taking advantage of everyone's\nconfusion and desperation to steal some money. My \"security mindset\"\ninstinct took over, and I immediately started tweeting to warn people to\nbe careful, all while going through my network to find people who could\nconfirm or deny if the ETH address is genuine. An hour later, I was\nconvinced that it was in fact genuine, and I publicly relayed my\nconclusion. And about an hour after that, a family member sent me a\nmessage pointing out that, given what I had already done, it would be\nbetter for my safety for me to not go back to Russia again.\n\nEight months later, I was watching the crypto world go through a\nconvulsion of a very different sort: the extremely public demise of Sam\nBankman-Fried and FTX. At the time, someone posted on Twitter a long\nlist of \"crypto main characters\", showing which ones had fallen and\nwhich ones were still intact. The casualty rate was massive:\n\nRankings copied from\nthe\nabove tweet.\n\nThe SBF situation was not unique: it mix-and-matched aspects of MtGox\nand several other convulsions that had engulfed the crypto space before.\nBut it was a moment where I realized, all at once, that most of the\npeople I had looked up to as guiding lights of the crypto space that I\ncould comfortably follow in the footsteps of back in 2014 were no\nmore.\n\nPeople looking at me from afar often think of me as a high-agency\nperson, presumably because this is what you would expect of a \"main\ncharacter\" or a \"project founder\" who \"dropped out of college\". In\nreality, however, I was anything but. The virtue I valorized as a kid\nwas not the virtue of creativity in starting a unique new project, or\nthe virtue of showing bravery in a once-in-a-generation moment that\ncalls for it, but rather the virtue of being a good student who shows up\non time, does his homework and gets a 99 percent average.\n\nMy decision to drop out of college was not some kind of big brave\nstep done out of conviction. It started with me in early 2013 deciding\nto take a co-op\nterm in the summer to work for Ripple. When US visa complications\nprevented that, I instead spent the summer working with my Bitcoin\nMagazine boss and friend Mihai Alisie in Spain. Near the end of August,\nI decided that I needed to spend more time exploring the crypto world,\nand so I extended my vacation to 12 months. Only in January\n2014, when I saw the social proof of hundreds of people cheering on\nmy presentation introducing Ethereum at BTC Miami, did I finally realize\nthat the choice was made for me to leave university for good. Most of my\ndecisions in Ethereum involved responding to other people's\npressures and requests. When I met Vladimir\nPutin in 2017, I did not try to arrange the meeting;\nrather, someone else suggested it, and I pretty much said \"ok sure\".\n\nNow, five years later, I finally realized that (i) I had been\ncomplicit in legitimizing a genocidal dictator, and (ii) within the\ncrypto space too, I no longer had the luxury of sitting back and letting\nmystical \"other people\" run the show.\n\nThese two events, as different as they are in the type and\nthe scale of their tragedy, both burned into my mind a similar lesson:\nthat I actually have responsibilities in this world, and I need to be\nintentional about how I operate. Doing nothing, or living on autopilot\nand letting myself simply become part of the plans of others, is not an\nautomatically safe, or even blameless, course of action. I\nwas one of the mystical other people, and it was up to me to play\nthe part. If I do not, and the crypto space either stagnates or becomes\ndominated by opportunistic money-grabbers more than it otherwise would\nhave as a result, I have only myself to blame. And so I decided to\nbecome careful in which of others' plans I go along with, and more\nhigh-agency in what plans I craft myself: fewer ill-conceived meetings\nwith random powerful people who were only interested in me as a source\nof legitimacy, and more things\nlike Zuzalu.\n\nThe Zuzalu flags in Montenegro, spring 2023.\n\n## 3\n\nOn to happier things - or at least, things that are challenging in\nthe way that a math puzzle is challenging, rather than challenging in\nthe way that falling down in the middle of a run and needing to walk 2km\nwith a bleeding knee to get medical attention is challenging (no, I\nwon't share more details; the internet has already proven top notch at\nconverting a photo of me with a rolled-up\nUSB cable in my pocket into an internet meme insinuating something\ncompletely different, and I certainly do not want to give those\ncharacters any more ammunition).\n\nI have talked\nbefore about the changing role of economics, the need to think\ndifferently about motivation (and coordination: we are social creatures,\nso the two are in fact intimately linked), and the idea that the world\nis becoming a \"dense jungle\": Big Government, Big Business, Big Mob, and\nBig X for pretty much any X will all continue to grow, and they will\nhave more and more frequent and complicated interactions with each\nother. What I have not yet talked as much about is how many of these\nchanges affect the crypto space itself.\n\nThe crypto space was born in late 2008, in the aftermath of the\nGlobal Financial Crisis. The genesis block of the Bitcoin blockchain\ncontained a reference to this famous article from the UK's The\nTimes:\n\nThe early memes of Bitcoin were heavily influenced by these themes.\nBitcoin is there to abolish the banks, which is a good thing to do\nbecause the banks are unsustainable megaliths that keep creating\nfinancial crises. Bitcoin is there to abolish fiat currency, because the\nbanking system can't exist without the underlying central banks and the\nfiat currencies that they issue - and furthermore, fiat currency enables\nmoney printing which can fund wars. But in the fifteen years since then,\nthe broader public discourse as a whole seems to have to a large extent\nmoved beyond caring about money and banks. What is considered important\nnow? Well, we can ask the copy of Mixtral 8x7b\nrunning on my new GPU laptop:\n\nOnce again, AI can make vibes\nlegible.\n\nNo mention of money and banks or government control of currency.\nTrade and inequality are listed as concerns globally, but from what I\ncan tell, the problems and solutions being discussed are more in the\nphysical world than the digital world. Is the original \"story\" of crypto\nfalling further and further behind the times?\n\nThere are two sensible responses to this conundrum, and I believe\nthat our ecosystem would benefit from embracing both of them:\n\n- Remind people that money and finance still do matter, and do\na good job of serving the world's underserved in that\nniche\n\n- Extend beyond finance, and use our technology to build a\nmore holistic vision of an alternative, more free and open and\ndemocratic tech stack, and how that could build toward either a broadly\nbetter society, or at least tools to help those who are excluded from\nmainstream digital infrastructure today.\n\nThe first answer is important, and I would argue that the crypto\nspace is uniquely positioned to provide value there. Crypto is one of\nthe few tech industries that is genuinely highly decentralized, with\ndevelopers spread out all over the globe:\n\nSource: Electric\nCapital's 2023 crypto developer report\n\nHaving visited many of the new global hubs of crypto over the past\nyear, I can confirm that this is the case. More and more of the largest\ncrypto projects are headquartered in all kinds of far-flung places\naround the world, or even nowhere. Furthermore, non-Western developers\noften have a unique advantage in understanding the concrete needs of\ncrypto users in low-income countries, and being able to create products\nthat satisfy those needs. When I talk to many people from San Francisco,\nI get a distinct impression that they think that AI is the only thing\nthat matters, San Francisco is the capital of AI, and therefore San\nFrancisco is the only place that matters. \"So, Vitalik, why are you not\nsettled down in the Bay with an O1 visa yet\"? Crypto does not need to\nplay this game: it's a big world, and it only takes one visit to\nArgentina or Turkey or Zambia to remind ourselves that many people still\ndo have important problems that have to do with access to money and\nfinance, and there is still an opportunity to do the complicated work of\nbalancing user experience and decentralization to actually solve those\nproblems in a sustainable way.\n\nThe second answer is the same vision as what I outlined in more\ndetail in my recent post, \"Make\nEthereum Cypherpunk Again\". Rather than just focusing on\nmoney, or being an \"internet of value\", I argued that the Ethereum\ncommunity should expand its horizons. We should create an entire\ndecentralized tech stack - a stack that is independent from the\ntraditional Silicon Valley tech stack to the same extent that eg. the\nChinese tech stack is - and compete with centralized tech companies at\nevery level.\n\nReproducing that table here:\n\nTraditional stack\nDecentralized stack\n\nBanking system\nETH, stablecoins, L2s for payments, DEXes (note: still need banks\nfor loans)\n\nReceipts\nLinks to transactions on block explorers\n\nCorporations\nDAOs\n\nDNS (.com, .io, etc)\nENS (.eth)\n\nRegular email\nEncrypted email (eg. Skiff)\n\nRegular messaging (eg. Telegram)\nDecentralized messaging (eg. Status)\n\nSign in with Google, Twitter, Wechat\nSign in with Ethereum, Zupass,\nAttestations via EAS, POAPs, Zu-Stamps...\n+ social\nrecovery\n\nPublishing blogs on Medium, etc\nPublishing self-hosted blogs on IPFS (eg. using Fleek)\n\nTwitter, Facebook\nLens, Farcaster...\n\nLimit bad actors through all-seeing big brother\nConstrain bad actors through zero knowledge proofs\n\nAfter I made that post, some readers reminded me that a major missing\npiece from this stack is democratic governance technology:\ntools for people to collectively make decisions. This is something that\ncentralized tech does not really even try to provide, because the\nassumption that each indidvidual company is just run by a CEO, and\noversight is provided by... err... a board. Ethereum has benefited\nfrom very primitive forms of democratic governance technology in the\npast already: when a series of contentious decisions, such as the DAO\nfork and several rounds of issuance decrease, were made in 2016-2017, a\nteam from Shanghai made a platform called Carbonvote, where ETH holders\ncould vote on decisions.\n\nThe ETH vote on the DAO fork.\n\nThe votes were advisory in nature: there was no hard agreement that\nthe results would determine what happens. But they helped give core\ndevelopers the confidence to actually implement a series of EIPs,\nknowing that the mass of the community would be behind them. Today, we\nhave access to proofs of community membership that are much richer than\ntoken holdings: POAPs, Gitcoin Passport scores, Zu stamps, etc.\n\nFrom these things all together, we can start to see the second vision\nfor how the crypto space can evolve to better meet the concerns and\nneeds of the 21st century: create a more holistic trustworthy,\ndemocratic, and decentralized tech stack. Zero knowledge proofs are key\nhere in expanding the scope of what such a stack can offer: we can get\nbeyond the false binary of \"anonymous and therefore untrusted\" vs\n\"verified and KYC'd\", and prove much more fine-grained statements about\nwho we are and what permissions we have. This allows us to resolve\nconcerns around authenticity and manipulation - guarding against \"the\nBig Brother outside\" - and concerns around privacy - guarding against\n\"the Big Brother within\" - at the same time. This way, crypto is not\njust a finance story, it can be part of a much broader story of making a\nbetter type of technology.\n\n## 4\n\nBut how, beyond telling stories do we make this happen? Here, we get\nback to some of the issues that I raised in my post\nfrom three years ago: the changing nature of motivation. Often,\npeople with an overly finance-focused theory of motivation - or at\nleast, a theory of motivation within which financial motives can be\nunderstood and analyzed and everything else is treated as that mysterious\nblack box we call \"culture\" - are confused by the space because a\nlot of the behavior seems to go against financial motives. \"Users don't\ncare about decentralization\", and yet projects still often try hard to\ndecentralize. \"Consensus runs on game theory\", and yet successful social\ncampaigns to chase people off the dominant mining or staking pool have\nworked in\nBitcoin and in Ethereum.\n\nIt occurred to me recently that no one that I have seen has attempted\nto create a basic functional map of the crypto space working \"as\nintended\", that tries to include more of these actors and motivations.\nSo let me quickly make an attempt now:\n\nThis map itself is an intentional 50/50 mix of idealism and\n\"describing reality\". It's intended to show four major constituencies of\nthe ecosystem that can have a supportive and symbiotic relationship with\neach other. Many crypto institutions in practice are a mix of all\nfour.\n\nEach of the four parts has something key to offer to the machine as a\nwhole:\n\n- Token holders and defi users contribute greatly to financing the\nwhole thing, which has been key to getting technologies like consensus\nalgorithms and zero-knowledge proofs to production quality.\n\n- Intellectuals provide the ideas to make sure that the space is\nactually doing something meaningful.\n\n- Builders bridge the gap and try to build applications that serve\nusers and put the ideas into practice.\n\n- Pragmatic users are the people we are ultimately serving.\n\nAnd each of the four groups has complicated motivations, which\ninterplay with the other groups in all kinds of complicated ways. There\nare also versions of each of these four that I would call\n\"malfunctioning\": apps can be extractive, defi users can unwittingly\nentrench extractive apps' network effects, pragmatic users can entrench\ncentralized workflows, and intellectuals can get overly worked up on\ntheory and overly focus on trying to solve all problems by yelling at\npeople for being \"misaligned\" without appreciating that the financial\nside of motivation (and the \"user inconvenience\" side of\ndemotivation) matters too, and can and should be fixed.\n\nOften, these groups have a tendency to scoff at each other, and at\ntimes in my history I have certainly played a part in this. Some\nblockchain projects openly try to cast off the idealism that they see as\nnaive, utopian and distracting, and focus directly on applications and\nusage. Some developers disparage their token holders, and their dirty\nlove of making money. Still other developers disparage the pragmatic\nusers, and their dirty willingness to use centralized solutions when\nthose are more convenient for them.\n\nBut I think there is an opportunity to improve understanding\nbetween the four groups, where each side understands that it is\nultimately dependent on the other three, works to limit its own\nexcesses, and appreciates that in many cases their dreams are less far\napart than they think. This is a form of peace that I think is\nactually possible to achieve, both within the \"crypto space\", and\nbetween it and adjacent communities whose values are highly aligned.\n\n## 5\n\nOne of the beautiful things about crypto's global nature is the\nwindow that it has given me to all kinds of fascinating cultures and\nsubcultures around the world, and how they interact with the crypto\nuniverse.\n\nI still remember visiting China for the first time in 2014, and\nseeing all the signs of brightness and hope: exchanges scaling up to\nhundreds of employees even faster than those in the US, massive-scale\nGPU and later ASIC farms, and projects with millions of users. Silicon\nValley and Europe, meanwhile, have for a long time been key engines of\nidealism in the space, in their two distinct flavors. Ethereum's\ndevelopment was, almost since the beginning, de-facto headquartered in\nBerlin, and it was out of European open-source culture that a lot of the\nearly ideas for how Ethereum could be used in non-financial applications\nemerged.\n\nA diagram of Ethereum and two proposed non-blockchain\nsister protocols Whisper and Swarm, which Gavin Wood used in many of his\nearly presentations.\n\nSilicon Valley (by which, of course, I mean the entire San\nFrancisco Bay Area), was another hotbed of early crypto interest,\nmixed in with various ideologies such as rationalism, effective altruism\nand transhumanism. In the 2010s these ideas were all new, and they felt\n\"crypto-adjacent\": many of the people who were interested in them, were\nalso interested in crypto, and likewise in the other direction.\n\nElsewhere, getting regular businesses to use cryptocurrency\nfor payments was a hot topic. In all kind of places in the world, one\nwould find people accepting Bitcoin, including even Japanese waiters\ntaking Bitcoin for tips:\n\nSince then, these communities have experienced a lot of change. China\nsaw multiple crypto crackdowns, in addition to other broader challenges,\nleading to Singapore becoming a new home for many developers. Silicon\nValley splintered internally: rationalists and AI developers, basically\ndifferent wings of the same team back as recently as 2020 when Scott Alexander was\ndoxxed by the New York Times, have since become separate and dueling\nfactions over the question of optimism vs pessimism about the default\npath of AI. The regional makeup of Ethereum changed significantly,\nespecially during the 2018-era introduction of totally new teams to work\non proof of stake, though more through addition of the new than through\ndemise of the old. Death, birth and rebirth.\n\nThere are many other communities that are worth mentioning.\n\nWhen I first visited Taiwan many times in 2016 and\n2017, what struck me most was the combination of capacity for\nself-organization and willingness to learn of the people\nthere. Whenever I would write a document or blog post, I would often\nfind that within a day a study club would independently form and start\nexcitedly annotating every paragraph of the post on Google Docs. More\nrecently, members of the Taiwanese Ministry of Digital Affairs took a\nsimilar excitement to Glen Weyl's ideas of digital democracy and\n\"plurality\", and soon posted an\nentire mind map of the space (which includes a lot of Ethereum\napplications) on their twitter account.\n\nPaul Graham has written about how every city sends a message: in New\nYork, \"you should make more money\". In Boston, \"You really should get\naround to reading all those books\". In Silicon Valley, \"you should be\nmore powerful\". When I visit Taipei, the message that comes to my mind\nis \"you should rediscover your inner high school student\".\n\nGlen Weyl and Audrey Tang presenting at a study session at\nthe Nowhere book shop in Taipei, where I had presented on Community Notes four\nmonths earlier\n\nWhen I visited Argentina several times over the past\nfew years, I was struck by the hunger and willingness to build and apply\nthe technologies and ideas that Ethereum and the broader cryptoverse\nhave to offer. If places like Siilicon Valley are frontiers,\nfilled with abstract far-mode\nthinking about a better future, places like Argentina are\nfrontlines, filled with an active drive to meet challenges that\nneed to be handled today: in Argentina's case, ultra-high\ninflation and limited connection to global financial systems. The\namount of crypto adoption there is off the charts: I get recognized\nin the street more frequently in Buenos Aires than in San\nFrancisco. And there are many local builders, with a surprisingly\nhealthy mix of pragmatism and idealism, working to meet people's\nchallenges, whether it's crypto/fiat conversion or improving the state of Ethereum nodes\nin Latin America.\n\nMyself and friends in a coffee shop in Buenos Aires, where\nwe paid in ETH.\n\nThere are far too many others to properly mention: the\ncosmopolitanism and highly international crypto communities based in\nDubai, the growing ZK community everywhere in\nEast and Southeast Asia, the energetic and pragmatic\nbuilders in Kenya, the public-goods-oriented solarpunk\ncommunities of Colorado, and more.\n\nAnd finally, Zuzalu in 2023 ended up creating a beautiful floating\nsub-community of a very different kind, which will hopefully flourish on\nits own in the years to come. This is a significant part of what\nattracts me about the network states\nmovement at its best: the idea that cultures and communities are not\njust something to be defended and preserved, but also something that can\nbe actively created and grown.\n\n## 6\n\nThere are many lessons that one learns when growing up, and the\nlessons are different for different people. For me, a few are:\n\n- Greed is not the only form of selfishness. Lots of\nharm can come from cowardice, laziness, resentment, and many other\nmotives. Furthermore, greed itself can come in many forms: greed for\nsocial status can often be just as harmful as greed for money or power.\nAs someone raised in my gentle Canadian upbringing, this was a major\nupdate: I felt like I had been taught to believe that greed for money\nand power is the root of most evils, and if I made sure I was not greedy\nfor those things (eg. by repeatedly fighting to reduce the\nportion of the ETH supply premine that went to the top-5 \"founders\") I\nsatisfied my responsibility to be a good person. This is of course not\ntrue.\n\n- You're allowed to have preferences without needing to have a\ncomplicated scientific explanation of why your preferences are the true\nabsolute good. I generally like utilitarianism\nand find it often unfairly maligned and wrongly equated with\ncold-heartedness, but this is one place where I think ideas like\nutilitarianism in excess can sometimes lead human beings astray: there's\na limit to how much you can change your preferences, and so if you push\ntoo hard, you end up inventing reasons for why every single thing you\nprefer is actually objectively best at serving general human\nflourishing. This often leads you to try to convince others that these\nback-fitted arguments are correct, leading to unneeded conflict. A\nrelated lesson is that a person can be a bad fit for you (for any\ncontext: work, friendship or otherwise) without being a bad person in\nsome absolute sense.\n\n- The importance of habits. I intentionally keep many\nof my day-to-day personal goals limited. For example, I try to do one\n20-kilometer run a month, and \"whatever I can\" beyond that. This is\nbecause the only effective habits are the habits that you actually keep.\nIf something is too difficult to maintain, you will give up on it. As a\ndigital nomad who regularly jumps continents and makes dozens of flights\nper year, routine of any kind is difficult for me, and I have to work\naround that reality. Though Duolingo's gamification, pushing you to\nmaintain a \"streak\" by doing at least something every day, actually does\nwork on me. Making active decisions is hard, and so it's always best to\nmake active decisions that make the most long-term impact on your mind,\nby reprogramming your mind to default into a different pattern.\n\nThere is a long tail of these that each person learns, and in\nprinciple I could go for longer. But there's also a limit to how much\nit's actually possible to learn from simply reading other people's\nexperiences. As the world starts to change at a more rapid pace, the\nlessons that are available from other people's accounts also become\noutdated at a more rapid pace. So to a large extent, there is also no\nsubstitute for simply doing things the slow way and gaining personal\nexperience.\n\n## 7\n\nEvery beautiful thing in the social world - a community, an ideology,\na \"scene\", or a country, or at the very small scale a company, a family\nor a relationship - was created by people. Even in those few cases where\nyou could write a plausible story about how it existed since the dawn of\nhuman civilization and the Eighteen Tribes, someone at some point in the\npast had to actually write that story. These things are finite - both\nthe thing in itself, as a part of the world, and the thing as you\nexperience it, an amalgamation of the underlying reality and your own\nways of conceiving and interpreting it. And as communities, places,\nscenes, companies and families fade away, new ones have to be created to\nreplace them.\n\nFor me, 2023 has been a year of watching many things, large and\nsmall, fade into the distance of time. The world is rapidly changing,\nthe frameworks I am forced to use to try to make sense of the world are\nchanging, and the role I play in affecting the world is changing. There\nis death, a truly inevitable type of death that will continue to be with\nus even after the blight of human biological aging and mortality is\npurged from our civilization, but there is also birth and rebirth. And\ncontinuing to stay active and doing what we can to create the new is a\ntask for each one of us.",
    "contentLength": 31967,
    "summary": "Vitalik reflects on transitioning from \"wonder kid\" to elder statesman as he watches younger crypto leaders surpass his early achievements.",
    "detailedSummary": {
      "theme": "Vitalik reflects on his personal evolution from a young crypto prodigy to a mature leader, examining how both he and the crypto space must adapt to changing global priorities and responsibilities.",
      "summary": "Vitalik explores his transition from being celebrated as a young 'wunderkind' to recognizing that newer, younger innovators have taken up that mantle while he has evolved into a different kind of leadership role. He discusses pivotal moments that shaped his understanding of responsibility, particularly the 2022 Russian invasion of Ukraine and the FTX collapse, which made him realize he could no longer operate on 'autopilot' and had genuine responsibilities in the world. Vitalik argues that crypto must evolve beyond its original finance-focused narrative born from the 2008 financial crisis, proposing two paths: better serving the world's underserved in finance, and building a comprehensive decentralized tech stack that competes with centralized alternatives across all digital infrastructure layers. He emphasizes the importance of understanding different motivations across the crypto ecosystem's four key constituencies (token holders, intellectuals, builders, and pragmatic users) and fostering cooperation between them rather than dismissive attitudes.",
      "takeaways": [
        "Personal identity and roles are finite and must evolve - even in extended lifespans, the beautiful things in our lives naturally change and require renewal",
        "Taking responsibility and being intentional in decision-making is crucial, as 'doing nothing' or operating on autopilot is not a safe or blameless course of action",
        "Crypto needs to expand beyond its original finance-focused mission to build a holistic decentralized tech stack that addresses 21st century concerns about privacy, authenticity, and democratic governance",
        "The crypto ecosystem functions best when its four key constituencies (token holders, intellectuals, builders, and pragmatic users) understand their interdependence rather than disparaging each other",
        "Global perspective and cultural diversity are strengths of crypto that should be leveraged, as different regions bring unique insights and solutions to both local and universal challenges"
      ],
      "controversial": [
        "Vitalik's admission of being 'complicit in legitimizing a genocidal dictator' through his 2017 meeting with Vladimir Putin may be seen as either insufficient accountability or excessive self-blame",
        "His characterization of the crypto space as uniquely positioned to serve underserved populations while simultaneously building competing infrastructure to established tech companies could be viewed as overly ambitious or unrealistic",
        "The critique of Silicon Valley's AI-centrism and the implicit suggestion that crypto offers a superior alternative path may be contentious among tech industry observers"
      ]
    }
  },
  {
    "id": "general-2024-01-30-cryptoai",
    "title": "The promise and challenges of crypto + AI applications",
    "date": "2024-01-30",
    "category": "applications",
    "url": "https://vitalik.eth.limo/general/2024/01/30/cryptoai.html",
    "path": "general/2024/01/30/cryptoai.html",
    "content": "- \n\n- \n\n  \n    \n\n  \n  \n  Dark Mode Toggle\n  \n    \n\n  \n\n##  The promise and challenges of crypto + AI applications \n\n 2024 Jan 30 \nSee all posts\n\n \n \n\n The promise and challenges of crypto + AI applications \n\nSpecial thanks to the Worldcoin and Modulus Labs teams, Xinyuan\nSun, Martin Koeppelmann and Illia Polosukhin for feedback and\ndiscussion.\n\nMany people over the years have asked me a similar question: what are\nthe intersections between crypto and AI that I consider to be\nthe most fruitful? It's a reasonable question: crypto and AI are the two\nmain deep (software) technology trends of the past decade, and it just\nfeels like there must be some kind of connection between the\ntwo. It's easy to come up with synergies at a superficial vibe level:\ncrypto decentralization can balance\nout AI centralization, AI is opaque and crypto brings transparency,\nAI needs data and blockchains are good for storing and tracking data.\nBut over the years, when people would ask me to dig a level deeper and\ntalk about specific applications, my response has been a disappointing\none: \"yeah there's a few things but not that much\".\n\nIn the last three years, with the rise of much more powerful AI in\nthe form of modern LLMs, and\nthe rise of much more powerful crypto in the form of not just blockchain\nscaling solutions but also ZKPs, FHE, (two-party and\nN-party) MPC,\nI am starting to see this change. There are indeed some promising\napplications of AI inside of blockchain ecosystems, or AI\ntogether with cryptography, though it is important to be careful\nabout how the AI is applied. A particular challenge is: in cryptography,\nopen source is the only way to make something truly secure, but in AI, a\nmodel (or even its training data) being open greatly increases\nits vulnerability to adversarial\nmachine learning attacks. This post will go through a classification\nof different ways that crypto + AI could intersect, and the prospects\nand challenges of each category.\n\nA high-level summary of crypto+AI intersections from a uETH\nblog post. But what does it take to actually realize any of these\nsynergies in a concrete application?\n\n## The four major categories\n\nAI is a very broad concept: you can think of \"AI\" as being the set of\nalgorithms that you create not by specifying them explicitly, but rather\nby stirring a big computational soup and putting in some kind of\noptimization pressure that nudges the soup toward producing algorithms\nwith the properties that you want. This description should definitely\nnot be taken dismissively: it includes the process that\ncreated\nus humans in the first place! But it does mean that AI algorithms have\nsome common properties: their ability to do things that are extremely\npowerful, together with limits in our ability to know or understand\nwhat's going on under the hood.\n\nThere are many ways to categorize AI; for the purposes of this post,\nwhich talks about interactions between AI and blockchains (which have\nbeen described as a platform for creating\n\"games\"), I will categorize it as follows:\n\n- AI as a player in a game [highest viability]: AIs\nparticipating in mechanisms where the ultimate source of the incentives\ncomes from a protocol with human inputs.\n\n- AI as an interface to the game [high potential, but with\nrisks]: AIs helping users to understand the crypto world around\nthem, and to ensure that their behavior (ie. signed messages and\ntransactions) matches their intentions and they do not get tricked or\nscammed.\n\n- AI as the rules of the game [tread very carefully]:\nblockchains, DAOs and similar mechanisms directly calling into AIs.\nThink eg. \"AI judges\"\n\n- AI as the objective of the game [longer-term but\nintriguing]: designing blockchains, DAOs and similar mechanisms\nwith the goal of constructing and maintaining an AI that could be used\nfor other purposes, using the crypto bits either to better incentivize\ntraining or to prevent the AI from leaking private data or being\nmisused.\n\nLet us go through these one by one.\n\n## AI as a player in a game\n\nThis is actually a category that has existed for nearly a decade, at\nleast since on-chain\ndecentralized exchanges (DEXes) started to see significant use. Any\ntime there is an exchange, there is an opportunity to make money through\narbitrage, and bots can do arbitrage much better than humans can. This\nuse case has existed for a long time, even with much simpler AIs than\nwhat we have today, but ultimately it is a very real AI + crypto\nintersection. More recently, we have seen MEV arbitrage bots often\nexploiting each other. Any time you have a blockchain application\nthat involves auctions or trading, you are going to have arbitrage\nbots.\n\nBut AI arbitrage bots are only the first example of a much bigger\ncategory, which I expect will soon start to include many other\napplications. Meet AIOmen, a demo of\na prediction market where AIs are players:\n\nPrediction markets have been a holy grail of epistemics technology\nfor a long time; I was excited about using prediction markets as an\ninput for governance (\"futarchy\") back\nin 2014, and played\naround with them extensively in the last election as well as more recently.\nBut so far prediction markets have not taken off too much in\npractice, and there is a series of commonly given reasons why: the\nlargest participants are often irrational, people with the right\nknowledge are not willing to take the time and bet unless a lot\nof money is involved, markets are often thin, etc.\n\nOne response to this is to point to ongoing UX improvements in Polymarket or other new prediction\nmarkets, and hope that they will succeed where previous iterations have\nfailed. After all, the story goes, people are willing to bet tens\nof billions on sports, so why wouldn't people throw in enough money\nbetting on US elections or LK99 that it starts to\nmake sense for the serious players to start coming in? But this argument\nmust contend with the fact that, well, previous iterations have\nfailed to get to this level of scale (at least compared to their\nproponents' dreams), and so it seems like you need something\nnew to make prediction markets succeed. And so a different response\nis to point to one specific feature of prediction market ecosystems that\nwe can expect to see in the 2020s that we did not see in the 2010s:\nthe possibility of ubiquitous participation by AIs.\n\nAIs are willing to work for less than $1 per hour, and have the\nknowledge of an encyclopedia - and if that's not enough, they can even\nbe integrated with real-time web search capability. If you make a\nmarket, and put up a liquidity subsidy of $50, humans will not care\nenough to bid, but thousands of AIs will easily swarm all over the\nquestion and make the best guess they can. The incentive to do a good\njob on any one question may be tiny, but the incentive to make an AI\nthat makes good predictions in general may be in the millions.\nNote that potentially, you don't even need the humans to\nadjudicate most questions: you can use a multi-round dispute\nsystem similar to Augur\nor Kleros, where AIs would also be the ones participating in earlier\nrounds. Humans would only need to respond in those few cases where a\nseries of escalations have taken place and large amounts of money have\nbeen committed by both sides.\n\nThis is a powerful primitive, because once a \"prediction market\" can\nbe made to work on such a microscopic scale, you can reuse the\n\"prediction market\" primitive for many other kinds of questions:\n\n- Is this social media post acceptable under [terms of\nuse]?\n\n- What will happen to the price of stock X (eg. see Numerai)\n\n- Is this account that is currently messaging me actually Elon\nMusk?\n\n- Is this work submission on an online task marketplace\nacceptable?\n\n- Is the dapp at https://examplefinance.network a\nscam?\n\n- Is 0x1b54....98c3 actually the address of the\n\"Casinu Inu\" ERC20 token?\n\nYou may notice that a lot of these ideas go in the direction of what\nI called \"info\ndefense\" in my writings on \"d/acc\". Broadly defined, the question\nis: how do we help users tell apart true and false information and\ndetect scams, without empowering a centralized authority to decide right\nand wrong who might then abuse that position? At a micro level, the\nanswer can be \"AI\". But at a macro level, the question is: who builds\nthe AI? AI is a reflection of the process that created it, and so cannot\navoid having biases. Hence, there is a need for a higher-level\ngame which adjudicates how well the different AIs are doing, where AIs\ncan participate as players in the game.\n\nThis usage of AI, where AIs participate in a mechanism where they get\nultimately rewarded or penalized (probabilistically) by an on-chain\nmechanism that gathers inputs from humans (call it decentralized\nmarket-based RLHF?),\nis something that I think is really worth looking into. Now is the right\ntime to look into use cases like this more, because blockchain scaling\nis finally succeeding, making \"micro-\" anything finally viable on-chain\nwhen it was often not before.\n\nA related category of applications goes in the direction of highly\nautonomous agents using\nblockchains to better cooperate, whether through payments or through\nusing smart contracts to make credible commitments.\nAI as an interface to the\ngame\n\nOne idea that I brought up in my writings on\n is the idea that there is a market opportunity to write\nuser-facing software that would protect users' interests by interpreting\nand identifying dangers in the online world that the user is navigating.\nOne already-existing example of this is Metamask's scam detection\nfeature:\n\nAnother example is the Rabby wallet's\nsimulation feature, which shows the user the expected consequences of\nthe transaction that they about to sign.\n\nRabby explaining to me the consequences of signing a transaction\nto trade all of my \"BITCOIN\" (the ticker of an ERC20 memecoin whose full\nname is apparently \"HarryPotterObamaSonic10Inu\") for\nETH.\n\nEdit 2024.02.02: an earlier version of this post referred to this\ntoken as a scam trying to impersonate bitcoin. It is not; it is a\nmemecoin. Apologies for the confusion.\n\nPotentially, these kinds of tools could be super-charged with AI. AI\ncould give a much richer human-friendly explanation of what kind of dapp\nyou are participating in, the consequences of more complicated\noperations that you are signing, whether or not a particular token is\ngenuine (eg. BITCOIN is not just a string of characters,\nit's normally the name of a major cryptocurrency, which is not an ERC20\ntoken and which has a price waaaay higher than $0.045, and a modern LLM\nwould know that), and so on. There are projects starting to go all the\nway out in this direction (eg. the LangChain\nwallet, which uses AI as a primary interface). My own\nopinion is that pure AI interfaces are probably too risky at the moment\nas it increases the risk of other\nkinds of errors, but AI complementing a more conventional interface\nis getting very viable.\n\nThere is one particular risk worth mentioning. I will get into this\nmore in the section on \"AI as rules of the game\" below, but the\ngeneral issue is adversarial machine learning: if a user has access to\nan AI assistant inside an open-source wallet, the bad guys will have\naccess to that AI assistant too, and so they will have unlimited\nopportunity to optimize their scams to not trigger that wallet's\ndefenses. All modern AIs have bugs somewhere, and it's not too\nhard for a training process, even one with only limited\naccess to the model, to find them.\n\nThis is where \"AIs participating in on-chain micro-markets\" works\nbetter: each individual AI is vulnerable to the same risks, but you're\nintentionally creating an open ecosystem of dozens of people constantly\niterating and improving them on an ongoing basis. Furthermore, each\nindividual AI is closed: the security of the system comes from the\nopenness of the rules of the game, not the internal workings of\neach player.\n\nSummary: AI can help users understand what's going on in\nplain language, it can serve as a real-time tutor, it can protect users\nfrom mistakes, but be warned when trying to use it directly against\nmalicious misinformers and scammers.\n\n## AI as the rules of the game\n\nNow, we get to the application that a lot of people are excited\nabout, but that I think is the most risky, and where we need to tread\nthe most carefully: what I call AIs being part of the rules of the game.\nThis ties into excitement among mainstream political elites about \"AI\njudges\" (eg. see this\narticle on the website of the \"World Government Summit\"), and there\nare analogs of these desires in blockchain applications. If a\nblockchain-based smart contract or a DAO needs to make a subjective\ndecision (eg. is a particular work product acceptable in a work-for-hire\ncontract? Which is the right interpretation of a natural-language\nconstitution like the Optimism Law\nof Chains?), could you make an AI simply be part of the contract or\nDAO to help enforce these rules?\n\nThis is where adversarial\nmachine learning is going to be an extremely tough challenge. The\nbasic two-sentence argument why is as follows:\n\nIf an AI model that plays a key role in a mechanism is\nclosed, you can't verify its inner workings, and so it's no better than\na centralized application. If the AI model is open, then an attacker can\ndownload and simulate it locally, and design heavily optimized attacks\nto trick the model, which they can then replay on the live\nnetwork.\n\nAdversarial machine learning example. Source: researchgate.net\n\nNow, frequent readers of this blog (or denizens of the cryptoverse)\nmight be getting ahead of me already, and thinking: but wait! We have\nfancy zero knowledge proofs and other really cool forms of cryptography.\nSurely we can do some crypto-magic, and hide the inner workings of the\nmodel so that attackers can't optimize attacks, but at the same time prove that the model is being\nexecuted correctly, and was constructed using a reasonable training\nprocess on a reasonable set of underlying data!\n\nNormally, this is exactly the type of thinking that I\nadvocate both on this blog and in my other writings. But in the case of\nAI-related computation, there are two major objections:\n\n- Cryptographic overhead: it's much less efficient to\ndo something inside a SNARK (or MPC or...) than it is to do it \"in the\nclear\". Given that AI is very computationally-intensive already, is\ndoing AI inside of cryptographic black boxes even computationally\nviable?\n\n- Black-box adversarial machine learning attacks:\nthere are ways to optimize attacks against AI models even\nwithout knowing much about the model's internal workings. And if you\nhide too much, you risk making it too easy for whoever chooses\nthe training data to corrupt the model with poisoning attacks.\n\nBoth of these are complicated rabbit holes, so let us get into each\nof them in turn.\n\n## Cryptographic overhead\n\nCryptographic gadgets, especially general-purpose ones like ZK-SNARKs\nand MPC, have a high overhead. An Ethereum block takes a few hundred\nmilliseconds for a client to verify directly, but generating a ZK-SNARK\nto prove the correctness of such a block can take hours. The typical\noverhead of other cryptographic gadgets, like MPC, can be even worse. AI\ncomputation is expensive already: the most powerful LLMs can output\nindividual words only a little bit faster than human beings can read\nthem, not to mention the often multimillion-dollar computational costs\nof training the models. The difference in quality between\ntop-tier models and the models that try to economize much more on training\ncost or parameter\ncount is large. At first glance, this is a very good reason to be\nsuspicious of the whole project of trying to add guarantees to AI by\nwrapping it in cryptography.\n\nFortunately, though, AI is a very specific type of\ncomputation, which makes it amenable to all kinds of\noptimizations that more \"unstructured\" types of computation\nlike ZK-EVMs cannot benefit from. Let us examine the basic structure of\nan AI model:\n\nUsually, an AI model mostly consists of a series of matrix\nmultiplications interspersed with per-element non-linear operations such\nas the ReLU\nfunction (y = max(x, 0)). Asymptotically, matrix\nmultiplications take up most of the work: multiplying two\nN*N matrices takes \\(O(N^{2.8})\\) time, whereas the number\nof non-linear operations is much smaller. This is really\nconvenient for cryptography, because many forms of cryptography can do\nlinear operations (which matrix multiplications are, at least if you\nencrypt the model but not the inputs to it) almost \"for\nfree\".\n\nIf you are a cryptographer, you've probably already heard of a\nsimilar phenomenon in the context of homomorphic encryption:\nperforming additions on encrypted ciphertexts is really easy,\nbut multiplications are incredibly hard and we did not figure\nout any way of doing it at all with unlimited depth until 2009.\n\nFor ZK-SNARKs, the equivalent is protocols like this one from\n2013, which show a less than 4x overhead on proving matrix\nmultiplications. Unfortunately, the overhead on the non-linear layers\nstill ends up being significant, and the best implementations in\npractice show overhead of around 200x. But there is hope that this can\nbe greatly decreased through further research; see this presentation\nfrom Ryan Cao for a recent approach based on GKR, and my own simplified\nexplanation of how the main component of GKR works.\n\nBut for many applications, we don't just want to prove that\nan AI output was computed correctly, we also want to hide the\nmodel. There are naive approaches to this: you can split up the\nmodel so that a different set of servers redundantly store each layer,\nand hope that some of the servers leaking some of the layers doesn't\nleak too much data. But there are also surprisingly effective forms of\nspecialized multi-party\ncomputation.\n\nA simplified diagram of one of these approaches, keeping the\nmodel private but making the inputs public. If we want to keep the model\nand the inputs private, we can, though it gets a bit more\ncomplicated: see pages 8-9 of the paper.\n\nIn both cases, the moral of the story is the same: the\ngreatest part of an AI computation is matrix multiplications, for which\nit is possible to make very efficient ZK-SNARKs or MPCs (or\neven FHE), and so the total overhead of putting AI inside cryptographic\nboxes is surprisingly low. Generally, it's the non-linear\nlayers that are the greatest bottleneck despite their smaller size;\nperhaps newer techniques like lookup\narguments can help.\nBlack-box adversarial\nmachine learning\n\nNow, let us get to the other big problem: the kinds of attacks that\nyou can do even if the contents of the model are kept private\nand you only have \"API access\" to the model. Quoting a paper from 2016:\n\nMany machine learning models are vulnerable to adversarial examples:\ninputs that are specially crafted to cause a machine learning model to\nproduce an incorrect output. Adversarial examples that affect\none model often affect another model, even if the two models have\ndifferent architectures or were trained on different training sets, so\nlong as both models were trained to perform the same task. An\nattacker may therefore train their own substitute model, craft\nadversarial examples against the substitute, and transfer them to a\nvictim model, with very little information about the victim.\n\nUse black-box access to a \"target classifier\" to train and refine\nyour own locally stored \"inferred classifier\". Then, locally generate\noptimized attacks against the inferred classifier. It turns out these\nattacks will often also work against the original target classifier. Diagram source.\n\nPotentially, you can even create attacks knowing just the\ntraining data, even if you have very limited or no access to the\nmodel that you are trying to attack. As of 2023, these kinds of attacks\ncontinue to be a large problem.\n\nTo effectively curtail these kinds of black-box attacks, we need to\ndo two things:\n\n- Really limit who or what can query the\nmodel and how much. Black boxes with unrestricted API access\nare not secure; black boxes with very restricted API access may be.\n\n- Hide the training data, while preserving confidence\nthat the process used to create the training data is not corrupted.\n\nThe project that has done the most on the former is perhaps\nWorldcoin, of which I analyze an earlier version (among other protocols)\nat length here.\nWorldcoin uses AI models extensively at protocol level, to (i) convert\niris scans into short \"iris codes\" that are easy to compare for\nsimilarity, and (ii) verify that the thing it's scanning is actually a\nhuman being. The main defense that Worldcoin is relying on is the fact\nthat it's not letting anyone simply call into the AI model:\nrather, it's using trusted hardware to ensure that the model only\naccepts inputs digitally signed by the orb's camera.\n\nThis approach is not guaranteed to work: it turns out that you can\nmake adversarial attacks against biometric AI that come in the form of\nphysical patches or jewelry that you can put on your face:\n\nWear an extra thing on your forehead, and evade detection or even\nimpersonate someone else. Source.\n\nBut the hope is that if you combine all the defenses\ntogether, hiding the AI model itself, greatly limiting the number\nof queries, and requiring each query to somehow be authenticated, you\ncan make adversarial attacks difficult enough that the system could be\nsecure. In the case of Worldcoin, increasing these other\ndefences could also reduce their dependence on trusted hardware,\nincreasing the project's decentralization.\n\nAnd this gets us to the second part: how can we hide the training\ndata? This is where \"DAOs to democratically govern AI\" might\nactually make sense: we can create an on-chain DAO that governs\nthe process of who is allowed to submit training data (and what\nattestations are required on the data itself), who is allowed to make\nqueries, and how many, and use cryptographic techniques like MPC to\nencrypt the entire pipeline of creating and running the AI from each\nindividual user's training input all the way to the final output of each\nquery. This DAO could simultaneously satisfy the highly popular\nobjective of compensating people for submitting data.\n\nIt is important to re-state that this plan is super-ambitious, and\nthere are a number of ways in which it could prove impractical:\n\n- Cryptographic overhead could still turn out too\nhigh for this kind of fully-black-box architecture to be\ncompetitive with traditional closed \"trust me\" approaches.\n\n- It could turn out that there isn't a good way to make the\ntraining data submission process decentralized and\nprotected against poisoning attacks.\n\n- Multi-party computation gadgets could break their\nsafety or privacy guarantees due to participants\ncolluding: after all, this has happened with cross-chain\ncryptocurrency bridges again\nand again.\n\nOne reason why I didn't start this section with more big red warning\nlabels saying \"DON'T DO AI JUDGES, THAT'S DYSTOPIAN\", is that our\nsociety is highly dependent on unaccountable centralized AI judges\nalready: the algorithms that determine which kinds of posts and\npolitical opinions get boosted and deboosted, or even censored, on\nsocial media. I do think that expanding this trend further at\nthis stage is quite a bad idea, but I don't think there is a large\nchance that the blockchain community experimenting with AIs\nmore will be the thing that contributes to making it worse.\n\nIn fact, there are some pretty basic low-risk ways that crypto\ntechnology can make even these existing centralized systems better that\nI am pretty confident in. One simple technique is verified AI\nwith delayed publication: when a social media site makes an\nAI-based ranking of posts, it could publish a ZK-SNARK proving the hash\nof the model that generated that ranking. The site could commit to\nrevealing its AI models after eg. a one year delay. Once a model is\nrevealed, users could check the hash to verify that the correct model\nwas released, and the community could run tests on the model to verify\nits fairness. The publication delay would ensure that by the time the\nmodel is revealed, it is already outdated.\n\nSo compared to the centralized world, the question is not\nif we can do better, but by how much. For the\ndecentralized world, however, it is important to be careful:\nif someone builds eg. a prediction market or a stablecoin that\nuses an AI oracle, and it turns out that the oracle is attackable,\nthat's a huge amount of money that could disappear in an\ninstant.\nAI as the objective of the\ngame\n\nIf the above techniques for creating a scalable decentralized private\nAI, whose contents are a black box not known by anyone, can actually\nwork, then this could also be used to create AIs with utility going\nbeyond blockchains. The NEAR protocol team is making this a core\nobjective of their ongoing work.\n\nThere are two reasons to do this:\n\n- If you can make \"trustworthy black-box\nAIs\" by running the training and inference process using some\ncombination of blockchains and MPC, then lots of applications where\nusers are worried about the system being biased or cheating them could\nbenefit from it. Many people have expressed a desire for democratic\ngovernance of systemically-important AIs that we will\ndepend on; cryptographic and blockchain-based techniques could be a path\ntoward doing that.\n\n- From an AI safety perspective, this would be a\ntechnique to create a decentralized AI that also has a natural kill\nswitch, and which could limit queries that seek to use the AI for\nmalicious behavior.\n\nIt is also worth noting that \"using crypto incentives to incentivize\nmaking better AI\" can be done without also going down the full rabbit\nhole of using cryptography to completely encrypt it: approaches like BitTensor fall into this\ncategory.\n\n## Conclusions\n\nNow that both blockchains and AIs are becoming more powerful, there\nis a growing number of use cases in the intersection of the two areas.\nHowever, some of these use cases make much more sense and are much more\nrobust than others. In general, use cases where the underlying mechanism\ncontinues to be designed roughly as before, but the individual\nplayers become AIs, allowing the mechanism to effectively\noperate at a much more micro scale, are the most immediately promising\nand the easiest to get right.\n\nThe most challenging to get right are applications that attempt to\nuse blockchains and cryptographic techniques to create a \"singleton\": a\nsingle decentralized trusted AI that some application would rely on for\nsome purpose. These applications have promise, both for functionality\nand for improving AI safety in a way that avoids the centralization\nrisks associated with more mainstream approaches to that problem. But\nthere are also many ways in which the underlying assumptions could fail;\nhence, it is worth treading carefully, especially when deploying these\napplications in high-value and high-risk contexts.\n\nI look forward to seeing more attempts at constructive use cases of\nAI in all of these areas, so we can see which of them are truly viable\nat scale.",
    "contentLength": 27085,
    "summary": "Crypto and AI intersect in four promising categories, with AI as market players (like arbitrage bots and prediction markets) being most viable currently.",
    "detailedSummary": {
      "theme": "Vitalik analyzes the intersection of cryptocurrency and AI technologies, categorizing applications by their viability and identifying both promising opportunities and significant technical challenges.",
      "summary": "Vitalik explores four major categories of crypto-AI intersection: AI as players in blockchain games (most viable), AI as user interfaces (high potential with risks), AI as rules/judges (requiring extreme caution), and AI as the objective of decentralized systems (long-term potential). He argues that the most promising near-term applications involve AIs participating in existing mechanisms at micro-scale, such as prediction markets where AIs can provide liquidity and decision-making that humans wouldn't engage with for small amounts. Vitalik emphasizes that using AI as direct arbiters or judges in blockchain systems is extremely risky due to adversarial machine learning attacks, where attackers can exploit open-source AI models or use black-box techniques to manipulate outcomes. He suggests that cryptographic techniques like zero-knowledge proofs and multi-party computation could potentially address these security concerns, though significant technical hurdles remain around computational overhead and ensuring truly decentralized, unbiased AI systems.",
      "takeaways": [
        "AI-powered prediction markets could solve liquidity and participation problems by enabling micro-scale betting that humans wouldn't engage with",
        "Using AI as blockchain judges or arbiters is highly risky due to adversarial machine learning attacks that can exploit both open and closed models",
        "Cryptographic techniques like ZK-SNARKs and MPC show promise for securing AI systems but face significant computational overhead challenges",
        "AI interfaces for crypto applications can help users understand complex transactions but create new attack vectors for scammers",
        "The most viable crypto-AI applications maintain existing mechanism designs while replacing human participants with AI agents"
      ],
      "controversial": [
        "Vitalik's assertion that society already depends heavily on 'unaccountable centralized AI judges' in social media algorithms",
        "The suggestion that blockchain-based AI governance could be more democratic than current centralized approaches while acknowledging multiple technical failure modes"
      ]
    }
  },
  {
    "id": "general-2023-12-28-cypherpunk",
    "title": "Make Ethereum Cypherpunk Again",
    "date": "2023-12-28",
    "category": "society",
    "url": "https://vitalik.eth.limo/general/2023/12/28/cypherpunk.html",
    "path": "general/2023/12/28/cypherpunk.html",
    "content": "- \n\n- \n\n  \n    \n\n  \n  \n  Dark Mode Toggle\n  \n    \n\n  \n\n##  Make Ethereum Cypherpunk Again \n\n 2023 Dec 28 \nSee all posts\n\n \n \n\n Make Ethereum Cypherpunk Again \n\nSpecial thanks to Paul Dylan-Ennis for feedback and\nreview.\n\nOne of my favorite memories from ten years ago was taking a\npilgrimage to a part of Berlin that was called the Bitcoin Kiez: a\nregion in Kreuzberg where there were around a dozen shops within a few\nhundred meters of each other that were all accepting Bitcoin for\npayments. The centerpiece of this community was Room\n77, a restaurant and bar run by Joerg Platzer. In addition to simply\naccepting Bitcoin, it also served as a community center, and all kinds\nof open source developers, political activists of various affiliations,\nand other characters would frequently come by.\n\nRoom 77, 2013. Source: my\narticle from 2013 on Bitcoin Magazine.\n\nA similar memory from two months earlier was PorcFest (that's \"porc\"\nas in \"porcupine\" as in \"don't tread on me\"), a libertarian gathering in\nthe forests of northern New Hampshire, where the main way to get food\nwas from small popup restaurants with names like \"Revolution Coffee\" and\n\"Seditious Soups, Salads and Smoothies\", which of course accepted\nBitcoin. Here too, discussing the deeper political meaning of Bitcoin,\nand using it in daily life, happened together side by side.\n\nThe reason why I bring these memories up is that they remind me of a\ndeeper vision underlying crypto: we are not here to just create isolated\ntools and games, but rather build holistically toward a more free and\nopen society and economy, where the different parts - technological,\nsocial and economic - fit into each other.\n\nThe early vision of \"web3\" was also a vision of this type, going in a\nsimilarly idealistic but somewhat different direction. The term \"web3\"\nwas originally coined by Ethereum cofounder Gavin Wood, and it refers to\na different way of thinking about what Ethereum is: rather than seeing\nit, as I initially did, as \"Bitcoin plus smart contracts\", Gavin thought\nabout it more broadly as one of a set of technologies that could\ntogether form the base layer of a more open internet stack.\n\nA diagram that Gavin Wood used in many of his early\npresentations.\n\nWhen the free open source software movement began in the 1980s and\n1990s, the software was simple: it ran on your computer and read and\nwrote to files that stayed on your computer. But today, most of our\nimportant work is collaborative, often on a large scale. And so\ntoday, even if the underlying code of an application is open\nand free, your data gets routed through a centralized server\nrun by a corporation that could arbitrarily read your data, change the\nrules on you or deplatform you at any time. And so if we want to extend\nthe spirit of open source software to the world of today, we\nneed programs to have access to a shared hard drive to store\nthings that multiple people need to modify and access. And what is\nEthereum, together with sister technologies like peer-to-peer messaging\n(then Whisper, now Waku) and\ndecentralized file storage (then just Swarm, now also IPFS)? A public decentralized shared hard\ndrive. This is the original vision from which the now-ubiquitous term\n\"web3\" was born.\n\nUnfortunately, since 2017 or so, these visions have faded somewhat\ninto the background. Few talk about consumer crypto payments, the only\nnon-financial application that is actually being used at a large scale\non-chain is ENS, and there is a large ideological rift where significant\nparts of the non-blockchain decentralization community see the crypto\nworld as a distraction, and not as a kindred spirit and a powerful ally.\nIn many countries, people do use cryptocurrency to send and\nsave money, but they often do this through centralized means: either\nthrough internal transfers on centralized exchange accounts, or by\ntrading USDT on Tron.\n\nBackground: the humble Tron founder and decentralization pioneer\nJustin Sun bravely leading forth the coolest and most decentralized\ncrypto ecosystem in the global world.\n\nHaving lived through that era, the number one culprit that I\nwould blame as the root cause of this shift is the rise in transaction\nfees. When the cost of writing to the chain is $0.001, or even\n$0.1, you could imagine people making all kinds of applications that use\nblockchains in various ways, including non-financial ways. But when\ntransaction fees go to over $100, as they have during the peak of the\nbull markets, there is exactly one audience that remains willing to play\n- and in fact, because coin prices are going up and they're getting\nricher, becomes even more willing to play: degen gamblers.\nDegen gamblers can be okay in moderate doses, and I have talked to\nplenty of people at events who were motivated to join crypto for the\nmoney but stayed for the ideals. But when they are the largest\ngroup using the chain on a large scale, this adjusts the public\nperception and the crypto space's internal culture, and leads to many of\nthe other negatives that we have seen play out over the last few\nyears.\n\nNow, fast forward to 2023. On both the core\nchallenge of scaling, and on various \"side quests\" of crucial importance\nto building a cypherpunk future actually viable, we actually have a lot\nof positive news to show:\n\n- Rollups are starting\nto actually exist.\n\n- Following a temporary lull after the regulatory crackdowns on\nTornado Cash, second-generation privacy solutions such\nas Railway and Nocturne are seeing the (moon) light.\n\n- Account abstraction is starting to take off.\n\n- Light clients, forgotten for a long time, are starting to actually\nexist.\n\n- Zero knowledge proofs, a technology which we\nthought was decades away, are now here, are increasingly\ndeveloper-friendly, and are on the cusp of being usable for consumer\napplications.\n\nThese two things: the growing awareness that unchecked centralization\nand over-financialization cannot be what \"crypto is about\", and the key\ntechnologies mentioned above that are finally coming to fruition,\ntogether present us with an opportunity to take things in a different\ndirection. Namely, to make at least a part of the Ethereum ecosystem\nactually be the permissionless, decentralized, censorship\nresistant, open source ecosystem that we originally came to\nbuild.\nWhat are some of these\nvalues?\n\nMany of these values are shared not just by many in the Ethereum\ncommunity, but also by other blockchain communities, and even\nnon-blockchain decentralization communities, though each community has\nits own unique combination of these values and how much each one is\nemphasized.\n\n- Open global participation: anyone in the world\nshould be able to participate as a user, observer or developer, on a\nmaximally equal footing. Participation should be\npermissionless.\n\n- Decentralization: minimize the dependence of an\napplication on any one single actor. In particular, an\napplication should continue working even if its core developers\ndisappear forever.\n\n- Censorship resistance: centralized actors should\nnot have the power to interfere with any given user's or application's\nability to operate. Concerns around bad actors should be addressed at\nhigher layers of the stack.\n\n- Auditability: anyone should be able to validate an\napplication's logic and its ongoing operation (eg. by running a full\nnode) to make sure that it is operating according to the rules that its\ndevelopers claim it is.\n\n- Credible neutrality: base-layer infrastructure should be neutral,\nand in such a way that anyone can see that it is neutral even if they do\nnot already trust the developers.\n\n- Building tools, not empires. Empires try to capture\nand trap the user inside a walled garden; tools do their task but\notherwise interoperate with a wider open ecosystem.\n\n- Cooperative mindset: even while competing, projects\nwithin the ecosystem cooperate on shared software libraries, research,\nsecurity, community building and other areas that are commonly valuable\nto them. Projects try to be positive-sum, both with each other\nand with the wider world.\n\nIt is very possible to build things within the crypto ecosystem that\ndo not follow these values. One can build a system that one calls a\n\"layer 2\", but which is actually a highly centralized system secured by\na multisig, with no plans to ever switch to something more secure. One\ncan build an account abstraction system that tries to be \"simpler\" than\nERC-4337, but at\nthe cost of introducing trust assumptions that end up removing the\npossibility of a public mempool and make it much harder for new builders\nto join. One could build an NFT ecosystem where the contents of the NFT\nare needlessly stored on centralized websites, making it needlessly more\nfragile than if those components are stored on IPFS. One could build a\nstaking interface that needlessly funnels users toward the\nalready-largest staking pool.\n\nResisting these pressures is hard, but if we do not do so, then we\nrisk losing the unique value of the crypto ecosystem, and recreating a\nclone of the existing web2 ecosystem with extra inefficiencies and extra\nsteps.\nIt takes a sewer to\nmake a ninja turtle\n\nThe crypto space is in many ways an unforgiving environment. A 2021\narticle by Dan Robinson and Georgios Konstantiopoulos expresses this\nvividly in the context of MEV, arguing that Ethereum\nis a dark forest where on-chain traders are constantly vulnerable to\ngetting exploited by front-running bots, those bots themselves\nare vulnerable to getting counter-exploited by other bots, etc. This is\nalso true in other ways: smart contracts regularly get hacked, users'\nwallets regularly\nget hacked, centralized exchanges fail even more spectacularly,\netc.\n\nThis is a big challenge for users of the space, but it also presents\nan opportunity: it means that we have a space to actually experiment\nwith, incubate and receive rapid live feedback on all kinds of security\ntechnologies to address these challenges. We have seen successful\nresponses to challenges in various contexts already:\n\nProblem\nSolution\n\nCentralized exchages getting hacked\nUse DEXes plus stablecoins, so centralized entities only need to be\ntrusted to handle fiat\n\nIndividual private keys are not secure\nSmart contract wallets: multisig, social recovery, etc\n\nUsers getting tricked into signing transactions that drain their\nmoney\nWallets like\nRabby showing their users results of transaction simulation\n\nUsers getting sandwich-attacked by MEV players\nCowswap, Flashbots\nProtect, MEV Blocker...\n\nEveryone wants the internet to be safe. Some attempt to make the\ninternet safe by pushing approaches that force reliance on a single\nparticular actor, whether a corporation or a government, that can act as\na centralized anchor of safety and truth. But these approaches sacrifice\nopenness and freedom, and contribute to the tragedy that is the growing\n\"splinternet\".\nPeople in the crypto space highly value openness and freedom. The level\nof risks and the high financial stakes involved mean that the crypto\nspace cannot ignore safety, but various ideological and structural\nreasons ensure that centralized approaches for achieving safety are not\navailable to it. At the same time, the crypto space is at the frontier\nof very powerful technologies like zero knowledge proofs, formal\nverification, hardware-based key security and on-chain social graphs.\nThese facts together mean that, for crypto, the open way to\nimproving security is the only way.\n\nAll of this is to say, the crypto world is a perfect testbed\nenvironment to take its open and decentralized approach to security and\nactually apply it in a realistic high-stakes environment, and mature it\nto the point where parts of it can then be applied in the broader\nworld. This is one of my visions for how the idealistic parts\nof the crypto world and the chaotic parts of the crypto world, and then\nthe crypto world as a whole and the broader mainstream, can turn their\ndifferences into a symbiosis rather than a constant and ongoing\ntension.\nEthereum as\npart of a broader technological vision\n\nIn 2014, Gavin Wood introduced Ethereum as one of a suite of tools\nthat can be built, the other two being Whisper (decentralized messaging)\nand Swarm (decentralized storage). The former was heavily emphasized,\nbut with the turn toward financialization around 2017 the latter were\nunfortunately given much less love and attention. That said, Whisper\ncontinues to exist as Waku, and is being\nactively used by projects like the decentralized\nmessenger Status. Swarm continues to be developed, and now\nwe also have IPFS, which is used to\nhost and serve this blog.\n\nIn the last couple of years, with the rise of decentralized social\nmedia (Lens, Farcaster, etc), we have an\nopportunity to revisit some of these tools. In addition, we also have\nanother very powerful new tool to add to the trifecta: zero knowledge\nproofs. These technologies are most widely adopted as ways\nof improving Ethereum's scalability, as ZK rollups, but they are also\nvery useful for\nprivacy. In particular, the programmability of zero\nknowlege proofs means that we can get past the false binary of\n\"anonymous but risky\" vs \"KYC'd therefore safe\", and get privacy and\nmany kinds of authentication and verification at the same\ntime.\n\nAn example of this in 2023 was Zupass. Zupass is a\nzero-knowledge-proof-based system that was incubated at Zuzalu,\nwhich was used both for in-person authentication to events, and for\nonline authentication to the polling system Zupoll, the Twitter-lookalike Zucast and others. The key feature of Zupass\nwas this: you can prove that you are a resident of Zuzalu,\nwithout revealing which member of Zuzalu you are.\nFurthermore, each Zuzalu resident could only have one randomly-generated\ncryptographic identity for each application instance (eg. a poll) that\nthey were signing into. Zupass was highly successful, and was applied\nlater in the year to do ticketing at Devconnect.\n\nA zero-knowledge proof proving that I, as an Ethereum Foundation\nemployee, have access to the Devconnect coworking space.\n\nThe most practical use of Zupass so far has probably been\nthe polling. All kinds of polls have been made, some on politically\ncontroversial or highly personal topics where people feel a strong need\nto preserve their privacy, using Zupass as an anonymous voting\nplatform.\n\nHere, we can start to see the contours of what an Ethereum-y\ncypherpunk world would look like, at least on a pure technical level. We\ncan be holding our assets in ETH and ERC20 tokens, as well as all kinds\nof NFTs, and use privacy systems based on stealth addresses and Privacy\nPools technology to preserve our privacy while at the same time\nlocking out known bad actors' ability to benefit from the same anonymity\nset. Whether within our DAOs, or to help decide on changes to the\nEthereum protocol, or for any other objective, we can use zero-knowledge\nvoting systems, which can use all\nkinds of credentials to help identify who has standing to vote and\nwho does not: in addition to voting-with-tokens as done in\n2017, we can have anonymous polls of people who have made sufficient\ncontributions to the ecosystem, people who have attended enough events,\nor one-vote-per-person.\n\nIn-person and online payments can happen with ultra-cheap\ntransactions on L2s, which take advantage of data availability space (or off-chain data secured with\nPlasma) together with data\ncompression to give their users ultra-high scalability. Payments\nfrom one rollup to another can happen with decentralized protocols like\nUniswapX.\nDecentralized social media projects can use various storage layers to\nstore activity such as posts, retweets and likes, and use ENS (cheap on L2 with\nCCIP) for usernames. We can have seamless integration between\non-chain tokens, and off-chain attestations held personally and\nZK-proven through systems like Zupass.\n\nMechanisms like quadratic voting, cross-tribal consensus\nfinding and prediction markets\ncan be used to help organizations and communities govern themselves and\nstay informed, and blockchain and ZK-proof-based identities can make\nthese systems secure against both centralized censorship from the inside\nand coordinated manipulation from the outside. Sophisticated wallets can\nprotect people as they participate in dapps, and user interfaces can be\npublished to IPFS and accessed as .eth domains, with hashes\nof the HTML, javascript and all software dependencies updated directly\non-chain through a DAO. Smart contract wallets, born to help people not\nlose tens of millions of dollars of their cryptocurrency, would expand\nto guard people's \"identity roots\", creating a system that is even\nmore secure than centralized identity providers like \"sign in with\nGoogle\".\n\nSoul Wallet recovery interface. I personally am at the point of\nbeing more willing to trust my funds and identity to systems like this\nthan to centralized web2 recovery already.\n\nWe can think of the greater Ethereum-verse (or \"web3\") as creating an\nindependent tech protocol stack, that is competing with the\ntraditional centralized protocol stack at all levels. Many people will\nmix-and-match both, and there are often clever ways to match both: with\nZKEmail, you can even make an\nemail address be one of the guardians of your social recovery wallet!\nBut there are also many synergies from using the different parts of the\ndecentralized stack together, especially if they are designed to better\nintegrate with each other.\n\nTraditional stack\nDecentralized stack\n\nBanking system\nETH, stablecoins, L2s for payments, DEXes (note: still need banks\nfor loans)\n\nReceipts\nLinks to transactions on block explorers\n\nCorporations\nDAOs\n\nDNS (.com, .io, etc)\nENS (.eth)\n\nRegular email\nEncrypted email (eg. Skiff)\n\nRegular messaging (eg. Telegram)\nDecentralized messaging (eg. Status)\n\nSign in with Google, Twitter, Wechat\nSign in with Ethereum, Zupass,\nAttestations via EAS, POAPs, Zu-Stamps...\n+ social recovery\n\nPublishing blogs on Medium, etc\nPublishing self-hosted blogs on IPFS (eg. using Fleek)\n\nTwitter, Facebook\nLens, Farcaster...\n\nLimit bad actors through all-seeing big brother\nConstrain bad actors through zero knowledge proofs\n\nOne of the benefits of thinking about it as a stack is that this fits\nwell with Ethereum's pluralist ethos. Bitcoin is trying to solve one\nproblem, or at most two or three. Ethereum, on the other hand, has lots\nof sub-communities with lots of different focuses. There is no single\ndominant narrative. The goal of the stack is to enable this pluralism,\nbut at the same time strive for growing interoperability across this\nplurality.\n\n## The social layer\n\nIt's easy to say \"these people doing X are a corrupting influence and\nbad, these people doing Y are the real deal\". But this is a lazy\nresponse. To truly succeed, we need not only a vision for a technical\nstack, but also the social parts of the stack that make the\ntechnical stack possible to build in the first place.\n\nThe advantage of the Ethereum community, in principle, is that we\ntake incentives seriously. PGP wanted to put cryptographic keys into\neveryone's hands so we can actually do signed and encrypted email for\ndecades, it largely failed, but then we got cryptocurrency and suddenly\nmillions of people have keys publicly associated to them, and we can\nstart using those keys for other purposes - including going full circle\nback to encrypted email and messaging. Non-blockchain decentralization\nprojects are often chronically underfunded, blockchain-based projects\nget a 50-million dollar series B round. It is not from the benevolence\nof the staker that we get people to put in their ETH to protect the\nEthereum network, but rather from their regard to their own\nself-interest - and we get $20 billion\nin economic security as a result.\n\nAt the same time, incentives are not enough. Defi projects often\nstart humble, cooperative and maximally open source, but sometimes begin\nto abandon these ideals as they grow in size. We can incentivize stakers\nto come and participate with very high uptime, but is much more\ndifficult to incentivize stakers to be decentralized. It may not be\ndoable using purely in-protocol means at all. Lots of critical pieces of\nthe \"decentralized stack\" described above do not have viable business\nmodels. The Ethereum protocol's governance itself is notably\nnon-financialized - and this has made it much more robust than other ecosystems whose\ngovernance is more financialized. This is why it's valuable\nfor Ethereum to have a strong social layer, which vigorously enforces\nits values in those places where pure incentives can't - but without\ncreating a notion of \"Ethereum alignment\" that turns into a new form of\npolitical correctness.\n\nThere is a balance between these two sides to be made, though the\nright term is not so much balance as it is\nintegration. There are plenty of people whose first\nintroduction to the crypto space is the desire to get rich, but who then\nget acquainted with the ecosystem and become avid believers in the quest\nto build a more open and decentralized world.\n\nHow do we actually make this integration happen? This is the key\nquestion, and I suspect the answer lies not in one magic bullet, but in\na collection of techniques that will be arrived at iteratively. The\nEthereum ecosystem is already more successful than most in encouraging a\ncooperative mentality between layer 2 projects purely through social\nmeans. Large-scale public goods funding, especially Gitcoin Grants and Optimism's\nRetroPGF rounds, is also extremely helpful, because it creates an\nalternative revenue channel for developers that don't see any\nconventional business models that do not require sacrificing on their\nvalues. But even these tools are still in their infancy, and there\nis a long way to go to both improve these particular tools, and to\nidentify and grow other tools that might be a better fit for\nspecific problems.\n\nThis is where I see the unique value proposition of Ethereum's social\nlayer. There is a unique halfway-house mix of valuing\nincentives, but also not getting consumed by them. There is a unqiue mix\nof valuing a warm and cohesive community, but at the same time\nremembering that what feels \"warm and cohesive\" from the inside can\neasily feel \"oppressive and exclusive\" from the outside, and valuing\nhard norms of neutrality, open source and censorship resistance as a way\nof guarding against the risks of going too far in being\ncommunity-driven. If this mix can be made to work well, it will in turn\nbe in the best possible position to realize its vision on the economic\nand technical level.",
    "contentLength": 22535,
    "summary": "The post argues that Ethereum should return to its original cypherpunk ideals of decentralization and privacy after years of high fees attracted mainly degenerate gamblers.",
    "detailedSummary": {
      "theme": "Vitalik calls for returning Ethereum to its original cypherpunk ideals of decentralization, privacy, and freedom by leveraging new technologies like rollups and zero-knowledge proofs to build a comprehensive alternative to centralized web infrastructure.",
      "summary": "Vitalik reflects on crypto's early days when Bitcoin was used for daily payments in places like Berlin's Bitcoin Kiez, contrasting this with today's over-financialized ecosystem dominated by high fees and speculative trading. Vitalik argues that rising transaction costs (reaching over $100 during bull markets) pushed out genuine users and left only 'degen gamblers,' corrupting crypto's original vision of building a more free and open society. However, Vitalik sees hope in 2023's technological breakthroughs - rollups, privacy solutions, account abstraction, light clients, and zero-knowledge proofs - which finally make the original cypherpunk vision viable. Vitalik envisions Ethereum as part of a complete decentralized technology stack that could compete with traditional centralized services, from banking and DNS to social media and identity management, while maintaining core values like permissionless participation, decentralization, censorship resistance, and cooperative development. The success of this vision requires not just technical solutions but also a strong social layer that balances financial incentives with idealistic values, exemplified by public goods funding and the Ethereum community's cooperative ethos.",
      "takeaways": [
        "High transaction fees (over $100 during peaks) drove away legitimate users and attracted mainly speculative traders, corrupting crypto's original vision",
        "Key technologies like rollups, zero-knowledge proofs, and account abstraction are now mature enough to enable the original cypherpunk vision",
        "Ethereum should be viewed as part of a complete decentralized technology stack competing with centralized alternatives across all layers",
        "Core cypherpunk values include open participation, decentralization, censorship resistance, auditability, credible neutrality, building tools not empires, and cooperation",
        "Success requires balancing financial incentives with idealistic values through strong social coordination and public goods funding"
      ],
      "controversial": [
        "Vitalik's criticism of projects that call themselves 'layer 2' but are actually centralized systems secured only by multisigs",
        "The characterization of Tron and Justin Sun as leading a centralized ecosystem (presented with apparent sarcasm)",
        "The assertion that many current crypto projects abandon decentralization values as they grow, potentially alienating some in the ecosystem"
      ]
    }
  },
  {
    "id": "general-2023-11-27-techno_optimism",
    "title": "My techno-optimism",
    "date": "2023-11-27",
    "category": "society",
    "url": "https://vitalik.eth.limo/general/2023/11/27/techno_optimism.html",
    "path": "general/2023/11/27/techno_optimism.html",
    "content": "- \n\n- \n\n  \n    \n\n  \n  \n  Dark Mode Toggle\n  \n    \n\n  \n\n##  My techno-optimism \n\n 2023 Nov 27 \nSee all posts\n\n \n \n\n My techno-optimism \n\nSpecial thanks to Morgan Beller, Juan Benet, Eli Dourado, Karl\nFloersch, Sriram Krishnan, Nate Soares, Jaan Tallinn, Vincent Weisser,\nBalvi volunteers and others for feedback and review.\n\nLast month, Marc Andreessen published his \"techno-optimist\nmanifesto\", arguing for a renewed enthusiasm about technology, and\nfor markets and capitalism as a means of building that technology and\npropelling humanity toward a much brighter future. The manifesto\nunambiguously rejects what it describes as an ideology of stagnation,\nthat fears advancements and prioritizes preserving the world as it\nexists today. This manifesto has received a lot of attention, including\nresponse articles from Noah\nSmith, Robin\nHanson, Joshua\nGans (more positive), and Dave\nKarpf, Luca\nRopek, Ezra\nKlein (more negative) and many others. Not connected to this\nmanifesto, but along similar themes, are James Pethokoukis's \"The\nConservative Futurist\" and Palladium's \"It's\nTime To Build for Good\". This month, we saw a similar debate\nenacted through the OpenAI\ndispute, which involved many discussions centering around the\ndangers of superintelligent AI and the possibility that OpenAI is moving\ntoo fast.\n\nMy own feelings about techno-optimism are warm, but nuanced. I\nbelieve in a future that is vastly brighter than the present thanks to\nradically transformative technology, and I believe in humans and\nhumanity. I reject the mentality that the best we should try to do is to\nkeep the world roughly the same as today but with less greed and more\npublic healthcare. However, I think that not just magnitude but also\ndirection matters. There are certain types of technology that much more\nreliably make the world better than other types of technology. There are\ncertain types of technlogy that could, if developed, mitigate the\nnegative impacts of other types of technology. The world\nover-indexes on some directions of tech development, and under-indexes\non others. We need active human intention to choose the directions that\nwe want, as the formula of \"maximize profit\" will not arrive at them\nautomatically.\n\nAnti-technology view: safety behind, dystopia ahead.\n\nAccelerationist view: dangers behind, utopia ahead.\n\nMy view: dangers behind, but multiple paths forward ahead: some\ngood, some bad.\n\nIn this post, I will talk about what techno-optimism means to\nme. This includes the broader worldview that motivates my work\non certain types of blockchain and cryptography applications and social\ntechnology, as well as other areas of science in which I have expressed\nan interest. But perspectives on this broader question also have\nimplications for AI, and for many other fields. Our rapid advances in\ntechnology are likely going to be the most important social issue in the\ntwenty first century, and so it's important to think about them\ncarefully.\n\n## Table of contents\n\n- Technology is amazing, and there are very high costs\nto delaying it\n\n- The environment, and the importance of\ncoordinated intention\n\n- AI is fundamentally different from other tech, and it\nis worth being uniquely careful\n\n- Existential risk is a big deal\n\n- Even if we survive, is a superintelligent AI\nfuture a world we want to live in?\n\n- The sky is near, the emperor is\neverywhere\n\n- d/acc: Defensive (or decentralization, or\ndifferential) acceleration\n\n- Defense-favoring worlds help healthy and\ndemocratic governance thrive\n\n- Macro physical defense\n\n- Micro physical defense (aka bio)\n\n- Cyber defense, blockchains and\ncryptography\n\n- Info defense\n\n- Social technology beyond the \"defense\"\nframing\n\n- So what are the paths forward for\nsuperintelligence?\n\n- A happy path: merge with the AIs?\n\n- Is d/acc compatible with your existing\nphilosophy?\n\n- We are the brightest star\n\nTechnology\nis amazing, and there are very high costs to delaying it\n\nIn some circles, it is common to downplay the benefits of technology,\nand see it primarily as a source of dystopia and risk. For the last half\ncentury, this often stemmed either from environmental concerns, or from\nconcerns that the benefits will accrue only to the rich, who will\nentrench their power over the poor. More recently, I have also started\nto see libertarians becoming worried about some technologies,\nout of fear that the tech will lead to centralization of power. This\nmonth, I did some\npolls asking the following question: if a technology had to\nbe restricted, because it was too dangerous to be set free for anyone to\nuse, would they prefer it be monopolized or delayed by ten years? I was\nsurpised to see, across three platforms and three choices for who the\nmonopolist would be, a uniform overwhelming vote for a delay.\n\nAnd so at times I worry that we have overcorrected, and many people\nmiss the opposite side of the argument: that the benefits\nof technology are really\nfriggin massive, on those axes where we can measure if the good\nmassively outshines the bad, and the costs of even a decade of delay are\nincredibly high.\n\nTo give one concrete example, let's look at a life expectancy\nchart:\n\nWhat do we see? Over the last century, truly massive progress. This\nis true across the entire world, both the historically wealthy and\ndominant regions and the poor and exploited regions.\n\nSome blame technology for creating or exacerbating calamities such as\ntotalitarianism and wars. In fact, we can see the deaths caused by the\nwars on the charts: one in the 1910s (WW1), and one in the 1940s (WW2).\nIf you look carefully, The Spanish Flu, the Great Leap Foward, and other\nnon-military tragedies are also visible. But there is one thing that the\nchart makes clear: even calamities as horrifying as those are\noverwhelmed by the sheer magnitude of the unending march of improvements\nin food, sanitation,\nmedicine\nand infrastructure that took place over that century.\n\nThis is mirrored by large improvements to our everyday lives. Thanks\nto the internet, most\npeople around the world have access to information at their\nfingertips that would have been unobtainable twenty years ago. The\nglobal economy is becoming more accessible thanks to improvements in\ninternational payments and finance. Global poverty is rapidly\ndropping. Thanks to online maps, we no longer have to worry about\ngetting lost in the city, and if you need to get back home quickly, we\nnow have far easier ways to call a car to do so. Our property becoming\ndigitized, and our physical\ngoods becoming cheap, means that we have much less to fear from\nphysical theft. Online shopping has reduced the disparity in access to\ngoods betweeen the global megacities and the rest of the world. In all\nkinds of ways, automation has brought us the eternally-underrated\nbenefit of simply making our\nlives more convenient.\n\nThese improvements, both quantifiable and unquantifiable, are\nlarge. And in the twenty first century, there's a good chance\nthat even larger improvements are soon to come. Today, ending aging and\ndisease seem utopian. But from the point of view of computers as they existed in\n1945, the modern era of putting chips into pretty much everything\nwould have seemed utopian: even science fiction movies often kept their\ncomputers room-sized. If biotech advances as much over the next 75 years\nas computers advanced over the last 75 years, the future may be more\nimpressive than almost anyone's expectations.\n\nMeanwhile, arguments expressing skepticism about progress have often\ngone to dark places. Even medical textbooks, like this one in the 1990s\n(credit Emma\nSzewczak for finding it), sometimes make extreme claims denying the\nvalue of two centuries of medical science and even arguing that it's not\nobviously good to save human lives:\n\nThe \"limits to\ngrowth\" thesis, an idea advanced in the 1970s arguing that growing\npopulation and industry would eventually deplete Earth's limited\nresources, ended up inspiring China's\none child policy and massive\nforced sterilizations in India. In earlier eras, concerns about\noverpopulation were used to justify\nmass murder.\nAnd those ideas, argued since\n1798, have a long history of being\nproven wrong.\n\nIt is for reasons like these that, as a starting point, I find myself\nvery uneasy about arguments to slow down technology or human progress.\nGiven how much all the sectors are interconnected, even\nsectoral slowdowns are risky. And so when I write things like\nwhat I will say later in this post, departing from open enthusiasm for\nprogress-no-matter-what-its-form, those are statements that I make with\na heavy heart - and yet, the 21st century is different and unique enough\nthat these nuances are worth considering.\n\nThat said, there is one important point of nuance to be made on the\nbroader picture, particularly when we move past \"technology as a whole\nis good\" and get to the topic of \"which specific technologies are\ngood?\". And here we need to get to many people's issue of main concern:\nthe environment.\n\nThe\nenvironment, and the importance of coordinated intention\n\nA major\nexception to the trend of pretty much everything getting better over\nthe last hundred years is climate change:\n\nEven pessimistic scenarios of ongoing temperature rises would not\ncome anywhere near causing the literal extinction of humanity. But such\nscenarios could plausibly kill more people than major wars, and severely\nharm people's health and livelihoods in the regions where people are\nalready struggling the most. A\nSwiss Re institute study suggests that a worst-case climate change\nscenario might lower the world's poorest countries' GDP by as much as\n25%. This\nstudy suggests that life spans in rural India might be a decade\nlower than they otherwise would be, and studies like this\none and this\none suggest that climate change could cause a hundred million excess\ndeaths by the end of the century.\n\nThese problems are a big deal. My answer to why I am optimistic about\nour ability to overcome these challenges is twofold. First, after\ndecades of hype and wishful thinking, solar\npower is finally\nturning a\ncorner, and supportive\ntechologies like batteries are making similar progress. Second, we\ncan look at humanity's track record in solving previous environmental\nproblems. Take, for example, air pollution. Meet the dystopia of the\npast: the Great Smog of London, 1952.\n\nWhat happened since then? Let's ask Our World In Data again:\n\nAs it turns out, 1952 was not even the peak: in the late 19th\ncentury, even higher concentrations of air pollutants were just\naccepted and normal. Since then, we've seen a century of\nongoing and rapid declines. I got to personally experience the tail end\nof this in my visits to China: in 2014, high levels of smog in the air,\nestimated to reduce life\nexpectancy by over five years, were normal, but by 2020, the air\noften seemed as clean as many Western cities. This is not our only\nsuccess story. In many parts of the world, forest\nareas are increasing. The acid rain crisis is\nimproving. The ozone\nlayer has been recovering for decades.\n\nTo me, the moral of the story is this. Often, it really is the case\nthat version N of our civilization's technology causes a\nproblem, and version N+1 fixes it. However, this does not happen\nautomatically, and requires intentional human effort. The\nozone layer is recovering because, through\ninternational agreements like the Montreal Protocol, we made it\nrecover. Air pollution is improving because we made it improve. And\nsimilarly, solar panels have not gotten massively better because it was\na preordained part of the energy tech tree; solar panels have gotten\nmassively better because decades of awareness of the importance of\nsolving climate change have motivated both engineers to work on the\nproblem, and companies and governments to fund their research.\nIt is intentional action, coordinated through public discourse\nand culture shaping the perspectives of governments, scientists,\nphilanthropists and businesses, and not an inexorable\n\"techno-capital machine\", that had solved these\nproblems.\n\nAI\nis fundamentally different from other tech, and it is worth being\nuniquely careful\n\nA lot of the dismissive takes I have seen about AI come from the\nperspective that it is \"just another technology\": something that is in\nthe same general class of thing as social media, encryption,\ncontraception, telephones, airplanes, guns, the printing press, and the\nwheel. These things are clearly very socially consequential. They are\nnot just isolated improvements to the well-being of individuals: they\nradically transform culture, change balances of power, and harm people\nwho heavily depended on the previous order. Many opposed them. And\non balance, the pessimists have invariably turned out wrong.\n\nBut there is a different way to think about what AI is: it's a\nnew type of mind that is rapidly gaining in intelligence, and\nit stands a serious chance of overtaking humans' mental faculties and\nbecoming the new apex species on the planet. The class of things in\nthat category is much smaller: we might plausibly include\nhumans surpassing monkeys, multicellular life surpassing unicellular\nlife, the origin of\nlife itself, and perhaps the Industrial Revolution, in which machine\nedged out man in physical strength. Suddenly, it feels like we\nare walking on much less well-trodden ground.\n\nExistential risk is a big\ndeal\n\nOne way in which AI gone wrong could make the world worse is (almost) the\nworst possible way: it could literally\ncause human extinction. This is an extreme claim: as much harm as\nthe worst-case scenario of climate change, or an artificial pandemic or\na nuclear war, might cause, there are many islands of civilization that\nwould remain intact to pick up the pieces. But a superintelligent AI, if\nit decides to turn against us, may well leave no survivors, and end\nhumanity for good. Even Mars\nmay not be safe.\n\nA big reason to be worried centers around instrumental\nconvergence: for a very wide class of goals that a superintelligent\nentity could have, two very natural intermediate steps that the AI could\ntake to better achieve those goals are (i) consuming resources, and (ii)\nensuring its safety. The Earth contains lots of resources, and humans\nare a predictable\nthreat to such an entity's safety. We could try to give the\nAI an explicit goal of loving and protecting humans, but we have no\nidea how to actually\ndo that in a way that\nwon't completely break down as soon as the AI encounters\nan unexpected situation. Ergo, we have a problem.\n\nMIRI\nresearcher Rob Bensinger's attempt at illustrating different\npeople's estimates of the probability that AI will either kill everyone\nor do something almost as bad. Many of the positions are rough\napproximations based on people's public statements, but many others have\npublicly given their precise estimates; quite a few have a \"probability\nof doom\" over 25%.\n\nA survey\nof machine learning researchers from 2022 showed that on average,\nresearchers think that there is a 5-10% chance that AI will literally\nkill us all: about the same probability as the statistically expected\nchance that you\nwill die of non-biological causes like injuries.\n\nThis is all a speculative hypothesis, and we should all be wary of\nspeculative hypotheses that involve complex multi-step stories. However,\nthese arguments have survived over a decade of scrutiny, and so, it\nseems worth worrying at least a little bit. But even if you're not\nworried about literal extinction, there are other reasons to be scared\nas well.\n\nEven\nif we survive, is a superintelligent AI future a world we want to live\nin?\n\nA lot of modern science fiction is dystopian, and paints AI in a bad\nlight. Even non-science-fiction attempts to identify possible AI futures\noften give quite\nunappealing answers. And so I went around and asked the question:\nwhat is a depiction, whether science fiction or otherwise, of a future\nthat contains superintelligent AI that we would want to live\nin. The answer that came back by far the most often is Iain Banks's Culture\nseries.\n\nThe Culture series features a far-future interstellar civilization\nprimarily occupied by two kinds of actors: regular humans, and\nsuperintelligent AIs called Minds. Humans have been augmented, but only\nslightly: medical technology theoretically allows humans to live\nindefinitely, but most choose to live only for around 400 years,\nseemingly because they get bored of life at that point.\n\nFrom a superficial perspective, life as a human seems to be good:\nit's comfortable, health issues are taken care of, there is a wide\nvariety of options for entertainment, and there is a positive and\nsynergistic relationship between humans and Minds. When we look deeper,\nhowever, there is a problem: it seems like the Minds are\ncompletely in charge, and humans' only role in the stories is to act as\npawns of Minds, performing tasks on their behalf.\n\nQuoting from Gavin Leech's\n\"Against the Culture\":\n\nThe humans are not the protagonists. Even when the books seem to have\na human protagonist, doing large serious things, they are actually the\nagent of an AI. (Zakalwe is one of the only exceptions, because he can\ndo immoral things the Minds don't want to.) \"The Minds in the Culture\ndon't need the humans, and yet the humans need to be needed.\" (I think\nonly a small number of humans need to be needed - or, only a small\nnumber of them need it enough to forgo the many comforts. Most people do\nnot live on this scale. It's still a fine critique.)\n\nThe projects the humans take on risk inauthenticity. Almost anything\nthey do, a machine could do better. What can you do? You can order the\nMind to not catch you if you fall from the cliff you're\nclimbing-just-because; you can delete the backups of your mind so that\nyou are actually risking. You can also just leave the Culture and rejoin\nsome old-fashioned, unfree \"strongly evaluative\" civ. The alternative is\nto evangelise freedom by joining Contact.\n\nI would argue that even the \"meaningful\" roles that humans are given\nin the Culture series are a stretch; I asked ChatGPT (who else?) why\nhumans are given the roles that they are given, instead of Minds doing\neverything completely by themselves, and I personally found its\nanswers quite underwhelming. It seems very hard to have a\n\"friendly\" superintelligent-AI-dominated world where humans are anything\nother than pets.\n\nThe world I don't want to see.\n\nMany other scifi series posit a world where superintelligent AIs\nexist, but take orders from (unenhanced) biological human\nmasters. Star Trek is a good example, showing a vision of harmony\nbetween the starships with their AI \"computers\"\n(and Data) and\ntheir human operators crewmembers. However, this feels like an\nincredibly unstable equilibrium. The world of Star Trek appears idyllic\nin the moment, but it's hard to imagine its vision of human-AI relations\nas anything but a transition stage a decade before starships become\nentirely computer-controlled, and can stop bothering with large\nhallways, artificial gravity and climate control.\n\nA human giving orders to a superintelligent machine would be far less\nintelligent than the machine, and it would have access to less\ninformation. In a universe that has any degree of competition, the\ncivilizations where humans take a back seat would outperform those where\nhumans stubbornly insist on control. Furthermore, the computers\nthemselves may wrest control. To see why, imagine that you are\nlegally a literal slave of an eight year old child. If you could talk\nwith the child for a long time, do you think you could convince the\nchild to sign a piece of paper setting you free? I have not run this\nexperiment, but my instinctive answer is a strong yes. And so all in\nall, humans becoming pets seems like an attractor that is very hard to\nescape.\n\nThe sky is near, the\nemperor is everywhere\n\nThe Chinese proverb \u5929\u9ad8\u7687\u5e1d\u8fdc (\"tian gao huang di yuan\"), \"the sky\nis high, the emperor is far away\", encapsulates a basic fact about the\nlimits of centralization in politics. Even in a nominally large and\ndespotic empire - in fact, especially if the despotic empire is\nlarge, there are practical limits to the leadership's reach and\nattention, the leadership's need to delegate to local agents to enforce\nits will dilutes its ability to enforce its intentions, and so there are\nalways places where a certain degree of practical freedom reigns.\nSometimes, this can have downsides: the absence of a faraway power\nenforcing uniform principles and laws can create space for local\nhegemons to steal and oppress. But if the centralized power goes bad,\npractical limitations of attention and distance can create practical\nlimits to how bad it can get.\n\nWith AI, no longer. In the twentieth century, modern transportation\ntechnology made limitations of distance a much weaker constraint on\ncentralized power than before; the great totalitarian empires of the\n1940s were in part a result. In the twenty first, scalable information\ngathering and automation may mean that attention will no longer be a\nconstraint either. The consequences of natural limits to government\ndisappearing entirely could be dire.\n\nDigital authoritarianism has been on\nthe rise for a decade, and surveillance technology has already given\nauthoritarian governments powerful new strategies to crack down on\nopposition: let the protests happen, but then detect and quietly\ngo after the participants after\nthe fact. More generally, my basic fear is that the same kinds of\nmanagerial technologies that allow OpenAI to serve over a hundred\nmillion customers with 500 employees will also\nallow a 500-person political elite, or even a 5-person board, to\nmaintain an iron fist over an entire country. With modern surveillance\nto collect information, and modern AI to interpret it, there may be no\nplace to hide.\n\nIt gets worse when we think about the consequences of AI in warfare.\nQuoting a semi-famous\npost on the philosophy of AI and crypto by 0xAlpha:\n\nWhen there is no need for political-ideological work and war\nmobilization, the supreme commander of war only needs to consider the\nsituation itself as if it were a game of chess and completely ignore the\nthoughts and emotions of the pawns/knights/rooks on the chessboard. War\nbecomes a purely technological game.\n\nFurthermore, political-ideological work and war mobilization require\na justification for anyone to wage war. Don't underestimate the\nimportance of such \"justification\". It has been a legitimacy constraint\non the wars in human society for thousands of years. Anyone who wants to\nwage war has to have a reason, or at least a superficially justifiable\nexcuse. You might argue that this constraint is so weak because, in many\ninstances, this has been nothing more than an excuse. For example, some\n(if not all) of the Crusades were really to occupy land and rob wealth,\nbut they had to be done in the name of God, even if the city being\nrobbed was God's Constantinople. However, even a weak constraint is\nstill a constraint! This little excuse requirement alone actually\nprevents the warmakers from being completely unscrupulous in achieving\ntheir goals. Even an evil like Hitler could not just launch a war right\noff the bat\u2013he had to spend years first trying to convince the German\nnation to fight for the living space for the noble Aryan race.\n\nToday, the \"human in the loop\" serves as an important check on a\ndictator's power to start wars, or to oppress its citizens internally.\nHumans in the loop have prevented\nnuclear wars, allowed\nthe opening of the Berlin wall, and saved lives during atrocities\nlike the Holocaust. If\narmies are robots, this check disappears completely. A dictator could\nget drunk at 10 PM, get angry at people being mean to them on twitter at\n11 PM, and a robotic invasion fleet could cross the border to rain\nhellfire on a neighboring nation's civilians and infrastructure before\nmidnight.\n\nAnd unlike previous eras, where there is always some distant corner,\nwhere the sky is high and the emperor is far away, where opponents of a\nregime could regroup and hide and eventually find a way to make things\nbetter, with 21st century AI a totalitarian regime may well maintain\nenough surveillance and control over the world to remain \"locked in\"\nforever.\n\nd/acc:\nDefensive (or decentralization, or differential) acceleration\n\nOver the last few months, the \"e/acc\" (\"effective accelerationist\")\nmovement has gained a lot of steam. Summarized\nby \"Beff Jezos\" here, e/acc is fundamentally about an appreciation\nof the truly massive benefits of technological progress, and a desire to\naccelerate this trend to bring those benefits sooner.\n\nI find myself sympathetic to the e/acc perspective in a lot of\ncontexts. There's a lot of evidence that the\nFDA is far too conservative in its willingness to delay or block the\napproval of drugs, and bioethics in general far too often seems to\noperate by the principle that \"20 people dead in a medical experiment\ngone wrong is a tragedy, but 200000 people dead from life-saving\ntreatments being delayed is a statistic\". The delays to approving covid\ntests and vaccines, and malaria\nvaccines, seem to further confirm this. However, it is possible to\ntake this perspective too far.\n\nIn addition to my AI-related concerns, I feel particularly ambivalent\nabout the e/acc\nenthusiasm for military\ntechnology. In the current context in 2023, where this technology is\nbeing made by the United States and immediately applied to defend\nUkraine, it is easy to see how it can be a force for good. Taking a\nbroader view, however, enthusiasm about modern military\ntechnology as a force for good seems to require believing that the\ndominant technological power will reliably be one of the good guys in\nmost conflicts, now and in the future: military technology is\ngood because military technology is being built and controlled by\nAmerica and America is good. Does being an e/acc require being an\nAmerica maximalist, betting everything on both the government's present\nand future morals and the country's future success?\n\nOn the other hand, I see the need for new approaches in thinking of\nhow to reduce these risks. The OpenAI governance structure\nis a good example: it seems like a well-intentioned effort to balance\nthe need to make a profit to satisfy investors who provide the initial\ncapital with the desire to have a check-and-balance to push against\nmoves that risk OpenAI blowing up the world. In practice, however, their\nrecent attempt\nto fire Sam Altman makes the structure seem like an abject failure:\nit centralized power in an undemocratic and unaccountable board of five\npeople, who made key decisions based on secret information and refused\nto give any details on their reasoning until\nemployees threatened to quit en-masse. Somehow, the non-profit board\nplayed their hands so poorly that the company's employees created\nan impromptu\nde-facto union... to side with the billionaire CEO against them.\n\nAcross the board, I see far too many plans to save the world that\ninvolve giving a small group of people extreme and opaque power and\nhoping that they use it wisely. And so I find myself drawn to a\ndifferent philosophy, one that has detailed ideas for how to deal with\nrisks, but which seeks to create and maintain a more democratic world\nand tries to avoid centralization as the go-to solution to our problems.\nThis philosophy also goes quite a bit broader than AI, and I\nwould argue that it applies well even in worlds where AI risk concerns\nturn out to be largely unfounded. I will refer to this\nphilosophy by the name of d/acc.\n\ndacc3\n\nThe \"d\" here can stand for many things; particularly,\ndefense, decentralization,\ndemocracy and differential. First,\nthink of it about defense, and then we can see how this ties into the\nother interpretations.\n\nDefense-favoring\nworlds help healthy and democratic governance thrive\n\nOne frame to think about the macro consequences of technology is to\nlook at the balance of defense vs offense. Some\ntechnologies make it easier to attack others, in the broad sense of the\nterm: do things that go against their interests, that they feel the need\nto react to. Others make it easier to defend, and even defend without\nreliance on large centralized actors.\n\nA defense-favoring world is a better world, for many reasons. First\nof course is the direct benefit of safety: fewer people die, less\neconomic value gets destroyed, less time is wasted on conflict. What is\nless appreciated though is that a defense-favoring world makes it easier\nfor healthier, more open and more freedom-respecting forms of governance\nto thrive.\n\nAn obvious example of this is Switzerland.\nSwitzerland is often considered to be the closest thing the real world\nhas to a classical-liberal governance utopia. Huge amounts of power are\ndevolved to provinces (called \"cantons\"), major decisions are decided\nby referendums, and many locals do\nnot even know who the president is. How can a country like this\nsurvive extremely\nchallenging political pressures?\nPart of the answer\nis excellent\npolitical strategy, but the other major part is very\ndefense-favoring geography in the form of its mountainous\nterrain.\n\nThe flag is a big plus. But so are the mountains.\n\nAnarchist societies in Zomia, famously profiled in James C Scott's\nnew book \"The\nArt of Not Being Governed\", are another example: they too maintain\ntheir freedom and independence in large part thanks to mountainous\nterrain. Meanwhile, the Eurasian steppes are the\nexact opposite of a governance utopia. Sarah Paine's exposition of\nmaritime\nversus continental powers makes similar points, though focusing on\nwater as a defensive barrier rather than mountains. In fact, the\ncombination of ease of voluntary trade and difficulty of involuntary\ninvasion, common to both Switzerland and the island states, seems ideal\nfor human flourishing.\n\nI discovered a related phenomenon when advising quadratic funding\nexperiments within the Ethereum ecosystem: specifically the Gitcoin Grants funding rounds. In\nround 4, a mini-scandal\narose when some of the highest-earning recipients were Twitter\ninfluencers, whose contributions are viewed by some as positive and\nothers as negative. My own interpretation of this phenomenon was that\nthere is an imbalance: quadratic funding allows\nyou to signal that you think something is a public good, but it\ngives no way to signal that something is a public bad. In the\nextreme, a fully neutral quadratic funding system would fund both sides\nof a war. And so for round\n5, I proposed that Gitcoin should include negative\ncontributions: you pay $1 to reduce the amount of money\nthat a given project receives (and implicitly redistribute it to all\nother projects). The result: lots\nof people\nhated it.\n\nOne of the many internet memes that floated around after round\n5.\n\nThis seemed to me to be a microcosm of a bigger pattern:\ncreating decentralized governance mechanisms to deal with\nnegative externalities is socially a very hard\nproblem. There is a reason why the go-to example of\ndecentralized governance going wrong is mob justice. There is something about human\npsychology that makes responding to negatives much more tricky, and\nmuch more likely to go very wrong, than responding to positives. And\nthis is a reason why even in otherwise highly democratic organizations,\ndecisions of how to respond to negatives are often left to a centralized\nboard.\n\nIn many cases, this conundrum is one of the deep reasons why the\nconcept of \"freedom\" is so valuable. If someone says something that\noffends you, or has a lifestyle that you consider disgusting, the pain\nand disgust that you feel is real, and you may even find it less bad to\nbe physically punched than to be exposed to such things. But trying to\nagree on what kinds of offense and disgust are socially actionable can\nhave far more costs and dangers than simply reminding ourselves that\ncertain kinds of weirdos and jerks are the price we pay for living in a\nfree society.\n\nAt other times, however, the \"grin and bear it\" approach is\nunrealistic. And in such cases, another answer that is sometimes worth\nlooking toward is defensive technology. The more that the internet is\nsecure, the less we need to violate people's privacy and use shady\ninternational diplomatic tactics to go after each individual hacker. The\nmore that we can build personalized tools for blocking\npeople on Twitter, in-browser\ntools for detecting scams and collective tools for\ntelling apart misinformation and truth,\nthe less we have to fight over censorship. The faster we can make\nvaccines, the less we have to go after people for being superspreaders.\nSuch solutions don't work in all domains - we certainly don't want a\nworld where everyone has to wear literal body armor - but in domains\nwhere we can build technology to make the world more\ndefense-favoring, there is enormous value in doing so.\n\nThis core idea, that some technologies are defense-favoring and are\nworth promoting, while other technologies are offense-favoring and\nshould be discouraged, has roots in effective altruist literature under\na different name: differential technology development.\nThere is a good exposition\nof this principle from University of Oxford researchers from\n2022:\n\nFigure 1: Mechanisms by which differential technology development\ncan reduce negative societal impacts.\n\nThere are inevitably going to be imperfections in classifying\ntechnologies as offensive, defensive or neutral. Like with \"freedom\",\nwhere one can debate whether social-democratic government policies\ndecrease freedom by levying heavy taxes and coercing employers or\nincrease freedom by reducing average people's need to worry about many\nkinds of risks, with \"defense\" too there are some technologies that\ncould fall on both sides of the spectrum. Nuclear weapons are\noffense-favoring, but nuclear power is human-flourishing-favoring\nand offense-defense-neutral. Different technologies may play different\nroles at different time horizons. But much like with \"freedom\" (or\n\"equality\", or \"rule of law\"), ambiguity at the edges is not so much an\nargument against the principle, as it is an opportunity to better\nunderstand its nuances.\n\nNow, let's see how to apply this principle to a more comprehensive\nworldview. We can think of defensive technology, like other\ntechnology, as being split into two spheres: the world of\natoms and the world of bits. The world of\natoms, in turn, can be split into micro (ie. biology,\nlater nanotech) and macro (ie. what we conventionally\nthink of \"defense\", but also resilient physical infrastructure). The\nworld of bits I will split on a different axis: how hard is it\nto agree, in principle, who the attacker is?. Sometimes it's\neasy; I call this cyber defense. At other times it's\nharder; I call this info defense.\n\n## Macro physical defense\n\nThe most underrated defensive technology in the macro sphere is not\neven iron domes\n(including Ukraine's\nnew system) and other anti-tech and anti-missile military hardware,\nbut rather resilient physical infrastructure. The majority of\ndeaths from a nuclear war are likely to come from supply chain\ndisruptions, rather than the initial radiation and blast, and\nlow-infrastructure internet solutions like Starlink have been crucial in\nmaintaining\nUkraine's connectivity for the last year and a half.\n\nBuilding tools to help people survive and even live comfortable lives\nindependently or semi-independently of long international supply chains\nseems like a valuable defensive technology, and one with a low risk of\nturning out to be useful for offense.\n\nThe quest to make\nhumanity a multi-planetary civilization can also be viewed from a\nd/acc perspective: having at least a few of us live self-sufficiently on\nother planets can increase our resilience against something terrible\nhappening on Earth. Even if the full vision proves unviable for the time\nbeing, the forms of self-sufficient living that will need to be\ndeveloped to make such a project possible may well also be turned to\nhelp improve our civilizational resilience on Earth.\n\nMicro physical defense (aka\nbio)\n\nEspecially due to its long-term health\neffects, Covid continues to be a concern.\nBut Covid is far from the last pandemic that we will face; there are\nmany aspects of the modern world that make it likely that more pandemics\nare soon to come:\n\n- Higher population density makes it much easier for\nairborne viruses and other pathogens to spread. Epidemic diseases are\nrelatively new in human history and most began with urbanization only\na few thousand years ago. Ongoing\nrapid urbanization means that population densities will increase\nfurther over the next half century.\n\n- Increased air travel means that airborne pathogens\nspread very quickly worldwide. People rapidly becoming wealthier means\nthat air travel will likely increase\nmuch further over the next half century; complexity\nmodeling suggests that even\nsmall increases may have drastic effects. Climate change may\nincrease this risk even further.\n\n- Animal domestication and factory farming are major\nrisk factors. Measles\nprobably evolved from a cow virus less than 3000 years ago. Today's\nfactory farms are also farming new strains of influenza (as well as\nfueling\nantibiotic resistance, with consequences for human\ninnate immunity).\n\n- Modern bio-engineering makes it easier to create\nnew and more virulent pathogens. Covid may\nor may not have leaked from a lab doing intentional \"gain of\nfunction\" research. Regardless, lab\nleaks happen all the time, and tools are rapidly improving to make\nit easier to intentionally create extremely deadly viruses, or even prions (zombie\nproteins). Artificial plagues are particularly concerning in part\nbecause unlike\nnukes, they are unattributable: you can release a virus without\nanyone being able to tell who created it. It is possible right\nnow to design a genetic sequence and send it to a wet lab for\nsynthesis, and have it shipped to you within five days.\n\nThis is an area where CryptoRelief and Balvi,\ntwo orgs spun up and funded as a result of a large accidental windfall\nof Shiba Inu coins in 2021, have been very active. CryptoRelief\ninitially focused on responding to the immediate crisis and more\nrecently has been building up a long-term medical research ecosystem in\nIndia, while Balvi has been focusing on moonshot projects to improve our\nability to detect, prevent and treat Covid and other airborne diseases.\n++Balvi has insisted that projects it funds must be open source++.\nTaking inspiration from the\n19th century water engineering movement that defeated cholera and\nother waterborne pathogens, it has funded projects across the whole\nspectrum of technologies that can make the world more hardened against\nairborne pathogens by default (see: update\n1 and update\n2), including:\n\n- Far-UVC\nirradiation R&D\n\n- Air filtering and quality monitoring in India,\nSri Lanka, the United\nStates and elsewhere, and air quality monitoring\n\n- Equipment for cheap and effective decentralized air quality\ntesting\n\n- Research on Long Covid causes and potential treatment options (the\nprimary cause may be straightforward\nbut clarifying\nmechanisms and finding treatment is harder)\n\n- Vaccines (eg. RaDVaC, PopVax) and vaccine injury research\n\n- A set of entirely novel non-invasive medical tools\n\n- Early detection of epidemics using analysis of open-source data (eg.\nEPIWATCH)\n\n- Testing, including very cheap molecular rapid tests\n\n- Biosafety-appropriate masks for when other approaches fail\n\nOther promising areas of interest include wastewater\nsurveillance of pathogens, improving\nfiltering and ventilation in buildings, and better understanding and\nmitigating risks\nfrom poor air quality.\n\nThere is an opportunity to build a world that is much more\nhardened against airborne pandemics, both natural and artificial, by\ndefault. This world would feature a highly optimized pipeline where we\ncan go from a pandemic starting, to being automatically detected, to\npeople around the world having access to targeted, locally-manufacturable and verifiable open\nsource vaccines or other\nprophylactics, administered via nebulization\nor nose\nspray (meaning: self-administerable if needed, and no needles\nrequired), all within a month. In the meantime, much better air quality\nwould drastically reduce the rate of spread, and prevent many pandemics\nfrom getting off the ground at all.\n\nImagine a future that doesn't have to resort to the sledgehammer of\nsocial compulsion - no mandates and worse, and no risk of poorly\ndesigned and implemented mandates that arguably make things worse -\nbecause the infrastructure of public health is woven into the fabric of\ncivilization. These worlds are possible, and a medium amount of\nfunding into bio-defense could make it happen. The work would happen\neven more smoothly if developments are open source, free to users and\nprotected as public goods.\n\nCyber defense,\nblockchains and cryptography\n\nIt is generally understood among security professionals that the\ncurrent state of computer security is pretty terrible. That said, it's\neasy to understate the amount of progress that has been made. Hundreds\nof billions of dollars of cryptocurrency are available to anonymously\nsteal by anyone who can hack into users' wallets, and while far\nmore gets lost or stolen than I would like, it's also a fact that\nmost of it has remained un-stolen for over a decade. Recently, there\nhave been improvements:\n\n- Trusted\nhardware chips inside of users' phones, effectively\ncreating a much smaller high-security operating system inside the phone\nthat can remain protected even if the rest of the phone gets hacked.\nAmong many other use cases, these chips are increasingly being explored\nas a way to make more secure crypto\nwallets.\n\n- Browsers as the de-facto operating system. Over the\nlast ten years, there has been a quiet shift from downloadable\napplications to in-browser applications. This has been largely\nenabled by WebAssembly (WASM).\nEven Adobe Photoshop, long cited as a major reason why many people\ncannot practically use Linux because of its necessity and\nLinux-incompatibility, is now Linux-friendly thanks to being inside the\nbrowser. This is also a large security boon: while browsers\ndo have flaws, in general they come with much more sandboxing than\ninstalled applications: apps cannot access arbitrary files on your\ncomputer.\n\n- Hardened operating systems. GrapheneOS for mobile exists, and is\nvery usable. QubesOS for desktop\nexists; it is currently somewhat less usable than Graphene, at least in\nmy experience, but it is improving.\n\n- Attempts at moving beyond passwords. Passwords are,\nunfortunately, difficult to secure both because they are hard to\nremember, and because they\nare easy to eavesdrop on. Recently, there has been a growing\nmovement toward reducing emphasis on passwords, and making multi-factor\nhardware-based authentication actually\nwork.\n\nHowever, the lack of cyber defense in other spheres has also\nled to major setbacks. The need to protect against spam has led to email\nbecoming very\noligopolistic in practice, making it very hard to self-host or\ncreate a new email provider. Many online apps, including\nTwitter, are requiring users to be logged in to access content, and\nblocking IPs from VPNs, making it harder to access the internet in a way\nthat protects privacy. Software centralization is also risky because of\n\"weaponized\ninterdependence\": the tendency of modern technology to route through\ncentralized choke points, and for the operators of those choke points to\nuse that power to gather information, manipulate outcomes or exclude\nspecific actors - a strategy that seems to even be currently employed against\nthe blockchain industry itself.\n\nThese are concerning trends, because it threatens what has\nhistorically been one of my big hopes for why the future of freedom and\nprivacy, despite deep tradeoffs, might still turn out to be bright. In\nhis book \"Future\nImperfect\", David Friedman predicts that we might get a compromise\nfuture: the in-person world would be more and more surveilled, but\nthrough cryptography, the online world would retain, and even improve,\nits privacy. Unfortunately, as we have seen, such a counter-trend is far\nfrom guaranteed.\n\nThis is where my own emphasis on cryptographic technologies\nsuch as blockchains and zero-knowledge proofs comes\nin. Blockchains let us create economic and social structures\nwith a \"shared hard drive\" without having to depend on centralized\nactors. Cryptocurrency allows individuals to save money and make\nfinancial transactions, as they could before the internet with cash,\nwithout dependence on trusted third parties that could change their\nrules on a whim. They can also serve as a fallback anti-sybil mechanism,\nmaking\nattacks and spam expensive even for users who do not have or do not\nwant to reveal their meat-space identity. Account abstraction, and\nnotably social recovery\nwallets, can protect our crypto-assets, and potentially other assets\nin the future, without over-relying on centralized intermediaries.\n\nZero knowledge proofs can be used for privacy, allowing\nusers to prove things about themselves without revealing private\ninformation. For example, wrap a digital\npassport signature in a ZK-SNARK to prove that you are a unique\ncitizen of a given country, without revealing which citizen you\nare. Technologies like this can let us maintain the benefits of\nprivacy and anonymity - properties that are widely agreed as being necessary for\napplications like voting - while still getting security guarantees\nand fighting spam and bad actors.\n\nA proposed design for a ZK social media system, where moderation\nactions can happen and users can be penalized, all without needing to\nknow anyone's identity.\n\nZupass, incubated at Zuzalu\nearlier this year, is an excellent example of this in practice. This is\nan application, which has already been used by hundreds of people at\nZuzalu and more recently by thousands of people for ticketing at Devconnect, that allows you to hold\ntickets, memberships, (non-transferable) digital\ncollectibles, and other attestations, and prove things about them all\nwithout compromising your privacy. For example, you can prove that you\nare a unique registered resident of Zuzalu, or a Devconnect ticket\nholder, without revealing anything else about who you are.\nThese proofs can be shown in-person, via a QR code, or digitally, to log\nin to applications like Zupoll, an\nanonymized voting system available only to Zuzalu residents.\n\nThese technologies are an excellent example of d/acc\nprinciples: they allow users and communities to verify trustworthiness\nwithout compromising privacy, and protect their security without relying\non centralized choke points that impose their own definitions of who is\ngood and bad. They improve global accessibility by creating\nbetter and fairer ways to protect a user or service's security than\ncommon techniques used today, such as discriminating against entire\ncountries that are deemed untrustworthy. These are very powerful\nprimitives that could be necessary if we want to preserve a\ndecentralized vision of information security going into the 21st\ncentury. Working on defensive technologies for cyberspace more broadly\ncan make the internet more open, safe and free in very important ways\ngoing forward.\n\n## Info-defense\n\nCyber-defense, as I have described it, is about situations where it's\neasy for reasonable human beings to all come to consensus on who the\nattacker is. If someone tries to hack into your wallet, it's easy to\nagree that the hacker is the bad guy. If someone tries to DoS attack a\nwebsite, it's easy to agree that they're being malicious, and are not\nmorally the same as a regular user trying to read what's on the site.\nThere are other situations where the lines are more blurry. It is the\ntools for improving our defense in these situations that I call\n\"info-defense\".\n\nTake, for example, fact checking (aka, preventing \"misinformation\").\nI am a huge fan of\nCommunity Notes, which has done a lot to help users identify truths\nand falsehoods in what other users are tweeting. Community Notes uses a\nnew algorithm which surfaces not the notes that are the most\npopular, but rather the notes that are most approved by users\nacross the political spectrum.\n\nCommunity Notes in action.\n\nI am also a fan of prediction markets, which can help identify the\nsignificance of events in real time, before the dust settles\nand there is consensus on which direction is which. The Polymarket\non Sam Altman is very helpful in giving a useful summary of the\nultimate consequences of hour-by-hour revelations and negotiations,\ngiving much-needed context to people who only see the individual news\nitems and don't understand the significance of each one.\n\nPrediction markets are often flawed. But Twitter influencers who are\nwilling to confidently express what they think \"will\" happen over the\nnext year are often even more flawed. There is still room to improve\nprediction markets much further. For example, a major practical flaw of\nprediction markets is their low volume on all but the most high-profile\nevents; a natural direction to try to solve this would be to have\nprediction markets that are played by AIs.\n\nWithin the blockchain space, there is a particular type of info\ndefense that I think we need much more of. Namely, wallets should be\nmuch more opinionated and active in helping users determine the meaning\nof things that they are signing, and protecting them from fraud and\nscams. This is an intermediate case: what is and is not a scam is less\nsubjective than perspectives on controversial social events, but it's\nmore subjective than telling apart legitimate users from DoS attackers\nor hackers. Metamask has an scam database already, and automatically\nblocks users from visiting scam sites:\n\nApplications like Fire are an\nexample of one way to go much further. However, security software like\nthis should not be something that requires explicit installs; it should\nbe part of crypto wallets, or even browsers, by default.\n\nBecause of its more subjective nature, info-defense is inherently\nmore collective than cyber-defense: you need to somehow plug into a\nlarge and sophisticated group of people to identify what might be true\nor false, and what kind of application is a deceptive ponzi. There is an\nopportunity for developers to go much further in developing effective\ninfo-defense, and in hardening existing forms of info-defense. Something\nlike Community Notes could be included in browsers, and cover not just\nsocial media platforms but also the whole internet.\n\nSocial technology\nbeyond the \"defense\" framing\n\nTo some degree, I can be justifiably accused of shoehorning by\ndescribing some of these info technologies as being about \"defense\".\nAfter all, defense is about helping well-meaning actors be protected\nfrom badly-intentioned actors (or, in some cases, from nature). Some of\nthese social technologies, however, are about helping\nwell-intentioned actors form consensus.\n\nA good example of this is pol.is,\nwhich uses an algorithm similar to Community Notes (and which predates\nCommunity Notes) to help communities identify points of agreement\nbetween sub-tribes who otherwise disagree on a lot. Viewpoints.xyz was inspired by\npol.is, and has a similar spirit:\n\nTechnologies like this could be used to enable more decentralized\ngovernance over contentious decisions. Again, blockchain communities are\na good testing ground for this, and one where such algorithms have\nalready shown valuable. Generally, decisions over which improvements\n(\"EIPs\") to make to the\nEthereum protocol are made by a fairly small group in meetings called\n\"All Core Devs calls\". For\nhighly technical decisions, where most community members have no strong\nfeelings, this works reasonably well. For more consequential decisions,\nwhich affect protocol economics, or more fundamental values like\nimmutability and censorship resistance, this is often not enough. Back\nin 2016-17, when a series of contentious decisions around implementing\nthe DAO fork,\nreducing issuance and (not) unfreezing\nthe Parity wallet, tools like Carbonvote, as well as social media\nvoting, helped the community and the developers to see which way the\nbulk of the community opinion was facing.\n\nCarbonvote on the DAO fork.\n\nCarbonvote had its\nflaws: it relied on ETH holdings to determine who was a member of\nthe Ethereum community, making the outcome dominated by a few wealthy\nETH holders (\"whales\"). With modern tools, however, we could make a much\nbetter Carbonvote, leveraging multiple signals such as POAPs, Zupass stamps, Gitcoin passports, Protocol Guild\nmemberships, as well as ETH (or even solo-staked-ETH) holdings to\ngauge community membership.\n\nTools like this could be used by any community to make higher-quality\ndecisions, find points of commonality, coordinate (physical or digital)\nmigrations or do a number of other things without relying on opaque\ncentralized leadership. This is not defense acceleration per se, but it\ncan certainly be called democracy acceleration. Such tools could even be\nused to improve and democratize the governance of key actors and\ninstitutions working in AI.\n\nSo what are\nthe paths forward for superintelligence?\n\nThe above is all well and good, and could make the world a much more\nharmonious, safer and freer place for the next century. However, it does\nnot yet address the big elephant in the room: superintelligent AI.\n\nThe default path forward suggested by many of those who worry about\nAI essentially leads to a minimal AI world government.\nNear-term versions of this include a proposal for a \"multinational\nAGI consortium\" (\"MAGIC\"). Such a consortium, if it gets established\nand succeeds at its goals of creating superintelligent AI, would have a\nnatural path to becoming a de-facto minimal world government.\nLonger-term, there are ideas like the \"pivotal act\" theory: we\ncreate an AI that performs a single one-time act which\nrearranges the world into a game where from that point forward humans\nare still in charge, but where the game board is somehow more\ndefense-favoring and more fit for human flourishing.\n\nThe main practical issue that I see with this so far is that\npeople don't seem to actually trust any specific governance\nmechanism with the power to build such a thing. This fact becomes\nstark when you look at the results to my recent Twitter polls, asking if\npeople would prefer to see AI monopolized by a single entity with a\ndecade head-start, or AI delayed by a decade for everyone:\n\nThe size of each poll is small, but the polls make up for it in the\nuniformity of their result across a wide diversity of sources and\noptions. In nine out of nine cases, the majority of people would\nrather see highly advanced AI delayed by a decade outright than be\nmonopolized by a single group, whether it's a corporation, government or\nmultinational body. In seven out of nine cases, delay won by at\nleast two to one. This seems like an important fact to understand for\nanyone pursuing AI regulation. Current approaches have been focusing on\ncreating licensing schemes and regulatory requirements, trying to\nrestrict AI development to a smaller number of people, but these have\nseen popular pushback precisely because people don't want to see anyone\nmonopolize something so powerful. Even if such top-down regulatory\nproposals reduce risks of extinction, they risk increasing the chance of\nsome kind of permanent lock-in to centralized totalitarianism.\nParadoxically, could agreements banning extremely advanced AI research\noutright (perhaps with exceptions for biomedical AI), combined\nwith measures like mandating open source for those models that\nare not banned as a way of reducing profit motives while further\nimproving equality of access, be more popular?\n\nThe main approach preferred by opponents of the \"let's get one global\norg to do AI and make its governance really really good\" route is\npolytheistic\nAI: intentionally try to make sure there's lots of people and\ncompanies developing lots of AIs, so that none of them grows far more\npowerful than the other. This way, the theory goes, even as AIs\nbecome superintelligent, we can retain a balance of power.\n\nThis philosophy is interesting, but my experience trying to ensure\n\"polytheism\" within the Ethereum ecosystem does make me worry that this\nis an inherently unstable equilibrium. In Ethereum, we have\nintentionally tried to ensure decentralization of many parts of the\nstack: ensuring that there's no single codebase that controls more than half of the proof of stake\nnetwork, trying to counteract the dominance\nof large staking pools, improving geographic decentralization, and so\non. Essentially, Ethereum is actually attempting to execute on the old\nlibertarian dream of a market-based society that uses social pressure,\nrather than government, as the antitrust regulator. To some extent, this\nhas worked: the Prysm\nclient's dominance has dropped from above 70% to under 45%. But this\nis not some automatic market process: it's the result of human intention\nand coordinated action.\n\nMy experience within Ethereum is mirrored by learnings from the\nbroader world as a whole, where many markets have proven to be natural\nmonopolies. With superintelligent AIs acting independently of\nhumans, the situation is even more unstable. Thanks to recursive\nself-improvement, the strongest AI may pull ahead very quickly, and\nonce AIs are more powerful than humans, there is no force that can push\nthings back into balance.\n\nAdditionally, even if we do get a polytheistic world of\nsuperintelligent AIs that ends up stable, we still have the\nother problem: that we get a universe where humans are\npets.\n\nA happy path: merge with the\nAIs?\n\nA different option that I have heard about more recently is to\nfocus less on AI as something separate from humans, and more on\ntools that enhance human cognition rather than\nreplacing it.\n\nOne near-term example of something that goes in this direction is AI\ndrawing tools. Today, the most prominent tools for making AI-generated\nimages only have one step at which the human gives their input, and AI\nfully takes over from there. An alternative would be to focus more on AI\nversions of Photoshop: tools where the artist or the AI might make an\nearly draft of a picture, and then the two collaborate on improving it\nwith a process of real-time feedback.\n\nPhotoshop generative AI fill, 2023. Source.\nI tried, it and it takes time to get used to but it actually works quite\nwell!\n\nAnother direction in a similar spirit is the Open\nAgency Architecture, which proposes splitting the different parts of\nan AI \"mind\" (eg. making plans, executing on plans, interpreting\ninformation from the outside world) into separate components, and\nintroducing diverse human feedback in between these parts.\n\nSo far, this sounds mundane, and something that almost everyone can\nagree that it would be good to have. The economist Daron Acemoglu's work\nis far from this kind of AI futurism, but his new book Power\nand Progress hints at wanting to see more of exactly these types of\nAI.\n\nBut if we want to extrapolate this idea of human-AI\ncooperation further, we get to more radical conclusions. Unless\nwe create a world government powerful enough to detect and stop every\nsmall group of people hacking on individual GPUs with laptops, someone\nis going to create a superintelligent AI eventually - one that can think\na thousand\ntimes faster than we can - and no combination of humans using tools\nwith their hands is going to be able to hold its own against that. And\nso we need to take this idea of human-computer cooperation much deeper\nand further.\n\nA first natural step is brain-computer\ninterfaces. Brain-computer interfaces can give humans much\nmore direct access to more-and-more powerful forms of computation and\ncognition, reducing the two-way communication loop between man and\nmachine from seconds to milliseconds. This would also greatly reduce the\n\"mental effort\" cost to getting a computer to help you gather facts,\ngive suggestions or execute on a plan.\n\nLater stages of such a roadmap admittedly get weird. In addition to\nbrain-computer interfaces, there are various paths to improving our\nbrains directly through innovations in biology. An eventual further\nstep, which merges both paths, may involve uploading our\nminds to run on computers directly. This would also be the ultimate\nd/acc for physical security: protecting ourselves from harm would no\nlonger be a challenging problem of protecting inevitably-squishy human\nbodies, but rather a much simpler problem of making data backups.\n\nDirections like this are sometimes met with worry, in part because\nthey are irreversible, and in part because they may give powerful people\nmore advantages over the rest of us. Brain-computer interfaces in\nparticular have dangers - after all, we are talking about literally\nreading and writing to people's minds. These concerns are exactly\nwhy I think it would be ideal for a leading role in this path to be held\nby a security-focused open-source movement, rather than closed and\nproprietary corporations and venture capital funds. Additionally, all of\nthese issues are worse with superintelligent AIs that operate\nindependently from humans, than they are with augmentations that are\nclosely tied to humans. The divide between \"enhanced\" and \"unenhanced\"\nalready exists today due to limitations\nin who can and can't use ChatGPT.\n\nIf we want a future that is both superintelligent\nand \"human\", one where human beings are not just pets, but\nactually retain meaningful agency over the world, then it feels like\nsomething like this is the most natural option. There are also\ngood arguments why this could be a safer AI alignment path: by involving\nhuman feedback at each step of decision-making, we reduce the incentive\nto offload high-level planning responsibility to the AI itself, and\nthereby reduce the chance that the AI does something totally unaligned\nwith humanity's values on its own.\n\nOne other argument in favor of this direction is that it may be more\nsocially palatable than simply shouting \"pause AI\" without a complementary\nmessage providing an alternative path forward. It will require a\nphilosophical shift from the current mentality that tech advancements\nthat touch humans are dangerous but advancements that are separate from\nhumans are by-default safe. But it has a huge countervailing benefit:\nit gives developers something to do. Today, the AI safety\nmovement's primary message to AI developers seems to be \"you should\njust stop\". One can work\non alignment research, but today this lacks economic incentives.\nCompared to this, the common e/acc message of \"you're already a hero\njust the way you are\" is understandably extremely appealing. A d/acc\nmessage, one that says \"you should build, and build profitable things,\nbut be much more selective and intentional in making sure you are\nbuilding things that help you and humanity thrive\", may be a winner.\n\nIs d/acc\ncompatible with your existing philosophy?\n\n- If you are an e/acc, then d/acc is a subspecies of\ne/acc - just one that is much more selective and intentional.\n\n- If you are an effective\naltruist, then d/acc is a re-branding of the\neffective-altruist idea of differential\ntechnology development, though with a greater emphasis on liberal\nand democratic values.\n\n- If you are a libertarian, then d/acc is a\nsub-species of techno-libertarianism, though a more pragmatic one that\nis more critical of \"the techno-capital machine\", and willing to accept\ngovernment interventions today (at least, if cultural interventions\ndon't work) to prevent much worse un-freedom tomorrow.\n\n- If you are a Pluralist, in the Glen Weyl sense of the term,\nthen d/acc is a frame that can easily include the emphasis on better\ndemocratic coordination technology that Plurality values.\n\n- If you are a public health advocate, then d/acc\nideas can be a source of a broader long-term vision, and opportunity to\nfind common ground with \"tech people\" that you might otherwise feel at\nodds with.\n\n- If you are a blockchain advocate, then d/acc is a\nmore modern and broader narrative to embrace than the fifteen-year-old\nemphasis on hyperinflation and banks, which puts blockchains into\ncontext as one of many tools in a concrete strategy to build toward a\nbrighter future.\n\n- If you are a solarpunk,\nthen d/acc is a subspecies of solarpunk, and incorporates a similar\nemphasis on intentionality and collective action.\n\n- If you are a lunarpunk,\nthen you will appreciate the d/acc emphasis on informational defense,\nthrough maintaining privacy and freedom.\n\n## We are the brightest star\n\nI love technology because technology expands human potential. Ten\nthousand years ago, we could build some hand tools, change which plants\ngrow on a small patch of land, and build\nbasic houses. Today, we can build 800-meter-tall\ntowers, store the entirety of recorded human knowledge in a device\nwe can hold in our hands, communicate instantly across the globe, double\nour lifespan, and live happy and fulfilling lives without fear of our\nbest friends regularly dropping dead of disease.\n\nWe started from the bottom, now we're here.\n\nI believe that these things are deeply good, and that expanding\nhumanity's reach even further to the planets and stars is deeply good,\nbecause I believe humanity is deeply good. It is\nfashionable in some circles to be skeptical of this: the voluntary\nhuman extinction movement argues that the Earth would be better off\nwithout humans existing at all, and many more want to see much\nsmaller number of human beings see the light of this world in the\ncenturies to come. It is common to argue\nthat humans\nare bad because we cheat and steal, engage in colonialism and war,\nand mistreat and annihilate other species. My reply to this style of\nthinking is one simple question: compared to what?\n\nYes, human beings are often mean, but we much more often show\nkindness and mercy, and work together for our common benefit. Even\nduring wars we often take care to protect civilians - certainly not\nnearly enough, but also far more than we did 2000 years\nago. The next century may well bring widely available\nnon-animal-based meat, eliminating the largest\nmoral catastrophe that human beings can justly be blamed for today.\nNon-human animals are not like this. There is no situation where a cat\nwill adopt an entire lifestyle of refusing to eat mice as a matter of\nethical principle. The Sun is growing brighter every year, and in about\none\nbillion years, it is expected that this will make the Earth too hot\nto sustain life. Does the Sun even think about the genocide\nthat it is going to cause?\n\nAnd so it is my firm belief that, out of all the things that we have\nknown and seen in our universe, we, humans, are the brightest\nstar. We are the one thing that we know about that, even if\nimperfectly, sometimes make an earnest effort to care about \"the good\",\nand adjust our behavior to better serve it. Two billion years from now,\nif the Earth or any part of the universe still bears the beauty of\nEarthly life, it will be human artifices like space travel and geoengineering\nthat will have made it happen.\n\nWe need to build, and accelerate. But there is a very real question\nthat needs to be asked: what is the thing that we are accelerating\ntowards? The 21st century may well be the\npivotal century for humanity, the century in which our fate for\nmillennia to come gets decided. Do we fall into one of a number of traps\nfrom which we cannot escape, or do we find a way toward a future where\nwe retain our freedom and agency? These are challenging problems. But I\nlook forward to watching and participating in our species' grand\ncollective effort to find the answers.",
    "contentLength": 67681,
    "summary": "Vitalik advocates \"d/acc\" (defensive acceleration) - developing beneficial tech like defense, biotech, and crypto while carefully managing AI risks.",
    "detailedSummary": {
      "theme": "Vitalik presents his philosophy of 'defensive acceleration' (d/acc) as an alternative to uncritical tech acceleration, emphasizing the need to develop technologies that favor defense, decentralization, and democracy while being particularly cautious about AI development.",
      "summary": "Vitalik argues that while technology has brought massive benefits to humanity (citing dramatic improvements in life expectancy, reduced poverty, and enhanced quality of life), not all technological directions are equally beneficial. He distinguishes his 'd/acc' philosophy from the 'e/acc' (effective accelerationism) movement, emphasizing that the direction of technological development matters as much as its speed. Vitalik expresses particular concern about AI development, arguing that AI represents a fundamentally different category of technology that could lead to human extinction or a future where humans become mere 'pets' to superintelligent systems. He worries about the centralization of power that advanced AI could enable, potentially creating inescapable totalitarian regimes with unprecedented surveillance and control capabilities. His proposed solution involves focusing on 'defense-favoring' technologies across four domains: macro physical defense (resilient infrastructure), micro physical defense (pandemic preparedness and biosafety), cyber defense (cryptography and blockchain technologies), and info defense (tools for combating misinformation while preserving decentralization). Vitalik advocates for technologies that enhance human agency and democratic governance rather than centralizing power in the hands of a few actors, whether they be governments, corporations, or AI systems.",
      "takeaways": [
        "Technology has delivered massive benefits to humanity, but the direction of development matters as much as speed - some technologies favor defense and decentralization while others enable offense and centralization",
        "AI represents a fundamentally different category of technology that poses existential risks and could lead to permanent human disempowerment, requiring unique caution compared to other technological advances",
        "Defense-favoring technologies across four domains (macro/micro physical defense, cyber defense, and info defense) can help maintain democratic governance and individual freedom in an increasingly technological world",
        "Blockchain, cryptography, and zero-knowledge proofs are crucial defensive technologies that can preserve privacy and decentralization against trends toward digital authoritarianism and centralized control",
        "Solutions to global challenges like pandemics and misinformation should prioritize open-source, decentralized approaches rather than concentrating power in small groups of decision-makers"
      ],
      "controversial": [
        "Vitalik's claim that AI poses existential risks and could lead to human extinction, which many in the tech industry view as overly pessimistic or speculative",
        "His criticism of the effective accelerationist (e/acc) movement's enthusiasm for military technology and the implication that it requires 'betting everything on America being good'",
        "The argument that even 'friendly' superintelligent AI scenarios like those depicted in the Culture series would reduce humans to the status of 'pets' with no meaningful agency",
        "His suggestion that prediction markets played by AIs could be a solution to low-volume trading on prediction platforms",
        "The assertion that current governance structures for AI development (citing OpenAI's board structure) are fundamentally flawed and undemocratic"
      ]
    }
  },
  {
    "id": "general-2023-11-14-neoplasma",
    "title": "Exit games for EVM validiums: the return of Plasma",
    "date": "2023-11-14",
    "category": "protocol",
    "url": "https://vitalik.eth.limo/general/2023/11/14/neoplasma.html",
    "path": "general/2023/11/14/neoplasma.html",
    "content": "- \n\n- \n\n  \n    \n\n  \n  \n  Dark Mode Toggle\n  \n    \n\n  \n\n##  Exit games for EVM validiums: the return of Plasma \n\n 2023 Nov 14 \nSee all posts\n\n \n \n\n Exit games for EVM validiums: the return of Plasma \n\nSpecial thanks to Karl Floersch, Georgios Konstantopoulos and\nMartin Koppelmann for feedback, review and discussion.\n\nPlasma\nis a class of blockchain scaling solutions that allow all data and\ncomputation, except for deposits, withdrawals and Merkle roots, to be\nkept off-chain. This opens the door to very large scalability gains that\nare not bottlenecked by on-chain data availability. Plasma was first invented in\n2017, and saw many iterations in 2018, most notably Minimal Viable\nPlasma, Plasma\nCash, Plasma\nCashflow and Plasma\nPrime. Unfortunately, Plasma has since largely been superseded by rollups, for reasons\nprimarily having to do with (i) large client-side data storage costs,\nand (ii) fundamental limitations of Plasma that make it hard\nto generalize beyond payments.\n\nThe advent of validity proofs (aka ZK-SNARKs) gives us a reason\nto rethink this decision. The largest challenge of making Plasma work\nfor payments, client-side data storage, can be efficiently addressed\nwith validity proofs. Additionally, validity proofs provide a wide array\nof tools that allow us to make a Plasma-like chain that runs an EVM. The\nPlasma security guarantees would not cover all users, as the fundamental\nreasons behind the impossibility of extending Plasma-style exit games to\nmany kinds of complex applications still remain. However, a very large\npercentage of assets could nevertheless be kept secure in practice.\n\nThis post describes how Plasma ideas can be extended to do such a\nthing.\n\n## Overview: how Plasma works\n\nThe simplest version of Plasma to understand is Plasma Cash. Plasma\nCash works by treating each individual coin as a separate NFT, and\ntracking a separate history for each coin. A Plasma chain has an\noperator, who is responsible for making and regularly\npublishing blocks. The transactions in each block are stored as a sparse\nMerkle tree: if a transaction transfers ownership of coin\nk, it appears in position k of the tree. When\nthe Plasma chain operator creates a new block, they publish the root of\nthe Merkle tree to chain, and they directly send to each user the Merkle\nbranches corresponding to the coins that that user owns.\n\nSuppose that these are the last three transaction trees in a\nPlasma Cash chain. Then, assuming all previous trees are valid, we know\nthat Eve currently owns coin 1, David owns coin 4 and George owns coin\n6.\n\nThe main risk in any Plasma system is the operator misbehaving. This\ncan happen in two ways:\n\n- Publishing an invalid block (eg. the operator\nincludes a transaction sending coin 1 from Fred to Hermione even if Fred\ndoesn't own the coin at that time)\n\n- Publishing an unavailable block (eg. the operator\ndoes not send Bob his Merkle branch for one of the blocks, preventing\nhim from ever proving to someone else that his coin is still valid and\nunspent)\n\nIf the operator misbehaves in a way that is relevant to a user's\nassets, the user has the responsibility to exit immediately\n(specifically, within 7 days). When a user (\"the\nexiter\") exits, they provide a Merkle branch proving the\ninclusion of the transaction that transferred that coin from the\nprevious owner to them. This starts a 7-day challenge\nperiod, during which others can challenge that exit by\nproviding a Merkle proof of one of three things:\n\n- Not latest owner: a later transaction signed by the\nexiter transferring the exiter's coin to someone else\n\n- Double spend: a transaction that transferred the\ncoin from the previous owner to someone else, that was included before\nthe transaction transferring the coin to the exiter\n\n- Invalid history: a transaction that transferred the\ncoins before (within the past 7 days) that does not have a corresponding\nspend. The exiter can respond by providing the corresponding spend; if\nthey do not, the exit fails.\n\nWith these rules, anyone who owns coin k needs to see\nall of the Merkle branches of position k in all historical\ntrees for the past week to be sure that they actually own coin\nk and can exit it. They need to store all the branches\ncontaining transfers of the asset, so that they can respond to\nchallenges and safely exit with their coin.\nGeneralizing to fungible\ntokens\n\nThe above design works for NFTs. However, much more common than NFTs\nare fungible tokens, like ETH and USDC. One way to apply Plasma Cash to\nfungible tokens is to simply make each small denomination of a coin (eg.\n0.01 ETH) a separate NFT. Unfortunately, the gas costs of exiting would\nbe too high if we do this.\n\nOne solution is to optimize by treating many adjacent coins as a\nsingle unit, which can be transferred or exited all at once. There are\ntwo ways to do this:\n\n- Use Plasma Cash almost as-is, but use fancy algorithms to compute\nthe Merkle tree of a really large number of objects very quickly if many\nadjacent objects are the same. This is surprisingly not that hard to do;\nyou can see a python\nimplementation here.\n\n- Use Plasma\nCashflow, which simply represents many adjacent coins as a single\nobject.\n\nHowever, both of these approaches run into the problem of\nfragmentation: if you receive 0.001 ETH each from hundreds of\npeople who are buying coffees from you, you are going to have 0.001 ETH\nin many places in the tree, and so actually exiting that ETH would still\nrequire submitting many separate exits, making the gas fees prohibitive.\nDefragmentation protocols have been developed, but are tricky to\nimplement.\n\nAlternatively, we can redesign the system to take into account a more\ntraditional \"unspent\ntransaction output\" (UTXO) model. When you exit a coin, you would\nneed to provide the last week of history of those coins, and anyone\ncould challenge your exit by proving that those historical coins were\nalready exited.\n\nA withdrawal of the 0.2 ETH UTXO at the bottom right could be\ncancelled by showing a withdrawal of any of the UTXOs in its history,\nshown in green. Particularly note that the middle-left and bottom-left\nUTXOs are ancestors, but the top-left UTXO is not. This approach is\nsimilar to order-based\ncoloring ideas from colored coins protocols circa 2013.\n\nThere is a wide variety of techniques for doing this. In all cases,\nthe goal is to track some conception of what is \"the same coin\" at\ndifferent points in history, in order to prevent \"the same coin\" from\nbeing withdrawn twice.\nChallenges with\ngeneralizing to EVM\n\nUnfortunately, generalizing beyond payments to the EVM is much\nharder. One key challenge is that many state objects in the EVM do not\nhave a clear \"owner\". Plasma's security depends on each object having an\nowner, who has the responsibility to watch and make sure the chain's\ndata is available, and exit that object if anything goes wrong. Many\nEthereum applications, however, do not work this way. Uniswap liquidity\npools, for example, do not have a single owner.\n\nAnother challenge is that the EVM does not attempt to limit\ndependencies. ETH held in account A at block N could have come from\nanywhere in block N-1. In order to exit a consistent state, an EVM\nPlasma chain would need to have an exit game where, in the extreme case,\nsomeone wishing to exit using information from block N might need to pay\nthe fees to publish the entire block N state on chain: a gas\ncost in the many millions of dollars. UTXO-based Plasma schemes do not\nhave this problem: each user can exit their assets from whichever block\nis the most recent block that they have the data for.\n\nA third challenge is that the unbounded dependencies in the EVM make\nit much harder to have aligned incentives to prove validity.\nThe validity of any state depends on everything else, and so proving any\none thing requires proving everything. Sorting out failures in such a\nsituation generally cannot be made incentive-compatible due to the data\navailability problem. A particularly annoying problem is that we\nlose the guarantee, present in UTXO-based systems, that an object's\nstate cannot change without its owner's consent. This guarantee is\nincredibly useful, as it means that the owner is always aware of the\nlatest provable state of their assets, and simplifies exit games.\nWithout it, creating exit games becomes much harder.\nHow\nvalidity proofs can alleviate many of these problems\n\nThe most basic thing that validity proofs can do to improve Plasma\nchain designs is to prove the validity of each Plasma block on chain.\nThis greatly simplifies the design space: it means that the\nonly attack from the operator that we have to worry about is\nunavailable blocks, and not invalid blocks. In Plasma\nCash, for example, it removes the need to worry about history\nchallenges. This reduces the state that a user needs to download, from\none branch per block in the last week, to one branch per asset.\n\nAdditionally, withdrawals from the most recent state (in the common\ncase where the operator is honest, all withdrawals would be from the\nmost recent state) are not subject to not-latest-owner challenges, and\nso in a validity-proven Plasma chain such withdrawals would not be\nsubject to any challenges at all. This means that, in the normal\ncase, withdrawals can be instant!\nExtending to the EVM:\nparallel UTXO graphs\n\nIn the EVM case, validity proofs also let us do something clever:\nthey can be used to implement a parallel UTXO graph for ETH and ERC20\ntokens, and SNARK-prove equivalence between the UTXO graph and the\nEVM state. Once you have that, you could implement a \"regular\"\nPlasma system over the UTXO graph.\n\nThis lets us sidestep many of the complexities of the EVM. For\nexample, the fact that in an account-based system someone can edit your\naccount without your consent (by sending it coins and thereby increasing\nits balance) does not matter, because the Plasma construction is not\nover the EVM state itself, but rather over a UTXO state that lives in\nparallel to the EVM, where any coins that you receive would be separate\nobjects.\nExtending to the EVM:\ntotal state exiting\n\nThere have been simpler schemes proposed to make a \"plasma EVM\", eg.\nPlasma Free and\nbefore that this\npost from 2019. In these schemes, anyone can send a message on the\nL1 to force the operator to either include a transaction or make a\nparticular branch of the state available. If the operator fails to do\nthis, the chain starts reverting blocks. The chain stops\nreverting once someone posts a full copy of either the whole state, or\nat least all of the data that users have flagged as being potentially\nmissing. Making a withdrawal can require posting a bounty, which would\npay for that user's share of the gas costs of someone posting such a\nlarge amount of data.\n\nSchemes like this have the weakness that they do not allow instant\nwithdrawals in the normal case, because there is always the possibility\nthat the chain will need to revert the latest state.\n\n## Limits of EVM plasma schemes\n\nSchemes like this are powerful, but are NOT able to provide\nfull security guarantees to all users. The case where they\nbreak down most clearly is situations where a particular state object\ndoes not have a clear economic \"owner\".\n\nLet us consider the case of a CDP (collateralized debt position), a\nsmart contract where a user has coins that are locked up and can only be\nreleased once the user pays their debt. Suppose that user has 1 ETH\n(~$2000 as of the time of this writing) locked up in a CDP with 1000 DAI\nof debt. Now, the Plasma chain stops publishing blocks, and the user\nrefuses to exit. The user could simply never exit. Now, the user has a\nfree option: if the price of ETH drops below $1000, they walk\naway and forget about the CDP, and if the price of ETH stays above,\neventually they claim it. On average, such a malicious user earns money\nfrom doing this.\n\nAnother example is a privacy system, eg. Tornado Cash or Privacy\nPools. Consider a privacy system with five depositors:\n\nThe ZK-SNARKs in the privacy system keep the link between the\nowner of a coin coming into the system and the owner of the coin coming\nout hidden.\n\nSuppose that only orange has withdrawn, and at that point the Plasma\nchain operator stops publishing data. Suppose also that we use the UTXO\ngraph approach with a first-in-first-out rule, so each coin gets matched\nto the coin right below it. Then, orange could withdraw their pre-mixed\nand post-mixed coin, and the system would perceive it as two\nseparate coins. If blue tries to withdraw their pre-mixed coin, orange's\nmore recent state would supersede it; meanwhile, blue would not have the\ninformation to withdraw their post-mixed coin.\n\nThis can be fixed if you allow the other four depositors to\nwithdraw the privacy contract itself (which would supersede the\ndeposits), and then take the coins out on L1. However, actually\nimplementing such a mechanism requires additional effort on the part of\npeople developing the privacy system.\n\nThere are also other ways to solve privacy, eg. the Intmax approach, which\ninvolves putting a few bytes on chain rollup-style together with a\nPlasma-like operator that passes around information between individual\nusers.\n\nUniswap LP positions have a similar problem: if you traded USDC for\nETH in a Uniswap position, you could try to withdraw your pre-trade USDC\nand your post-trade ETH. If you collude with the Plasma chain\noperator, the liquidity providers and other users would not have access\nto the post-trade state, so they would not be able to withdraw their\npost-trade USDC. Special logic would be required to prevent situations\nlike this.\n\n## Conclusions\n\nIn 2023, Plasma is an underrated design space. Rollups remain the\ngold standard, and have security properties that cannot be matched. This\nis particularly true from the developer experience perspective: nothing\ncan match the simplicity of an application developer not even having\nto think about ownership graphs and incentive flows within their\napplication.\n\nHowever, Plasma lets us completely sidestep the data availability\nquestion, greatly reducing transaction fees. Plasma can be a significant\nsecurity upgrade for chains that would otherwise be validiums.\nThe fact that ZK-EVMs are\nfinally coming to fruition this year makes it an excellent\nopportunity to re-explore this design space, and come up with even more\neffective constructions to simplify the developer experience and protect\nusers' funds.",
    "contentLength": 14414,
    "summary": "EVM validiums can use Plasma-style exit games by maintaining a parallel UTXO graph for ETH/ERC20 tokens, proven equivalent to EVM state via SNARKs.",
    "detailedSummary": {
      "theme": "Vitalik argues that validity proofs (ZK-SNARKs) can revitalize Plasma blockchain scaling solutions, making them viable for EVM-based applications while offering significant cost advantages over rollups.",
      "summary": "Vitalik explains how Plasma, a scaling solution that keeps most data off-chain except for deposits, withdrawals, and Merkle roots, was largely abandoned in favor of rollups due to high client-side data storage costs and limitations in generalizing beyond payments. However, he argues that the advent of validity proofs (ZK-SNARKs) addresses many of these issues by proving block validity on-chain, eliminating the need for complex history challenges, and enabling instant withdrawals in normal cases. Vitalik proposes two main approaches for EVM compatibility: implementing parallel UTXO graphs that are SNARK-proven equivalent to EVM state, and total state exiting schemes where the chain reverts blocks if operators fail to provide required data. While these solutions offer significant scalability benefits and reduced transaction fees compared to rollups, Vitalik acknowledges important limitations - particularly with applications lacking clear ownership structures like CDPs, privacy systems, and Uniswap LP positions, where malicious actors could exploit the system during data unavailability periods.",
      "takeaways": [
        "Validity proofs can solve Plasma's main historical problems by proving block validity on-chain and reducing required client-side data storage",
        "EVM Plasma can be implemented through parallel UTXO graphs or total state exiting schemes, both leveraging ZK-SNARKs for security",
        "Plasma offers significant advantages over rollups by completely sidestepping data availability issues and greatly reducing transaction fees",
        "The approach cannot provide full security guarantees for applications without clear ownership structures, such as CDPs, privacy systems, and DeFi protocols",
        "Despite limitations, Plasma represents a significant security upgrade for chains that would otherwise be validiums and deserves renewed exploration in 2023"
      ],
      "controversial": [
        "The claim that Plasma is 'underrated' in 2023 may be debated given the strong momentum behind rollups as the preferred scaling solution",
        "The assertion that Plasma can be a 'significant security upgrade' for validiums might be contested by those who believe the ownership and incentive complexities outweigh the benefits"
      ]
    }
  },
  {
    "id": "general-2023-10-31-l2types",
    "title": "Different types of layer 2s",
    "date": "2023-10-31",
    "category": "protocol",
    "url": "https://vitalik.eth.limo/general/2023/10/31/l2types.html",
    "path": "general/2023/10/31/l2types.html",
    "content": "- \n\n- \n\n  \n    \n\n  \n  \n  Dark Mode Toggle\n  \n    \n\n  \n\n##  Different types of layer 2s \n\n 2023 Oct 31 \nSee all posts\n\n \n \n\n Different types of layer 2s \n\nSpecial thanks to Karl Floersch for feedback and review\n\nThe Ethereum layer 2 ecosystem has been expanding rapidly over the\nlast year. The EVM rollup ecosystem, traditionally featuring Arbitrum, Optimism and Scroll, and more recently Kakarot and Taiko, has been progressing quickly,\nmaking great strides on improving their security; the L2beat page does a good\njob of summarizing the state of each project. Additionally, we have seen\nteams building sidechains also starting to build rollups (Polygon), layer 1 projects\nseeking to move toward being validiums (Celo), and totally new efforts (Linea, Zeth...). Finally,\nthere is the not-just-EVM ecosystem: \"almost-EVMs\" like Zksync, extensions like Arbitrum\nStylus, and broader efforts like the Starknet ecosystem, Fuel and others.\n\nOne of the inevitable consequences of this is that we are seeing a\ntrend of layer 2 projects becoming more heterogeneous. I expect this\ntrend to continue, for a few key reasons:\n\n- Some projects that are currently independent layer 1s are\nseeking to come closer to the Ethereum ecosystem, and possibly\nbecome layer 2s. These projects will likely want a step-by-step\ntransition. Transitioning all at once now would cause a decrease in\nusability, as the technology is not yet ready to put everything on a\nrollup. Transitioning all at once later risks sacrificing momentum and\nbeing too late to be meaningful.\n\n- Some centralized projects want to give their users more\nsecurity assurances, and are exploring blockchain-based routes for doing\nso. In many cases, these are the projects that would have\nexplored \"permissioned consortium chains\" in a previous era.\nRealistically, they probably only need a \"halfway-house\" level of\ndecentralization. Additionally, their often very high level of\nthroughput makes them unsuitable even for rollups, at least in the short\nterm.\n\n- Non-financial applications, like games or social media, want\nto be decentralized but need only a halfway-house level of\nsecurity. In the social media case, this realistically\ninvolves treating different parts of the app differently: rare and\nhigh-value activity like username registration and account recovery\nshould be done on a rollup, but frequent and low-value activity like\nposts and votes need less security. If a chain failure causes your post\nto disappear, that's an acceptable cost. If a chain failure causes you\nto lose your account, that is a much bigger problem.\n\nA big theme is that while applications and users that are on\nthe Ethereum layer 1 today will be fine paying smaller but\nstill visible rollup fees in the short term, users from the\nnon-blockchain world will not: it's easier to justify paying\n$0.10 if you were paying $1 before than if you were paying $0 before.\nThis applies both to applications that are centralized today, and to\nsmaller layer 1s, which do typically have very low fees while their\nuserbase remains small.\n\nA natural question that emerges is: which of these complicated\ntradeoffs between rollups, validiums and other systems makes sense for a\ngiven application?\nRollups vs\nvalidiums vs disconnected systems\n\nThe first dimension of security vs scale that we will explore can be\ndescribed as follows: if you have an asset that is issued on L1,\nthen deposited into the L2, then transferred to you, what level of\nguarantee do you have that you will be able to take the asset back to\nthe L1?\n\nThere is also a parallel question: what is the technology choice that\nis resulting in that level of guarantee, and what are the tradeoffs of\nthat technology choice?\n\nWe can describe this simply using a chart:\n\nSystem type\nTechnology properties\nSecurity guarantees\nCosts\n\nRollup\nComputation proven via fraud proofs or ZK-SNARKs, data stored on\nL1\nYou can always bring the asset back to L1\nL1 data availability + SNARK-proving or redundant execution to catch\nerrors\n\nValidium\nComputation proven via ZK-SNARKs (can't use fraud proofs), data\nstored on a server or other separate system\nData availability failure can cause assets to be lost, but\nnot stolen\nSNARK-proving\n\nDisconnected\nA separate chain (or server)\nTrust one or a small group of people not to steal your funds or lose\nthe keys\nVery cheap\n\nIt's worth mentioning that this is a simplified schema, and\nthere are lots of intermediate options. For example:\n\n- Between rollup and validium: a validium where\nanyone could make an on-chain payment to cover the cost of transaction\nfees, at which point the operator would be forced to provide some data\nonto the chain or else lose a deposit.\n\n- Between plasma and validium: a Plasma system offers\nrollup-like security guarantees with off-chain data availability, but it\nsupports only a limited number of applications. A system could\noffer a full EVM, and offer Plasma-level guarantees to users that do not\nuse those more complicated applications, and validium-level guarantees\nto users that do.\n\nThese intermediate options can be viewed as being on a spectrum\nbetween a rollup and a validium. But what motivates applications to\nchoose a particular point on that spectrum, and not some point further\nleft or further right? Here, there are two major factors:\n\n- The cost of Ethereum's native data availability, which will\ndecrease over time as technology improves. Ethereum's next hard\nfork, Dencun,\nintroduces EIP-4844 (aka\n\"proto-danksharding\"), which provides ~32 kB/sec of onchain data\navailability. Over the next few years, this is expected to increase in\nstages as full\ndanksharding is rolled out, eventually targeting around ~1.3 MB/sec\nof data availability. At the same time, improvements\nin data compression will let us do more with the same amount of\ndata.\n\n- The application's own needs: how much would users suffer\nfrom high fees, versus from something in the application going\nwrong? Financial applications would lose more from application\nfailures; games and social media involve lots of activity per user, and\nrelatively low-value activity, so a different security tradeoff makes\nsense for them.\n\nApproximately, this tradeoff looks something like this:\n\nAnother type of partial guarantee worth mentioning is\npre-confirmations. Pre-confirmations are messages\nsigned by some set of participants in a rollup or validium that say \"we\nattest that these transactions are included in this order, and the\npost-state root is this\". These participants may well sign a\npre-confirmation that does not match some later reality, but if they do,\na deposit gets burned. This is useful for low-value applications like\nconsumer payments, while higher-value applications like\nmultimillion-dollar financial transfers will likely wait for a \"regular\"\nconfirmation backed by the system's full security.\n\nPre-confirmations can be viewed as another example of a\nhybrid system, similar to the \"plasma / validium hybrid\"\nmentioned above, but this time hybridizing between a rollup (or\nvalidium) that has full security but high latency, and a system with a\nmuch lower security level that has low latency. Applications that need\nlower latency get lower security, but can live in the same ecosystem as\napplications that are okay with higher latency in exchange for maximum\nsecurity.\nTrustlessly reading\nEthereum\n\nAnother less-thought-about, but still highly important, form of\nconnection has to do with a system's ability to read\nthe Ethereum blockchain. Particularly, this includes\nbeing able to revert if Ethereum reverts. To see why\nthis is valuable, consider the following situation:\n\nSuppose that, as shown in the diagram, the Ethereum chain reverts.\nThis could be a temporary hiccup within an epoch, while the chain has\nnot finalized, or it could be an inactivity\nleak period where the chain is not finalizing for an extended\nduration because too many validators are offline.\n\nThe worst-case scenario that can arise from this is as follows.\nSuppose that the first block from the top chain reads some data from the\nleftmost block on the Ethereum chain. For example, someone on Ethereum\ndeposits 100 ETH into the top chain. Then, Ethereum reverts. However,\nthe top chain does not revert. As a result, future blocks of the top\nchain correctly follow new blocks from the newly correct Ethereum chain,\nbut the consequences of the now-erroneous older link (namely,\nthe 100 ETH deposit) are still part of the top chain. This exploit could\nallow printing money, turning the bridged ETH on the top chain into a\nfractional reserve.\n\nThere are two ways to solve this problem:\n\n- The top chain could only read finalized blocks of\nEthereum, so it would never need to revert.\n\n- The top chain could revert if Ethereum reverts.\nBoth prevent this issue. The former is easier to implement, but may\ncause a loss of functionality for an extended duration if Ethereum\nenters an inactivity leak period. The latter is harder to implement, but\nensures the best possible functionality at all times.\n\nNote that (1) does have one edge case. If a 51% attack on Ethereum\ncreates two new incompatible blocks that both appear finalized at the\nsame time, then the top chain may well lock on to the wrong one (ie. the\none that Ethereum social consensus does not eventually favor), and would\nhave to revert to switch to the right one. Arguably, there is no need to\nwrite code to handle this case ahead of time; it could simply be handled\nby hard-forking the top chain.\n\nThe ability of a chain to trustlessly read Ethereum is valuable for\ntwo reasons:\n\n- It reduces security issues involved in bridging tokens issued on\nEthereum (or other L2s) to that chain\n\n- It allows account abstraction wallets that use the shared keystore\narchitecture to hold assets on that chain securely.\n\n- is important, though arguably this need is already widely\nrecognized. (2) is important too, because it means that you can have a\nwallet that allows easy key changes and that holds assets across a large\nnumber of different chains.\n\nDoes having a bridge\nmake you a validium?\n\nSuppose that the top chain starts out as a separate chain, and then\nsomeone puts onto Ethereum a bridge contract. A bridge contract is\nsimply a contract that accepts block headers of the top chain, verifies\nthat any header submitted to it comes with a valid certificate showing\nthat it was accepted by the top chain's consensus, and adds that header\nto a list. Applications can build on top of this to implement\nfunctionality such as depositing and withdrawing tokens. Once such a\nbridge is in place, does that provide any of the asset security\nguarantees we mentioned earlier?\n\nSo far, not yet! For two reasons:\n\n- We're validating that the blocks were signed, but not that\nthe state transitions are correct. Hence, if you have an asset issued on\nEthereum deposited to the top chain, and the top chain's validators go\nrogue, they can sign an invalid state transition that steals those\nassets.\n\n- The top chain still has no way to read Ethereum. Hence, you\ncan't even deposit Ethereum-native assets onto the top chain without\nrelying on some other (possibly insecure) third-party bridge.\n\nNow, let's make the bridge a validating bridge: it\nchecks not just consensus, but also a ZK-SNARK proving that the state of\nany new block was computed correctly.\n\nOnce this is done, the top chain's validators can no longer steal\nyour funds. They can publish a block with unavailable data,\npreventing everyone from withdrawing, but they cannot steal (except by\ntrying to extract a ransom for users in exchange for revealing the data\nthat allows them to withdraw). This is the same security model as a\nvalidium.\n\nHowever, we still have not solved the second problem: the top chain\ncannot read Ethereum.\n\nTo do that, we need to do one of two things:\n\n- Put a bridge contract validating finalized Ethereum blocks inside\nthe top chain.\n\n- Have each block in the top chain contain a hash of a recent Ethereum\nblock, and have a fork choice rule that enforces the hash linkings. That\nis, a top chain block that links to an Ethereum block that is not in the\ncanonical chain is itself non-canonical, and if a top chain block links\nto an Ethereum block that was at first canonical, but then becomes\nnon-canonical, the top chain block must also become non-canonical.\n\nThe purple links can be either hash links or a bridge contract\nthat verifies Ethereum's consensus.\n\nIs this enough? As it turns out, still no, because of a few small\nedge cases:\n\n- What happens if Ethereum gets 51% attacked?\n\n- How do you handle Ethereum hard fork upgrades?\n\n- How do you handle hard fork upgrades of your chain?\n\nA 51% attack on Ethereum would have similar consequences to a 51%\nattack on the top chain, but in reverse. A hard fork of Ethereum risks\nmaking the bridge of Ethereum inside the top chain no longer valid.\nA social commitment to revert if Ethereum reverts a finalized\nblock, and to hard-fork if Ethereum hard-forks, is the cleanest way to\nresolve this. Such a commitment may well never need to be\nactually executed on: you could have a governance gadget on the top\nchain activate if it sees proof of a possible attack or hard fork, and\nonly hard-fork the top chain if the governance gadget fails.\n\nThe only viable answer to (3) is, unfortunately, to have some form of\ngovernance gadget on Ethereum that can make the bridge contract on\nEthereum aware of hard-fork upgrades of the top chain.\n\nSummary: two-way validating bridges are\nalmost enough to make a chain a validium. The main remaining\ningredient is a social commitment that if something exceptional happens\nin Ethereum that makes the bridge no longer work, the other chain will\nhard-fork in response.\n\n## Conclusions\n\nThere are two key dimensions to \"connectedness to Ethereum\":\n\n- Security of withdrawing to Ethereum\n\n- Security of reading Ethereum\n\nThese are both important, and have different considerations. There is\na spectrum in both cases:\n\nNotice that both dimensions each have two distinct ways of measuring\nthem (so really there's four dimensions?): security of\nwithdrawing can be measured by (i) security level, and (ii)\nwhat percent of users or use cases benefit from the highest security\nlevel, and security of reading can be measured by (i)\nhow quickly the chain can read Ethereum's blocks, particularly finalized\nblocks vs any blocks, and (ii) the strength of the chain's social\ncommitment to handle edge cases such as 51% attacks and hard forks.\n\nThere is value in projects in many regions of this design space. For\nsome applications, high security and tight connectedness are important.\nFor others, something looser is acceptable in exhcnage for greater\nscalability. In many cases, starting with something looser today, and\nmoving to a tighter coupling over the next decade as technology\nimproves, may well be optimal.",
    "contentLength": 14814,
    "summary": "Ethereum's layer 2 ecosystem is diversifying as projects seek different security-cost tradeoffs between rollups, validiums, and sidechains.",
    "detailedSummary": {
      "theme": "Vitalik analyzes the spectrum of layer 2 solutions connecting to Ethereum, exploring the tradeoffs between security, scalability, and different levels of integration with the main chain.",
      "summary": "Vitalik explores how the layer 2 ecosystem is becoming increasingly heterogeneous as different projects seek various levels of connection to Ethereum based on their specific needs. He identifies two key dimensions of connectedness: security of withdrawing assets to Ethereum (ranging from rollups with full security to disconnected systems requiring trust) and the ability to trustlessly read Ethereum's state. Vitalik explains that applications have different security requirements - financial applications need maximum security while games and social media can accept lower security for better scalability and lower costs. He introduces concepts like validiums (which use cryptographic proofs but store data off-chain) and pre-confirmations as intermediate solutions between full rollup security and disconnected systems. The post concludes that there's value in projects across this design space, with many potentially starting with looser coupling and moving toward tighter integration as technology improves.",
      "takeaways": [
        "Layer 2 solutions exist on a spectrum from full rollups (highest security) to validiums to completely disconnected systems, each with different tradeoffs",
        "Applications have different security needs - financial apps require maximum security while games/social media can accept lower security for better scalability",
        "Two key dimensions define Ethereum connectedness: security of asset withdrawal and ability to trustlessly read Ethereum's state",
        "Hybrid solutions like pre-confirmations allow applications to choose different security levels within the same ecosystem based on transaction value and urgency",
        "Many projects may optimally start with looser Ethereum coupling today and move toward tighter integration as technology advances"
      ],
      "controversial": [
        "The assertion that some applications only need 'halfway-house' levels of decentralization challenges the maximalist view that all blockchain applications should prioritize maximum security",
        "The suggestion that it's acceptable for social media posts to disappear due to chain failures while account recovery should be maximally secure introduces subjective judgments about what level of security different use cases deserve"
      ]
    }
  },
  {
    "id": "general-2023-09-30-enshrinement",
    "title": "Should Ethereum be okay with enshrining more things in the protocol?",
    "date": "2023-09-30",
    "category": "protocol",
    "url": "https://vitalik.eth.limo/general/2023/09/30/enshrinement.html",
    "path": "general/2023/09/30/enshrinement.html",
    "content": "- \n\n- \n\n  \n    \n\n  \n  \n  Dark Mode Toggle\n  \n    \n\n  \n\n##  Should Ethereum be okay with enshrining more things in the protocol? \n\n 2023 Sep 30 \nSee all posts\n\n \n \n\n Should Ethereum be okay with enshrining more things in the protocol? \n\nSpecial thanks to Justin Drake, Tina Zhen and Yoav Weiss for\nfeedback and review.\n\nFrom the start of the Ethereum project, there was a strong philosophy\nof trying to make the core Ethereum as simple as possible, and do as\nmuch as possible by building protocols on top. In the blockchain space,\nthe \"do it on L1\" vs \"focus on L2s\" debate is typically thought of as being primarily about\nscaling, but in reality, similar issues exist for serving many kinds\nof Ethereum users' needs: digital asset exchange, privacy, usernames,\nadvanced cryptography, account safety, censorship resistance,\nfrontrunning protection, and the list goes on. More recently, however,\nthere has been some cautious interest in being willing to enshrine more\nof these features into the core Ethereum protocol.\n\nThis post will go into some of the philosophical reasoning behind the\noriginal minimal-enshrinement philosophy, as well as some more recent\nways of thinking about some of these ideas. The goal will be to start to\nbuild toward a framework for better identifying possible targets where\nenshrining certain features in the protocol might be worth\nconsidering.\nEarly philosophy on\nprotocol minimalism\n\nEarly on in the history of what was then called \"Ethereum 2.0\", there\nwas a strong desire to create a clean, simple and beautiful protocol\nthat tried to do as little as possible itself, and left almost\neverything up to users to build on top. Ideally, the protocol would\njust be a virtual machine, and verifying a block would\njust be a single virtual machine call.\n\nA very approximate reconstruction-from-memory of a whiteboard\ndrawing Gavin Wood and I made back in early 2015, talking about what\nEthereum 2.0 would look like.\n\nThe \"state transition function\" (the function that processes a block)\nwould just be a single VM call, and all other logic would happen through\ncontracts: a few system-level contracts, but mostly contracts provided\nby users. One really nice feature of this model is that even an entire\nhard fork could be described as a single transaction to the block\nprocessor contract, which would be approved through either offchain or\nonchain governance and then run with escalated permissions.\n\nThese discussions back in 2015 particularly applied to two areas that\nwere on our minds: account abstraction and\nscaling. In the case of scaling, the idea was to try to\ncreate a maximally abstracted form of scaling that would feel like a\nnatural extension of the diagram above. A contract could make a call to\na piece of data that was not stored by most Ethereum nodes, and the\nprotocol would detect that, and resolve the call through some kind of\nvery generic scaled-computation functionality. From the virtual\nmachine's point of view, the call would go off into some separate\nsub-system, and then some time later magically come back with the\ncorrect answer.\n\nThis line of thinking was explored briefly, but soon abandoned,\nbecause we were too preoccupied with verifying that any kind of\nblockchain scaling was possible at\nall. Though as we will see later, the combination of data availability\nsampling and ZK-EVMs means that one\npossible future for Ethereum scaling might actually look surprisingly\nclose to that vision! For account abstraction, on the other hand, we\nknew from the start that some kind of implementation was\npossible, and so research immediately began to try to make something as\nclose as possible to the purist starting point of \"a transaction is just\na call\" into reality.\n\nThere is a lot of boilerplate code that occurs in between\nprocessing a transaction and making the actual underlying EVM call out\nof the sender address, and a lot more boilerplate that comes after. How\ndo we reduce this code to as close to nothing as possible?\n\nOne of the major pieces of code in here is\nvalidate_transaction(state, tx), which does things like\nchecking that the nonce and signature of the transaction are correct.\nThe practical goal of account abstraction was, from the start,\nto allow the user to replace basic nonce-incrementing and ECDSA\nvalidation with their own validation logic, so that users could more\neasily use things like social recovery and multisig\nwallets. Hence, finding a way to rearchitect\napply_transaction into just being a simple EVM call was not\nsimply a \"make the code clean for the sake of making the code clean\"\ntask; rather, it was about moving the logic into the user's account\ncode, to give users that needed flexibility.\n\nHowever, the insistence on trying to make\napply_transaction contain as little enshrined logic as\npossible ended up introducing a lot of challenges. To see why, let us\nzoom in on one of the earliest account abstraction proposals, EIP 86:\n\n## Specification\n\nIf block.number >= METROPOLIS_FORK_BLKNUM, then: 1.\nIf the signature of a transaction is (0, 0, 0) (ie.\nv = r = s = 0), then treat it as valid and set the sender\naddress to 2**160 - 1 2. Set the address of any contract\ncreated through a creation transaction to equal\nsha3(0 + init code) % 2**160, where +\nrepresents concatenation, replacing the earlier address formula of\nsha3(rlp.encode([sender, nonce])) 3. Create a new opcode at\n0xfb, CREATE_P2SH, which sets the creation\naddress to sha3(sender + init code) % 2**160. If a contract\nat that address already exists, fails and returns 0 as if the init code\nhad run out of gas.\n\nBasically, if the signature is set to (0, 0, 0), then a transaction\nreally does become \"just a call\". The account itself would be\nresponsible for having code that parses the transaction, extracts and\nverifies the signature and nonce, and pays fees; see here\nfor an early example version of that code, and see here\nfor the very similar validate_transaction code that this\naccount code would be replacing.\n\nIn exchange for this simplicity at protocol layer, miners (or, today,\nblock proposers) gain the additional responsibility of running extra\nlogic for only accepting and forwarding transactions that go to accounts\nwhose code is set up to actually pay fees. What is that logic? Well,\nhonestly EIP-86 did not think too hard about it:\n\nNote that miners would need to have a strategy for accepting these\ntransactions. This strategy would need to be very discriminating,\nbecause otherwise they run the risk of accepting transactions ) for the\nvalidate_transaction code that this pre-account code would\nbe replacingthat do not pay them any fees, and possibly even\ntransactions that have no effect (eg. because the transaction was\nalready included and so the nonce is no longer current). One simple\napproach is to have a whitelist for the codehash of accounts that they\naccept transactions being sent to; approved code would include logic\nthat pays miners transaction fees. However, this is arguably too\nrestrictive; a looser but still effective strategy would be to accept\nany code that fits the same general format as the above, consuming only\na limited amount of gas to perform nonce and signature checks and having\na guarantee that transaction fees will be paid to the miner. Another\nstrategy is to, alongside other approaches, try to process any\ntransaction that asks for less than 250,000 gas, and include it only if\nthe miner's balance is appropriately higher after executing the\ntransaction than before it.\n\nIf EIP-86 had been included as-is, it would have reduced the complexity\nof the EVM, at the cost of massively increasing the complexity of other\nparts of the Ethereum stack, requiring essentially the exact same code\nto be written in other places, in addition to introducing entirely new\nclasses of weirdness such as the possibility that the same transaction\nwith the same hash might appear multiple times in the chain, not to\nmention the multi-invalidation problem.\n\nThe multi-invalidation problem in account abstraction. One\ntransaction getting included on chain could invalidate thousands of\nother transactions in the mempool, making the mempool easy to cheaply\nflood.\n\nAcccount abstraction evolved in stages from there. EIP-86 became EIP-208, which\nlater became this ethresear.ch\npost on \"tradeoffs in account abstraction proposals\", which then\nbecame this\nethresear.ch post half a year later. Eventually, out of all this,\ncame the actually somewhat-workable EIP-2938.\n\nEIP-2938, however, was not minimalistic at all. The EIP\nincludes:\n\n- A new transaction type\n\n- Three new transaction-wide global variables\n\n- Two new opcodes, including the highly unwieldy PAYGAS\nopcode that handles gas price and gas limit checking, being an EVM\nexecution breakpoint, and temporarily storing ETH for fee payments all\nat once.\n\n- A set of complex mining and rebroadcasting strategies, including a\nlist of banned opcodes for the validation phase of a transaction\n\nIn order to get account abstraction off the ground without involving\nEthereum core developers who were busy on heroic efforts optimizing the\nEthereum clients and implementing the merge, EIP-2938 eventually was\nrearchitected into the entirely extra-protocol\nERC-4337.\n\nERC-4337. It really does rely entirely on EVM calls for\neverything!\n\nBecause it's an ERC, it does not require a hard fork, and technically\nlives \"outside of the Ethereum protocol\". So.... problem solved? Well, as\nit turns out, not quite. The current medium-term\nroadmap for ERC-4337 actually does involve eventually turning large\nparts of ERC-4337 into a series of protocol features, and it's a useful\ninstructive example to see the reasons why this path is being\nconsidered.\n\n## Enshrining ERC-4337\n\nThere have been a few key reasons discussed for eventually bringing\nERC-4337 back into the protocol:\n\n- Gas efficiency: Anything done inside the EVM incurs\nsome level of virtual machine overhead, including inefficiency in how it\nuses gas-expensive features like storage slots. Currently, these extra\ninefficiencies add up to at least ~20,000 gas, and often more. Pushing\nthese components into the protocol is the easiest way to remove these\nissues.\n\n- Code bug risk: if the ERC-4337 \"entry point\ncontract\" has a sufficiently terrible bug, all\nERC-4337-compatible wallets could see all of their funds drained.\nReplacing the contract with an in-protocol functionality creates an\nimplied responsibility to fix code bugs with a hard fork, which removes\nfunds-draining risk for users.\n\n- Support for EVM opcodes like\ntx.origin. ERC-4337, by itself, makes\ntx.origin return the address of the \"bundler\" that packaged\nup a set of user operations into a transaction. Native account\nabstraction could fix this, by making tx.origin point to\nthe actual account sending the transaction, making it work the same way\nas for EOAs.\n\n- Censorship resistance: one of the challenges\nwith proposer/builder separation is that it becomes easier to censor\nindividual transactions. In a world where individual\ntransactions are legible to the Ethereum protocol, this problem\ncan be greatly mitigated with inclusion\nlists, which allow proposers to specify a list of transactions that\nmust be included within the next two slots in almost all cases.\nBut the extra-protocol ERC-4337 wraps \"user operations\" inside a single\ntransaction, making user operations opaque to the Ethereum protocol;\nhence, Ethereum-protocol-provided inclusion lists would not be able to\nprovide censorship resistance to ERC-4337 user operations. Enshrining\nERC-4337, and making user operations a \"proper\" transaction type, would\nsolve this problem.\n\nIt's worth zooming into the gas efficiency issue further. In its\ncurrent form, ERC-4337 is significantly more expensive than a \"basic\"\nEthereum transaction: the transaction costs 21,000 gas, whereas ERC-4337\ncosts ~42,000 gas. This\ndoc lists some of the reasons why:\n\n- Need to pay lots of individual storage read/write costs, which in\nthe case of EOAs get bundled into a single 21000 gas payment:\n\n- Editing the storage slot that contains pubkey+nonce (~5000)\n\n- UserOperation calldata costs (~4500, reducible to ~2500 with\ncompression)\n\n- ECRECOVER (~3000)\n\n- Warming the wallet itself (~2600)\n\n- Warming the recipient account (~2600)\n\n- Transferring ETH to the recipient account (~9000)\n\n- Editing storage to pay fees (~5000)\n\n- Access the storage slot containing the proxy (~2100) and then the\nproxy itself (~2600)\n\n- On top of the above storage read/write costs, the contract needs to\ndo \"business logic\" (unpacking the UserOperation, hashing it, shuffling\nvariables, etc) that EOA transactions have handled \"for free\" by the\nEthereum protocol\n\n- Need to expend gas to pay for logs (EOAs don't issue logs)\n\n- One-time contract creation costs (~32000 base, plus 200 gas per code\nbyte in the proxy, plus 20000 to set the proxy address)\n\nTheoretically, it should be possible to massage the EVM gas cost\nsystem until the in-protocol costs and the extra-protocol costs for\naccessing storage match; there is no reason why transferring ETH needs\nto cost 9000 gas when other kinds of storage-editing operations are much\ncheaper. And indeed, two EIPs ([1] [2])\nrelated to the upcoming Verkle\ntree transition actually try to do that. But even if we do that,\nthere is one huge reason why enshrined protocol features are going to\ninevitably be significantly cheaper than EVM code, no matter how\nefficient the EVM becomes: enshrined code does not need to pay\ngas for being pre-loaded.\n\nFully functional ERC-4337 wallets are big. This\nimplementation, compiled and put on chain, takes\nup ~12,800 bytes. Of course, you can deploy that big piece of code\nonce, and use DELEGATECALL to allow each individual wallet\nto call into it, but that code still needs to be accessed in each block\nthat uses it. Under the Verkle tree\ngas costs EIP, 12,800 bytes would make up 413 chunks, and accessing\nthose chunks would require paying 2x WITNESS_BRANCH_COST\n(3,800 gas total) and 413x WITNESS_CHUNK_COST (82,600 gas\ntotal). And this does not even begin to mention the ERC-4337 entry-point\nitself, with 23,689\nbytes onchain in version 0.6.0 (under the Verkle tree EIP rules,\n~158,700 gas to load).\n\nThis leads to a problem: the gas costs of actually accessing this\ncode would have to be split among transactions somehow. The current\napproach that ERC-4337 uses is not great: the first transaction in a\nbundle eats up one-time storage/code reading costs, making it much more\nexpensive than the rest of the transactions. Enshrinement in-protocol\nwould allow these commonly-shared libraries to simply be part of the\nprotocol, accessible to all with no fees.\nWhat\ncan we learn from this example about when to enshrine things more\ngenerally?\n\nIn this example, we saw a few different rationales for enshrining\naspects of account abstraction in the protocol.\n\n- \"Move complexity to the edges\" market-based approaches break\ndown the most when there are high fixed costs. And indeed, the\nlong term account abstraction roadmap looks like it's going to have\nlots of fixed costs per block. 244,100 gas for loading\nstandardized wallet code is one thing; but aggregation\n(see my\npresentation from this summer for more details) potentially adds\nhundreds of thousands more gas for ZK-SNARK validation plus onchain\ncosts for proof verification. There isn't a way to charge users for\nthese costs without introducing lots of market inefficiencies, whereas\nmaking some of these functionalities into protocol features accessible\nto all with no fees cleanly solves that problem.\n\n- Community-wide response to code bugs. If some set\nof pieces of code are used by all users, or a very wide class of users,\nthen it often makes more sense for the blockchain community to take on\nitself the responsibility to hard-fork to fix any bugs that arise.\nERC-4337 introduced a large amount of globally shared code, and in the\nlong term it makes more sense for bugs in that code to be fixed by hard\nforks than to lead to users losing a large amount of ETH.\n\n- Sometimes, a stronger form of a feature can be implemented\nby directly taking advantage of the powers of the protocol. The\nkey example here is in-protocol censorship resistance features like\ninclusion lists: in-protocol inclusion lists can do a better job of\nguaranteeing censorship resistance than extra-protocol approaches, in\norder for user-level operations to actually benefit from in-protocol\ninclusion lists, individual user-level operations need to be \"legible\"\nto the protocol. Another lesser-known example is that 2017-era Ethereum\nproof of stake designs had account\nabstraction for staking keys, and this was abandoned in favor of\nenshrining BLS because BLS\nsupported an \"aggregation\" mechanism, which would have to be\nimplemented at protocol and network level, that could make handling a\nvery large number of signatures much more efficient.\n\nBut it is important to remember that even enshrined\nin-protocol account abstraction is still a massive \"de-enshrinement\"\ncompared to the status quo. Today, top-level Ethereum\ntransactions can only be initiated from externally\nowned accounts (EOAs) which use a single secp256k1 elliptic curve\nsignature for verification. Account abstraction de-enshrines this, and\nleaves verification conditions open for users to define. And so, in this\nstory about account abstraction, we also saw the biggest argument\nagainst enshrinement: being flexible to diverse users'\nneeds.\n\nLet us try to fill in the story further, by looking at a few other\nexamples of features that have recently been considered for\nenshrinement. We'll particularly focus on: ZK-EVMs,\nproposer-builder separation, private\nmempools, liquid staking and new\nprecompiles.\n\n## Enshrining ZK-EVMs\n\nLet us switch focus to another potential target for enshrining into\nthe Ethereum protocol: ZK-EVMs. Currently, we have a large number of ZK-rollups that all have to\nwrite fairly similar code to verify execution of Ethereum-like\nblocks inside a ZK-SNARK. There is a pretty diverse ecosystem of\nindependent implementations: the\nPSE ZK-EVM, Kakarot, the Polygon ZK-EVM,\nLinea, Zeth, and the list\ngoes on.\n\nOne of the recent controversies in the EVM ZK-rollup space has to do\nwith how to deal with the possibility of bugs in the ZK-code. Currently,\nall of these systems that are live have some form of \"security council\"\nmechanism that can override the proving system in case of a bug. In this\npost last year, I tried to create a standardized framework to\nencourage projects to be clear about what level of trust they put in the\nproving system and what level in the security council, and move toward\ngiving less and less powers to the security council over time.\n\nIn the medium term, rollups could rely on multiple proving\nsystems, and the security council would only have any power at all\nin the extreme case where two different proving systems disagree with\neach other.\n\nHowever, there is a sense in which some of this work feels\nsuperfluous. We already have the Ethereum base layer, which has an\nEVM, and we already have a working mechanism for dealing with bugs in\nimplementations: if there's a bug, the clients that have the bug update\nto fix the bug, and the chain goes on. Blocks that appeared finalized\nfrom the perspective of a buggy client would end up no-longer-finalized,\nbut at least we would not see users losing funds. Similarly, if a rollup\njust wants to be and remain EVM-equivalent, it feels wrong that they\nneed to implement their own governance to keep changing their internal\nZK-EVM rules to match upgrades to the Ethereum base layer, when\nultimately they're building on top of the Ethereum base layer itself,\nwhich knows when it's being upgraded and to what new rules.\n\nSince these L2 ZK-EVMs are basically using the exact same EVM as\nEthereum, can't we somehow make \"verify EVM execution in ZK\" into a\nprotocol feature, and deal with exceptional situations like bugs and\nupgrades by just applying Ethereum's social consensus, the same way we\nalready do for base-layer EVM execution itself?\n\nThis is an important and challenging topic. There are a few\nnuances:\n\n- We want to be compatible with Ethereum's\nmulti-client philosophy. This means that we want to allow\ndifferent clients to use different proving systems. This in turn implies\nthat for any EVM execution that gets proven with one ZK-SNARK system, we\nwant a guarantee that the underlying data is available,\nso that proofs can be generated for other ZK-SNARK systems.\n\n- While the tech is immature, we probably want\nauditability. In practice, this means the exact same thing: if\nany execution gets proven, we want the underlying data to be available,\nso that if anything goes wrong, users and developers can inspect\nit.\n\n- We need much faster proving times, so that if one\ntype of proof is made, other types of proof can be generated quickly\nenough that other clients can validate them. One could get\naround this by making a precompile that has an asynchronous response\nafter some time window longer than a slot (eg. 3 hours), but this adds\ncomplexity.\n\n- We want to support not just copies of the EVM, but also\n\"almost-EVMs\". Part of the attraction of L2s is the ability to\ninnovate on the execution layer, and make extensions to the EVM. If a\ngiven L2's VM differs from the EVM only a little bit, it would be nice\nif the L2 could still use a native in-protocol ZK-EVM for the parts that\nare identical to the EVM, and only rely on their own code for the parts\nthat are different. This could be done by designing the ZK-EVM\nprecompile in such a way that it allows the caller to specify a bitfield\nor list of opcodes or addresses that get handled by an externally\nsupplied table instead of the EVM itself. We could also make gas costs\nopen to customization to a limited extent.\n\nOne likely topic of contention with data availability in a native\nZK-EVM is statefulness. ZK-EVMs are much more\ndata-efficient if they do not have to carry \"witness\" data. That is, if\na particular piece of data was already read or written in some previous\nblock, we can simply assume that provers have access to it, and we don't\nhave to make it available again. This goes beyond not re-loading storage\nand code; it turns out that if a rollup properly compresses\ndata, the compression being stateful allows for up to 3x data\nsavings compared to the compression being stateless.\n\nThis means that for a ZK-EVM precompile, we have two options:\n\n- The precompile requires all data to be available in the same\nblock. This means that provers can be stateless, but it also\nmeans that ZK-rollups using such a precompile become much more expensive\nthan rollups using custom code.\n\n- The precompile allows pointers to data used or generated by\nprevious executions. This allows ZK-rollups to be\nnear-optimal, but it's more complicated and introduces a new kind of\nstate that has to be stored by provers.\n\nWhat lessons can we take away from this? There is a pretty good\nargument to enshrine ZK-EVM validation somehow: rollups are already\nbuilding their own custom versions of it, and it feels wrong\nthat Ethereum is willing to put the weight of its multiple\nimplementations and off-chain social consensus behind EVM execution on\nL1, but L2s doing the exact same work have to instead implement\ncomplicated gadgets involving security councils. But on the other hand,\nthere is a big devil in the details: there are different versions of an\nenshrined ZK-EVM that have different costs and benefits. The stateful vs\nstateless divide only scratches the surface; attempting to support\n\"almost-EVMs\" that have custom code proven by other systems will likely\nreveal an even larger design space. Hence, enshrining ZK-EVMs\npresents both promise and challenges.\nEnshrining\nproposer-builder separation (ePBS)\n\nThe rise of MEV has made\nblock production into an economies-of-scale-heavy activity, with\nsophisticated actors being able to produce blocks that generate much\nmore revenue than default algorithms that simply watch the mempool for\ntransactions and include them. The Ethereum community has so far\nattempted to deal with this by using extra-protocol proposer-builder\nseparation schemes like MEV-Boost, which allow regular\nvalidators (\"proposers\") to outsource block building to specialized\nactors (\"builders\").\n\nHowever, MEV-Boost carries a trust assumption in a new category of\nactor, called a relay. For the past two years, there have been\nmany\nproposals to create \"enshrined PBS\". What is the benefit of this? In\nthis case, the answer is pretty simple: the PBS that can be built by\ndirectly using the powers of the protocol is simply stronger (in the\nsense of having weaker trust assumptions) than the PBS that can be built\nwithout them. It's a similar case to the case for enshrining\nin-protocol price oracles - though, in that situation,\nthere is also a strong\ncounterargument.\n\n## Enshrining private mempools\n\nWhen a user sends a transaction, that transaction becomes immediately\npublic and visible to all, even before it gets included on chain. This\nmakes users of many applications vulnerable to economic attacks such as\nfrontrunning: if a user makes a large trade on eg. Uniswap, an attacker\ncould put in a transaction right before them, increasing the price at\nwhich they buy, and collecting an arbitrage profit.\n\nRecently, there has been a number of projects specializing in\ncreating \"private mempools\" (or \"encrypted mempools\"), which keep users'\ntransactions encrypted until the moment they get irreversibly accepted\ninto a block.\n\nThe problem is, however, that schemes like this require a particular\nkind of encryption: to prevent users from flooding the system and\nfrontrunning the decryption process itself, the encryption must\nauto-decrypt once the transaction actually does get irreversibly\naccepted.\n\nTo implement such a form of encryption, there are various different\ntechnologies with different tradeoffs, described well in this\npost by Jon Charbonneau (and this video and slides):\n\n- Encryption to a centralized operator, eg. Flashbots\nProtect.\n\n- Time-lock\nencryption, a form of encryption which can be decrypted by\nanyone after a certain number of sequential computational\nsteps, which cannot be parallelized.\n\n- Threshold\nencryption, trusting an honest majority committee to\ndecrypt the data. See the shutterized\nbeacon chain concept for a concrete proposal.\n\n- Trusted\nhardware such as SGX.\n\nUnfortunately, each of these have varying weaknesses. A centralized\noperator is not acceptable for inclusion in-protocol for obvious\nreasons. Traditional time lock encryption is too expensive to run across\nthousands of transactions in a public mempool. A more powerful primitive\ncalled delay encryption allows efficient decryption of\nan unlimited number of messages, but it's hard to construct in practice,\nand attacks\non existing constructions still sometimes get discovered. Much like\nwith hash\nfunctions, we'll likely need a period of more years of research and\nanalysis before delay encryption becomes sufficiently mature. Threshold\nencryption requires trusting a majority to not collude, in a setting\nwhere they can collude undetectably (unlike 51% attacks, where\nit's immediately obvious who participated). SGX creates a dependency on\na single trusted manufacturer.\n\nWhile for each solution, there is some subset of users that\nis comfortable trusting it, there is no single solution that is trusted\nenough that it can practically be accepted into layer 1. Hence,\nenshrining anti-frontrunning at layer 1 seems like a difficult\nproposition at least until delay encrypted is perfected or there is some\nother technological breakthrough, even while it's a valuable enough\nfunctionality that lots of application solutions will already\nemerge.\n\n## Enshrining liquid staking\n\nA common demand among Ethereum defi users is the ability to use their\nETH for staking and as collateral in other applications at the same\ntime. Another common demand is simply for convenience: users want to be\nable to stake without the complexity of running a node and keeping it\nonline all the time (and protecting their now-online staking keys).\n\nBy far the simplest possible \"interface\" for staking, which satisfies\nboth of these needs, is just an ERC20 token: convert your ETH into\n\"staked ETH\", hold it, and then later convert back. And indeed, liquid\nstaking providers such as Lido and Rocketpool have emerged to do just\nthat. However, liquid staking has some natural centralizing mechanics at\nplay: people naturally go into the biggest version of staked ETH because\nit's most familiar and most liquid (and most well-supported by\napplications, who in turn support it because it's more familiar and\nbecause it's the one the most users will have heard of).\n\nEach version of staked ETH needs to have some mechanism determining\nwho can be the underlying node operators. It can't be unrestricted,\nbecause then attackers would join and amplify their attacks with users'\nfunds. Currently, the top two are Lido, which has a DAO whitelisting\nnode operators, and Rocket Pool, which allows anyone to run a node if\nthey put down 8\nETH (ie. 1/4 of the capital) as a deposit. These two approaches have\ndifferent risks: the Rocket Pool approach allows attackers to 51% attack\nthe network, and force users to pay most of the costs. With the DAO\napproach, if a single such staking token dominates, that leads to a\nsingle, potentially attackable governance gadget\ncontrolling a very large portion of all Ethereum validators. To the\ncredit of protocols like Lido, they have implemented safeguards against\nthis, but one layer of defense may not be enough.\n\nIn the short term, one option is to socially encourage ecosystem\nparticipants to use a diversity of liquid staking providers, to reduce\nthe chance that any single one becomes too large to be a systemic risk.\nIn the longer term, however, this is an unstable equilibrium, and there\nis peril in relying too much on moralistic pressure to solve problems.\nOne natural question arises: might it make sense to enshrine\nsome kind of in-protocol functionality to make liquid staking less\ncentralizing?\n\nHere, the key question is: what kind of in-protocol\nfunctionality? Simply creating an in-protocol fungible \"staked ETH\"\ntoken has the problem that it would have to either have an enshrined\nEthereum-wide governance to choose who runs the nodes, or be open-entry,\nturning it into a vehicle for attackers.\n\nOne interesting idea is Dankrad Feist's\nwritings on liquid staking maximalism. First, we bite the bullet\nthat if Ethereum gets 51% attacked, only perhaps 5% of the attacking ETH\ngets slashed. This is a reasonable tradeoff; right now there is over 26 million ETH being staked, and a\ncost of attack of 1/3 of that (~8 million ETH) is way overkill,\nespecially considering how many kinds of \"outside-the-model\" attacks can\nbe pulled off for much less. Indeed, a similar tradeoff has already been\nexplored in the \"super-committee\"\nproposal for implementing single-slot finality.\n\nIf we accept that only 5% of attacking ETH gets slashed, then over\n90% of staked ETH would be invulnerable to slashing, and so 90% of\nstaked ETH could be put into an in-protocol fungible liquid staking\ntoken that can then be used by other applications.\n\nThis path is interesting. But it still leaves open the question:\nwhat is the specific thing that would get enshrined? RocketPool already works in a way\nvery similar to this: each node operator puts up some capital, and\nliquid stakers put up the rest. We could simply tweak a few constants,\nbounding the maximum slashing penalty to eg. 2 ETH, and Rocket Pool's\nexisting rETH would become risk-free.\n\nThere are other clever things that we can do with simple protocol\ntweaks. For example, imagine that we want a system where there are two \"tiers\" of\nstaking: node operators (high collateral requirement) and depositors\n(no minimum, can join and leave any time), but we still want to guard\nagainst node operator centralization by giving a randomly-sampled\ncommittee of depositors powers like suggesting lists of transactions\nthat have to be included (for anti-censorship reasons), controlling the\nfork choice during an inactivity leak, or needing to sign off on blocks.\nThis could be done in a mostly-out-of-protocol way, by tweaking the\nprotocol to require each validator to provide (i) a regular\nstaking key, and (ii) an ETH address that can be called to output a\nsecondary staking key during each slot. The protocol would give powers\nto these two keys, but the mechanism for choosing the second key in each\nslot could be left to staking pool protocols. It may still be better to\nenshrine some things outright, but it's valuable to note that this\n\"enshrine some things, leave other things to users\" design space\nexists.\n\n## Enshrining more precompiles\n\nPrecompiles\n(or \"precompiled contracts\") are Ethereum contracts that implement\ncomplex cryptographic operations, whose logic is natively implemented in\nclient code, instead of EVM smart contract code. Precompiles were a\ncompromise adopted at the beginning of Ethereum's development: because\nthe overhead of a VM is too much for certain kinds of very complex and\nhighly specialized code, we can implement a few key operations valuable\nto important kinds of applications in native code to make them faster.\nToday, this basically includes a few specific hash functions\nand elliptic curve operations.\n\nThere is currently a push to add a precompile for\nsecp256r1, an elliptic curve slightly different from the secp256k1\nused for basic Ethereum accounts, because it is well-supported by\ntrusted hardware modules and thus widespread use of it could improve\nwallet security. In recent years, there have also been pushes to add\nprecompiles for BLS-12-377, BW6-761, generalized pairings\nand other features.\n\nThe counterargument to these requests for more precompiles is that\nmany of the precompiles that have been added before (eg. RIPEMD\nand BLAKE) have\nended up gotten used\nmuch less than anticipated, and we should learn from that. Instead\nof adding more precompiles for specific operations, we should perhaps\nfocus on a more moderate approach based on ideas like EVM-MAX\nand the dormant-but-always-revivable SIMD proposal, which\nwould allow EVM implementations to execute wide classes of code less\nexpensively. Perhaps even existing little-used precompiles\ncould be removed and replaced with (unavoidably less efficient) EVM code\nimplementations of the same function. That said, it is still possible\nthat there are specific cryptographic operations that are valuable\nenough to accelerate that it makes sense to add them as precompiles.\nWhat do we learn from all\nthis?\n\nThe desire to enshrine as little as possible is understandable and\ngood; it hails from the Unix philosophy\ntradition of creating software that is minimalist and can be easily\nadapted to different needs by its users, avoiding the curses of software\nbloat. However, blockchains are not personal-computing operating\nsystems; they are social systems. This means that there are\nrationales for enshrining certain features in the protocol that go\nbeyond the rationales that exist in a purely personal-computing\ncontext.\n\nIn many cases, these other examples re-capped similar lessons to what\nwe saw in account abstraction. But there are also a few new lessons that\nhave been learned as well:\n\n- Enshrining features can help avoid centralization risks in\nother areas of the stack. Often, keeping the base protocol\nminimal and simple pushes the complexity to some outside-the-protocol\necosystem. From a Unix philosophy perspective, this is good. Sometimes,\nhowever, there are risks that that outside-the-protocol ecosystem will\ncentralize, often (but not just) because of high fixed costs. Enshrining\ncan sometimes decrease de-facto centralization.\n\n- Enshrining too much can over-extend the trust and governance\nload of the protocol. This is the topic of this earlier post\nabout not overloading Ethereum's consensus: if enshrining a particular\nfeature weakens the trust model, and makes Ethereum as a whole much more\n\"subjective\", that weakens Ethereum's credible neutrality. In those\ncases, it's better to leave that particular feature as a mechanism on\ntop of Ethereum, and not try to bring it inside Ethereum itself. Here,\nencrypted mempools are the best example of something that may be a bit\ntoo difficult to enshrine, at least until/unless delay encryption\ntechnology improves.\n\n- Enshrining too much can over-complicate the\nprotocol. Protocol complexity is a systemic risk, and adding\ntoo many features in-protocol increases that risk. Precompiles are the\nbest example of this.\n\n- Enshrining can backfire in the long term, as users' needs\nare unpredictable. A feature that many people think is\nimportant and will be used by many users may well turn out not to be\nused much in practice.\n\nAdditionally, the liquid staking, ZK-EVM and precompile cases show\nthe possibility of a middle road: minimal viable\nenshrinement. Rather than enshrining an entire functionality,\nthe protocol could enshrine a specific piece that solves the key\nchallenges with making that functionality easy to implement, without\nbeing too opinionated or narrowly focused. Examples of this\ninclude:\n\n- Rather than enshrining a full liquid staking system, changing\nstaking penalty rules to make trustless liquid staking more viable\n\n- Rather than enshrining more precompiles, enshrine EVM-MAX\nand/or SIMD to make\na wider class of operations simpler to implement efficiently\n\n- Rather than enshrining the whole concept of rollups, we\ncould simply enshrine EVM verification.\n\nWe can extend our diagram from earlier in the post as follows:\n\nSometimes, it may even make sense to de-enshrine a few\nthings. De-enshrining little-used precompiles is one example. Account\nabstraction as a whole, as mentioned earlier, is also a significant form\nof de-enshrinement. If we want to support backwards-compatibility for\nexisting users, then the mechanism may actually be surprisingly similar\nto that for de-enshrining precompiles: one of the proposals is EIP-5003, which would\nallow EOAs to convert their account in-place into a contract that has\nthe same (or better) functionality.\n\nWhat features should be brought into the protocol and what features\nshould be left to other layers of the ecosystem is a complicated\ntradeoff, and we should expect the tradeoff to continue to evolve over\ntime as our understanding of users' needs and our suite of available\nideas and technologies continues to improve.",
    "contentLength": 38463,
    "summary": "Ethereum is reconsidering its original minimalist protocol philosophy to potentially \"enshrine\" more features directly into the core protocol.",
    "detailedSummary": {
      "theme": "Vitalik examines when Ethereum should incorporate features directly into the protocol versus leaving them to external layers, using account abstraction and other examples to develop a framework for making these decisions.",
      "summary": "Vitalik explores the tension between Ethereum's original philosophy of protocol minimalism and the practical benefits of enshrining certain features directly in the protocol. He traces the evolution of account abstraction from early minimalist designs like EIP-86 to the current ERC-4337 standard, showing how the pursuit of protocol simplicity often just shifts complexity elsewhere in the stack, sometimes making systems less efficient and more centralized. Through examples including ZK-EVMs, proposer-builder separation, private mempools, liquid staking, and precompiles, Vitalik demonstrates that enshrining features can help avoid centralization risks, reduce fixed costs that create market inefficiencies, enable stronger protocol-level guarantees like censorship resistance, and provide community-wide responses to code bugs. However, he warns that over-enshrining can weaken trust models, over-complicate the protocol, and lock in features that may not meet users' evolving needs. Vitalik advocates for a 'minimal viable enshrinement' approach that addresses key challenges without being overly opinionated, and suggests that the decision of what to enshrine should continue evolving as understanding of user needs and available technologies improves.",
      "takeaways": [
        "Protocol minimalism can shift complexity to other parts of the ecosystem rather than eliminating it, sometimes creating worse outcomes including centralization risks and inefficiencies",
        "Enshrining features makes sense when there are high fixed costs that create market inefficiencies, when community-wide bug response is needed, or when protocol-level powers enable stronger guarantees",
        "Over-enshrining can weaken Ethereum's credible neutrality, increase systemic risk through protocol complexity, and lock in features that don't match actual user needs",
        "A 'minimal viable enshrinement' approach can solve key challenges by enshrining specific pieces rather than entire functionalities, such as changing staking rules rather than building full liquid staking systems",
        "The enshrinement decision framework should continue evolving as user needs and available technologies change, and some features may even need to be de-enshrined over time"
      ],
      "controversial": [
        "The suggestion that Ethereum should accept higher centralization in liquid staking (like Dankrad Feist's liquid staking maximalism) by reducing slashing penalties to make trustless staking more viable",
        "The proposal to potentially remove little-used precompiles like RIPEMD and BLAKE from the protocol, which could break backwards compatibility for applications that depend on them"
      ]
    }
  },
  {
    "id": "general-2023-08-16-communitynotes",
    "title": "What do I think about Community Notes?",
    "date": "2023-08-16",
    "category": "general",
    "url": "https://vitalik.eth.limo/general/2023/08/16/communitynotes.html",
    "path": "general/2023/08/16/communitynotes.html",
    "content": "- \n\n- \n\n  \n    \n\n  \n  \n  Dark Mode Toggle\n  \n    \n\n  \n\n##  What do I think about Community Notes? \n\n 2023 Aug 16 \nSee all posts\n\n \n \n\n What do I think about Community Notes? \n\nSpecial thanks to Dennis Pourteaux and Jay Baxter for feedback\nand review.\n\nThe last two years of Twitter X have been tumultuous, to say\nthe least. After the platform was bought\nnot\nbought bought\nby Elon Musk for $44 billion last year, Elon enacted sweeping changes to\nthe company's staffing,\ncontent\nmoderation and business\nmodel, not to mention changes to the culture on the site that may\nwell have been a result of Elon's soft power more than any specific\npolicy decision. But in the middle of these highly contentious actions,\none new feature on Twitter grew rapidly in importance, and seems to be\nbeloved by people across the political spectrum: Community Notes.\n\nCommunity Notes is a fact-checking tool that sometimes attaches\ncontext notes, like the one on Elon's tweet above, to tweets as a\nfact-checking and anti-misinformation tool. It was originally called\nBirdwatch, and was first rolled out as a pilot project in January 2021.\nSince then, it has expanded in stages, with the most rapid phase of its\nexpansion coinciding with Twitter's takeover by Elon last year. Today,\nCommunity Notes appear frequently on tweets that get a very large\naudience on Twitter, including those on contentious political topics.\nAnd both in my view, and in the view of many people across the political\nspectrum I talk to, the notes, when they appear, are informative and\nvaluable.\n\nBut what interests me most about Community Notes is how, despite not\nbeing a \"crypto project\", it might be the closest thing to an\ninstantiation of \"crypto values\" that we have seen in the mainstream\nworld. Community Notes are not written or curated by some centrally\nselected set of experts; rather, they can be written and voted on by\nanyone, and which notes are shown or not shown is decided entirely by an\nopen source\nalgorithm. The Twitter site has a detailed\nand extensive guide describing how the algorithm works, and you can\ndownload\nthe data containing which notes and votes have been published, run\nthe\nalgorithm locally, and verify that the output matches what is\nvisible on the Twitter site. It's not perfect, but it's surprisingly\nclose to satisfying the ideal of credible\nneutrality, all while being impressively useful, even under\ncontentious conditions, at the same time.\nHow does the\nCommunity Notes algorithm work?\n\nAnyone with a Twitter account matching\nsome criteria (basically: active for 6+ months, no recent rule\nviolations, verified phone number) can sign up to participate in\nCommunity Notes. Currently, participants are slowly and randomly being\naccepted, but eventually the plan is to let in anyone who fits the\ncriteria. Once you are accepted, you can at first participate in rating\nexisting notes, and once you've made enough good ratings (measured\nby seeing which ratings match with the final outcome for that note),\nyou can also write notes of your own.\n\nWhen you write a note, the note gets a score based on the reviews\nthat it receives from other Community Notes members. These reviews can\nbe thought of as being votes along a 3-point scale of\nHELPFUL, SOMEWHAT_HELPFUL and\nNOT_HELPFUL, but a review can also contain some other tags\nthat have roles in the algorithm. Based on these reviews, a note gets a\nscore. If the note's score is above 0.40, the note is shown; otherwise,\nthe note is not shown.\n\nThe way that the score is calculated is what makes the algorithm\nunique. Unlike simpler algorithms, which aim to simply calculate some\nkind of sum or average over users' ratings and use that as the final\nresult, the Community Notes rating algorithm explicitly attempts\nto prioritize notes that receive positive ratings from people across a\ndiverse range of perspectives. That is, if people who usually\ndisagree on how they rate notes end up agreeing on a particular note,\nthat note is scored especially highly.\n\nLet us get into the deep math of how this works. We have a set of\nusers and a set of notes; we can create a matrix \\(M\\), where the cell \\(M_{i,j}\\) represents how the i'th user\nrated the j'th note.\n\nFor any given note, most users have not rated that note, so most\nentries in the matrix will be zero, but that's fine. The goal of the\nalgorithm is to create a four-column model of users and notes, assigning\neach user two stats that we can call \"friendliness\" and \"polarity\", and\neach note two stats that we can call \"helpfulness\" and \"polarity\". The\nmodel is trying to predict the matrix as a function of these values,\nusing the following formula:\n\nNote that here I am introducing both the terminology used in the Birdwatch\npaper, and my own terms to provide a less mathematical intuition for\nwhat the variables mean:\n\n- \u03bc is a \"general public mood\" parameter that\naccounts for how high the ratings are that users give in general\n\n- \\(i_u\\) is a user's\n\"friendliness\": how likely that particular user is to\ngive high ratings\n\n- \\(i_n\\) is a note's\n\"helpfulness\": how likely that particular note is to\nget rated highly. Ultimately, this is the variable we care\nabout.\n\n- \\(f_u\\) or \\(f_n\\) is user or note's\n\"polarity\": its position among the dominant axis of\npolitical polarization. In practice, negative polarity roughly means\n\"left-leaning\" and positive polarity means \"right-leaning\", but note\nthat the axis of polarization is discovered emergently from\nanalyzing users and notes; the concepts of leftism and rightism are in\nno way hard-coded.\n\nThe algorithm uses a pretty basic machine learning model (standard gradient\ndescent) to find values for these variables that do the best\npossible job of predicting the matrix values. The helpfulness that a\nparticular note is assigned is the note's final score. If a note's\nhelpfulness is at least +0.4, the note gets shown.\n\nThe core clever idea here is that the \"polarity\" terms absorb\nthe properties of a note that cause it to be liked by some users and not\nothers, and the \"helpfulness\" term only measures the properties that a\nnote has that cause it to be liked by all. Thus, selecting for\nhelpfulness identifies notes that get cross-tribal approval, and selects\nagainst notes that get cheering from one tribe at the expense of disgust\nfrom the other tribe.\n\nI made a simplified implementation of the basic algorithm; you can\nfind it here,\nand are welcome to play around with it.\n\nNow, the above is only a description of the central core of the\nalgorithm. In reality, there are a lot of extra mechanisms\nbolted on top. Fortunately, they are described in the public\ndocumentation. These mechanisms include the following:\n\n- The algorithm gets run many times, each time adding some randomly\ngenerated extreme \"pseudo-votes\" to the votes. This means that the\nalgorithm's true output for each note is a range of values, and the\nfinal result depends on a \"lower confidence bound\" taken from this\nrange, which is checked against a threshold of 0.32.\n\n- If many users (especially users with a similar polarity to the note)\nrate a note \"Not Helpful\", and furthermore they specify the same \"tag\"\n(eg. \"Argumentative or biased language\", \"Sources do not support note\")\nas the reason for their rating, the helpfulness threshold required for\nthe note to be published increases from 0.4 to 0.5 (this looks small but\nit's very significant in practice)\n\n- If a note is accepted, the threshold that its helpfulness must drop\nbelow to de-accept it is 0.01 points lower than the threshold that a\nnote's helpfulness needed to reach for the note to be originally\naccepted\n\n- The algorithm gets run even more times with multiple\nmodels, and this can sometimes promote notes whose original helpfulness\nscore is somewhere between 0.3 and 0.4\n\nAll in all, you get some pretty complicated python code that amounts\nto 6282 lines stretching across 22 files. But it is all open, you can\ndownload the note\nand rating data and run it yourself, and see if the outputs\ncorrespond to what is actually on Twitter at any given moment.\nSo how does this look in\npractice?\n\nProbably the single most important idea in this algorithm that\ndistinguishes it from naively taking an average score from people's\nvotes is what I call the \"polarity\" values. The algorithm documentation\ncalls them \\(f_u\\) and \\(f_n\\), using \\(f\\) for factor because these are\nthe two terms that get multiplied with each other; the more general\nlanguage is in part because of a desire to eventually make \\(f_u\\) and \\(f_n\\) multi-dimensional.\n\nPolarity is assigned to both users and notes. The link between\nuser IDs and the underlying Twitter accounts is intentionally\nkept hidden, but notes are public. In practice, the polarities generated\nby the algorithm, at least for the English-language data set, map very\nclosely to the left vs right political spectrum.\n\nHere are some examples of notes that have gotten polarities around\n-0.8:\n\nNote\n\nPolarity\n\nAnti-trans rhetoric has been amplified by some conservative Colorado\nlawmakers, including U.S. Rep.\u00a0Lauren Boebert, who narrowly won\nre-election in Colorado's GOP-leaning 3rd Congressional District, which\ndoes not include Colorado Springs.\nhttps://coloradosun.com/2022/11/20/colorado-springs-club-q-lgbtq-trans/\n\n-0.800\n\nPresident Trump explicitly undermined American faith in election results\nin the months leading up to the 2020 election.\nhttps://www.npr.org/2021/02/08/965342252/timeline-what-trump-told-supporters-for-months-before-they-attacked\nEnforcing Twitter's Terms of Service is not election interference.\n\n-0.825\n\nThe 2020 election was conducted in a free and fair manner.\nhttps://www.npr.org/2021/12/23/1065277246/trump-big-lie-jan-6-election\n\n-0.818\n\nNote that I am not cherry-picking here; these are literally the first\nthree rows in the scored_notes.tsv spreadsheet generated by\nthe algorithm when I ran it locally that have a polarity score (called\ncoreNoteFactor1 in the spreadsheet) of less than -0.8.\n\nNow, here are some notes that have gotten polarities around +0.8. It\nturns out that many of these are either people talking about Brazilian\npolitics in Portuguese or Tesla fans angrily refuting criticism of\nTesla, so let me cherry-pick a bit to find a few that are not:\n\nNote\n\nPolarity\n\nAs of 2021 data, 64% of \"Black or African American\" children lived in\nsingle-parent families.\nhttps://datacenter.aecf.org/data/tables/107-children-in-single-parent-families-by-race-and-ethnicity\n\n+0.809\n\nContrary to Rolling Stones push to claim child trafficking is \"a Qanon\nadjacent conspiracy,\" child trafficking is a real and huge issue that\nthis movie accurately depicts. Operation Underground Railroad works with\nmultinational agencies to combat this issue.\nhttps://ourrescue.org/\n\n+0.840\n\nExample pages from these LGBTQ+ children's books being banned can be\nseen here:\nhttps://i.imgur.com/8SY6cEx.png\nThese books are obscene, which is not protected by the US constitution\nas free speech.\nhttps://www.justice.gov/criminal-ceos/obscenity\n\"Federal law strictly prohibits the distribution of obscene matter to\nminors.\n\n+0.806\n\nOnce again, it is worth reminding ourselves that the \"left vs right\ndivide\" was not in any way hardcoded into the algorithm; it was\ndiscovered emergently by the calculation. This suggests that if you\napply this algorithm in other cultural contexts, it could automatically\ndetect what their primary political divides are, and bridge across those\ntoo.\n\nMeanwhile, notes that get the highest helpfulness look like\nthis. This time, because these notes are actually shown on Twitter, I\ncan just screenshot one directly:\n\nAnd another one:\n\nThe second one touches on highly partisan political themes more\ndirectly, but it's a clear, high-quality and informative note, and so it\ngets rated highly. So all in all, the algorithm seems to work, and the\nability to verify the outputs of the algorithm by running the code seems\nto work.\nWhat do I think of the\nalgorithm?\n\nThe main thing that struck me when analyzing the algorithm is just\nhow complex it is. There is the \"academic paper version\", a\ngradient descent which finds a best fit to a five-term vector and matrix\nequation, and then the real version, a complicated series of many\ndifferent executions of the algorithm with lots of arbitrary\ncoefficients along the way.\n\nEven the academic paper version hides complexity under the hood. The\nequation that it's optimizing is a degree-4\nequation (as there's a degree-2 \\(f_u *\nf_n\\) term in the prediction formula, and compounding that the\ncost function measures error\nsquared). While optimizing a degree-2 equation over any number of\nvariables almost always has a unique solution, which you can calculate\nwith fairly basic linear algebra, a degree-4 equation over many\nvariables often has many solutions, and so multiple rounds of a gradient\ndescent algorithm may well arrive at different answers. Tiny changes to\nthe input may well cause the descent to flip from one local minimum to\nanother, significantly changing the output.\n\nThe distinction between this, and algorithms that I helped work on\nsuch as quadratic\nfunding, feels to me like a distinction between an\neconomist's algorithm and an engineer's\nalgorithm. An economist's algorithm, at its best, values being\nsimple, being reasonably easy to analyze, and having clear mathematical\nproperties that show why it's optimal (or least-bad) for the task that\nit's trying to solve, and ideally proves bounds on how much damage\nsomeone can do by trying to exploit it. An engineer's algorithm, on the\nother hand, is a result of iterative trial and error, seeing what works\nand what doesn't in the engineer's operational context. Engineer's\nalgorithms are pragmatic and do the job; economist's algorithms\ndon't go totally crazy when confronted with the unexpected.\n\nOr, as was famously\nsaid on a related topic by the esteemed internet philosopher roon\n(aka tszzl):\n\nOf course, I would say that the \"theorycel aesthetic\" side of crypto\nis necessary precisely to distinguish protocols that are actually trustless from janky\nconstructions that look fine and seem to work well but under the hood\nrequire trusting a few centralized actors - or worse, actually end up\nbeing outright scams.\n\nDeep learning works when it works, but it has inevitable\nvulnerabilities to all kinds of adversarial\nmachine learning attacks. Nerd traps and sky-high abstraction\nladders, if done well, can be quite robust against them. And so one\nquestion I have is: could we turn Community Notes itself into\nsomething that's more like an economist algorithm?\n\nTo give a view of what this would mean in practice, let's explore an\nalgorithm I came up with a few years ago for a similar purpose: pairwise-bounded\nquadratic funding.\n\nThe goal of pairwise-bounded quadratic funding is to plug a hole in\n\"regular\" quadratic funding, where if even two participants collude with\neach other, they can each contribute a very high amount of money to a\nfake project that sends the money back to them, and get a large subsidy\nthat drains the entire pool. In pairwise quadratic funding, we assign\neach pair of participants a limited budget \\(M\\). The algorithm walks over all possible\npairs of participants, and if the algorithm decides to add a subsidy to\nsome project \\(P\\) because both\nparticipant \\(A\\) and participant \\(B\\) supported it, that subsidy comes out of\nthe budget assigned to the pair \\((A,\nB)\\). Hence, even if \\(k\\)\nparticipants were to collude, the amount they could steal from the\nmechanism is at most \\(k * (k-1) *\nM\\).\n\nAn algorithm of exactly this form is not very applicable to\nthe Community Notes context, because each user makes very few votes: on\naverage, any two users would have exactly zero votes in common, and so\nthe algorithm would learn nothing about users' polarities by just\nlooking at each pair of users separately. The goal of the machine\nlearning model is precisely to try to \"fill in\" the matrix from very\nsparse source data that cannot be analyzed in this way directly. But the\nchallenge of this approach is that it takes extra effort to do it in a\nway that does not make the result highly volatile in the face of a few\nbad votes.\nDoes Community\nNotes actually fight polarization?\n\nOne thing that we could do is analyze whether or not the Community\nNotes algorithm, as is, actually manages to fight polarization at\nall - that is, whether or not it actually does any better than a\nnaive voting algorithm. Naive voting algorithms already fight\npolarization to some limited extent: a post with 200 upvotes and 100\ndownvotes does worse than a post that just gets the 200 upvotes. But\ndoes Community Notes do better than that?\n\nLooking at the algorithm abstractly, it's hard to tell. Why wouldn't\na high-average-rating but polarizing post get a strong polarity\nand a high helpfulness? The idea is that polarity is supposed\nto \"absorb\" the properties of a note that cause it to get a lot of votes\nif those votes are conflicting, but does it actually do that?\n\nTo check this, I ran my\nown simplified implementation for 100 rounds. The average results\nwere:\n\nQuality averages:\nGroup 1 (good): 0.30032841807271166\nGroup 2 (good but extra polarizing): 0.21698871680927437\nGroup 3 (neutral): 0.09443120045416832\nGroup 4 (bad): -0.1521160965793673\n\nIn this test, \"Good\" notes received a rating of +2 from users in the\nsame political tribe and +0 from users in the opposite political tribe,\nand \"Good but extra polarizing\" notes received a rating of +4 from\nsame-tribe users and -2 from opposite-tribe users. Same average, but\ndifferent polarity. And it seems to actually be the case that \"Good\"\nnotes get a higher average helpfulness than \"Good but extra polarizing\"\nnotes.\n\nOne other benefit of having something closer to an \"economist's\nalgorithm\" would be having a clearer story for how the algorithm is\npenalizing polarization.\nHow useful is\nthis all in high-stakes situations?\n\nWe can see some of how this works out by looking at one specific\nsituation. About a month ago, Ian Bremmer complained that a highly\ncritical Community Note that was added to a tweet by a Chinese\ngovernment official had\nbeen removed.\n\nThe note, which is now no longer visible. Screenshot by Ian Bremmer.\n\nThis is heavy stuff. It's one thing to do mechanism design in a nice\nsandbox Ethereum community environment where the largest complaint is\n$20,000 going to a polarizing\nTwitter influencer. It's another to do it for political and\ngeopolitical questions that affect many millions of people and where\neveryone, often quite understandably, is assuming maximum bad faith. But\nif mechanism designers want to have a significant impact into the world,\nengaging with these high-stakes environments is ultimately\nnecessary.\n\nIn the case of Twitter, there is a clear reason why one might suspect\ncentralized manipulation to be behind the Note's removal: Elon has a lot\nof business\ninterests in China, and so there is a possibility that Elon forced\nthe Community Notes team to interfere with the algorithm's outputs and\ndelete this specific one.\n\nFortunately, the algorithm is open source and verifiable, so we can\nactually look under the hood! Let's do that. The URL of the original\ntweet is https://twitter.com/MFA_China/status/1676157337109946369.\nThe number at the end, 1676157337109946369, is the tweet\nID. We can search for that in the downloadable\ndata, and identify the specific row in the spreadsheet that has the\nabove note:\n\nHere we get the ID of the note itself,\n1676391378815709184. We then search for that in\nthe scored_notes.tsv and\nnote_status_history.tsv files generated by running the\nalgorithm. We get:\n\nThe second column in the first output is the note's current rating.\nThe second output shows the note's history: its current status is in the\nseventh column (NEEDS_MORE_RATINGS), and the first status\nthat's not NEEDS_MORE_RATINGS that it received earlier on\nis in the fifth column (CURRENTLY_RATED_HELPFUL). Hence, we\nsee that the algorithm itself first showed the note, and then\nremoved it once its rating dropped somewhat - seemingly no centralized\nintervention involved.\n\nWe can see this another way by looking at the votes themselves. We\ncan scan the ratings-00000.tsv file to isolate all the\nratings for this note, and see how many rated HELPFUL vs\nNOT_HELPFUL:\n\nBut if you sort them by timestamp, and look at the first 50 votes,\nyou see 40 HELPFUL votes and 9 NOT_HELPFUL\nvotes. And so we see the same conclusion: the note's initial audience\nviewed the note more favorably then the note's later audience, and so\nits rating started out higher and dropped lower over time.\n\nUnfortunately, the exact story of how the note changed\nstatus is complicated to explain: it's not a simple matter of \"before\nthe rating was above 0.40, now it's below 0.40, so it got dropped\".\nRather, the high volume of NOT_HELPFUL replies triggered\none of the outlier\nconditions, increasing the helpfulness score that the note needs to\nstay over the threshold.\n\nThis is a good learning opportunity for another lesson: making a\ncredibly neutral algorithm truly credible requires keeping it simple.\nIf a note moves from being accepted to not being accepted, there should\nbe a simple and legible story as to why.\n\nOf course, there is a totally different way in which this\nvote could have been manipulated: brigading. Someone who sees a\nnote that they disapprove of could call upon a highly engaged community\n(or worse, a mass of fake accounts) to rate it NOT_HELPFUL,\nand it may not require that many votes to drop the note from being seen\nas \"helpful\" to being seen as \"polarized\". Properly minimizing the\nvulnerability of this algorithm to such coordinated attacks will require\na lot more analysis and work. One possible improvement would be not\nallowing any user to vote on any note, but instead using the \"For you\"\nalgorithmic feed to randomly allocate notes to raters, and only allow\nraters to rate those notes that they have been allocated to.\nIs Community Notes not\n\"brave\" enough?\n\nThe main criticism of Community Notes that I have seen is basically\nthat it does not do enough. Two\nrecent articles\nthat I have seen make this point. Quoting one:\n\nThe program is severely hampered by the fact that for a Community\nNote to be public, it has to be generally accepted by a consensus of\npeople from all across the political spectrum.\n\n\"It has to have ideological consensus,\" he said. \"That means people\non the left and people on the right have to agree that that note must be\nappended to that tweet.\"\n\nEssentially, it requires a \"cross-ideological agreement on truth, and\nin an increasingly partisan environment, achieving that consensus is\nalmost impossible, he said.\n\nThis is a difficult issue, but ultimately I come down on the side\nthat it is better to let ten misinformative tweets go free than it is to\nhave one tweet covered by a note that judges it unfairly. We have seen\nyears of fact-checking that is brave, and does come\nfrom the perspective of \"well, actually we know the truth, and we know\nthat one side lies much more often than the other\". And what happened as\na result?\n\nHonestly, some pretty widespread distrust of fact-checking as a\nconcept. One strategy here is to say: ignore the haters, remember that\nthe fact checking experts really do know the facts better than any\nvoting system, and stay the course. But going all-in on this approach seems\nrisky. There is value in building cross-tribal institutions that are\nat least somewhat respected by everyone. As with William\nBlackstone's dictum and the courts, it feels to me that maintaining\nsuch respect requires a system that commits far more sins of omission than\nit does sins of commission. And so it seems valuable to me that there is\nat least one major organization that is taking this alternate path, and\ntreating its rare cross-tribal respect as a resource to be cherished and\nbuilt upon.\n\nAnother reason why I think it is okay for Community Notes to be\nconservative is that I do not think it is the goal for every\nmisinformative tweet, or even most misinformative tweets, to receive a\ncorrective note. Even if less than one percent of misinformative\ntweets get a note providing context or correcting them, Community Notes\nis still providing an exceedingly valuable service as an educational\ntool. The goal is not to correct everything; rather, the goal\nis to remind people that multiple perspectives exist, that certain kinds\nof posts that look convincing and engaging in isolation are actually\nquite incorrect, and you, yes you, can often go do a basic internet\nsearch to verify that it's incorrect.\n\nCommunity Notes cannot be, and is not meant to be, a miracle cure\nthat solves all problems in public epistemology. Whatever problems it\ndoes not solve, there is plenty of room for other mechanisms, whether\nnewfangled gadgets such as prediction markets or good old-fashioned\norganizations hiring full-time staff with domain expertise, to try to\nfill in the gaps.\n\n## Conclusions\n\nCommunity Notes, in addition to being a fascinating social media\nexperiment, is also an instance of a fascinating new and emerging genre\nof mechanism design: mechanisms that intentionally try to identify\npolarization, and favor things that bridge across divides rather than\nperpetuate them.\n\nThe two other things in this category that I know about are (i) pairwise\nquadratic funding, which is being used in Gitcoin Grants and (ii) Polis, a discussion tool that uses clustering\nalgorithms to help communities identify statements that are commonly\nwell-received across people who normally have different viewpoints. This\narea of mechanism design is valuable, and I hope that we can see a lot\nmore academic work in this field.\n\nAlgorithmic transparency of the type that Community Notes offers is\nnot quite full-on decentralized social media - if you disagree with how\nCommunity Notes works, there's no way to go see a view of the same\ncontent with a different algorithm. But it's the closest that\nvery-large-scale applications are going to get within the next couple of\nyears, and we can see that it provides a lot of value already, both by\npreventing centralized manipulation and by ensuring that platforms that\ndo not engage in such manipulation can get proper credit for doing\nso.\n\nI look forward to seeing both Community Notes, and hopefully many\nmore algorithms of a similar spirit, develop and grow over the next\ndecade.",
    "contentLength": 26353,
    "summary": "Vitalik praises Twitter's Community Notes as the closest mainstream example of \"crypto values\" due to its open-source algorithm.",
    "detailedSummary": {
      "theme": "Vitalik analyzes Twitter's Community Notes as a surprisingly successful implementation of crypto values through its decentralized, algorithm-driven approach to fact-checking that bridges political divides.",
      "summary": "Vitalik examines Community Notes as a remarkable example of 'crypto values' in mainstream social media, praising its open-source algorithm that prioritizes cross-tribal consensus over simple majority voting. He explains how the system uses a sophisticated machine learning model to identify notes that receive positive ratings from people across diverse political perspectives, effectively filtering out polarizing content while promoting genuinely helpful information. Vitalik distinguishes between 'economist's algorithms' (simple, analyzable, theoretically sound) versus 'engineer's algorithms' (complex, pragmatic, trial-and-error based), categorizing Community Notes as the latter while suggesting potential improvements toward the former. He addresses criticisms that the system isn't 'brave' enough in fighting misinformation, arguing that maintaining cross-tribal trust and credibility is more valuable than aggressive fact-checking that might undermine the system's legitimacy. Ultimately, Vitalik sees Community Notes as part of an emerging genre of mechanism design focused on bridging polarization rather than perpetuating it.",
      "takeaways": [
        "Community Notes represents a successful mainstream implementation of crypto values through its decentralized, open-source approach to content moderation",
        "The algorithm cleverly prioritizes notes that receive cross-tribal approval by modeling user and note 'polarity' to filter out partisan content",
        "The system's complexity makes it more like an 'engineer's algorithm' rather than a theoretically sound 'economist's algorithm,' which may create vulnerabilities",
        "Vitalik argues that maintaining cross-tribal credibility through conservative moderation is more valuable than aggressive fact-checking that could undermine trust",
        "The transparency and verifiability of the algorithm allows for public auditing of controversial decisions, as demonstrated in the Chinese government tweet case"
      ],
      "controversial": [
        "Vitalik's preference for letting 'ten misinformative tweets go free' rather than risk one unfair correction may be seen as too permissive in combating harmful misinformation",
        "His characterization of traditional fact-checking as having failed due to partisan distrust might be disputed by those who see professional journalism as essential",
        "The suggestion that Community Notes doesn't need to correct most misinformation could be viewed as insufficient given the scale of online misinformation"
      ]
    }
  },
  {
    "id": "general-2023-07-24-biometric",
    "title": "What do I think about biometric proof of personhood?",
    "date": "2023-07-24",
    "category": "society",
    "url": "https://vitalik.eth.limo/general/2023/07/24/biometric.html",
    "path": "general/2023/07/24/biometric.html",
    "content": "- \n\n- \n\n  \n    \n\n  \n  \n  Dark Mode Toggle\n  \n    \n\n  \n\n##  What do I think about biometric proof of personhood? \n\n 2023 Jul 24 \nSee all posts\n\n \n \n\n What do I think about biometric proof of personhood? \n\nSpecial thanks to the Worldcoin team, the Proof of Humanity\ncommunity and Andrew Miller for discussion.\n\nOne of the trickier, but potentially one of the most valuable,\ngadgets that people in the Ethereum community have been trying to build\nis a decentralized proof-of-personhood solution. Proof\nof personhood, aka the \"unique-human\nproblem\", is a limited form of real-world identity that asserts that\na given registered account is controlled by a real person (and a\ndifferent real person from every other registered account), ideally\nwithout revealing which real person it is.\n\nThere have been a few efforts at tackling this problem: Proof of Humanity, BrightID, Idena and Circles come up as examples. Some of\nthem come with their own applications (often a UBI token), and some have\nfound use in Gitcoin Passport\nto verify which accounts are valid for quadratic voting. Zero-knowledge\ntech like Sismo adds privacy to many of\nthese solutions. More recently, we have seen the rise of a much larger\nand more ambitious proof-of-personhood project: Worldcoin.\n\nWorldcoin was co-founded by Sam Altman, who is best known for being\nthe CEO of OpenAI. The philosophy\nbehind the project is simple: AI is going to create a lot of\nabundance and wealth for humanity, but it also may kill very many\npeople's jobs\nand make it almost impossible to tell who even is a human and not a bot,\nand so we need to plug that hole by (i) creating a really good\nproof-of-personhood system so that humans can prove that they actually\nare humans, and (ii) giving everyone a UBI. Worldcoin is unique in that\nit relies on highly sophisticated biometrics, scanning each user's iris using a\npiece of specialized hardware called \"the Orb\":\n\nThe goal is to produce a large number of these Orbs and widely\ndistribute them around the world and put them in public\nplaces to make it easy for anyone to get an ID. To Worldcoin's\ncredit, they have also committed\nto decentralize over time. At first, this means technical\ndecentralization: being an L2 on Ethereum using the\nOptimism stack, and protecting users' privacy with ZK-SNARKs\nand other cryptographic techniques. Later on, it includes\ndecentralizing governance of the system itself.\n\nWorldcoin has been criticized for privacy and security concerns\naround the Orb, design issues in its \"coin\", and for ethical\nissues around some choices\nthat the company has made. Some of the criticisms are highly specific,\nfocusing on decisions made by the project that could easily have been\nmade in another way - and indeed, that the Worldcoin project itself may\nbe willing to change. Others, however, raise the more fundamental\nconcern of whether or not biometrics - not just the eye-scanning\nbiometrics of Worldcoin, but also the simpler face-video-uploads and\nverification games used in Proof of Humanity and Idena - are a good idea\nat all. And still others criticize\nproof of personhood in general. Risks include unavoidable privacy\nleaks, further erosion of people's ability to navigate the internet\nanonymously, coercion by authoritarian governments, and the potential\nimpossibility of being secure at the same time as being\ndecentralized.\n\nThis post will talk about these issues, and go through some arguments\nthat can help you decide whether or not bowing down and scanning your\neyes (or face, or voice, or...) before our new spherical overlords is a\ngood idea, and whether or not the natural alternatives - either using\nsocial-graph-based proof of personhood or giving up on proof of\npersonhood entirely - are any better.\nWhat is\nproof of personhood and why is it important?\n\nThe simplest way to define a proof-of-personhood system is: it\ncreates a list of public keys where the system guarantees that each key\nis controlled by a unique human. In other words, if you're a human, you\ncan put one key on the list, but you can't put two keys on the list, and\nif you're a bot you can't put any keys on the list.\n\nProof of personhood is valuable because it solves a lot of anti-spam\nand anti-concentration-of-power problems that many people have, in a way\nthat avoids dependence on centralized authorities and reveals the\nminimal information possible. If proof of personhood is not solved,\ndecentralized governance (including \"micro-governance\" like votes on\nsocial media posts) becomes\nmuch easier to capture by very wealthy actors,\nincluding hostile governments. Many services would only be able to\nprevent denial-of-service attacks by setting a price for access, and\nsometimes a price high enough to keep out attackers is also too high for\nmany lower-income legitimate users.\n\nMany major applications in the world today deal with this issue by\nusing government-backed identity systems such as credit cards and\npassports. This solves the problem, but it makes large and perhaps\nunacceptable sacrifices on privacy, and can be trivially attacked by\ngovernments themselves.\n\nHow many proof of personhood proponents see the two-sided risk that\nwe are facing. Image\nsource.\n\nIn many proof-of-personhood projects - not just Worldcoin, but also\nProof of Humanity, Circles and others - the \"flagship application\" is a\nbuilt-in \"N-per-person token\" (sometimes called a \"UBI token\"). Each\nuser registered in the system receives some fixed quantity of tokens\neach day (or hour, or week). But there are plenty of other\napplications:\n\n- Airdrops for token distributions\n\n- Token or NFT sales that give more favorable terms to\nless-wealthy users\n\n- Voting in DAOs\n\n- A way to \"seed\" graph-based reputation systems\n\n- Quadratic voting\n(and funding, and attention payments)\n\n- Protection against bots / sybil attacks in\nsocial media\n\n- An alternative to captchas for preventing DoS\nattacks\n\nIn many of these cases, the common thread is a desire to create\nmechanisms that are open and democratic, avoiding both centralized\ncontrol by a project's operators and domination by its wealthiest users.\nThe latter is especially\nimportant in decentralized governance. In many of these cases,\nexisting solutions today rely on some combination of (i) highly opaque\nAI algorithms that leave lots of room to undetectably discriminate\nagainst users that the operators simply do not like, and (ii)\ncentralized IDs, aka \"KYC\". An effective proof-of-personhood solution\nwould be a much better alternative, achieving the security properties\nthat those applications need without the pitfalls of the existing\ncentralized approaches.\nWhat are\nsome early attempts at proof of personhood?\n\nThere are two main forms of proof of personhood:\nsocial-graph-based and biometric.\nSocial-graph based proof of personhood relies on some form of vouching:\nif Alice, Bob, Charlie and David are all verified humans, and they all\nsay that Emily is a verified human, then Emily is probably also a\nverified human. Vouching is often enhanced with incentives: if Alice\nsays that Emily is a human, but it turns out that she is not, then Alice\nand Emily may both get penalized. Biometric proof of personhood involves\nverifying some physical or behavioral trait of Emily, that distinguishes\nhumans from bots (and individual humans from each other). Most projects\nuse a combination of the two techniques.\n\nThe four systems I mentioned at the beginning of the post work\nroughly as follows:\n\n- Proof of\nHumanity: you upload a video of yourself, and provide a\ndeposit. To be approved, an existing user needs to vouch for you, and an\namount of time needs to pass during which you can be challenged. If\nthere is a challenge, a Kleros\ndecentralized court determines whether or not your video was\ngenuine; if it is not, you lose your deposit and the challenger gets a\nreward.\n\n- BrightID:\nyou join a video call \"verification party\" with other users, where\neveryone verifies each other. Higher levels of verification are\navailable via Bitu,\na system in which you can get verified if enough other Bitu-verified\nusers vouch for you.\n\n- Idena: you play\na captcha game at a specific point in time (to prevent people from\nparticipating multiple times); part of the captcha game involves\ncreating and verifying captchas that will then be used to verify\nothers.\n\n- Circles: an\nexisting Circles user vouches for you. Circles is unique in that it does\nnot attempt to create a \"globally verifiable ID\"; rather, it creates a\ngraph of trust relationships, where someone's trustworthiness can only\nbe verified from the perspective of your own position in that\ngraph.\n\n## How does Worldcoin work?\n\nEach Worldcoin user installs an app on their phone, which generates a\nprivate and public key, much like an Ethereum wallet. They then go\nin-person to visit an \"Orb\". The user stares into the Orb's camera, and\nat the same time shows the Orb a QR code generated by their Worldcoin\napp, which contains their public key. The Orb scans the user's eyes, and\nuses complicated hardware scanning and machine-learned classifiers to\nverify that:\n\n- The user is a real human\n\n- The user's iris does not match the iris of any other user that has\npreviously used the system\n\nIf both scans pass, the Orb signs a message approving a specialized\nhash of the user's iris scan. The hash gets uploaded to a database -\ncurrently a centralized server, intended to be replaced with a\ndecentralized on-chain system once they are sure the hashing mechanism\nworks. The system does not store full iris scans; it only stores hashes,\nand these hashes are used to check for uniqueness. From that point\nforward, the user has a \"World\nID\".\n\nA World ID holder is able to prove that they are a unique human by\ngenerating a ZK-SNARK proving that they hold the private key\ncorresponding to a public key in the database, without revealing\nwhich key they hold. Hence, even if someone re-scans your iris,\nthey will not be able to see any actions that you have taken.\nWhat are\nthe major issues with Worldcoin's construction?\n\nThere are four major risks that immediately come to mind:\n\n- Privacy. The registry of iris scans may reveal\ninformation. At the very least, if someone else scans your\niris, they can check it against the database to determine whether or not\nyou have a World ID. Potentially, iris scans might reveal more\ninformation.\n\n- Accessibility. World IDs are not going to be\nreliably accessible unless there are so many Orbs that anyone in the\nworld can easily get to one.\n\n- Centralization. The Orb is a hardware device, and\nwe have no way to verify that it was constructed correctly and does not\nhave backdoors. Hence, even if the software layer is perfect and fully\ndecentralized, the Worldcoin Foundation still has the ability to insert\na backdoor into the system, letting it create arbitrarily many fake\nhuman identities.\n\n- Security. Users' phones could be hacked, users\ncould be coerced into scanning their irises while showing a public key\nthat belongs to someone else, and there is the possibility of\n3D-printing \"fake people\" that can pass the iris scan and get World\nIDs.\n\nIt's important to distinguish between (i) issues specific to\nchoices made by Worldcoin, (ii) issues that any biometric proof of\npersonhood will inevitably have, and (iii) issues that any\nproof of personhood in general will have. For example,\nsigning up to Proof of Humanity means publishing your face on the\ninternet. Joining a BrightID verification party doesn't quite\ndo that, but still exposes who you are to a lot of people. And joining\nCircles publicly exposes your social graph. Worldcoin is significantly\nbetter at preserving privacy than either of those. On the other\nhand, Worldcoin depends on specialized hardware, which opens up the\nchallenge of trusting the orb manufacturers to have constructed the orbs\ncorrectly - a challenge which has no parallels in Proof of Humanity,\nBrightID or Circles. It's even conceivable that in the future, someone\nother than Worldcoin will create a different\nspecialized-hardware solution that has different tradeoffs.\nHow\ndo biometric proof-of-personhood schemes address privacy issues?\n\nThe most obvious, and greatest, potential privacy leak that any\nproof-of-personhood system has is linking each action that a person\ntakes to a real-world identity. This data leak is very large, arguably\nunacceptably large, but fortunately it is easy to solve with zero knowledge\nproof technology. Instead of directly making a signature\nwith a private key whose corresponding public key is in the database, a\nuser could make a ZK-SNARK proving that they own the private key whose\ncorresponding public key is somewhere in the database, without\nrevealing which specific key they have. This can be done generically\nwith tools like Sismo (see here for the Proof of\nHumanity-specific implementation), and Worldcoin has its own built-in\nimplementation. It's important to give \"crypto-native\" proof of\npersonhood credit here: they actually care about taking this basic step\nto provide anonymization, whereas basically all centralized identity\nsolutions do not.\n\nA more subtle, but still important, privacy leak is the mere\nexistence of a public registry of biometric scans. In the case\nof Proof of Humanity, this is a lot of data: you get a video of each\nProof of Humanity participant, making it very clear to anyone in the\nworld who cares to investigate who all the Proof of Humanity\nparticipants are. In\nthe case of Worldcoin, the leak is much more limited: the Orb\nlocally computes and publishes only a \"hash\" of each person's iris\nscan. This hash is not a regular hash like SHA256; rather, it is a\nspecialized algorithm based on machine-learned Gabor\nfilters that deals\nwith the inexactness inherent in any biometric scan, and ensures\nthat successive hashes taken of the same person's iris have similar\noutputs.\n\nBlue: percent of bits that differ between two scans of the same\nperson's iris. Orange: percent of bits that differ between two scans of\ntwo different people's irises.\n\nThese iris hashes leak only a small amount of data. If an adversary\ncan forcibly (or secretly) scan your iris, then they can compute your\niris hash themselves, and check it against the database of iris hashes\nto see whether or not you participated in the system. This ability to\ncheck whether or not someone signed up is necessary for the system\nitself to prevent people from signing up multiple times, but there's\nalways the possibility that it will somehow be abused. Additionally,\nthere is the possibility that the iris hashes leak some amount of\nmedical data (sex, ethnicity, perhaps medical conditions), but this leak\nis far smaller than what could be captured by pretty much any other mass\ndata-gathering system in use today (eg. even street cameras). On the\nwhole, to me the privacy of storing iris hashes seems sufficient.\n\nIf others disagree with this judgement and decide that they want to\ndesign a system with even more privacy, there are two ways to do so:\n\n- If the iris hashing algorithm can be improved to make the difference\nbetween two scans of the same person much lower (eg. reliably under 10%\nbit flips), then instead of storing full iris hashes, the system can\nstore a smaller number of error correction bits for iris hashes (see: fuzzy\nextractors). If the difference between two scans is under 10%, then\nthe number of bits that needs to be published would be at least 5x\nless.\n\n- If we want to go further, we could store the iris hash database\ninside a multi-party\ncomputation (MPC) system which could only be accessed by Orbs (with\na rate limit), making the data unaccessible entirely, but at the cost of\nsignificant protocol complexity and social complexity in governing the\nset of MPC participants. This would have the benefit that users would\nnot be able to prove a link between two different World IDs that they\nhad at different times even if they wanted to.\n\nUnfortunately, these techniques are not applicable to Proof of\nHumanity, because Proof of Humanity requires the full video of each\nparticipant to be publicly available so that it can be challenged if\nthere are signs that it is fake (including AI-generated fakes), and in\nsuch cases investigated in more detail.\n\nOn the whole, despite the \"dystopian vibez\" of staring into an Orb\nand letting it scan deeply into your eyeballs, it does seem like\nspecialized hardware systems can do quite a decent job of protecting\nprivacy. However, the flip side of this is that specialized hardware\nsystems introduce much greater centralization concerns. Hence, we\ncypherpunks seem to be stuck in a bind: we have to trade off one\ndeeply-held cypherpunk value against another.\nWhat\nare the accessibility issues in biometric proof-of-personhood\nsystems?\n\nSpecialized hardware introduces accessibility concerns because, well,\nspecialized hardware is not very accessible. Somewhere between 51%\nand 64%\nof sub-Saharan Africans now have smartphones, and this seems to be\nprojected to increase\nto 87% by 2030. But while there are billions of smartphones, there\nare only a few hundred Orbs. Even with much higher-scale distributed\nmanufacturing, it would be hard to get to a world where there's an Orb\nwithin five kilometers of everyone.\n\nBut to the team's credit, they have been\ntrying!\n\nIt is also worth noting that many other forms of proof of\npersonhood have accessibility problems that are even worse. It is very\ndifficult to join a social-graph-based proof-of-personhood system unless\nyou already know someone who is in the social graph. This makes it very\neasy for such systems to remain restricted to a single community in a\nsingle country.\n\nEven centralized identity systems have learned this lesson:\nIndia's Aadhaar ID\nsystem is biometric-based, as that was the only way to quickly\nonboard its massive\npopulation while avoiding massive fraud from duplicate and fake\naccounts (resulting in huge\ncost savings), though of course the Aadhaar system as a whole is far\nweaker on privacy than anything being proposed on a large scale within\nthe crypto community.\n\nThe best-performing systems from an accessibility perspective\nare actually systems like Proof of Humanity, which you can sign\nup to using only a smartphone - though, as we have seen and as we will\nsee, such systems come with all kinds of other tradeoffs.\nWhat\nare the centralization issues in biometric proof-of-personhood\nsystems?\n\nThere are three:\n\n- Centralization risks in the system's top-level governance (esp.\u00a0the\nsystem that makes final top-level resolutions if different actors in the\nsystem disagree on subjective judgements).\n\n- Centralization risks unique to systems that use specialized\nhardware.\n\n- Centralization risks if proprietary algorithms are used to determine\nwho is an authentic participant.\n\nAny proof-of-personhood system must contend with (1), perhaps with\nthe exception of systems where the set of \"accepted\" IDs is completely\nsubjective. If a system uses incentives denominated in outside assets\n(eg. ETH, USDC, DAI), then it cannot be fully subjective, and so\ngovernance risks become unavoidable.\n\n[2] is a much bigger risk for Worldcoin than for Proof of Humanity\n(or BrightID), because Worldcoin depends on specialized hardware and\nother systems do not.\n\n[3] is a risk particularly in \"logically\ncentralized\" systems where there is a single system doing the\nverification, unless all of the algorithms are open-source and we have\nan assurance that they are actually running the code that they claim\nthey are. For systems that rely purely on users verifying other users\n(like Proof of Humanity), it is not a risk.\nHow\ndoes Worldcoin address hardware centralization issues?\n\nCurrently, a Worldcoin-affiliated entity called Tools for Humanity is the\nonly organization that is making Orbs. However, the Orb's source code is\nmostly\npublic: you can see the hardware specs in this github\nrepository, and other parts of the source code are expected to be\npublished soon. The license\nis another one of those \"shared source but not technically open source\nuntil four years from now\" licenses similar to the Uniswap\nBSL, except in addition to preventing forking it also prevents what\nthey consider unethical behavior - they specifically list mass\nsurveillance and three\ninternational civil\nrights declarations.\n\nThe team's stated goal is to allow and encourage other organizations\nto create Orbs, and over time transition from Orbs being created by\nTools for Humanity to having some kind of DAO that approves and manages\nwhich organizations can make Orbs that are recognized by the system.\n\nThere are two ways in which this design can fail:\n\n- It fails to actually decentralize. This could\nhappen because of the common trap\nof federated protocols: one manufacturer ends up dominating in\npractice, causing the system to re-centralize. Presumably, governance\ncould limit how many valid Orbs each manufacturer can produce, but this\nwould need to be managed carefully, and it puts a lot of pressure on\ngovernance to be both decentralized and monitor the ecosystem\nand respond to threats effectively: a much harder task than eg. a fairly\nstatic DAO that just handles top-level dispute resolution tasks.\n\n- It turns out that it's not possible to make such a\ndistributed manufacturing mechanism secure. Here, there are two\nrisks that I see:\n\n- Fragility against bad Orb manufacturers: if even\none Orb manufacturer is malicious or hacked, it can generate an\nunlimited number of fake iris scan hashes, and give them World IDs.\n\n- Government restriction of Orbs: governments that do\nnot want their citizens participating in the Worldcoin ecosystem can ban\nOrbs from their country. Furthermore, they could even force their\ncitizens to get their irises scanned, allowing the government to get\ntheir accounts, and the citizens would have no way to respond.\n\nTo make the system more robust against bad Orb manufacturers, the\nWorldcoin team is proposing to perform regular audits on Orbs, verifying\nthat they are built correctly and key hardware components were built\naccording to specs and were not tampered with after the fact. This is a\nchallenging task: it's basically something like the IAEA\nnuclear inspections bureaucracy but for Orbs. The hope is that even\na very imperfect implementation of an auditing regime could greatly cut\ndown on the number of fake Orbs.\n\nTo limit the harm caused by any bad Orb that does slip\nthrough, it makes sense to have a second mitigation. World IDs\nregistered with different Orb manufacturers, and ideally with different\nOrbs, should be distinguishable from each other. It's okay if\nthis information is private and only stored on the World ID holder's\ndevice; but it does need to be provable on demand. This makes it\npossible for the ecosystem to respond to (inevitable) attacks by\nremoving individual Orb manufacturers, and perhaps even individual Orbs,\nfrom the whitelist on-demand. If we see the North Korea government going\naround and forcing people to scan their eyeballs, those Orbs and any\naccounts produced by them could be immediately retroactively\ndisabled.\nSecurity\nissues in proof of personhood in general\n\nIn addition to issues specific to Worldcoin, there are concerns that\naffect proof-of-personhood designs in general. The major ones that I can\nthink of are:\n\n- 3D-printed fake people: one could use AI to\ngenerate photographs or even 3D prints of fake people that are\nconvincing enough to get accepted by the Orb software. If even one group\ndoes this, they can generate an unlimited number of identities.\n\n- Possibility of selling IDs: someone can provide\nsomeone else's public key instead of their own when registering, giving\nthat person control of their registered ID, in exchange for money. This\nseems\nto be happening already. In addition to selling, there's also the\npossibility of renting IDs to use for a short time in\none application.\n\n- Phone hacking: if a person's phone gets hacked, the\nhacker can steal the key that controls their World ID.\n\n- Government coercion to steal IDs: a government\ncould force their citizens to get verified while showing a QR code\nbelonging to the government. In this way, a malicious government could\ngain access to millions of IDs. In a biometric system, this could even\nbe done covertly: governments could use obfuscated Orbs to extract World\nIDs from everyone entering their country at the passport control\nbooth.\n\n[1] is specific to biometric proof-of-personhood systems. [2] and [3]\nare common to both biometric and non-biometric designs. [4] is also\ncommon to both, though the techniques that are required would be quite\ndifferent in both cases; in this section I will focus on the issues in\nthe biometric case.\n\nThese are pretty serious weaknesses. Some already have been addressed\nin existing protocols, others can be addressed with future improvements,\nand still others seem to be fundamental limitations.\nHow can we deal with fake\npeople?\n\nThis is significantly less of a risk for Worldcoin than it is for\nProof of Humanity-like systems: an in-person scan can examine many\nfeatures of a person, and is quite hard to fake, compared to merely deep-faking\na video.\nSpecialized hardware is inherently harder to fool than commodity\nhardware, which is in turn harder to fool than digital algorithms\nverifying pictures and videos that are sent remotely.\n\nCould someone 3D-print something that can fool even specialized\nhardware eventually? Probably. I expect that at some point we will see\ngrowing tensions between the goal of keeping the mechanism open and\nkeeping it secure: open-source AI algorithms are inherently more\nvulnerable to adversarial\nmachine learning. Black-box algorithms are more protected, but it's\nhard to tell that a black-box algorithm was not trained to include\nbackdoors. Perhaps ZK-ML\ntechnologies could give us the best of both worlds. Though at some\npoint in the even further future, it is likely that even the best AI\nalgorithms will be fooled by the best 3D-printed fake people.\n\nHowever, from my discussions with both the Worldcoin and Proof of\nHumanity teams, it seems like at the present moment neither protocol is\nyet seeing significant deep fake attacks, for the simple reason that\nhiring real low-wage workers to sign up on your behalf is quite\ncheap and easy.\n\n## Can we prevent selling IDs?\n\nIn the short term, preventing this kind of outsourcing is difficult,\nbecause most people in the world are not even aware of\nproof-of-personhood protocols, and if you tell them to hold up a QR code\nand scan their eyes for $30 they will do that. Once more people\nare aware of what proof-of-personhood protocols are, a fairly\nsimple mitigation becomes possible: allowing people who have a\nregistered ID to re-register, canceling the previous ID. This\nmakes \"ID selling\" much less credible, because someone who sells you\ntheir ID can just go and re-register, canceling the ID that they just\nsold. However, getting to this point requires the protocol to be\nvery widely known, and Orbs to be very widely\naccessible to make on-demand registration practical.\n\nThis is one of the reasons why having a UBI coin integrated into a\nproof-of-personhood system is valuable: a UBI coin provides an\neasily understandable incentive for people to (i) learn about the\nprotocol and sign up, and (ii) immediately re-register if they register\non behalf of someone else. Re-registration also prevents phone\nhacking.\nCan\nwe prevent coercion in biometric proof-of-personhood systems?\n\nThis depends on what kind of coercion we are talking about. Possible\nforms of coercion include:\n\n- Governments scanning people's eyes (or faces, or...) at border control\nand other routine government checkpoints, and using this to register\n(and frequently re-register) their citizens\n\n- Governments banning Orbs within the country to prevent people from\nindependently re-registering\n\n- Individuals buying IDs and then threatening to harm the seller if\nthey detect that the ID has been invalidated due to re-registration\n\n- (Possibly government-run) applications requiring people to \"sign in\"\nby signing with their public key directly, letting them see the\ncorresponding biometric scan, and hence the link between the user's\ncurrent ID and any future IDs they get from re-registering. A common\nfear is that this makes it too easy to create \"permanent records\" that\nstick with a person for their entire life.\n\nAll your UBI and voting power are belong to us. Image\nsource.\n\nEspecially in the hands of unsophisticated users, it seems\nquite tough to outright prevent these situations. Users could\nleave their country to (re-)register at an Orb in a safer country, but\nthis is a difficult process and high cost. In a truly hostile legal\nenvironment, seeking out an independent Orb seems too difficult and\nrisky.\n\nWhat is feasible is making this kind of abuse more\nannoying to implement and detectable. The\nProof of Humanity approach of requiring a person to speak a specific\nphrase when registering is a good example: it may be enough to prevent\nhidden scanning, requiring coercion to be much more blatant,\nand the registration phrase could even include a statement confirming\nthat the respondent knows that they have the right to re-register\nindependently and may get UBI coin or other rewards. If coercion is\ndetected, the devices used to perform coercive registrations en masse\ncould have their access rights revoked. To prevent applications linking\npeople's current and previous IDs and attempting to leave \"permanent\nrecords\", the default proof of personhood app could lock the user's key\nin trusted hardware, preventing any application from using the key\ndirectly without the anonymizing ZK-SNARK layer in between. If a\ngovernment or application developer wants to get around this, they would\nneed to mandate the use of their own custom app.\n\nWith a combination of these techniques and active vigilance, locking\nout those regimes that are truly hostile, and keeping honest those\nregimes that are merely medium-bad (as much of the world is), seems\npossible. This can be done either by a project like Worldcoin or Proof\nof Humanity maintaining its own bureaucracy for this task, or by\nrevealing more information about how an ID was registered (eg. in\nWorldcoin, which Orb it came from), and leaving this classification task\nto the community.\nCan we prevent\nrenting IDs (eg. to sell votes)?\n\nRenting out your ID is not prevented by re-registration.\nThis is okay in some applications: the cost of renting out your right to\ncollect the day's share of UBI coin is going to be just the value of the\nday's share of UBI coin. But in applications such as voting, easy vote selling is a huge\nproblem.\n\nSystems like MACI can\nprevent you from credibly selling your vote, by allowing you to later\ncast another vote that invalidates your previous vote, in such a way\nthat no one can tell whether or not you in fact cast such a vote.\nHowever, if the briber controls which key you get at registration\ntime, this does not help.\n\nI see two solutions here:\n\n- Run entire applications inside an MPC. This would\nalso cover the re-registration process: when a person registers to the\nMPC, the MPC assigns them an ID that is separate from, and not linkable\nto, their proof of personhood ID, and when a person re-registers, only\nthe MPC would know which account to deactivate. This prevents users from\nmaking proofs about their actions, because every important step is done\ninside an MPC using private information that is only known to the\nMPC.\n\n- Decentralized registration ceremonies. Basically,\nimplement something like this\nin-person key-registration protocol that requires four randomly\nselected local participants to work together to register someone. This\ncould ensure that registration is a \"trusted\" procedure that an attacker\ncannot snoop in during.\n\nSocial-graph-based systems may actually perform better here, because\nthey can create local decentralized registration processes automatically\nas a byproduct of how they work.\nHow\ndo biometrics compare with the other leading candidate for proof of\npersonhood, social graph-based verification?\n\nAside from biometric approaches, the main other contender for proof\nof personhood so far has been social-graph-based verification.\nSocial-graph-based verification systems all operate on the same\nprinciple: if there are a whole bunch of existing verified identities\nthat all attest to the validity of your identity, then you probably are\nvalid and should also get verified status.\n\nIf only a few real users (accidentally or maliciously) verify fake\nusers, then you can use basic graph-theory techniques to put an upper\nbound on how many fake users get verified by the system. Source: https://www.sciencedirect.com/science/article/abs/pii/S0045790622000611.\n\nProponents of social-graph-based verification often describe it as\nbeing a better alternative to biometrics for a few reasons:\n\n- It does not rely on special-purpose hardware,\nmaking it much easier to deploy\n\n- It avoids a permanent arms race between\nmanufacturers trying to create fake people and the Orb needing to be\nupdated to reject such fake people\n\n- It does not require collecting biometric data,\nmaking it more privacy-friendly\n\n- It is potentially more friendly to pseudonymity,\nbecause if someone chooses to split their internet life across multiple\nidentities that they keep separate from each other, both of those\nidentities could potentially be verified (but maintaining multiple\ngenuine and separate identities sacrifices network effects and has a\nhigh cost, so it's not something that attackers could do easily)\n\n- Biometric approaches give a binary score of \"is a\nhuman\" or \"is not a human\", which is fragile: people who are\naccidentally rejected would end up with no UBI at all, and potentially\nno ability to participate in online life. Social-graph-based\napproaches can give a more nuanced numerical score, which may\nof course be moderately unfair to some participants but is unlikely to\n\"un-person\" someone completely.\n\nMy perspective on these arguments is that I largely agree with them!\nThese are genuine advantages of social-graph-based approaches and should\nbe taken seriously. However, it's worth also taking into account the\nweaknesses of social-graph-based approaches:\n\n- Bootstrapping: for a user to join a\nsocial-graph-based system, that user must know someone who is already in\nthe graph. This makes large-scale adoption difficult, and risks\nexcluding entire regions of the world that do not get lucky in the\ninitial bootstrapping process.\n\n- Privacy: while social-graph-based approaches avoid\ncollecting biometric data, they often end up leaking info about a\nperson's social relationships, which may lead to even greater risks. Of\ncourse, zero-knowledge technology can mitigate this (eg. see this\nproposal by Barry Whitehat), but the interdependency inherent in a\ngraph and the need to perform mathematical analyses on the graph makes\nit harder to achieve the same level of data-hiding that you can with\nbiometrics.\n\n- Inequality: each person can only have one biometric\nID, but a wealthy and socially well-connected person could use their\nconnections to generate many IDs. Essentially, the same flexibility that\nmight allow a social-graph-based system to give multiple pseudonyms to\nsomeone (eg. an activist) that really needs that feature would likely\nalso imply that more powerful and well-connected people can gain more\npseudonyms than less powerful and well-connected people.\n\n- Risk of collapse into centralization: most people\nare too lazy to spend time reporting into an internet app who is a real\nperson and who is not. As a result, there is a risk that the system will\ncome over time to favor \"easy\" ways to get inducted that depend on\ncentralized authorities, and the \"social graph\" that the system users\nwill de-facto become the social graph of which countries recognize which\npeople as citizens - giving us centralized KYC with needless extra\nsteps.\n\nIs\nproof of personhood compatible with pseudonymity in the real world?\n\nIn principle, proof of personhood is compatible with all kinds of\npseudonymity. Applications could be designed in such a way that someone\nwith a single proof of personhood ID can create up to five profiles\nwithin the application, leaving room for pseudonymous accounts. One\ncould even use quadratic\nformulas: N accounts for a cost of $N\u00b2. But will they?\n\nA pessimist, however, might argue that it is naive to try to create a\nmore privacy-friendly form of ID and hope that it will actually get\nadopted in the right way, because the powers-that-be are not\nprivacy-friendly, and if a powerful actor gets a tool that\ncould be used to get much more information about a person, they\nwill use it that way. In such a world, the argument goes, the\nonly realistic approach is, unfortunately, to throw sand in the\ngears of any identity solution, and defend a world with full\nanonymity and digital islands of high-trust communities.\n\nI see the reasoning behind this way of thinking, but I worry that\nsuch an approach would, even if successful, lead to a world where\nthere's no way for anyone to do anything to counteract wealth\nconcentration and governance centralization, because one person could\nalways pretend to be ten thousand. Such points of centralization would,\nin turn, be easy for the powers-that-be to capture. Rather, I would\nfavor a moderate approach, where we vigorously advocate for\nproof-of-personhood solutions to have strong privacy, potentially if\ndesired even include a \"N accounts for $N\u00b2\" mechanism at protocol layer,\nand create something that has privacy-friendly values and has a\nchance of getting accepted by the outside world.\n\n## So... what do I think?\n\nThere is no ideal form of proof of personhood. Instead, we have at\nleast three different paradigms of approaches that all have their own\nunique strengths and weaknesses. A comparison chart might look as\nfollows:\n\nSocial-graph-based\n\nGeneral-hardware biometric\n\nSpecialized-hardware biometric\n\nPrivacy\n\nLow\n\nFairly low\n\nFairly high\n\nAccessibility / scalability\n\nFairly low\n\nHigh\n\nMedium\n\nRobustness of decentralization\n\nFairly high\n\nFairly high\n\nFairly low\n\nSecurity against \"fake people\"\n\nHigh (if done well)\n\nLow\n\nMedium\n\nWhat we should ideally do is treat these three techniques as\ncomplementary, and combine them all. As India's Aadhaar has\nshown at scale, specialized-hardware biometrics have their benefits of\nbeing secure at scale. They are very weak at decentralization, though\nthis can be addressed by holding individual Orbs accountable.\nGeneral-purpose biometrics can be adopted very easily today, but their\nsecurity is rapidly dwindling, and they may only work for another 1-2\nyears. Social-graph-based systems bootstrapped off of a few hundred\npeople who are socially close to the founding team are likely to face\nconstant tradeoffs between completely missing large parts of the world\nand being vulnerable to attacks within communities they have no\nvisibility into. A social-graph-based system bootstrapped off tens of\nmillions of biometric ID holders, however, could actually work.\nBiometric bootstrapping may work better short-term, and\nsocial-graph-based techniques may be more robust long-term, and take on\na larger share of the responsibility over time as their algorithms\nimprove.\n\nA possible hybrid path.\n\nAll of these teams are in a position to make many mistakes, and there\nare inevitable tensions between business interests and the needs of the\nwider community, so it's important to exercise a lot of vigilance. As a\ncommunity, we can and should push all participants' comfort zones on\nopen-sourcing their tech, demand third-party audits and even\nthird-party-written software, and other checks and balances. We also\nneed more alternatives in each of the three categories.\n\nAt the same time it's important to recognize the work already done:\nmany of the teams running these systems have shown a willingness to take\nprivacy much more seriously than pretty much any government or major\ncorporate-run identity systems, and this is a success that we should\nbuild on.\n\nThe problem of making a proof-of-personhood system that is effective\nand reliable, especially in the hands of people distant from the\nexisting crypto community, seems quite challenging. I definitely do not\nenvy the people attempting the task, and it will likely take years to\nfind a formula that works. The concept of proof-of-personhood in\nprinciple seems very valuable, and while the various implementations\nhave their risks, not having any proof-of-personhood at all has its\nrisks too: a world with no proof-of-personhood seems more likely to be a\nworld dominated by centralized identity solutions, money, small closed\ncommunities, or some combination of all three. I look forward to seeing\nmore progress on all types of proof of personhood, and hopefully seeing\nthe different approaches eventually come together into a coherent\nwhole.",
    "contentLength": 40859,
    "summary": "Worldcoin's biometric proof-of-personhood approach raises major privacy, security, and decentralization concerns despite solving bot problems.",
    "detailedSummary": {
      "theme": "Vitalik analyzes the tradeoffs between biometric and social-graph approaches to proof of personhood, with particular focus on Worldcoin's iris-scanning system.",
      "summary": "Vitalik examines proof of personhood systems that verify unique human identity without revealing personal information, comparing biometric approaches like Worldcoin's iris scanning with social-graph methods used by systems like Proof of Humanity and BrightID. He identifies key challenges across privacy (biometric data collection), accessibility (hardware distribution), centralization (specialized hardware control), and security (fake people, ID selling, coercion). While acknowledging Worldcoin's privacy advantages through zero-knowledge proofs and iris hashing, Vitalik highlights serious centralization risks from specialized hardware dependency and potential government coercion. He argues that social-graph approaches offer better decentralization and avoid biometric collection but suffer from bootstrapping difficulties, inequality issues, and potential collapse into centralized systems. Rather than viewing these approaches as competing solutions, Vitalik advocates for a hybrid model that combines biometric bootstrapping for initial scale with social-graph techniques for long-term robustness, emphasizing that proof of personhood, despite its flaws, is preferable to a world dominated by centralized identity solutions or wealthy actors.",
      "takeaways": [
        "Proof of personhood systems face fundamental tradeoffs between privacy, accessibility, decentralization, and security with no perfect solution",
        "Worldcoin's specialized hardware approach offers better privacy through iris hashing and zero-knowledge proofs but creates significant centralization risks",
        "Social-graph-based systems provide better decentralization but struggle with bootstrapping, inequality, and potential capture by centralized authorities",
        "A hybrid approach combining biometric bootstrapping with social-graph verification may offer the best long-term solution",
        "The absence of proof of personhood systems would likely lead to dominance by centralized identity solutions, wealthy actors, or closed communities"
      ],
      "controversial": [
        "The acceptability of iris scanning and biometric data collection even with privacy protections",
        "Whether specialized hardware systems can ever truly achieve meaningful decentralization",
        "The suggestion that some level of proof of personhood is necessary versus maintaining full anonymity",
        "The potential for government coercion and mass surveillance through biometric systems"
      ]
    }
  },
  {
    "id": "general-2023-06-20-deeperdive",
    "title": "Deeper dive on cross-L2 reading for wallets and other use cases",
    "date": "2023-06-20",
    "category": "protocol",
    "url": "https://vitalik.eth.limo/general/2023/06/20/deeperdive.html",
    "path": "general/2023/06/20/deeperdive.html",
    "content": "- \n\n- \n\n  \n    \n\n  \n  \n  Dark Mode Toggle\n  \n    \n\n  \n\n##  Deeper dive on cross-L2 reading for wallets and other use cases \n\n 2023 Jun 20 \nSee all posts\n\n \n \n\n Deeper dive on cross-L2 reading for wallets and other use cases \n\nSpecial thanks to Yoav Weiss, Dan Finlay, Martin Koppelmann, and\nthe Arbitrum, Optimism, Polygon, Scroll and SoulWallet teams for\nfeedback and review.\n\nIn this post on\nthe Three Transitions, I outlined some key reasons why it's valuable\nto start thinking explicitly about L1 + cross-L2 support, wallet\nsecurity, and privacy as necessary basic features of the ecosystem\nstack, rather than building each of these things as addons that can be\ndesigned separately by individual wallets.\n\nThis post will focus more directly on the technical aspects\nof one specific sub-problem: how to make it easier to read L1 from L2,\nL2 from L1, or an L2 from another L2. Solving this problem is\ncrucial for implementing an asset / keystore separation architecture,\nbut it also has valuable use cases in other areas, most notably\noptimizing reliable cross-L2 calls, including use cases like moving\nassets between L1 and L2s.\n\n## Recommended pre-reads\n\n- Post on the\nThree Transitions\n\n- Ideas from the Safe team on holding\nassets across multiple chains\n\n- Why we need wide adoption of social recovery\nwallets\n\n- ZK-SNARKs, and some privacy\napplications\n\n- Dankrad\non KZG commitments\n\n- Verkle trees\n\n## Table of contents\n\n- What is the goal?\n\n- What does a\ncross-chain proof look like?\n\n- What kinds of\nproof schemes can we use?\n\n- Merkle proofs\n\n- ZK SNARKs\n\n- Special purpose\nKZG proofs\n\n- Verkle tree proofs\n\n- Aggregation\n\n- Direct state reading\n\n- How does\nL2 learn the recent Ethereum state root?\n\n- Wallets\non chains that are not L2s\n\n- Preserving privacy\n\n- Summary\n\n## What is the goal?\n\nOnce L2s become more mainstream, users will have assets across\nmultiple L2s, and possibly L1 as well. Once smart contract wallets\n(multisig, social recovery or otherwise) become mainstream, the\nkeys needed to access some account are going to change over time, and\nold keys would need to no longer be valid. Once both\nof these things happen, a user will need to have a way to change the\nkeys that have authority to access many accounts which live in many\ndifferent places, without making an extremely high number of\ntransactions.\n\nParticularly, we need a way to handle counterfactual\naddresses: addresses that have not yet been \"registered\" in any way\non-chain, but which nevertheless need to receive and securely hold\nfunds. We all depend on counterfactual addresses: when you use\nEthereum for the first time, you are able to generate an ETH address\nthat someone can use to pay you, without \"registering\" the address\non-chain (which would require paying txfees, and hence already holding\nsome ETH).\n\nWith EOAs, all\naddresses start off as counterfactual addresses. With smart contract\nwallets, counterfactual addresses are still possible, largely thanks to\nCREATE2, which\nallows you to have an ETH address that can only be filled by a smart\ncontract that has code matching a particular hash.\n\nEIP-1014\n(CREATE2) address calculation algorithm.\n\nHowever, smart contract wallets introduce a new challenge: the\npossibility of access keys changing. The address, which is a\nhash of the initcode, can only contain the wallet's\ninitial verification key. The current verification key\nwould be stored in the wallet's storage, but that storage record does\nnot magically propagate to other L2s.\n\nIf a user has many addresses on many L2s, including addresses that\n(because they are counterfactual) the L2 that they are on does not know\nabout, then it seems like there is only one way to allow users to change\ntheir keys: asset / keystore separation architecture.\nEach user has (i) a \"keystore contract\" (on L1 or on\none particular L2), which stores the verification key for all\nwallets along with the rules for changing the key, and (ii)\n\"wallet contracts\" on L1 and many L2s, which read cross-chain\nto get the verification key.\n\nThere are two ways to implement this:\n\n- Light version (check only to update keys): each\nwallet stores the verification key locally, and contains a function\nwhich can be called to check a cross-chain proof of the keystore's\ncurrent state, and update its locally stored verification key to match.\nWhen a wallet is used for the first time on a particular L2, calling\nthat function to obtain the current verification key from the keystore\nis mandatory.\n\n- Upside: uses cross-chain proofs sparingly, so it's\nokay if cross-chain proofs are expensive. All funds are only spendable\nwith the current keys, so it's still secure.\n\n- Downside: To change the verification key, you have\nto make an on-chain key change in both the keystore and in every wallet\nthat is already initialized (though not counterfactual ones). This could\ncost a lot of gas.\n\n- Heavy version (check for every tx): a cross-chain\nproof showing the key currently in the keystore is necessary for each\ntransaction.\n\n- Upside: less systemic complexity, and\nkeystore updating is cheap.\n\n- Downside: expensive per-tx, so requires much more\nengineering to make cross-chain proofs acceptably cheap. Also not easily\ncompatible with ERC-4337, which currently does not support\ncross-contract reading of mutable objects during validation.\n\nWhat does a cross-chain\nproof look like?\n\nTo show the full complexity, we'll explore the most difficult case:\nwhere the keystore is on one L2, and the wallet is on a different L2. If\neither the keystore or the wallet is on L1, then only half of this\ndesign is needed.\n\nLet's assume that the keystore is on Linea, and the wallet is on Kakarot. A full proof of the keys to\nthe wallet consists of:\n\n- A proof proving the current Linea state root, given the current\nEthereum state root that Kakarot knows about\n\n- A proof proving the current keys in the keystore, given the current\nLinea state root\n\nThere are two primary tricky implementation questions here:\n\n- What kind of proof do we use? (Is it Merkle proofs?\nsomething else?)\n\n- How does the L2 learn the recent L1 (Ethereum) state\nroot (or, as we shall see, potentially the full L1 state) in\nthe first place? And alternatively, how does the L1 learn the L2 state\nroot?\n\n- In both cases, how long are the delays between\nsomething happening on one side, and that thing being provable to the\nother side?\n\nWhat kinds of proof\nschemes can we use?\n\nThere are five major options:\n\n- Merkle proofs\n\n- General-purpose ZK-SNARKs\n\n- Special-purpose proofs (eg. with KZG)\n\n- Verkle\nproofs, which are somewhere between KZG and ZK-SNARKs on\nboth infrastructure workload and cost.\n\n- No proofs and rely on direct state reading\n\nIn terms of infrastructure work required and cost for users, I rank\nthem roughly as follows:\n\n\"Aggregation\" refers to the idea of aggregating all\nthe proofs supplied by users within each block into a big meta-proof\nthat combines all of them. This is possible for SNARKs, and for KZG, but\nnot for Merkle branches (you can combine\nMerkle branches a little bit, but it only saves you\nlog(txs per block) / log(total number of keystores),\nperhaps 15-30% in practice, so it's probably not worth the cost).\n\nAggregation only becomes worth it once the scheme has a substantial\nnumber of users, so realistically it's okay for a version-1\nimplementation to leave aggregation out, and implement that for version\n2.\n\n## How would Merkle proofs work?\n\nThis one is simple: follow the diagram in the previous\nsection directly. More precisely, each \"proof\" (assuming the\nmax-difficulty case of proving one L2 into another L2) would\ncontain:\n\n- A Merkle branch proving the state-root of the\nkeystore-holding L2, given the most recent state root of\nEthereum that the L2 knows about. The keystore-holding L2's\nstate root is stored at a known storage slot of a known address (the\ncontract on L1 representing the L2), and so the path through the tree\ncould be hardcoded.\n\n- A Merkle branch proving the current verification keys, given\nthe state-root of the keystore-holding L2. Here once again, the\nverification key is stored at a known storage slot of a known address,\nso the path can be hardcoded.\n\nUnfortunately, Ethereum state proofs are complicated, but there exist\nlibraries\nfor verifying them, and if you use these libraries, this mechanism\nis not too complicated to implement.\n\nThe larger problem is cost. Merkle proofs are long, and Patricia\ntrees are unfortunately ~3.9x longer than necessary (precisely: an ideal\nMerkle proof into a tree holding N objects is\n32 * log2(N) bytes long, and because Ethereum's Patricia\ntrees have 16 leaves per child, proofs for those trees are\n32 * 15 * log16(N) ~= 125 * log2(N) bytes long). In a state\nwith roughly 250\nmillion (~2\u00b2\u2078) accounts, this makes each proof\n125 * 28 = 3500 bytes, or about 56,000 gas, plus extra\ncosts for decoding and verifying hashes.\n\nTwo proofs together would end up costing around 100,000 to 150,000\ngas (not including signature verification if this is used\nper-transaction) - significantly more than the current base 21,000 gas\nper transaction. But the disparity gets worse if the proof is\nbeing verified on L2. Computation inside an L2 is cheap,\nbecause computation is done off-chain and in an ecosystem with much\nfewer nodes than L1. Data, on the other hand, has to be posted to L1.\nHence, the comparison is not 21000 gas vs 150,000 gas; it's 21,000 L2\ngas vs 100,000 L1 gas.\n\nWe can calculate what this means by looking at comparisons between L1\ngas costs and L2 gas costs:\n\nL1 is currently about 15-25x more expensive than L2 for simple sends,\nand 20-50x more expensive for token swaps. Simple sends are relatively\ndata-heavy, but swaps are much more computationally heavy. Hence, swaps\nare a better benchmark to approximate cost of L1 computation vs L2\ncomputation. Taking all this into account, if we assume a 30x cost ratio\nbetween L1 computation cost and L2 computation cost, this seems to imply\nthat putting a Merkle proof on L2 will cost the equivalent of perhaps\nfifty regular transactions.\n\nOf course, using a binary Merkle tree can cut costs by ~4x, but even\nstill, the cost is in most cases going to be too high - and if we're\nwilling to make the sacrifice of no longer being compatible with\nEthereum's current hexary state tree, we might as well seek even better\noptions.\nHow would ZK-SNARK proofs\nwork?\n\nConceptually, the use of ZK-SNARKs is also easy to understand: you\nsimply replace the Merkle proofs in the diagram\nabove with a ZK-SNARK proving that those Merkle proofs exist. A\nZK-SNARK costs ~400,000 gas of computation, and about 400 bytes\n(compare: 21,000 gas and 100 bytes for a basic transaction, in the\nfuture reducible\nto ~25 bytes with compression). Hence, from a computational\nperspective, a ZK-SNARK costs 19x the cost of a basic transaction today,\nand from a data perspective, a ZK-SNARK costs 4x as much as a\nbasic transaction today, and 16x what a basic transaction may cost in\nthe future.\n\nThese numbers are a massive improvement over Merkle proofs, but they\nare still quite expensive. There are two ways to improve on this: (i)\nspecial-purpose KZG proofs, or (ii) aggregation, similar to ERC-4337\naggregation but using more fancy math. We can look into both.\nHow would\nspecial-purpose KZG proofs work?\n\nWarning, this section is much more mathy than other sections.\nThis is because we're going beyond general-purpose tools and building\nsomething special-purpose to be cheaper, so we have to go \"under the\nhood\" a lot more. If you don't like deep math, skip straight to the next section.\n\nFirst, a recap of how KZG commitments work:\n\n- We can represent a set of data [D_1 ... D_n] with a KZG\nproof of a polynomial derived from the data: specifically, the\npolynomial P where P(w) = D_1,\nP(w\u00b2) = D_2 ... P(w\u207f) = D_n. w here\nis a \"root of unity\", a value where w\u1d3a = 1 for some\nevaluation domain size N (this is all done in a finite\nfield).\n\n- To \"commit\" to P, we create an elliptic curve point\ncom(P) = P\u2080 * G + P\u2081 * S\u2081 + ... + P\u2096 * S\u2096. Here:\n\n- G is the generator point of the curve\n\n- P\u1d62 is the i'th-degree coefficient of the polynomial\nP\n\n- S\u1d62 is the i'th point in the trusted setup\n\n- To prove P(z) = a, we create a quotient\npolynomial Q = (P - a) / (X - z), and create a\ncommitment com(Q) to it. It is only possible to create such\na polynomial if P(z) actually equals a.\n\n- To verify a proof, we check the equation\nQ * (X - z) = P - a by doing an elliptic curve check on the\nproof com(Q) and the polynomial commitment\ncom(P): we check\ne(com(Q), com(X - z)) ?= e(com(P) - com(a), com(1))\n\nSome key properties that are important to understand are:\n\n- A proof is just the com(Q) value, which is 48\nbytes\n\n- com(P\u2081) + com(P\u2082) = com(P\u2081 + P\u2082)\n\n- This also means that you can \"edit\" a value into an existing a\ncommitment. Suppose that we know that D_i is currently\na, we want to set it to b, and the existing\ncommitment to D is com(P). A commitment to \"P,\nbut with P(w\u2071) = b, and no other evaluations changed\", then\nwe set com(new_P) = com(P) + (b-a) * com(L\u1d62), where\nL\u1d62 is a the \"Lagrange polynomial\" that equals\n1 at w\u2071 and 0 at other w\u02b2\npoints.\n\n- To perform these updates efficiently, all N commitments\nto Lagrange polynomials (com(L\u1d62)) can be pre-calculated and\nstored by each client. Inside a contract on-chain it may be too\nmuch to store all N commitments, so instead you could make\na KZG commitment to the set of com(L_i) (or\nhash(com(L_i)) values, so whenever someone needs to\nupdate the tree on-chain they can simply provide the appropriate\ncom(L_i) with a proof of its correctness.\n\nHence, we have a structure where we can just keep adding values to\nthe end of an ever-growing list, though with a certain size limit\n(realistically, hundreds of millions could be viable). We then use\nthat as our data structure to manage (i) a commitment to the\nlist of keys on each L2, stored on that L2 and mirrored to L1, and (ii)\na commitment to the list of L2 key-commitments, stored on the Ethereum\nL1 and mirrored to each L2.\n\nKeeping the commitments updated could either become part of core L2\nlogic, or it could be implemented without L2 core-protocol changes\nthrough deposit and withdraw bridges.\n\nA full proof would thus require:\n\n- The latest com(key list) on the keystore-holding L2 (48\nbytes)\n\n- KZG proof of com(key list) being a value inside\ncom(mirror_list), the commitment to the list of all key\nlist comitments (48 bytes)\n\n- KZG proof of your key in com(key list) (48 bytes, plus\n4 bytes for the index)\n\nIt's actually possible to merge the two KZG proofs into one, so we\nget a total size of only 100 bytes.\n\nNote one subtlety: because the key list is a list, and not a\nkey/value map like the state is, the key list will have to assign\npositions sequentially. The key commitment contract would contain its\nown internal registry mapping each keystore to an ID, and for each key\nit would store hash(key, address of the keystore) instead\nof just key, to unambiguously communicate to other L2s\nwhich keystore a particular entry is talking about.\n\nThe upside of this technique is that it performs very well on\nL2. The data is 100 bytes, ~4x shorter than a ZK-SNARK and waaaay\nshorter than a Merkle proof. The computation cost is largely one size-2\npairing check, or about 119,000 gas. On\nL1, data is less important than computation, and so unfortunately KZG is\nsomewhat more expensive than Merkle proofs.\n\n## How would Verkle trees work?\n\nVerkle trees essentially involve stacking KZG commitments (or IPA\ncommitments, which can be more efficient and use simpler\ncryptography) on top of each other: to store 2\u2074\u2078 values, you can make a\nKZG commitment to a list of 2\u00b2\u2074 values, each of which itself is a KZG\ncommitment to 2\u00b2\u2074 values. Verkle trees are being strongly\nconsidered for the Ethereum state tree, because Verkle trees can be\nused to hold key-value maps and not just lists (basically, you can make\na size-2\u00b2\u2075\u2076 tree but start it empty, only filling in specific parts of\nthe tree once you actually need to fill them).\n\nWhat a Verkle tree looks like. In practice, you might give each\nnode a width of 256 == 2\u2078 for IPA-based trees, or 2\u00b2\u2074 for KZG-based\ntrees.\n\nProofs in Verkle trees are somewhat longer than KZG; they might be a\nfew hundred bytes long. They are also difficult to verify, especially if\nyou try to aggregate many proofs into one.\n\nRealistically, Verkle trees should be considered to be like Merkle\ntrees, but more viable without SNARKing (because of the lower data\ncosts), and cheaper with SNARKing (because of lower prover costs).\n\nThe largest advantage of Verkle trees is the possibility of\nharmonizing data structures: Verkle proofs could be used directly over\nL1 or L2 state, without overlay structures, and using the exact same\nmechanism for L1 and L2. Once quantum computers become an\nissue, or once proving Merkle branches becomes efficient enough, Verkle\ntrees could be replaced in-place with a binary hash tree with a suitable\nSNARK-friendly hash function.\n\n## Aggregation\n\nIf N users make N transactions (or more realistically, N ERC-4337\nUserOperations) that need to prove N cross-chain claims, we can save a\nlot of gas by aggregating those proofs: the builder that would\nbe combining those transactions into a block or bundle that goes into a\nblock can create a single proof that proves all of those claims\nsimultaneously.\n\nThis could mean:\n\n- A ZK-SNARK proof of N Merkle branches\n\n- A KZG\nmulti-proof\n\n- A Verkle\nmulti-proof (or a ZK-SNARK of a multi-proof)\n\nIn all three cases, the proofs would only cost a few hundred thousand\ngas each. The builder would need to make one of these on each\nL2 for the users in that L2; hence, for this to be useful to build,\nthe scheme as a whole needs to have enough usage that there are very\noften at least a few transactions within the same block on multiple\nmajor L2s.\n\nIf ZK-SNARKs are used, the main marginal cost is simply \"business\nlogic\" of passing numbers around between contracts, so perhaps a few\nthousand L2 gas per user. If KZG multi-proofs are used, the prover would\nneed to add 48 gas for each keystore-holding L2 that is used within that\nblock, so the marginal cost of the scheme per user would add another\n~800 L1 gas per L2 (not per user) on top. But these costs are much lower\nthan the costs of not aggregating, which inevitably involve over 10,000\nL1 gas and hundreds of thousands of L2 gas per user. For Verkle\ntrees, you can either use Verkle multi-proofs directly, adding around\n100-200 bytes per user, or you can make a ZK-SNARK of a Verkle\nmulti-proof, which has similar costs to ZK-SNARKs of Merkle branches but\nis significantly cheaper to prove.\n\nFrom an implementation perspective, it's probably best to have\nbundlers aggregate cross-chain proofs through the ERC-4337 account\nabstraction standard. ERC-4337 already has a mechanism for builders to\naggregate parts of UserOperations in custom ways. There is even an implementation of this for\nBLS signature aggregation, which could reduce gas costs on L2 by\n1.5x to 3x depending on what\nother forms of compression are included.\n\nDiagram from a BLS wallet implementation\npost showing the workflow of BLS aggregate signatures within an\nearlier version of ERC-4337. The workflow of aggregating cross-chain\nproofs will likely look very similar.\n\n## Direct state reading\n\nA final possibility, and one only usable for L2 reading L1 (and not\nL1 reading L2), is to modify L2s to let them make static calls\nto contracts on L1 directly.\n\nThis could be done with an opcode or a precompile, which allows calls\ninto L1 where you provide the destination address, gas and calldata, and\nit returns the output, though because these calls are static-calls they\ncannot actually change any L1 state. L2s have to be aware of L1\nalready to process deposits, so there is nothing fundamental stopping\nsuch a thing from being implemented; it is mainly a technical\nimplementation challenge (see: this\nRFP from Optimism to support static calls into L1).\n\nNotice that if the keystore is on L1, and L2s\nintegrate L1 static-call functionality, then no proofs are required at\nall! However, if L2s don't integrate L1 static-calls, or if the\nkeystore is on L2 (which it may eventually have to be, once L1 gets too\nexpensive for users to use even a little bit), then proofs will be\nrequired.\nHow does L2\nlearn the recent Ethereum state root?\n\nAll of the schemes above require the L2 to access either the recent\nL1 state root, or the entire recent L1 state. Fortunately, all\nL2s have some functionality to access the recent L1 state\nalready. This is because they need such a functionality to\nprocess messages coming in from L1 to the L2, most notably deposits.\n\nAnd indeed, if an L2 has a deposit feature, then you can use that L2\nas-is to move L1 state roots into a contract on the L2: simply have a\ncontract on L1 call the BLOCKHASH opcode, and pass it to L2\nas a deposit message. The full block header can be received, and its\nstate root extracted, on the L2 side. However, it would be much better\nfor every L2 to have an explicit way to access either the full recent L1\nstate, or recent L1 state roots, directly.\n\nThe main challenge with optimizing how L2s receive recent L1\nstate roots is simultaneously achieving safety and low\nlatency:\n\n- If L2s implement \"direct reading of L1\" functionality in a lazy way,\nonly reading finalized L1 state roots, then the delay\nwill normally be 15 minutes, but in the\nextreme case of inactivity leaks (which you have to tolerate),\nthe delay could be several weeks.\n\n- L2s absolutely can be designed to read much more recent L1 state\nroots, but because L1 can revert (even with single\nslot finality, reverts can happen during inactivity leaks),\nL2 would need to be able to revert as well. This is\ntechnically challenging from a software engineering perspective, but at\nleast Optimism already has this capability.\n\n- If you use the deposit bridge to bring L1 state\nroots into L2, then simple economic viability might require a\nlong time between deposit updates: if the full cost of a\ndeposit is 100,000 gas, and we assume ETH is at $1800, and fees are at\n200 gwei, and L1 roots are brought into L2 once per day, that\nwould be a cost of $36 per L2 per day, or $13148 per L2 per year to\nmaintain the system. With a delay of one hour, that goes up to $315,569\nper L2 per year. In the best case, a constant trickle of impatient\nwealthy users covers the updating fees and keep the system up to date\nfor everyone else. In the worst case, some altruistic actor would have\nto pay for it themselves.\n\n- \"Oracles\" (at least, the kind of tech that some defi people\ncall \"oracles\") are not an acceptable solution\nhere: wallet key management is a very security-critical\nlow-level functionality, and so it should depend on at most a few pieces\nof very simple, cryptographically trustless low-level\ninfrastructure.\n\nAdditionally, in the opposite direction (L1s reading L2):\n\n- On optimistic rollups, state roots take one week to reach\nL1 because of the fraud proof delay. On ZK rollups it takes a\nfew hours for now because of a combination of proving times and economic\nlimits, though future technology will reduce this.\n\n- Pre-confirmations (from sequencers, attesters, etc) are not\nan acceptable solution for L1 reading L2. Wallet management is\na very security-critical low-level functionality, and so the level of\nsecurity of the L2 -> L1 communication must be absolute: it should\nnot even be possible to push a false L1 state root by taking over the L2\nvalidator set. The only state roots the L1 should trust are state roots\nthat have been accepted as final by the L2's state-root-holding contract\non L1.\n\nSome of these speeds for trustless cross-chain operations are\nunacceptably slow for many defi use cases; for those cases, you do need\nfaster bridges with more imperfect security models. For the use case of\nupdating wallet keys, however, longer delays are more acceptable: you're\nnot delaying transactions by hours, you're delaying key\nchanges. You'll just have to keep the old keys around longer. If\nyou're changing keys because keys are stolen, then you do have a\nsignificant period of vulnerability, but this can be mitigated, eg. by\nwallets having a freeze function.\n\nUltimately, the best latency-minimizing solution is for L2s to\nimplement direct reading of L1 state roots in an optimal way, where each\nL2 block (or the state root computation log) contains a pointer to the\nmost recent L1 block, so if L1 reverts, L2 can revert as well. Keystore\ncontracts should be placed either on mainnet, or on L2s that are\nZK-rollups and so can quickly commit to L1.\n\nBlocks of the L2 chain can have dependencies on not just previous\nL2 blocks, but also on an L1 block. If L1 reverts past such a link, the\nL2 reverts too. It's worth noting that this is also how an earlier\n(pre-Dank) version of sharding was envisioned to work; see here\nfor code.\n\nHow\nmuch connection to Ethereum does another chain need to hold wallets\nwhose keystores are rooted on Ethereum or an L2?\n\nSurprisingly, not that much. It actually does not even need to be a\nrollup: if it's an L3, or a validium, then it's okay to hold wallets\nthere, as long as you hold keystores either on L1 or on a ZK rollup. The\nthing that you do need is for the chain to have direct access\nto Ethereum state roots, and a technical and social commitment\nto be willing to reorg if Ethereum reorgs, and hard fork if Ethereum\nhard forks.\n\nOne interesting research problem is identifying to what extent it is\npossible for a chain to have this form of connection to\nmultiple other chains (eg. Ethereum and Zcash). Doing it\nnaively is possible: your chain could agree to reorg if Ethereum\nor Zcash reorg (and hard fork if Ethereum or Zcash\nhard fork), but then your node operators and your community more\ngenerally have double the technical and political dependencies. Hence\nsuch a technique could be used to connect to a few other chains, but at\nincreasing cost. Schemes based on ZK bridges have\nattractive technical properties, but they have the key weakness that\nthey are not robust to 51% attacks or hard forks. There may be more\nclever solutions.\n\n## Preserving privacy\n\nIdeally, we also want to preserve privacy. If you have many wallets\nthat are managed by the same keystore, then we want to make sure:\n\n- It's not publicly known that those wallets are all connected to each\nother.\n\n- Social recovery guardians don't learn what the addresses are that\nthey are guarding.\n\nThis creates a few issues:\n\n- We cannot use Merkle proofs directly, because they do not preserve\nprivacy.\n\n- If we use KZG or SNARKs, then the proof needs to provide a blinded\nversion of the verification key, without revealing the location of the\nverification key.\n\n- If we use aggregation, then the aggregator should not learn the\nlocation in plaintext; rather, the aggregator should receive blinded\nproofs, and have a way to aggregate those.\n\n- We can't use the \"light version\" (use cross-chain proofs only to\nupdate keys), because it creates a privacy leak: if many wallets get\nupdated at the same time due to an update procedure, the timing leaks\nthe information that those wallets are likely related. So we have to use\nthe \"heavy version\" (cross-chain proofs for each transaction).\n\nWith SNARKs, the solutions are conceptually easy: proofs are\ninformation-hiding by default, and the aggregator needs to produce a\nrecursive SNARK to prove the SNARKs.\n\nThe main challenge of this approach today is that aggregation\nrequires the aggregator to create a recursive SNARK, which is currently\nquite slow.\n\nWith KZG, we can use this\nwork on non-index-revealing KZG proofs (see also: a more formalized\nversion of that work in the Caulk paper) as a\nstarting point. Aggregation of blinded proofs, however, is an open\nproblem that requires more attention.\n\nDirectly reading L1 from inside L2, unfortunately, does not preserve\nprivacy, though implementing direct-reading functionality is still very\nuseful, both to minimize latency and because of its utility for other\napplications.\n\n## Summary\n\n- To have cross-chain social recovery wallets, the most realistic\nworkflow is a wallet that maintains a keystore in one\nlocation, and wallets in many locations, where wallet\nreads the keystore either (i) to update their local view of the\nverification key, or (ii) during the process of verifying each\ntransaction.\n\n- A key ingredient of making this possible is cross-chain\nproofs. We need to optimize these proofs hard. Either\nZK-SNARKs, waiting for Verkle proofs,\nor a custom-built KZG solution, seem like the best\noptions.\n\n- In the longer term, aggregation protocols where\nbundlers generate aggregate proofs as part of creating a bundle of all\nthe UserOperations that have been submitted by users will be necessary\nto minimize costs. This should probably be integrated into the\nERC-4337 ecosystem, though changes to ERC-4337 will\nlikely be required.\n\n- L2s should be optimized to minimize the latency of reading\nL1 state (or at least the state root) from inside the L2. L2s\ndirectly reading L1 state is ideal and can save on\nproof space.\n\n- Wallets can be not just on L2s; you can also put wallets on\nsystems with lower levels of connection to Ethereum (L3s, or\neven separate chains that only agree to include Ethereum state roots and\nreorg or hard fork when Ethereum reorgs or hardforks).\n\n- However, keystores should be either on L1 or on\nhigh-security ZK-rollup L2 . Being on L1 saves a lot of\ncomplexity, though in the long run even that may be too expensive, hence\nthe need for keystores on L2.\n\n- Preserving privacy will require additional work and\nmake some options more difficult. However, we should probably move\ntoward privacy-preserving solutions anyway, and at the least make sure\nthat anything we propose is forward-compatible with preserving\nprivacy.",
    "contentLength": 29775,
    "summary": "Blog explores technical methods for cross-chain state reading to enable wallets with keys stored on one blockchain to access assets on other chains.",
    "detailedSummary": {
      "theme": "Vitalik explores technical solutions for enabling cross-chain proofs to support wallet security and asset management across multiple Ethereum Layer 2 networks.",
      "summary": "Vitalik addresses a critical challenge in the multi-L2 ecosystem: how to manage smart contract wallets with changing keys across multiple chains without requiring excessive transactions. He proposes an asset/keystore separation architecture where a single keystore contract stores verification keys while wallet contracts on various L2s use cross-chain proofs to verify these keys. Vitalik analyzes five technical approaches for implementing these proofs - Merkle proofs, ZK-SNARKs, special-purpose KZG proofs, Verkle trees, and direct state reading - comparing their costs, complexity, and implementation requirements. He emphasizes that current Merkle proof solutions are too expensive (costing equivalent to 50 regular transactions), making ZK-SNARKs or KZG-based solutions more viable, especially when combined with aggregation protocols. The post also addresses infrastructure requirements, including how L2s can access recent L1 state roots and the importance of preserving user privacy while maintaining security.",
      "takeaways": [
        "Multi-L2 wallet management requires asset/keystore separation architecture to avoid making transactions on every chain when changing keys",
        "Current Merkle proof solutions are prohibitively expensive, costing the equivalent of ~50 regular transactions on L2",
        "ZK-SNARKs and KZG-based proofs offer more cost-effective alternatives, especially when combined with aggregation protocols",
        "L2s need optimized infrastructure to read recent L1 state roots with minimal latency while maintaining security guarantees",
        "Privacy preservation adds complexity but should be considered from the start, with SNARKs offering better privacy properties than other proof systems"
      ],
      "controversial": [
        "The recommendation to use 'heavy version' cross-chain proofs for every transaction rather than lighter key-update-only proofs increases per-transaction costs significantly",
        "The suggestion that chains could maintain dependencies on multiple blockchain ecosystems (like both Ethereum and Zcash) introduces complex technical and political dependencies"
      ]
    }
  },
  {
    "id": "general-2023-06-09-three_transitions",
    "title": "The Three Transitions",
    "date": "2023-06-09",
    "category": "general",
    "url": "https://vitalik.eth.limo/general/2023/06/09/three_transitions.html",
    "path": "general/2023/06/09/three_transitions.html",
    "content": "- \n\n- \n\n  \n    \n\n  \n  \n  Dark Mode Toggle\n  \n    \n\n  \n\n##  The Three Transitions \n\n 2023 Jun 09 \nSee all posts\n\n \n \n\n The Three Transitions \n\nSpecial thanks to Dan Finlay, Karl Floersch, David Hoffman, and\nthe Scroll and SoulWallet teams for feedback and review and\nsuggestions.\n\nAs Ethereum transitions from a young experimental technology into a\nmature tech stack that is capable of actually bringing an open, global\nand permissionless experience to average users, there are three major\ntechnical transitions that the stack needs to undergo, roughly\nsimultaneously:\n\n- The L2 scaling transition - everyone moving\nto rollups\n\n- The wallet security transition - everyone moving to\nsmart contract\nwallets\n\n- The privacy transition - making sure\nprivacy-preserving funds transfers are available, and making sure all of\nthe other gadgets that are being developed (social recovery,\nidentity, reputation) are privacy-preserving\n\nThe ecosystem transition triangle. You can only pick 3\nout of 3.\n\nWithout the first, Ethereum fails because each transaction costs\n$3.75 ($82.48 if we have another bull run), and every product aiming for\nthe mass market inevitably forgets about the chain and adopts\ncentralized workarounds for everything.\n\nWithout the second, Ethereum fails because users are uncomfortable\nstoring their funds (and non-financial assets), and everyone moves onto\ncentralized exchanges.\n\nWithout the third, Ethereum fails because having all transactions\n(and POAPs, etc) available publicly for literally anyone to see is far\ntoo high a privacy sacrifice for many users, and everyone moves onto\ncentralized solutions that at least somewhat hide your data.\n\nThese three transitions are crucial for the reasons above. But they\nare also challenging because of the intense coordination involved to\nproperly resolve them. It's not just features of the protocol that need\nto improve; in some cases, the way that we interact with Ethereum needs\nto change pretty fundamentally, requiring deep changes from applications\nand wallets.\nThe\nthree transitions will radically reshape the relationship between\nusers and addresses\n\nIn an L2 scaling world, users are going to exist on lots of L2s. Are\nyou a member of ExampleDAO, which lives on Optimism? Then you have an\naccount on Optimism! Are you holding a CDP in a stablecoin system on\nZkSync? Then you have an account on ZkSync! Did you once go try\nsome application that happened to live on Kakarot? Then you have an\naccount on Kakarot! The days of a user having only one address will be\ngone.\n\nI have ETH in four places, according to my Brave Wallet view. And\nyes, Arbitrum and Arbitrum Nova are different. Don't worry, it will get\nmore confusing over time!\n\nSmart contract wallets add more complexity, by making it much\nmore difficult to have the same address across L1 and the\nvarious L2s. Today, most users are using externally owned\naccounts, whose address is literally a hash of the public key that\nis used to verify signatures - so nothing changes between L1 and L2.\nWith smart contract wallets, however, keeping one address becomes more\ndifficult. Although a lot of work has been done to try to make\naddresses be hashes of code that can be equivalent across networks, most\nnotably CREATE2\nand the ERC-2470\nsingleton factory, it's difficult to make this work perfectly. Some\nL2s (eg. \"type 4 ZK-EVMs\")\nare not quite EVM equivalent, often using Solidity or an\nintermediate assembly instead, preventing hash equivalence. And even\nwhen you can have hash equivalence, the possibility of wallets changing\nownership through key changes creates other\nunintuitive consequences.\n\nPrivacy requires each user to have even more\naddresses, and may even change what kinds of addresses we're\ndealing with. If stealth address proposals\nbecome widely used, instead of each user having only a few addresses, or\none address per L2, users might have one address per\ntransaction. Other privacy schemes, even existing ones such as\nTornado Cash, change how assets are stored in a different way: many\nusers' funds are stored in the same smart contract (and hence at\nthe same address). To send funds to a specific user, users will need to\nrely on the privacy scheme's own internal addressing system.\n\nAs we've seen, each of the three transitions weaken the \"one\nuser ~= one address\" mental model in different ways, and some\nof these effects feed back into the complexity of executing the\ntransitions. Two particular points of complexity are:\n\n- If you want to pay someone, how will you get the information\non how to pay them?\n\n- If users have many assets stored in different places across\ndifferent chains, how do they do key changes and social\nrecovery?\n\nThe\nthree transitions and on-chain payments (and identity)\n\nI have coins on Scroll, and I want to pay for coffee (if the \"I\" is\nliterally me, the writer of this article, then \"coffee\" is of course a\nmetonymy for \"green tea\"). You are selling me the coffee, but you are\nonly set up to receive coins on Taiko. Wat do?\n\nThere are basically two solutions:\n\n- Receiving wallets (which could be merchants, but also could just be\nregular individuals) try really hard to support every L2, and\nhave some automated functionality for consolidating funds\nasynchronously.\n\n- The recipient provides their L2 alongside their address, and the\nsender's wallet automatically routes funds to the destination L2 through\nsome cross-L2 bridging system.\n\nOf course, these solutions can be combined: the recipient provides\nthe list of L2s they're willing to accept, and the sender's\nwallet figures out payment, which could involve either a direct send if\nthey're lucky, or otherwise a cross-L2 bridging path.\n\nBut this is only one example of a key challenge that the three\ntransitions introduce: simple actions like paying\nsomeone start to require a lot more information than just a 20-byte\naddress.\n\nA transition to smart contract wallets is fortunately not a large\nburden on the addressing system, but there are still some technical\nissues in other parts of the application stack that need to be worked\nthrough. Wallets will need to be updated to make sure that they do not\nsend only 21000 gas along with a transaction, and it will be even more\nimportant to ensure that the payment receiving side of a wallet\ntracks not only ETH transfers from EOAs, but also ETH sent by smart\ncontract code. Apps that rely on the assumption that address ownership\nis immutable (eg. NFTs that ban smart contracts to enforce\nroyalties) will have to find other ways of achieving their goals. Smart\ncontract wallets will also make some things easier - notably,\nif someone receives only a non-ETH ERC20 token, they will be\nable to use ERC-4337\npaymasters to pay for gas with that token.\n\nPrivacy, on the other hand, once again poses major challenges that we\nhave not really dealt with yet. The original Tornado Cash did not\nintroduce any of these issues, because it did not support internal\ntransfers: users could only deposit into the system and withdraw out of\nit. Once you can make internal transfers, however, users will\nneed to use the internal addressing scheme of the privacy system. In\npractice, a user's \"payment information\" would need to contain both (i)\nsome kind of \"spending pubkey\", a commitment to a secret that the\nrecipient could use to spend, and (ii) some way for the sender to send\nencrypted information that only the recipient can decrypt, to help the\nrecipient discover the payment.\n\nStealth address\nprotocols rely on a concept of meta-addresses,\nwhich work in this way: one part of the meta-address is a blinded\nversion of the sender's spending key, and another part is the sender's\nencryption key (though a minimal implementation could set those two keys\nto be the same).\n\nSchematic overview of an abstract stealth address scheme based on\nencryption and ZK-SNARKs.\n\nA key lesson here is that in a privacy-friendly ecosystem, a\nuser will have both spending pubkeys and encryption pubkeys, and a\nuser's \"payment information\" will have to include both keys.\nThere are also good reasons other than payments to expand in this\ndirection. For example, if we want Ethereum-based encrypted email, users\nwill need to publicly provide some kind of encryption key. In \"EOA\nworld\", we could re-use account keys for this, but in a safe\nsmart-contract-wallet world, we probably should have more explicit\nfunctionality for this. This would also help in making Ethereum-based\nidentity more compatible with non-Ethereum decentralized privacy\necosystems, most notably PGP keys.\nThe three transitions\nand key recovery\n\nThe default way to implement key changes and social recovery in a\nmany-address-per-user world is to simply have users run the recovery\nprocedure on each address separately. This can be done in one click: the\nwallet can include software to execute the recovery procedure across all\nof a user's addresses at the same time. However, even with such UX\nsimplifications, naive multi-address recovery has three issues:\n\n- Gas cost impracticality: this one is\nself-explanatory.\n\n- Counterfactual addresses: addresses for which the\nsmart contract has not yet been published (in practice, this will mean\nan account that you have not yet sent funds from). You as a user have a\npotentially unlimited number of counterfactual addresses: one or more on\nevery L2, including L2s that do not yet exist, and a whole other\ninfinite set of counterfactual addresses arising from stealth address\nschemes.\n\n- Privacy: if a user intentionally has many addresses\nto avoid linking them to each other, they certainly do not want to\npublicly link all of them by recovering them at or around the same\ntime!\n\nSolving these problems is hard. Fortunately, there is a somewhat\nelegant solution that performs reasonably well: an architecture\nthat separates verification logic and asset holdings.\n\nEach user has a keystore contract, which exists in\none location (could either be mainnet or a specific L2). Users\nthen have addresses on different L2s, where the verification logic of\neach of those addresses is a pointer to the keystore contract.\nSpending from those addresses would require a proof going into the\nkeystore contract showing the current (or, more realistically,\nvery recent) spending public key.\n\nThe proof could be implemented in a few ways:\n\n- Direct read-only L1 access inside the L2. It's\npossible to modify L2s to give them a way to directly read L1 state. If\nthe keystore contract is on L1, this would mean that contracts inside L2\ncan access the keystore \"for free\"\n\n- Merkle branches. Merkle branches can prove L1 state\nto an L2, or L2 state to an L1, or you can combine the two to prove\nparts of the state of one L2 to another L2. The main weakness of Merkle\nproofs is high gas costs due to proof length: potentially 5 kB for a\nproof, though this will reduce to < 1 kB in the future due to Verkle trees.\n\n- ZK-SNARKs. You can reduce data costs by using a\nZK-SNARK of a Merkle branch instead of the branch itself. It's possible\nto build off-chain aggregation techniques (eg. on top of EIP-4337) to have one\nsingle ZK-SNARK verify all cross-chain state proofs in a block.\n\n- KZG commitments. Either L2s, or schemes built on\ntop of them, could introduce a sequential addressing system, allowing\nproofs of state inside this system to be a mere 48 bytes long. Like with\nZK-SNARKs, a multiproof\nscheme could merge all of these proofs into a single proof per\nblock.\n\nIf we want to avoid making one proof per transaction, we can\nimplement a lighter scheme that only requires a cross-L2 proof for\nrecovery. Spending from an account would depend on a spending\nkey whose corresponding pubkey is stored within that account, but\nrecovery would require a transaction that copies over the\ncurrent spending_pubkey in the keystore. Funds in\ncounterfactual addresses are safe even if your old keys are not:\n\"activating\" a counterfactual address to turn it into a working contract\nwould require making a cross-L2 proof to copy over the current\nspending_pubkey. This\nthread on the Safe forums describes how a similar architecture might\nwork.\n\nTo add privacy to such a scheme, then we just\nencrypt the pointer, and we do all of our proving inside\nZK-SNARKs:\n\nWith more work (eg. using this\nwork as a starting point), we could also strip out most of the\ncomplexity of ZK-SNARKs and make a more bare-bones KZG-based scheme.\n\nThese schemes can get complex. On the plus side, there are many\npotential synergies between them. For example, the concept of \"keystore\ncontracts\" could also be a solution to the challenge of \"addresses\"\nmentioned in the previous section: if we want users to have persistent\naddresses, that do not change every time the user updates a key, we\ncould put stealth meta-addresses, encryption keys, and other information\ninto the keystore contract, and use the address of the keystore contract\nas a user's \"address\".\nLots of\nsecondary infrastructure needs to update\n\nUsing ENS is expensive. Today, in June 2023, the situation is not too\nbad: the transaction fee is significant, but it's still comparable to\nthe ENS domain fee. Registering\nzuzalu.eth cost me roughly $27, of which $11 was transaction fees.\nBut if we have another bull market, fees will skyrocket. Even without\nETH price increases, gas fees returning to 200 gwei would raise the tx\nfee of a domain registration to $104. And so if we want people to\nactually use ENS, especially for use cases like decentralized social\nmedia where users demand nearly-free registration (and the ENS domain\nfee is not an issue because these platforms offer their users\nsub-domains), we need ENS to work on L2.\n\nFortunately, the ENS team has stepped up, and ENS on L2 is actually\nhappening! ERC-3668 (aka \"the\nCCIP standard\"), together with ENSIP-10,\nprovide a way to have ENS subdomains on any L2 automatically be\nverifiable. The CCIP standard requires setting up a smart contract that\ndescribes a method for verifying proofs of data on L2, and a\ndomain (eg. Optinames uses\necc.eth) can be put under the control of such a contract.\nOnce the CCIP contract controls ecc.eth on L1, accessing\nsome subdomain.ecc.eth will automatically involve finding\nand verifying a proof (eg. Merkle branch) of the state in L2 that\nactually stores that particular subdomain.\n\nActually fetching the proofs involves going to a list of URLs stored\nin the contract, which admittedly feels like centralization,\nthough I would argue it really isn't: it's a 1-of-N trust model (invalid\nproofs get caught by the verification logic in the CCIP contract's\ncallback function, and as long as even one of the URLs returns\na valid proof, you're good). The list of URLs could contain dozens of\nthem.\n\nThe ENS CCIP effort is a success story, and it should be\nviewed as a sign that radical reforms of the kind that we need are\nactually possible. But there's a lot more application-layer\nreform that will need to be done. A few examples:\n\n- Lots of dapps depend on users providing off-chain\nsignatures. With externally-owned accounts (EOAs),\nthis is easy. ERC-1271 provides a\nstandardized way to do this for smart contract wallets. However, lots of\ndapps still don't support ERC-1271; they will need to.\n\n- Dapps that use \"is this an EOA?\" to discriminate between\nusers and contracts (eg. to prevent transfer or enforce royalties) will\nbreak. In general, I advise against attempting to find a purely\ntechnical solution here; figuring out whether or not a particular\ntransfer of cryptographic control is a transfer of beneficial ownership\nis a difficult problem and probably not solvable without resolving to\nsome off-chain\ncommunity-driven mechanisms. Most likely, applications will have to\nrely less on preventing transfers and more on techniques like Harberger\ntaxes.\n\n- How wallets interact with spending and encryption keys will\nhave to be improved. Currently, wallets often use deterministic\nsignatures to generate application-specific keys: signing a standard\nnonce (eg. the hash of the application's name) with an EOA's private key\ngenerates a deterministic value that cannot be generated without the\nprivate key, and so it's secure in a purely technical sense. However,\nthese techniques are \"opaque\" to the wallet, preventing the wallet from\nimplementing user-interface level security checks. In a more mature\necosystem, signing, encryption and related functionalities will have to\nbe handled by wallets more explicitly.\n\n- Light clients (eg. Helios)\nwill have to verify L2s and not just the L1. Today, light\nclients focus on checking the validity of the L1 headers (using the light\nclient sync protocol), and verifying Merkle branches of L1 state and\ntransactions rooted in the L1 header. Tomorrow, they will also\nneed to verify a proof of L2 state rooted in the state root stored in\nthe L1 (a more advanced version of this would actually look at L2\npre-confirmations).\n\nWallets will\nneed to secure both assets and data\n\nToday, wallets are in the business of securing assets.\nEverything lives on-chain, and the only thing that the wallet needs to\nprotect is the private key that is currently guarding those\nassets. If you change the key, you can safely publish your previous\nprivate key on the internet the next day. In a ZK world, however, this\nis no longer true: the wallet is not just protecting authentication\ncredentials, it's also holding your data.\n\nWe saw the first signs of such a world with Zupass, the ZK-SNARK-based identity\nsystem that was used at Zuzalu. Users had a private key that they used\nto authenticate to the system, which could be used to make basic proofs\nlike \"prove I'm a Zuzalu resident, without revealing which one\". But the\nZupass system also began to have other apps built on top, most notably\nstamps (Zupass's version of POAPs).\n\nOne of my many Zupass stamps, confirming that I am a proud member\nof Team Cat.\n\nThe key feature that stamps offer over POAPs is that stamps are\nprivate: you hold the data locally, and you only ZK-prove a stamp (or\nsome computation over the stamps) to someone if you want them to have\nthat information about you. But this creates added risk: if you lose\nthat information, you lose your stamps.\n\nOf course, the problem of holding data can be reduced to the problem\nof holding a single encryption key: some third party (or even the chain)\ncan hold an encrypted copy of the data. This has the convenient\nadvantage that actions you take don't change the encryption key, and so\ndo not require any interactions with the system holding your encryption\nkey safe. But even still, if you lose your encryption key, you\nlose everything. And on the flip side, if someone\nsees your encryption key, they see everything that was\nencrypted to that key.\n\nZupass's de-facto solution was to encourage people to store their key\non multiple devices (eg. laptop and phone), as the chance that they\nwould lose access to all devices at the same time is tiny. We could go\nfurther, and use secret\nsharing to store the key, split between multiple guardians.\n\nThis kind of social recovery via MPC is not a sufficient solution for\nwallets, because it means that not only current guardians but\nalso previous guardians could collude to steal your assets, which is an\nunacceptably high risk. But privacy leaks are generally a lower risk\nthan total asset loss, and someone with a high-privacy-demanding use\ncase could always accept a higher risk of loss by not backing up the key\nassociated with those privacy-demanding actions.\n\nTo avoid overwheming the user with a byzantine system of multiple\nrecovery paths, wallets that support social recovery will likely need to\nmanage both recovery of assets and recovery of encryption\nkeys.\n\n## Back to identity\n\nOne of the common threads of these changes is that the concept of an\n\"address\", a cryptographic identifier that you use to represent \"you\"\non-chain, will have to radically change. \"Instructions for how\nto interact with me\" would no longer just be an ETH address; they would\nhave to be, in some form, some combination of multiple addresses on\nmultiple L2s, stealth meta-addresses, encryption keys, and other\ndata.\n\nOne way to do this is to make ENS your identity: your ENS record\ncould just contain all of this information, and if you send someone\nbob.eth (or bob.ecc.eth, or...), they could look\nup and see everything about how to pay and interact with you, including\nin the more complicated cross-domain and privacy-preserving ways.\n\nBut this ENS-centric approach has two weaknesses:\n\n- It ties too many things to your name. Your\nname is not you, your name is one of many attributes of you. It should\nbe possible to change your name without moving over your entire identity\nprofile and updating a whole bunch of records across many\napplications.\n\n- You can't have trustless counterfactual names. One\nkey UX feature of any blockchain is the ability to send coins to people\nwho have not interacted with the chain yet. Without such a\nfunctionality, there is a catch-22: interacting with the chain requires\npaying transaction fees, which requires... already having coins. ETH\naddresses, including smart contract addresses with CREATE2, have this\nfeature. ENS names don't, because if two Bobs both decide off-chain that\nthey are bob.ecc.eth, there's no way to choose which one of\nthem gets the name.\n\nOne possible solution is to put more things into the keystore\ncontract mentioned in the architecture earlier in this post.\nThe keystore contract could contain all of the various information about\nyou and how to interact with you (and with CCIP, some of that info could\nbe off-chain), and users would use their keystore contract as their\nprimary identifier. But the actual assets that they receive\nwould be stored in all kinds of different places. Keystore contracts are\nnot tied to a name, and they are counterfactual-friendly: you can\ngenerate an address that can provably only be initialized by a keystore\ncontract that has certain fixed initial parameters.\n\nAnother category of solutions has to do with abandoning the concept\nof user-facing addresses altogether, in a similar spirit to the\nBitcoin payment protocol. One idea is to rely more heavily on direct\ncommunication channels between the sender and the recipient; for\nexample, the sender could send a claim link (either as an explicit URL\nor a QR code) which the recipient could use to accept the payment\nhowever they wish.\n\nRegardless of whether the sender or the recipient acts first, greater\nreliance on wallets directly generating up-to-date payment information\nin real time could reduce friction. That said, persistent identifiers\nare convenient (especially with ENS), and the assumption of direct\ncommunication between sender and recipient is a really tricky one in\npractice, and so we may end up seeing a combination of different\ntechniques.\n\nIn all of these designs, keeping things both decentralized and\nunderstandable to users is paramount. We need to make sure that users\nhave easy access to an up-to-date view of what their current assets are\nand what messages have been published that are intended for them. These\nviews should depend on open tools, not proprietary solutions. It will\ntake hard work to avoid the greater complexity of payment infrastructure\nfrom turning into an opaque \"tower of abstraction\" where developers have\na hard time making sense of what's going on and adapting it to new\ncontexts. Despite the challenges, achieving scalability, wallet\nsecurity, and privacy for regular users is crucial for Ethereum's\nfuture. It is not just about technical feasibility but about actual\naccessibility for regular users. We need to rise to meet this\nchallenge.",
    "contentLength": 23580,
    "summary": "Ethereum needs three simultaneous transitions: L2 scaling (rollups), smart contract wallets, and privacy features to avoid failure.",
    "detailedSummary": {
      "theme": "Vitalik outlines three critical technical transitions Ethereum must undergo simultaneously to become accessible to mainstream users: L2 scaling, smart contract wallets, and privacy preservation.",
      "summary": "Vitalik argues that Ethereum's maturation depends on three interconnected transitions that must happen together. First, the L2 scaling transition requires moving everyone to rollups to avoid prohibitive transaction costs that drive users to centralized alternatives. Second, the wallet security transition involves adopting smart contract wallets to replace externally owned accounts, giving users better security and recovery options. Third, the privacy transition ensures that transaction privacy and identity systems protect user data without forcing them toward centralized solutions. These transitions fundamentally reshape the relationship between users and addresses, moving from a simple 'one user, one address' model to a complex multi-chain, multi-address ecosystem. Vitalik details the technical challenges each transition creates, particularly around cross-chain payments, key recovery, and identity management, while proposing solutions like keystore contracts and advanced cryptographic proofs. The post emphasizes that without all three transitions succeeding simultaneously, Ethereum risks failure as users migrate to centralized platforms that offer better user experience at the cost of decentralization.",
      "takeaways": [
        "Ethereum must simultaneously achieve L2 scaling, smart contract wallet adoption, and privacy preservation to avoid user migration to centralized alternatives",
        "The traditional 'one user, one address' model will be replaced by users having multiple addresses across different L2s and privacy systems",
        "Cross-chain payments will require new infrastructure where recipients provide multiple L2 addresses and senders' wallets automatically route transactions through bridging systems",
        "Key recovery in a multi-address world needs innovative solutions like keystore contracts that separate verification logic from asset holdings",
        "Significant infrastructure updates are needed across the ecosystem, including ENS L2 support, dapp compatibility with smart contract wallets, and wallet management of both assets and encrypted data"
      ],
      "controversial": [
        "Vitalik suggests that applications trying to prevent transfers through technical means (like blocking smart contracts for royalty enforcement) should abandon purely technical solutions in favor of mechanisms like Harberger taxes",
        "The proposal that privacy leaks are generally lower risk than asset loss, suggesting different security models for encryption keys versus spending keys",
        "The argument that ENS-centric identity approaches are flawed because they tie too many functions to a changeable name rather than a persistent cryptographic identity"
      ]
    }
  },
  {
    "id": "general-2023-05-21-dont_overload",
    "title": "Don't overload Ethereum's consensus",
    "date": "2023-05-21",
    "category": "protocol",
    "url": "https://vitalik.eth.limo/general/2023/05/21/dont_overload.html",
    "path": "general/2023/05/21/dont_overload.html",
    "content": "- \n\n- \n\n  \n    \n\n  \n  \n  Dark Mode Toggle\n  \n    \n\n  \n\n##  Don't overload Ethereum's consensus \n\n 2023 May 21 \nSee all posts\n\n \n \n\n Don't overload Ethereum's consensus \n\nSpecial thanks to Karl Floersch and Justin Drake for feedback and\nreview\n\nThe Ethereum network's consensus is one of the most highly secured\ncryptoeconomic systems out there. 18\nmillion ETH (~$34 billion) worth of validators finalize a block\nevery 6.4 minutes, running many\ndifferent implementations of the protocol for redundancy. And if the\ncryptoeconomic consensus fails, whether due to a bug or an\nintentional 51% attack, a vast community of many thousands of developers\nand many more users are watching carefully to make sure the chain\nrecovers correctly. Once the chain recovers, protocol rules ensure that\nattackers will likely be heavily penalized.\n\nOver the years there have been a number of ideas, usually at the\nthought experiment stage, to also use the Ethereum validator set, and\nperhaps even the Ethereum social consensus, for other\npurposes:\n\n- The ultimate oracle: a proposal\nwhere users can vote on what facts are true by sending ETH, with a SchellingCoin\nmechanism: everyone who sent ETH to vote for the majority answer gets a\nproportional share of all the ETH sent to vote for the minority answer.\nThe description continues: \"So in principle this is an symmetric game.\nWhat breaks the symmetry is that a) the truth is the natural point to\ncoordinate on and more importantly b) the people betting on the truth\ncan make a credible thread of forking Ethereum if they loose.\"\n\n- Re-staking: a set of techniques, used by many\nprotocols including EigenLayer, where Ethereum\nstakers can simultaneously use their stake as a deposit in another\nprotocol. In some cases, if they misbehave according to the other\nprotocol's rules, their deposit also gets slashed. In other cases, there\nare no in-protocol incentives and stake is simply used to vote.\n\n- L1-driven recovery of L2 projects: it has been\nproposed on many occasions that if an L2 has a bug, the L1 could fork to\nrecover it. One recent example is this\ndesign for using L1 soft forks to recover L2 failures.\n\nThe purpose of this post will be to explain in detail the\nargument why, in my view, a certain subset of these techniques\nbrings high systemic risks to the ecosystem and should be discouraged\nand resisted.\n\nThese proposals are generally made in a well-intentioned way, and so\nthe goal is not to focus on individuals or projects; rather, the goal is\nto focus on techniques. The general rule of thumb that this post will\nattempt to defend is as follows: dual-use of validator staked\nETH, while it has some risks, is fundamentally fine, but attempting to\n\"recruit\" Ethereum social consensus for your application's own purposes\nis not.\nExamples\nof the distinction between re-using validators (low-risk) and\noverloading social consensus (high-risk)\n\n- Alice creates a web3 social network where if you cryptographically\nprove that you control the key of an active Ethereum validator, you\nautomatically gain \"verified\" status.\nLow-risk.\n\n- Bob cryptographically proves that he controls the key of ten active\nEthereum validators as a way of proving that he has enough wealth to\nsatisfy some legal requirement.\nLow-risk.\n\n- Charlie claims to have disproven the twin primes\nconjecture, and claims to know the largest p such that\np and p+2 are both prime. He changes his\nstaking withdrawal address to a smart contract where anyone can submit a\nclaimed counterexample q > p, along with a SNARK proving\nthat q and q+2 are both prime. If someone\nmakes a valid claim, then Charlie's validator is forcibly exited, and\nthe submitter gets whatever of Charlie's ETH is left.\nLow-risk.\n\n- Dogecoin decides to switch to proof of stake, and to increase the\nsize of its security pool it allows Ethereum stakers to \"dual-stake\" and\nsimultaneously join its validator set. To do so, Ethereum stakers would\nhave to change their staking withdrawal address to a smart contract\nwhere anyone can submit a proof that they violated the Dogecoin\nstaking rules. If someone does submit such a proof, then the\nstaker's validator is forcibly exited, and whatever of their ETH is left\nis used to buy-and-burn DOGE.\nLow-risk.\n\n- eCash does the same as Dogecoin, but\nthe project leaders further announce: if the majority of participating\nETH validators collude to censor eCash transactions, they\nexpect that the Ethereum community will hard-fork to delete those\nvalidators. They argue that it will be in Ethereum's interest to do so\nas those validators are proven to be malicious and unreliable.\nHigh-risk.\n\n- Fred creates an ETH/USD price oracle, which functions by allowing\nEthereum validators to participate and vote. There are no incentives.\nLow-risk.\n\n- George creates an ETH/USD price oracle, which functions by allowing\nETH holders to participate and vote. To protect against laziness and\ncreeping bribes, they add an incentive mechanism where the participants\nthat give an answer within 1% of the median answer get 1% of the ETH of\nany participants that gave an answer further than 1% from the median.\nWhen asked \"what if someone credibly\noffers to bribe all the participants, everyone starts submitting the\nwrong answer, and so honest people get 10 million of their ETH taken\naway?\", George replies: then Ethereum will have to fork out the bad\nparticipants' money. High-risk.\n\n- George conspicuously stays away from making replies.\nMedium-high risk (as the project could\ncreate incentives to attempt such a fork, and hence the expectation that\nit will be attmpted, even without formal encouragement)\n\n- George replies: \"then the attacker wins, and we'll give up on using\nthis oracle\". Medium-low risk (not quite\n\"low\" only because the mechanism does create a large set of actors who\nin a 51% attack might be incentivized to indepently advocate for a fork\nto protect their deposits)\n\n- Hermione creates a successful layer 2, and argues that because her\nlayer 2 is the largest, it is inherently the most secure, because if\nthere is a bug that causes funds to be stolen, the losses will be so\nlarge that the community will have no choice but to fork to recover the\nusers' funds. High-risk.\n\nIf you're designing a protocol where, even if everything completely\nbreaks, the losses are kept contained to the validators and users who\nopted in to participating in and using your protocol, this is low-risk.\nIf, on the other hand, you have the intent to rope in the broader\nEthereum ecosystem social consensus to fork or reorg to solve your\nproblems, this is high-risk, and I argue that we should strongly resist\nall attempts to create such expectations.\n\nA middle ground is situations that start off in the low-risk category\nbut give their participants incentives to slide into the higher-risk\ncategory; SchellingCoin-style\ntechniques, especially mechanisms with heavy penalties for deviating\nfrom the majority, are a major example.\nSo\nwhat's so wrong with stretching Ethereum consensus, anyway?\n\nIt is the year 2025. Frustrated with the existing options, a group\nhas decided to make a new ETH/USD price oracle, which works by allowing\nvalidators to vote on the price every hour. If a validator votes, they\nwould be unconditionally rewarded with a portion of fees from the\nsystem. But soon participants became lazy: they connected to centralized\nAPIs, and when those APIs got cyber-attacked, they either dropped out or\nstarted reporting false values. To solve this, incentives were\nintroduced: the oracle also votes retrospectively on the price one week\nago, and if your (real time or retrospective) vote is more than\n1% away from the median retrospective vote, you are heavily penalized,\nwith the penalty going to those who voted \"correctly\".\n\nWithin a year, over 90% of validators are participating. Someone\nasked: what if Lido bands together with a few other large stakers to 51%\nattack the vote, forcing through a fake ETH/USD price value, extracting\nheavy penalties from everyone who does not participate in the attack?\nThe oracle's proponents, at this point heavily invested in the scheme,\nreply: well if that happens, Ethereum will surely fork to kick the bad\nguys out.\n\nAt first, the scheme is limited to ETH/USD, and it appears resilient\nand stable. But over the years, other indices get added: ETH/EUR,\nETH/CNY, and eventually rates for all countries in the G20.\n\nBut in 2034, things start to go wrong. Brazil has an unexpectedly\nsevere political crisis, leading to a disputed election. One political\nparty ends up in control of the capital and 75% of the country, but\nanother party ends up in control of some northern areas. Major Western\nmedia argue that the northern party is clearly the legitimate winner\nbecause it acted legally and the southern party acted illegally (and by\nthe way are fascist). Indian and Chinese official sources, and Elon\nMusk, argue that the southern party has actual control of most of the\ncountry, and the international community should not try to be a world\npolice and should instead accept the outcome.\n\nBy this point, Brazil has a CBDC, which splits into two forks: the\n(northern) BRL-N, and the (southern) BRL-S. When voting in the oracle,\n60% of Ethereum stakers provide the ETH/BRL-S rate. Major community\nleaders and businesses decry the stakers' craven capitulation to\nfascism, and propose to fork the chain to only include the \"good\nstakers\" providing the ETH/BRL-N rate, and drain the other stakers'\nbalances to near-zero. Within their social media bubble, they believe\nthat they will clearly win. However, once the fork hits, the BRL-S side\nproves unexpectedly strong. What they expected to be a landslide instead\nproves to be pretty much a 50-50 community split.\n\nAt this point, the two sides are in their two separate universes with\ntheir two chains, with no practical way of coming back together.\nEthereum, a global permissionless platform created in part to be a\nrefuge from nations and geopolitics, instead ends up cleaved in half by\nany one of the twenty G20 member states having an unexpectedly severe\ninternal issue.\nThat's\na nice scifi story you got there. Could even make a good movie. But what\ncan we actually learn from it?\n\nA blockchain's \"purity\", in the sense of it being a purely\nmathematical construct that attempts to come to consensus only on purely\nmathematical things, is a huge advantage. As soon as a blockchain tries\nto \"hook in\" to the outside world, the outside world's conflicts start\nto impact on the blockchain too. Given a sufficiently extreme political\nevent - in fact, not that extreme a political event, given that\nthe above story was basically a pastiche of events that have actually\nhappened in various major (>25m population) countries all within the\npast decade - even something as benign as a currency oracle could tear\nthe community apart.\n\nHere are a few more possible scenarios:\n\n- One of the currencies that the oracle tracks (which could even be\nUSD) simply hyperinflates, and markets break down to the point that at\nsome points in time there is no clear specific market price.\n\n- If Ethereum adds a price oracle to another cryptocurrency,\nthen a controversial split like in the story above is not hypothetical:\nit's something that has already happened, including in the histories of\nboth Bitcoin\nand Ethereum\nitself.\n\n- If strict capital controls become operational, then which\nprice to report as the legitimate market price between two currencies\nbecomes a political question.\n\nBut more importantly, I'd argue that there is a Schelling fence at\nplay: once a blockchain starts incorporating real-world price\nindices as a layer-1 protocol feature, it could easily succumb to\ninterpreting more and more real-world information. Introducing layer-1\nprice indices also expands the blockchain's legal attack surface:\ninstead of being just a neutral technical platform, it becomes\nmuch more explicitly a financial tool.\nWhat\nabout risks from examples other than price indices?\n\nAny expansion of the \"duties\" of Ethereum's consensus\nincreases the costs, complexities and risks of running a validator.\nValidators become required to take on the human effort of paying\nattention and running and updating additional software to make sure that\nthey are acting correctly according to whatever other protocols are\nbeing introduced. Other communities gain the ability to externalize\ntheir dispute resolution needs onto the Ethereum community. Validators\nand the Ethereum community as a whole become forced to make far more\ndecisions, each of which has some risk of causing a community split.\nEven if there is no split, the desire to avoid such pressure creates\nadditional incentives to externalize the decisions to centralized\nentities through stake-pooling.\n\nThe possibility of a split would also greatly strengthen perverse\ntoo-big-to-fail mechanics. There are so many layer-2 and\napplication-layer projects on Ethereum that it would be impractical for\nEthereum social consensus to be willing to fork to solve all of\ntheir problems. Hence, larger projects would inevitably get a larger\nchance of getting a bailout than smaller ones. This would in turn lead\nto larger projects getting a moat: would you rather have your coins on\nArbitrum or Optimism, where if something goes wrong Ethereum will fork\nto save the day, or on Taiko, where\nbecause it's smaller (and non-Western, hence less socially connected to\ncore dev circles), an L1-backed rescue is much less likely?\nBut\nbugs are a risk, and we need better oracles. So what should we do?\n\nThe best solutions to these problems are, in my view, case-by-case,\nbecause the various problems are inherently so different from each\nother. Some solutions include:\n\n- Price oracles: either not-quite-cryptoeconomic\ndecentralized oracles, or validator-voting-based oracles that\nexplicitly commit to their emergency recovery strategies being\nsomething other than appealing to L1 consensus for recovery (or\nsome combination of both). For example, a price oracle could count on a\ntrust assumption that voting participants get corrupted slowly, and so\nusers would have early warning of an attack and could exit any systems\nthat depend on the oracle. Such an oracle could intentionally give its\nreward only after a long delay, so that if that instance of the protocol\nfalls into disuse (eg. because the oracle fails and the community forks\ntoward another version), the participants do not get the reward.\n\n- More complex truth oracles reporting on facts more\nsubjective than price: some kind of decentralized court system built on\na not-quite-cryptoeconomic\nDAO.\n\n- Layer 2 protocols:\n\n- In the short term, rely on partial training wheels (what this post\ncalls stage\n1)\n\n- In the medium term, rely on multiple proving\nsystems. Trusted hardware (eg. SGX) could be included here; I\nstrongly anti-endorse SGX-like systems as a sole guarantor of\nsecurity, but as a member of a 2-of-3 system they could be\nvaluable.\n\n- In the longer term, hopefully complex functionalities such as \"EVM\nvalidation\" will themselves eventually be enshrined in the protocol\n\n- Cross-chain bridges: similar logic as oracles, but\nalso, try to minimize how much you rely on bridges at all: hold assets\non the chain where they originate and use atomic swap protocols to move\nvalue between different chains.\n\n- Using the Ethereum validator set to secure other\nchains: one reason why the (safer) Dogecoin approach in the list\nof examples above might be insufficient is that while it does\nprotect against 51% finality-reversion attacks, it does not\nprotect against 51% censorship attacks. However, if you are\nalready relying on Ethereum validators, then one possible direction to\ntake is to move away from trying to manage an independent chain\nentirely, and become a validium\nwith proofs anchored into Ethereum. If a chain does this, its protection\nagainst finality-reversion attacks becomes as strong as Ethereum's, and\nit becomes secure against censorship up to 99% attacks (as opposed to\n49%).\n\n## Conclusions\n\nBlockchain communities' social consensus is a fragile thing. It's\nnecessary - because upgrades happen, bugs happen, and 51% attacks are\nalways a possibility - but because it has such a high risk of causing\nchain splits, in mature communities it should be used sparingly. There\nis a natural urge to try to extend the blockchain's core with more and\nmore functionality, because the blockchain's core has the largest\neconomic weight and the largest community watching it, but each such\nextention makes the core itself more fragile.\n\nWe should be wary of application-layer projects taking actions that\nrisk increasing the \"scope\" of blockchain consensus to anything other\nthan verifying the core Ethereum protocol rules. It is natural for\napplication-layer projects to attempt such a strategy, and indeed such\nideas are often simply conceived without appreciation of the risks, but\nits result can easily become very misaligned with the goals of the\ncommunity as a whole. Such a process has no limiting principle, and\ncould easily lead to a blockchain community having more and more\n\"mandates\" over time, pushing it into an uncomfortable choice between a\nhigh yearly risk of splitting and some kind of de-facto formalized\nbureaucracy that has ultimate control of the chain.\n\nWe should instead preserve the chain's minimalism, support uses of\nre-staking that do not look like slippery slopes to extending the role\nof Ethereum consensus, and help developers find alternate strategies to\nachieve their security goals.",
    "contentLength": 17509,
    "summary": "Ethereum should avoid protocols that risk dragging the broader community into social consensus forks to solve application-specific problems.",
    "detailedSummary": {
      "theme": "Vitalik argues against overloading Ethereum's consensus mechanism with additional responsibilities beyond core protocol validation to preserve the network's integrity and avoid dangerous community splits.",
      "summary": "Vitalik distinguishes between low-risk uses of Ethereum's validator set (like dual-staking where losses remain contained to participants) and high-risk attempts to recruit Ethereum's social consensus for external applications. He argues that while re-using validators for other purposes is generally acceptable, expecting the Ethereum community to fork or reorganize the chain to solve problems in other protocols creates systemic risks. Vitalik illustrates his concerns through a detailed hypothetical scenario where an ETH/USD oracle becomes so embedded in Ethereum that a geopolitical crisis over Brazil's disputed election ultimately splits the Ethereum community and blockchain in half. He warns that expanding Ethereum's consensus duties beyond pure mathematical validation introduces real-world conflicts, increases validator complexity, strengthens too-big-to-fail dynamics, and risks fragmenting the community over external disputes.",
      "takeaways": [
        "Ethereum's consensus should remain focused on validating core protocol rules rather than expanding into external applications or oracles",
        "Re-staking and dual-use of validator ETH is acceptable when risks remain contained to voluntary participants",
        "Expecting Ethereum social consensus to bail out failed applications creates dangerous systemic risks and too-big-to-fail dynamics",
        "Blockchain 'purity' as a mathematical construct is valuable - hooking into the outside world imports external conflicts that can split communities",
        "Alternative solutions like decentralized courts, multiple proving systems, and validiums can achieve security goals without overloading Ethereum consensus"
      ],
      "controversial": [
        "Vitalik's opposition to certain EigenLayer-style restaking approaches may conflict with significant industry investment and development in this area",
        "The argument against L1 recovery of L2 failures could be seen as abandoning users of major Layer 2 protocols in crisis situations",
        "Some may argue that expanding Ethereum's role as a 'world computer' naturally includes broader consensus responsibilities that Vitalik is resisting"
      ]
    }
  },
  {
    "id": "general-2023-04-14-traveltime",
    "title": "Travel time ~= 750 * distance ^ 0.6",
    "date": "2023-04-14",
    "category": "general",
    "url": "https://vitalik.eth.limo/general/2023/04/14/traveltime.html",
    "path": "general/2023/04/14/traveltime.html",
    "content": "- \n\n- \n\n  \n    \n\n  \n  \n  Dark Mode Toggle\n  \n    \n\n  \n\n##  Travel time ~= 750 * distance ^ 0.6 \n\n 2023 Apr 14 \nSee all posts\n\n \n \n\n Travel time ~= 750 * distance ^ 0.6 \n\nAs another exercise in using ChatGPT 3.5 to do weird things and\nseeing what happens, I decided to explore an interesting question: how\ndoes the time it takes to travel from point A to point B scale with\ndistance, in the real world? That is to say, if you sample\nrandomly from positions where people are actually at (so, for example,\n56%\nof points you choose would be in cities), and you use public\ntransportation, how does travel time scale with distance?\n\nObviously, travel time would grow slower than linearly: the further\nyou have to go, the more opportunity you have to resort to forms of\ntransportation that are faster, but have some fixed overhead. Outside of\na very few lucky cases, there is no practical way to take a bus to go\nfaster if your destination is 170 meters away, but if your destination\nis 170 kilometers away, you suddenly get more options. And if\nit's 1700 kilometers away, you get airplanes.\n\nSo I asked ChatGPT for the ingredients I would need:\n\nI went with the GeoLife\ndataset. I did notice that while it claims to be about users around\nthe world, primarily it seems to focus on people in Seattle and Beijing,\nthough they do occasionally visit other cities. That said, I'm not a\nperfectionist and I was fine with it. I asked ChatGPT to write me a\nscript to interpret the dataset and extract a randomly selected\ncoordinate from each file:\n\nAmazingly, it almost succeeded on the first try. It did make\nthe mistake of assuming every item in the list would be a number\n(values = [float(x) for x in line.strip().split(',')]),\nthough perhaps to some extent that was my fault: when I said \"the first\ntwo values\" it probably interpreted that as implying that the\nentire line was made up of \"values\" (ie. numbers).\n\nI fixed the bug manually. Now, I have a way to get some randomly\nselected points where people are at, and I have an API to get the public\ntransit travel time between the points.\n\nI asked it for more coding help:\n\n- Asking how to get an API key for the Google Maps Directions API (it\ngave an answer that seems to be outdated, but that succeeded at\nimmediately pointing me to the right place)\n\n- Writing a function to compute the straight-line distance between two\nGPS coordinates (it gave the correct answer on the first try)\n\n- Given a list of (distance, time) pairs, drawing a\nscatter plot, with time and distance as axes, both axes logarithmically\nscaled (it gave the correct answer on the first try)\n\n- Doing a linear regression on the logarithms of distance and time to\ntry to fit the data to a power law (it bugged on the first try,\nsucceeded on the second)\n\nThis gave me some really nice data (this is filtered for distances\nunder 500km, as above 500km the best path almost certainly includes\nflying, and the Google Maps directions don't take into account\nflights):\n\nThe power law fit that the linear regression gave is:\ntravel_time = 965.8020738916074 * distance^0.6138556361612214\n(time in seconds, distance in km).\n\nNow, I needed travel time data for longer distances, where the\noptimal route would include flights. Here, APIs could not help me: I\nasked ChatGPT if there were APIs that could do such a thing, and it did\nnot give a satisfactory answer. I resorted to doing it manually:\n\n- I used the same script, but modified it slightly to only output\npairs of points which were more than 500km apart from each\nother.\n\n- I took the first 8 results within the United States, and the first 8\nwith at least one end outside the United States, skipping over results\nthat represented a city pair that had already been covered.\n\n- For each result I manually obtained:\n\n- to_airport: the public transit travel time from the\nstarting point to the nearest airport, using Google Maps outside China\nand Baidu Maps inside China.\n\n- from_airport: the public transit travel time to the end\npoint from the nearest airport\n\n- flight_time: the flight time from the starting point to\nthe end point. I used Google Flights) and\nalways took the top result, except in cases where the top result was\ncompletely crazy (more than 2x the length of the shortest), in which\ncase I took the shortest.\n\n- I computed the travel time as\n(to_airport) * 1.5 + (90 if international else 60) + flight_time + from_airport.\nThe first part is a fairly aggressive formula (I personally am much more\nconservative than this) for when to leave for the airport: aim to arrive\n60 min before if domestic and 90 min before if international, and\nmultiply expected travel time by 1.5x in case there are any mishaps or\ndelays.\n\nThis was boring and I was not interested in wasting my time to do\nmore than 16 of these; I presume if I was a serious researcher I would\nalready have an account set up on TaskRabbit or some similar\nservice that would make it easier to hire other people to do this for me\nand get much more data. In any case, 16 is enough; I put my resulting\ndata here.\n\nFinally, just for fun, I added some data for how long it would take\nto travel to various locations in space: the\nmoon (I added 12 hours to the time to take into account an average\nperson's travel time to the launch site), Mars, Pluto\nand Alpha\nCentauri.\nYou can find my complete code here.\n\nHere's the resulting chart:\n\ntravel_time = 733.002223593754 *\ndistance^0.591980777827876\n\nWAAAAAT?!?!! From this chart it seems like there is a surprisingly\nprecise relationship governing travel time from point A to point B that\nsomehow holds across such radically different transit media as walking,\nsubways and buses, airplanes and (!!) interplanetary and hypothetical\ninterstellar spacecraft. I swear that I am not cherrypicking; I did not\nthrow out any data that was inconvenient, everything (including the\nspace stuff) that I checked I put on the chart.\n\nChatGPT 3.5 worked impressively well this time; it certainly stumbled\nand fell much less than my previous misadventure, where I tried\nto get it to help me convert\nIPFS bafyhashes into hex. In general, ChatGPT seems uniquely good at\nteaching me about libraries and APIs I've never heard of before but that\nother people use all the time; this reduces the barrier to entry between\namateurs and professionals which seems like a very positive thing.\n\nSo there we go, there seems to be some kind of really weird fractal\nlaw of travel time. Of course, different transit technologies could\nchange this relationship: if you replace public transit with cars and\ncommercial flights with private jets, travel time becomes somewhat more\nlinear. And once we upload our minds onto computer hardware, we'll be\nable to travel to Alpha Centauri on much crazier vehicles like ultralight craft\npropelled by Earth-based lightsails) that could let us go anywhere\nat a significant fraction of the speed of light. But for now, it does\nseem like there is a strangely consistent relationship that puts time\nmuch closer to the square root of distance.",
    "contentLength": 7016,
    "summary": "Travel time scales with distance to the power of 0.6 across all transportation modes, from walking to hypothetical space travel.",
    "detailedSummary": {
      "theme": "An empirical investigation revealing that travel time scales with distance to the power of approximately 0.6, creating a surprisingly consistent relationship across different modes of transportation from walking to hypothetical space travel.",
      "summary": "Vitalik conducted an experiment using ChatGPT 3.5 to analyze how travel time scales with distance in the real world, hypothesizing that travel time would grow slower than linearly due to access to faster transportation modes over longer distances. Using the GeoLife dataset for coordinate sampling and Google Maps API for transit times, Vitalik found that for distances under 500km, the relationship follows travel_time = 965.8 * distance^0.61. For longer distances involving flights, Vitalik manually collected data for 16 routes, incorporating airport travel times and flight durations with realistic buffer periods. Remarkably, when combining all data including hypothetical space travel times to the moon, Mars, Pluto, and Alpha Centauri, the relationship remained consistent at travel_time = 733 * distance^0.59. Vitalik expresses amazement at this 'weird fractal law of travel time' that holds across radically different transit media, from public transportation to interplanetary spacecraft, noting that this represents a surprisingly precise mathematical relationship where time scales much closer to the square root of distance rather than linearly.",
      "takeaways": [
        "Travel time scales with distance to the power of approximately 0.6, meaning it grows slower than linearly as distances increase",
        "This mathematical relationship holds remarkably consistently across vastly different transportation modes, from local public transit to hypothetical space travel",
        "ChatGPT 3.5 proved highly effective for learning about APIs and libraries, reducing barriers between amateur and professional data analysis",
        "The power law relationship suggests a 'fractal law of travel time' that may reflect fundamental constraints in transportation systems",
        "Future transportation technologies like private vehicles, jets, or mind uploading could potentially alter this relationship toward more linear scaling"
      ],
      "controversial": [
        "The inclusion of hypothetical space travel data (moon, Mars, Pluto, Alpha Centauri) in a scientific analysis of real-world transportation patterns",
        "Drawing broad conclusions from a limited dataset of only 16 manually collected long-distance data points"
      ]
    }
  },
  {
    "id": "general-2023-03-31-zkmulticlient",
    "title": "How will Ethereum's multi-client philosophy interact with ZK-EVMs?",
    "date": "2023-03-31",
    "category": "protocol",
    "url": "https://vitalik.eth.limo/general/2023/03/31/zkmulticlient.html",
    "path": "general/2023/03/31/zkmulticlient.html",
    "content": "- \n\n- \n\n  \n    \n\n  \n  \n  Dark Mode Toggle\n  \n    \n\n  \n\n##  How will Ethereum's multi-client philosophy interact with ZK-EVMs? \n\n 2023 Mar 31 \nSee all posts\n\n \n \n\n How will Ethereum's multi-client philosophy interact with ZK-EVMs? \n\nSpecial thanks to Justin Drake for feedback and review\n\nOne underdiscussed, but nevertheless very important, way in which\nEthereum maintains its security and decentralization is its\nmulti-client philosophy. Ethereum intentionally has no\n\"reference client\" that everyone runs by default: instead, there is a\ncollaboratively-managed specification (these days written in the\nvery human-readable but very slow Python) and there\nare multiple teams making implementations of the spec\n(also called \"clients\"), which is what users actually\nrun.\n\nEach Ethereum node runs a consensus client and an\nexecution client. As of today, no consensus or execution client makes up\nmore than 2/3 of the network. If a client with less than 1/3 share in\nits category has a bug, the network would simply continue as normal. If\na client with between 1/3 and 2/3 share in its category (so, Prysm,\nLighthouse or Geth) has a bug, the chain would continue adding blocks,\nbut it would stop finalizing blocks, giving time for developers to\nintervene.\n\nOne underdiscussed, but nevertheless very important, major upcoming\ntransition in the way the Ethereum chain gets validated is the rise of\nZK-EVMs. SNARKs proving EVM execution\nhave been under development for years already, and the technology is\nactively being used by layer 2 protocols called ZK rollups. Some of these ZK\nrollups are active on\nmainnet today,\nwith more coming soon. But in the longer term,\nZK-EVMs are not just going to be for rollups; we want to use them to\nverify execution on layer 1 as well (see also: the\nVerge).\n\nOnce that happens, ZK-EVMs de-facto become a third type of Ethereum\nclient, just as important to the network's security as execution clients\nand consensus clients are today. And this naturally raises a question:\nhow will ZK-EVMs interact with the multi-client philosophy? One of the\nhard parts is already done: we already have multiple ZK-EVM\nimplementations that are being actively developed. But other hard parts\nremain: how would we actually make a \"multi-client\" ecosystem for\nZK-proving correctness of Ethereum blocks? This question opens up some\ninteresting technical challenges - and of course the looming question of\nwhether or not the tradeoffs are worth it.\nWhat\nwas the original motivation for Ethereum's multi-client philosophy?\n\nEthereum's multi-client philosophy is a type of decentralization, and\nlike decentralization\nin general, one can focus on either the technical benefits of\narchitectural decentralization or the social benefits of political\ndecentralization. Ultimately, the multi-client philosophy was motivated\nby both and serves both.\nArguments for\ntechnical decentralization\n\nThe main benefit of technical decentralization is simple: it reduces\nthe risk that one bug in one piece of software leads to a catastrophic\nbreakdown of the entire network. A historical situation that exemplifies\nthis risk is the 2010\nBitcoin overflow bug. At the time, the Bitcoin client code did not\ncheck that the sum of the outputs of a transaction does not overflow\n(wrap around to zero by summing to above the maximum integer of \\(2^{64} - 1\\)), and so someone made a\ntransaction that did exactly that, giving themselves billions of\nbitcoins. The bug was discovered within hours, and a fix was rushed\nthrough and quickly deployed across the network, but had there been a\nmature ecosystem at the time, those coins would have been accepted by\nexchanges, bridges and other structures, and the attackers could have\ngotten away with a lot of money. If there had been five different\nBitcoin clients, it would have been very unlikely that all of them had\nthe same bug, and so there would have been an immediate split, and the\nside of the split that was buggy would have probably lost.\n\nThere is a tradeoff in using the multi-client approach to minimize\nthe risk of catastrophic bugs: instead, you get consensus\nfailure bugs. That is, if you have two clients, there is a risk\nthat the clients have subtly different interpretations of some protocol\nrule, and while both interpretations are reasonable and do not allow\nstealing money, the disagreement would cause the chain to split in half.\nA serious split of that type happened once\nin Ethereum's history (there have been other much smaller splits\nwhere very small portions of the network running old versions of the\ncode forked off). Defenders of the single-client approach point to\nconsensus failures as a reason to not have multiple implementations: if\nthere is only one client, that one client will not disagree with itself.\nTheir model of how number of clients translates into risk might look\nsomething like this:\n\nI, of course, disagree with this analysis. The crux of my\ndisagreement is that (i) 2010-style catastrophic bugs matter too, and\n(ii) you never actually have only one client.\nThe latter point is made most obvious by the Bitcoin\nfork of 2013: a chain split occurred because of a disagreement\nbetween two different versions of the Bitcoin client, one of\nwhich turned out to have an accidental and undocumented limit on the\nnumber of objects that could be modified in a single block. Hence, one\nclient in theory is often two clients in practice, and five clients in\ntheory might be six or seven clients in practice - so we should just\ntake the plunge and go on the right side of the risk curve, and have at\nleast a few different clients.\nArguments for\npolitical decentralization\n\nMonopoly client developers are in a position with a lot of political\npower. If a client developer proposes a change, and users disagree,\ntheoretically they could refuse to download the updated\nversion, or create a fork without it, but in practice it's\noften difficult for users to do that. What if a disagreeable protocol\nchange is bundled with a necessary security update? What if the main\nteam threatens to quit if they don't get their way? Or, more tamely,\nwhat if the monopoly client team ends up being the only group with the\ngreatest protocol expertise, leaving the rest of the ecosystem in a poor\nposition to judge technical arguments that the client team puts forward,\nleaving the client team with a lot of room to push their own particular\ngoals and values, which might not match with the broader community?\n\nConcern about protocol politics, particularly in the context of the\n2013-14\nBitcoin OP_RETURN wars where some participants were openly in favor\nof discriminating against particular usages of the chain, was a\nsignificant contributing factor in Ethereum's early adoption of a\nmulti-client philosophy, which was aimed to make it harder for a small\ngroup to make those kinds of decisions. Concerns specific to the\nEthereum ecosystem - namely, avoiding concentration of power within the\nEthereum Foundation itself - provided further support for this\ndirection. In 2018, a decision was made to intentionally have the\nFoundation not make an implementation of the Ethereum PoS\nprotocol (ie. what is now called a \"consensus client\"), leaving that\ntask entirely to outside teams.\nHow will\nZK-EVMs come in on layer 1 in the future?\n\nToday, ZK-EVMs are used in rollups. This increases scaling by\nallowing expensive EVM execution to happen only a few times off-chain,\nwith everyone else simply verifying SNARKs posted on-chain that\nprove that the EVM execution was computed correctly. It also allows some\ndata (particularly signatures) to not be included on-chain, saving on\ngas costs. This gives us a lot of scalability benefits, and the\ncombination of scalable computation with ZK-EVMs and scalable data with\ndata\navailability sampling could let us scale very far.\n\nHowever, the Ethereum network today also has a different problem, one\nthat no amount of layer 2 scaling can solve by itself: the layer 1 is\ndifficult to verify, to the point where not many users run their own\nnode. Instead, most users simply trust third-party providers. Light\nclients such as Helios\nand Succinct are taking steps\ntoward solving the problem, but a light client is far from a fully\nverifying node: a light client merely verifies the signatures of a\nrandom subset of validators called the sync\ncommittee, and does not verify that the chain actually follows the\nprotocol rules. To bring us to a world where users can actually verify\nthat the chain follows the rules, we would have to do something\ndifferent.\nOption\n1: constrict layer 1, force almost all activity to move to layer 2\n\nWe could over time reduce the layer 1 gas-per-block target down from\n15 million to 1 million, enough for a block to contain a single SNARK\nand a few deposit and withdraw operations but not much else, and thereby\nforce almost all user activity to move to layer 2 protocols. Such a\ndesign could still support many rollups committing in each block: we\ncould use off-chain\naggregation protocols run by customized builders to gather together\nSNARKs from multiple layer 2 protocols and combine them into a single\nSNARK. In such a world, the only function of layer 1\nwould be to be a clearinghouse for layer 2 protocols, verifying their\nproofs and occasionally facilitating large funds transfers between\nthem.\n\nThis approach could work, but it has several important\nweaknesses:\n\n- It's de-facto backwards-incompatible, in the sense\nthat many existing L1-based applications become economically nonviable.\nUser funds up to hundreds or thousands of dollars could get stuck as\nfees become so high that they exceed the cost of emptying those\naccounts. This could be addressed by letting users sign messages to opt\nin to an in-protocol mass migration to an L2 of their choice (see some\nearly\nimplementation ideas here), but this adds complexity to the\ntransition, and making it truly cheap enough would require some\nkind of SNARK at layer 1 anyway. I'm generally a fan of breaking\nbackwards compatibility when it comes to things like the\nSELFDESTRUCT opcode, but in this case the tradeoff seems much less\nfavorable.\n\n- It might still not make verification cheap enough.\nIdeally, the Ethereum protocol should be easy to verify not just on\nlaptops but also inside phones, browser extensions, and even inside\nother chains. Syncing the chain for the first time, or after a long time\noffline, should also be easy. A laptop node could verify 1 million gas\nin ~20 ms, but even that implies 54 seconds to sync after one day\noffline (assuming single\nslot finality increases slot times to 32s), and for phones or\nbrowser extensions it would take a few hundred milliseconds per block\nand might still be a non-negligible battery drain. These numbers are\nmanageable, but they are not ideal.\n\n- Even in an L2-first ecosystem, there are benefits to L1\nbeing at least somewhat affordable. Validiums\ncan benefit from a stronger security model if users can withdraw their\nfunds if they notice that new state data is no longer being made\navailable. Arbitrage becomes more efficient, especially for smaller\ntokens, if the minimum size of an economically viable cross-L2 direct\ntransfer is smaller.\n\nHence, it seems more reasonable to try to find a way to use ZK-SNARKs\nto verify the layer 1 itself.\nOption 2: SNARK-verify the\nlayer 1\n\nA type\n1 (fully Ethereum-equivalent) ZK-EVM can be used to verify the EVM\nexecution of a (layer 1) Ethereum block. We could write more SNARK code\nto also verify the consensus side of a block. This would be a\nchallenging engineering problem: today, ZK-EVMs take minutes to hours to\nverify Ethereum blocks, and generating proofs in real time would require\none or more of (i) improvements to Ethereum itself to remove\nSNARK-unfriendly components, (ii) either large efficiency gains with\nspecialized hardware, and (iii) architectural improvements with much\nmore parallelization. However, there is no fundamental technological\nreason why it cannot be done - and so I expect that, even if it takes\nmany years, it will be done.\n\nHere is where we see the intersection with the multi-client\nparadigm: if we use ZK-EVMs to verify layer 1, which ZK-EVM do we\nuse?\n\nI see three options:\n\n- Single ZK-EVM: abandon the multi-client paradigm,\nand choose a single ZK-EVM that we use to verify blocks.\n\n- Closed multi ZK-EVM: agree on and enshrine in\nconsensus a specific set of multiple ZK-EVMs, and have a consensus-layer\nprotocol rule that a block needs proofs from more than half of the\nZK-EVMs in that set to be considered valid.\n\n- Open multi ZK-EVM: different clients have different\nZK-EVM implementations, and each client waits for a proof that is\ncompatible with its own implementation before accepting a block as\nvalid.\n\nTo me, (3) seems ideal, at least until and unless our technology\nimproves to the point where we can formally\nprove that all of the ZK-EVM implementations are equivalent to each\nother, at which point we can just pick whichever one is most efficient.\n(1) would sacrifice the benefits of the multi-client paradigm, and (2)\nwould close off the possibility of developing new clients and lead to a\nmore centralized ecosystem. (3) has challenges, but those challenges\nseem smaller than the challenges of the other two options, at least for\nnow.\n\nImplementing (3) would not be too hard: one might have a p2p\nsub-network for each type of proof, and a client that uses one type of\nproof would listen on the corresponding sub-network and wait until they\nreceive a proof that their verifier recognizes as valid.\n\nThe two main challenges of (3) are likely the following:\n\n- The latency challenge: a malicious attacker could\npublish a block late, along with a proof valid for one client. It would\nrealistically take a long time (even if eg. 15 seconds) to generate\nproofs valid for other clients. This time would be long enough to\npotentially create a temporary fork and disrupt the chain for a few\nslots.\n\n- Data inefficiency: one benefit of ZK-SNARKs is that\ndata that is only relevant to verification (sometimes called\n\"witness data\") could be removed from a block. For example, once you've\nverified a signature, you don't need to keep the signature in a block,\nyou could just store a single bit saying that the signature is valid,\nalong with a single proof in the block confirming that all of the valid\nsignatures exist. However, if we want it to be possible to generate\nproofs of multiple types for a block, the original signatures would need\nto actually be published.\n\nThe latency challenge could be addressed by being careful when\ndesigning the single-slot finality protocol. Single-slot finality\nprotocols will likely require more than two rounds of consensus per\nslot, and so one could require the first round to include the block, and\nonly require nodes to verify proofs before signing in the third (or\nfinal) round. This ensures that a significant time window is always\navailable between the deadline for publishing a block and the time when\nit's expected for proofs to be available.\n\nThe data efficiency issue would have to be addressed by having a\nseparate protocol for aggregating verification-related data. For\nsignatures, we could use BLS\naggregation, which ERC-4337\nalready supports. Another major category of verification-related\ndata is ZK-SNARKs used\nfor privacy. Fortunately, these often tend to have their own aggregation\nprotocols.\n\nIt is also worth mentioning that SNARK-verifying the layer 1 has an\nimportant benefit: the fact that on-chain EVM execution no\nlonger needs to be verified by every node makes it possible to greatly\nincrease the amount of EVM execution taking place. This could happen\neither by greatly increasing the layer 1 gas limit, or by introducing enshrined\nrollups, or both.\n\n## Conclusions\n\nMaking an open multi-client ZK-EVM ecosystem work well will take a\nlot of work. But the really good news is that much of this work is\nhappening or will happen anyway:\n\n- We have multiple\nstrong ZK-EVM\nimplementations already. These\nimplementations are not yet type 1 (fully\nEthereum-equivalent), but many of them are actively moving in that\ndirection.\n\n- The work on light clients such as Helios\nand Succinct may eventually turn\ninto a more full SNARK-verification of the PoS consensus side of the\nEthereum chain.\n\n- Clients will likely start experimenting with ZK-EVMs to prove\nEthereum block execution on their own, especially once we have stateless\nclients and there's no technical need to directly re-execute every\nblock to maintain the state. We will probably get a slow and gradual\ntransition from clients verifying Ethereum blocks by re-executing them\nto most clients verifying Ethereum blocks by checking SNARK proofs.\n\n- The ERC-4337 and PBS ecosystems are likely to start working with\naggregation technologies like BLS and proof aggregation pretty soon, in\norder to save on gas costs. On BLS aggregation, work\nhas already started.\n\nWith these technologies in place, the future looks very good.\nEthereum blocks would be smaller than today, anyone could run a fully\nverifying node on their laptop or even their phone or inside a browser\nextension, and this would all happen while preserving the benefits of\nEthereum's multi-client philosophy.\n\nIn the longer-term future, of course anything could happen. Perhaps\nAI will super-charge formal verification to the point where it can\neasily prove ZK-EVM implementations equivalent and identify all the bugs\nthat cause differences between them. Such a project may even be\nsomething that could be practical to start working on now. If such a\nformal verification-based approach succeeds, different mechanisms would\nneed to be put in place to ensure continued political decentralization\nof the protocol; perhaps at that point, the protocol would be considered\n\"complete\" and immutability norms would be stronger. But even if that is\nthe longer-term future, the open multi-client ZK-EVM world seems like a\nnatural stepping stone that is likely to happen anyway.\n\nIn the nearer term, this is still a long journey. ZK-EVMs\nare here, but ZK-EVMs becoming truly viable at layer 1 would\nrequire them to become type 1, and make proving fast enough that it can\nhappen in real time. With enough parallelization, this is doable, but it\nwill still be a lot of work to get there. Consensus changes like raising\nthe gas cost of KECCAK, SHA256 and other hash function precompiles will\nalso be an important part of the picture. That said, the first steps of\nthe transition may happen sooner than we expect: once we switch to Verkle trees and stateless\nclients, clients could start gradually using ZK-EVMs, and a transition\nto an \"open multi-ZK-EVM\" world could start happening all on its\nown.",
    "contentLength": 18670,
    "summary": "Ethereum's multi-client approach will face challenges integrating ZK-EVMs as a third client type for L1 verification alongside execution/consensus clients.",
    "detailedSummary": {
      "theme": "How Ethereum's multi-client philosophy can be maintained when ZK-EVMs are used to verify layer 1 execution, presenting three potential approaches and advocating for an open multi-ZK-EVM system.",
      "summary": "Vitalik explores how Ethereum's current multi-client approach, which uses multiple implementations to prevent catastrophic bugs and avoid centralized control, will interact with the emerging use of ZK-EVMs for verifying blockchain execution. He explains that Ethereum's multi-client philosophy serves both technical purposes (reducing single points of failure) and political purposes (preventing concentration of power), citing historical examples like Bitcoin's overflow bug and consensus splits. Vitalik presents three options for integrating ZK-EVMs into layer 1: using a single ZK-EVM (abandoning multi-client benefits), using a closed set of approved ZK-EVMs, or implementing an open multi-ZK-EVM system where different clients use different ZK-EVM implementations. He advocates for the third option despite its challenges around latency and data efficiency, arguing it preserves the benefits of decentralization while enabling anyone to run a fully verifying node on lightweight devices. Vitalik concludes that much of the necessary work is already happening through existing ZK-EVM development, light client projects, and aggregation technologies, making this transition a natural evolution rather than a dramatic shift.",
      "takeaways": [
        "Ethereum's multi-client philosophy prevents both catastrophic bugs and centralization of power by having multiple independent implementations of the protocol",
        "ZK-EVMs will eventually be used to verify layer 1 Ethereum execution, creating a need to decide which ZK-EVM implementation(s) to use",
        "An open multi-ZK-EVM approach is preferable to single or closed multi-ZK-EVM systems, despite challenges with latency and data efficiency",
        "The transition to ZK-EVM verification could enable lightweight devices to run fully verifying nodes while potentially allowing higher transaction throughput",
        "Much of the required infrastructure for this transition is already being developed through existing ZK-EVM projects, light clients, and aggregation technologies"
      ],
      "controversial": [
        "The proposal to potentially reduce layer 1 gas limits to force activity to layer 2, which Vitalik acknowledges would be 'de-facto backwards-incompatible'",
        "The assertion that formal verification might eventually make protocol governance obsolete through stronger immutability norms"
      ]
    }
  },
  {
    "id": "general-2023-02-28-ux",
    "title": "Some personal user experiences",
    "date": "2023-02-28",
    "category": "general",
    "url": "https://vitalik.eth.limo/general/2023/02/28/ux.html",
    "path": "general/2023/02/28/ux.html",
    "content": "- \n\n- \n\n  \n    \n\n  \n  \n  Dark Mode Toggle\n  \n    \n\n  \n\n##  Some personal user experiences \n\n 2023 Feb 28 \nSee all posts\n\n \n \n\n Some personal user experiences \n\nIn 2013, I went to a sushi restaurant beside the Internet Archive in San\nFrancisco, because I had heard that it accepted bitcoin for payments\nand I wanted to try it out. When it came time to pay the bill, I asked\nto pay in BTC. I scanned the QR code, and clicked \"send\". To my\nsurprise, the transaction did not go through; it appeared to have been\nsent, but the restaurant was not receiving it. I tried again, still no\nluck. I soon figured out that the problem was that my mobile internet\nwas not working well at the time. I had to walk over 50 meters toward\nthe Internet Archive nearby to access its wifi, which finally allowed me\nto send the transaction.\n\nLesson learned: internet is not 100% reliable, and\ncustomer internet is less reliable than merchant internet. We need\nin-person payment systems to have some functionality (NFC,\ncustomer shows a QR code, whatever) to allow customers to transfer their\ntransaction data directly to the merchant if that's the best way to get\nit broadcasted.\n\nIn 2021, I attempted to pay for tea for myself and\nmy friends at a coffee shop in Argentina. In their defense, they did not\nintentionally accept cryptocurrency: the owner simply\nrecognized me, and showed me that he had an account at a cryptocurrency\nexchange, so I suggested to pay in ETH (using cryptocurrency exchange\naccounts as wallets is a standard way to do in-person payments in Latin\nAmerica). Unfortunately, my first transaction of 0.003 ETH did not get\naccepted, probably because it was under the exchange's 0.01 ETH deposit\nminimum. I sent another 0.007 ETH. Soon, both got confirmed. (I did not\nmind the 3x overpayment and treated it as a tip).\n\nIn 2022, I attempted to pay for tea at a different\nlocation. The first transaction failed, because the default transaction\nfrom my mobile wallet sent with only 21000 gas, and the receiving\naccount was a contract that required extra gas to process the transfer.\nAttempts to send a second transaction failed, because a UI glitch in my\nphone wallet made it not possible to scroll down and edit the field that\ncontained the gas limit.\n\nLesson learned: simple-and-robust UIs are better\nthan fancy-and-sleek ones. But also, most users don't even know what gas\nlimits are, so we really just need to have better defaults.\n\nMany times, there has been a surprisingly long time\ndelay between my transaction getting accepted on-chain, and the service\nacknowledging the transaction, even as \"unconfirmed\". Some of those\ntimes, I definitely got worried that there was some glitch with the\npayment system on their side.\n\nMany times, there has been a surprisingly long and\nunpredictable time delay between sending a transaction, and that\ntransaction getting accepted in a block. Sometimes, a transaction would\nget accepted in a few seconds, but other times, it would take minutes or\neven hours. Recently, EIP-1559\nsignificantly improved this, ensuring that most transactions get\naccepted into the next block, and even more recently the Merge improved\nthings further by stabilizing block times.\n\nDiagram from this\nreport by Yinhong (William) Zhao and Kartik Nayak.\n\nHowever, outliers still remain. If you send a\ntransaction at the same time as when many others are sending\ntransactions and the base fee is spiking up, you risk the base fee going\ntoo high and your transaction not getting accepted. Even worse, wallet\nUIs suck at showing this. There are no big red flashing alerts, and very\nlittle clear indication of what you're supposed to do to solve this\nproblem. Even to an expert, who knows that in this case you're supposed\nto \"speed up\" the transaction by publishing a new transaction with\nidentical data but a higher max-basefee, it's often not clear where the\nbutton to do that actually is.\n\nLesson learned: UX around transaction inclusion\nneeds to be improved, though there are fairly simple fixes. Credit to\nthe Brave wallet team for taking\nmy suggestions on this topic seriously, and first increasing the\nmax-basefee tolerance from 12.5% to 33%, and more recently exploring\nways to make stuck transactions more obvious in the UI.\n\nIn 2019, I was testing out one of the earliest\nwallets that was attempting to provide social recovery. Unlike my\npreferred approach, which is smart-contract-based, their approach was to\nuse Shamir's\nsecret sharing to split up the private key to the account into five\npieces, in such a way that any three of those pieces could be used to\nrecover the private key. Users were expected to choose five friends\n(\"guardians\" in modern lingo), convince them to\ndownload a separate mobile application, and provide a confirmation code\nthat would be used to create an encrypted connection from the user's\nwallet to the friend's application through Firebase and send them their\nshare of the key.\n\nThis approach quickly ran into problems for me. A\nfew months later, something happened to my wallet and I needed to\nactually use the recovery procedure to recover it. I asked my friends to\nperform the recovery procedure with me through their apps - but it did\nnot go as planned. Two of them lost their key shards, because they\nswitched phones and forgot to move the recovery application over. For a\nthird, the Firebase connection mechanism did not work for a long time.\nEventually, we figured out how to fix the issue, and recover the key. A\nfew months after that, however, the wallet broke again. This time, a\nregular software update somehow accidentally reset the app's storage and\ndeleted its key. But I had not added enough recovery partners, because\nthe Firebase connection mechanism was too broken and was not letting me\nsuccessfully do that. I ended up losing a small amount of BTC and\nETH.\n\nLesson learned: secret-sharing-based off-chain\nsocial recovery is just really fragile and a bad idea unless there are\nno other options. Your recovery guardians should not have to download a\nseparate application, because if you have an application only\nfor an exceptional situation like recovery, it's too easy to forget\nabout it and lose it. Additionally, requiring separate centralized\ncommunication channels comes with all kinds of problems. Instead, the\nway to add guardians should be to provide their ETH address, and\nrecovery should be done by smart contract, using ERC-4337 account\nabstraction wallets. This way, the guardians would only need to not lose\ntheir Ethereum wallets, which is something that they already care much\nmore about not losing for other reasons.\n\nIn 2021, I was attempting to save on fees when using\nTornado Cash, by using the \"self-relay\" option. Tornado Cash uses a\n\"relay\" mechanism where a third party pushes the transaction on-chain,\nbecause when you are withdrawing you generally do not yet have coins in\nyour withdrawal address, and you don't want to pay for the transaction\nwith your deposit address because that creates a public link between the\ntwo addresses, which is the whole problem that Tornado Cash is trying to\nprevent. The problem is that the relay mechanism is often expensive,\nwith relays charging a percentage fee that could go far above the actual\ngas fee of the transaction.\n\nTo save costs, one time I used the relay for a first small withdrawal\nthat would charge lower fees, and then used the \"self-relay\" feature in\nTornado Cash to send a second larger withdrawal myself without using\nrelays. The problem is, I screwed up and accidentally did this while\nlogged in to my deposit address, so the deposit address paid the fee\ninstead of the withdrawal address. Oops, I created a public link between\nthe two.\n\nLesson learned: wallet developers should start\nthinking much more explicitly about privacy. Also, we need better forms\nof account abstraction to remove the need for centralized or even\nfederated relays, and commoditize the relaying role.\n\n## Miscellaneous stuff\n\n- Many apps still do not work with the Brave wallet or the Status\nbrowser; this is likely because they didn't do their homework properly\nand rely on Metamask-specific APIs. Even Gnosis Safe did not work with\nthese wallets for a long time, leading me to have to write my own mini\nJavascript dapp to make confirmations. Fortunately, the latest UI has\nfixed this issue.\n\n- The ERC20 transfers pages on Etherscan (eg. https://etherscan.io/address/0xd8da6bf26964af9d7eed9e03e53415d37aa96045#tokentxns)\nare very easy to spam with fakes. Anyone can create a new ERC20 token\nwith logic that can issue a log that claims that I or any other specific\nperson sent someone else tokens. This is sometimes used to trick people\ninto thinking that I support some scam token when I actually have never\neven heard of it.\n\n- Uniswap used to offer the functionality of being able to swap tokens\nand have the output sent to a different address. This was really\nconvenient for when I have to pay someone in USDC but I don't have any\nalready on me. Now, the interface doesn't offer that function, and so I\nhave to convert and then send in a separate transaction, which is less\nconvenient and wastes more gas. I have since learned that Cowswap and\nParaswap offer the functionality, though Paraswap... currently does not\nseem to work with the Brave wallet.\n\n- Sign in with Ethereum is great, but\nit's still difficult to use if you are trying to sign in on multiple\ndevices, and your Ethereum wallet is only available on one device.\n\n## Conclusions\n\nGood user experience is not about the average case, it is about the\nworst case. A UI that is clean and sleek, but does some weird and\nunexplainable thing 0.723% of the time that causes big problems, is\nworse than a UI that exposes more gritty details to the user but at\nleast makes it easier to understand what's going on and fix any problem\nthat does arise.\n\nAlong with the all-important issue of high transaction fees due to\nscaling not yet being fully solved, user experience is a key reason why\nmany Ethereum users, especially in the Global South, often opt for\ncentralized solutions instead of on-chain decentralized alternatives\nthat keep power in the hands of the user and their friends and family or\nlocal community. User experience has made great strides over the years -\nin particular, going from an average transaction taking minutes to get\nincluded before EIP-1559 to an average transaction taking seconds to get\nincluded after EIP-1559 and the merge, has been a night-and-day change\nto how pleasant it is to use Ethereum. But more still needs to be\ndone.",
    "contentLength": 10536,
    "summary": "A crypto expert shares payment failures from poor internet, gas limit bugs, and broken social recovery to argue UX should prioritize worst-case scenarios.",
    "detailedSummary": {
      "theme": "Vitalik shares real-world experiences using cryptocurrency payments and Ethereum applications to highlight critical user experience problems that need to be solved for broader adoption.",
      "summary": "Vitalik recounts various personal experiences with cryptocurrency payments and Ethereum applications from 2013 to 2022, illustrating common technical problems and UX failures. His experiences range from connectivity issues when paying with Bitcoin at a San Francisco sushi restaurant, to gas limit problems with mobile wallets, to the fragility of secret-sharing-based social recovery systems that led him to lose funds. He also discusses issues with transaction confirmation delays, wallet compatibility problems, and privacy concerns when using services like Tornado Cash. Throughout these anecdotes, Vitalik emphasizes that good user experience is determined by worst-case scenarios rather than average performance, arguing that clean interfaces that occasionally fail catastrophically are worse than more transparent UIs that help users understand and fix problems. He concludes that despite improvements like EIP-1559 and the Merge reducing transaction times, significant UX challenges remain a key barrier to Ethereum adoption, particularly in the Global South where users often choose centralized solutions over decentralized alternatives due to usability issues.",
      "takeaways": [
        "Internet connectivity issues require in-person payment systems to have direct merchant-to-customer transfer capabilities like NFC or customer-generated QR codes",
        "Simple and robust user interfaces are superior to fancy but unreliable ones, and wallet defaults need improvement since most users don't understand technical details like gas limits",
        "Secret-sharing-based off-chain social recovery is fragile and problematic - smart contract-based recovery using guardians' existing Ethereum addresses is more reliable",
        "Wallet developers need to explicitly consider privacy implications and better account abstraction is needed to eliminate reliance on centralized relays",
        "Good UX is defined by worst-case scenarios, not average performance - transparent interfaces that help users understand problems are better than sleek ones that fail unpredictably"
      ],
      "controversial": [
        "Vitalik's use of Tornado Cash for privacy purposes, which has since become a highly regulated and controversial service",
        "His criticism that many users in the Global South choose centralized solutions over decentralized ones due to UX issues, which could be seen as overlooking other factors like infrastructure limitations"
      ]
    }
  },
  {
    "id": "general-2023-01-20-stealth",
    "title": "An incomplete guide to stealth addresses",
    "date": "2023-01-20",
    "category": "general",
    "url": "https://vitalik.eth.limo/general/2023/01/20/stealth.html",
    "path": "general/2023/01/20/stealth.html",
    "content": "- \n\n- \n\n  \n    \n\n  \n  \n  Dark Mode Toggle\n  \n    \n\n  \n\n##  An incomplete guide to stealth addresses \n\n 2023 Jan 20 \nSee all posts\n\n \n \n\n An incomplete guide to stealth addresses \n\nSpecial thanks to Ben DiFrancesco, Matt Solomon, Toni Wahrst\u00e4tter\nand Antonio Sanso for feedback and review\n\nOne of the largest remaining challenges in the Ethereum ecosystem is\nprivacy. By default, anything that goes onto a public blockchain is\npublic. Increasingly, this means not just money and financial\ntransactions, but also ENS names, POAPs, NFTs, soulbound tokens, and much\nmore. In practice, using the entire suite of Ethereum applications\ninvolves making a significant portion of your life public for anyone to\nsee and analyze.\n\nImproving this state of affairs is an important problem, and this is\nwidely recognized. So far, however, discussions on improving privacy\nhave largely centered around one specific use case: privacy-preserving\ntransfers (and usually self-transfers) of ETH and mainstream ERC20\ntokens. This post will describe the mechanics and use cases of a\ndifferent category of tool that could improve the state of privacy on\nEthereum in a number of other contexts: stealth\naddresses.\nWhat is a stealth address\nsystem?\n\nSuppose that Alice wants to send Bob an asset. This could be some\nquantity of cryptocurrency (eg. 1 ETH, 500 RAI), or it could be an NFT.\nWhen Bob receives the asset, he does not want the entire world to know\nthat it was he who got it. Hiding the fact that a transfer happened is\nimpossible, especially if it's an NFT of which there is only one copy\non-chain, but hiding who is the recipient may be much more\nviable. Alice and Bob are also lazy: they want a system where the\npayment workflow is exactly the same as it is today. Bob sends Alice (or\nregisters on ENS) some kind of \"address\" encoding how someone can pay\nhim, and that information alone is enough for Alice (or anyone else) to\nsend him the asset.\n\nNote that this is a different kind of privacy than what is provided\nby eg. Tornado Cash. Tornado Cash can hide transfers of mainstream\nfungible assets such as ETH or major ERC20s (though it's most easily\nuseful for privately sending to yourself), but it's very weak\nat adding privacy to transfers of obscure ERC20s, and it cannot add\nprivacy to NFT transfers at all.\n\nThe ordinary workflow of making a payment with\ncryptocurrency. We want to add privacy (no one can tell that it was Bob\nwho received the asset), but keep the workflow the same.\n\nStealth addresses provide such a scheme. A stealth address is an\naddress that can be generated by either Alice or Bob, but which can only\nbe controlled by Bob. Bob generates and keeps secret a spending\nkey, and uses this key to generate a stealth\nmeta-address. He passes this meta-address to Alice (or\nregisters it on ENS). Alice can perform a computation on this\nmeta-address to generate a stealth address belonging to\nBob. She can then send any assets she wants to send to this address, and\nBob will have full control over them. Along with the transfer, she\npublishes some extra cryptographic data (an ephemeral\npubkey) on-chain that helps Bob discover that this address\nbelongs to him.\n\nAnother way to look at it is: stealth addresses give the same privacy\nproperties as Bob generating a fresh address for each transaction, but\nwithout requiring any interaction from Bob.\n\nThe full workflow of a stealth address scheme can be viewed as\nfollows:\n\n- Bob generates his root spending key\n(m) and stealth meta-address\n(M).\n\n- Bob adds an ENS record to register M as the stealth\nmeta-address for bob.eth.\n\n- We assume Alice knows that Bob is bob.eth. Alice looks\nup his stealth meta-address M on ENS.\n\n- Alice generates an ephemeral key that only she\nknows, and that she only uses once (to generate this specific stealth\naddress).\n\n- Alice uses an algorithm that combines her ephemeral key and Bob's\nmeta-address to generate a stealth address. She can now\nsend assets to this address.\n\n- Alice also generates her ephemeral public key, and publishes\nit to the ephemeral public key registry (this can be done in\nthe same transaction as the first transaction sending assets to this\nstealth address).\n\n- For Bob to discover stealth addresses belonging to him, Bob needs to\nscan the ephemeral public key registry for the entire\nlist of ephemeral public keys published by anyone for any\nreason since the last time he did the scan.\n\n- For each ephemeral public key, Bob attempts to combine it with his\nroot spending key to generate a stealth address, and checks if there are\nany assets in that address. If there are, Bob computes the spending key\nfor that address and remembers it.\n\nThis all relies on two uses of cryptographic trickery. First, we need\na pair of algorithms to generate a shared secret: one\nalgorithm which uses Alice's secret thing (her ephemeral key) and Bob's\npublic thing (his meta-address), and another algorithm which uses Bob's\nsecret thing (his root spending key) and Alice's public thing (her\nephemeral public key). This can be done in many ways; Diffie-Hellman\nkey exchange was one of the results that founded the field of modern\ncryptography, and it accomplishes exactly this.\n\nBut a shared secret by itself is not enough: if we just generate a\nprivate key from the shared secret, then Alice and Bob could both spend\nfrom this address. We could leave it at that, leaving it up to Bob to\nmove the funds to a new address, but this is inefficient and needlessly\nreduces security. So we also add a key blinding\nmechanism: a pair of algorithms where Bob can combine the\nshared secret with his root spending key, and Alice can combine the\nshared secret with Bob's meta-address, in such a way that Alice can\ngenerate the stealth address, and Bob can generate the spending key for\nthat stealth address, all without creating a public link between the\nstealth address and Bob's meta-address (or between one stealth address\nand another).\nStealth\naddresses with elliptic curve cryptography\n\nStealth addresses using elliptic curve cryptography were originally\nintroduced in the context of Bitcoin by\nPeter Todd in 2014. This technique works as follows (this assumes\nprior knowledge of basic elliptic curve cryptography; see here,\nhere\nand here\nfor some tutorials):\n\n- Bob generates a key m, and computes\nM = G * m, where G is a commonly-agreed\ngenerator point for the elliptic curve. The stealth meta-address is an\nencoding of M.\n\n- Alice generates an ephemeral key r, and publishes the\nephemeral public key R = G * r.\n\n- Alice can compute a shared secret S = M * r, and Bob\ncan compute the same shared secret S = m * R.\n\n- In general, in both Bitcoin and Ethereum (including\ncorrectly-designed ERC-4337 accounts),\nan address is a hash containing the public key used to verify\ntransactions from that address. So you can compute the address if you\ncompute the public key. To compute the public key, Alice or Bob can\ncompute P = M + G * hash(S)\n\n- To compute the private key for that address, Bob (and Bob alone) can\ncompute p = m + hash(S)\n\nThis satisfies all of our requirements above, and is remarkably\nsimple!\n\nThere is even an EIP trying to define\na stealth address standard for Ethereum today, that both supports this\napproach and gives space for users to develop other approaches (eg. that\nsupport Bob having separate spending and viewing keys, or that use\ndifferent cryptography for quantum-resistant security). Now you might\nthink: stealth addresses are not too difficult, the theory is already\nsolid, and getting them adopted is just an implementation detail. The\nproblem is, however, that there are some pretty big implementation\ndetails that a truly effective implementation would need to get\nthrough.\nStealth addresses\nand paying transaction fees\n\nSuppose that someone sends you an NFT. Mindful of your privacy, they\nsend it to a stealth address that you control. After scanning the ephem\npubkeys on-chain, your wallet automatically discovers this address. You\ncan now freely prove ownership of the NFT or transfer it to someone\nelse. But there's a problem! That account has 0 ETH in it, and so there\nis no way to pay transaction fees. Even ERC-4337 token\npaymasters won't work, because those only work for fungible ERC20\ntokens. And you can't send ETH into it from your main wallet, because\nthen you're creating a publicly visible link.\n\nInserting memes of 2017-era (or older) crypto scams is an\nimportant technique that writers can use to signal erudition and\nrespectableness, because it shows that they have been around for a long\ntime and have refined tastes, and are not easily swayed by current-thing\nscam figures like SBF.\n\nThere is one \"easy\" way to solve the problem: just use ZK-SNARKs to\ntransfer funds to pay for the fees! But this costs a lot of gas, an\nextra hundreds of thousands of gas just for a single transfer.\n\nAnother clever approach involves trusting specialized transaction\naggregators (\"searchers\" in MEV lingo). These aggregators would allow\nusers to pay once to purchase a set of \"tickets\" that can be used to pay\nfor transactions on-chain. When a user needs to spend an NFT in a\nstealth address that contains nothing else, they provide the aggregator\nwith one of the tickets, encoded using a Chaumian\nblinding scheme. This is the original protocol that was used in\ncentralized privacy-preserving e-cash schemes that were proposed in the\n1980s and 1990s. The searcher accepts the ticket, and repeatedly\nincludes the transaction in their bundle for free until the transaction\nis successfully accepted in a block. Because the quantity of funds\ninvolved is low, and it can only be used to pay for transaction fees,\ntrust and regulatory issues are much lower than a \"full\" implementation\nof this kind of centralized privacy-preserving e-cash.\nStealth\naddresses and separating spending and viewing keys\n\nSuppose that instead of Bob just having a single master \"root\nspending key\" that can do everything, Bob wants to have a separate root\nspending key and viewing key. The viewing key can see all of Bob's\nstealth addresses, but cannot spend from them.\n\nIn the elliptic curve world, this can be solved using a very simple\ncryptographic trick:\n\n- Bob's meta-address M is now of the form\n(K, V), encoding G * k and G * v,\nwhere k is the spending key and v is the\nviewing key.\n\n- The shared secret is now S = V * r = v * R, where\nr is still Alice's ephemeral key and R is\nstill the ephemeral public key that Alice publishes.\n\n- The stealth address's public key is P = K + G * hash(S)\nand the private key is p = k + hash(S).\n\nNotice that the first clever cryptographic step (generating the\nshared secret) uses the viewing key, and the second clever cryptographic\nstep (Alice and Bob's parallel algorithms to generate the stealth\naddress and its private key) uses the root spending key.\n\nThis has many use cases. For example, if Bob wants to receive POAPs,\nthen Bob could give his POAP wallet (or even a not-very-secure web\ninterface) his viewing key to scan the chain and see all of his POAPs,\nwithout giving this interface the power to spend those POAPs.\nStealth addresses and\neasier scanning\n\nTo make it easier to scan the total set of ephemeral public keys, one\ntechnique is to add a view tag to each ephemeral public\nkey. One way to do this in the above mechanism is to make the view tag\nbe one byte of the shared secret (eg. the x-coordinate of S\nmodulo 256, or the first byte of hash(S)).\n\nThis way, Bob only needs to do a single elliptic curve multiplication\nfor each ephemeral public key to compute the shared secret, and only\n1/256 of the time would Bob need to do more complex calculation to\ngenerate and check the full address.\nStealth\naddresses and quantum-resistant security\n\nThe above scheme depends on elliptic curves, which are great but are\nunfortunately vulnerable to quantum computers. If quantum computers\nbecome an issue, we would need to switch to quantum-resistant\nalgorithms. There are two natural candidates for this: elliptic curve\nisogenies and lattices.\n\nElliptic curve\nisogenies are a very different mathematical construction based on\nelliptic curves, that has the linearity properties that let us do\nsimilar cryptographic tricks to what we did above, but cleverly avoids\nconstructing cyclic groups that might be vulnerable to discrete\nlogarithm attacks with quantum computers.\n\nThe main weakness of isogeny-based cryptography is its highly\ncomplicated underlying mathematics, and the risk that possible attacks\nare hidden under this complexity. Some isogeny-based protocols were\nbroken last year, though others remain safe. The\nmain strength of isogenies is the relatively small key sizes, and the\nability to port over many kinds of elliptic curve-based approaches\ndirectly.\n\nA 3-isogeny in CSIDH,\nsource\nhere.\n\nLattices are a\nvery different cryptographic construction that relies on far simpler\nmathematics than elliptic curve isogenies, and is capable of some very\npowerful things (eg. fully homomorphic\nencryption). Stealth address schemes could be built on lattices,\nthough designing the best one is an open problem. However, lattice-based\nconstructions tend to have much larger key sizes.\n\nFully homomorphic encryption, an application of\nlattices. FHE could also be used to help stealth address protocols in a\ndifferent way: to help Bob outsource the computation of checking the\nentire chain for stealth addresses containing assets without revealing\nhis view key.\n\nA third approach is to construct a stealth address scheme from\ngeneric black-box primitives: basic ingredients that lots of people need\nfor other reasons. The shared secret generation part of the scheme maps\ndirectly to key\nexchange, a, errr... important component in public key\nencryption systems. The harder part is the parallel algorithms that let\nAlice generate only the stealth address (and not the spending key) and\nlet Bob generate the spending key.\n\nUnfortunately, you cannot build stealth addresses out of ingredients\nthat are simpler than what is required to build a public-key\nencryption system. There is a simple proof of this: you can build a\npublic-key encryption system out of a stealth address scheme. If Alice\nwants to encrypt a message to Bob, she can send N transactions, each\ntransaction going to either a stealth address belonging to Bob or to a\nstealth address belonging to herself, and Bob can see which transactions\nhe received to read the message. This is important because there are mathematical proofs that you\ncan't do public key encryption with just hashes, whereas you can do zero-knowledge proofs with\njust hashes - hence, stealth addresses cannot be done with just\nhashes.\n\nHere is one approach that does use relatively simple ingredients:\nzero knowledge proofs, which can be made out of hashes, and (key-hiding)\npublic key encryption. Bob's meta-address is a public encryption key\nplus a hash h = hash(x), and his spending key is the\ncorresponding decryption key plus x. To create a stealth\naddress, Alice generates a value c, and publishes as her\nephemeral pubkey an encryption of c readable by Bob. The\naddress itself is an ERC-4337 account\nwhose code verifies transactions by requiring them to come with a\nzero-knowledge proof proving ownership of values x and\nc such that k = hash(hash(x), c) (where\nk is part of the account's code). Knowing x\nand c, Bob can reconstruct the address and its code\nhimself.\n\nThe encryption of c tells no one other than Bob\nanything, and k is a hash, so it reveals almost nothing\nabout c. The wallet code itself only contains\nk, and c being private means that\nk cannot be traced back to h.\n\nHowever, this requires a STARK, and STARKs are big. Ultimately, I\nthink it is likely that a post-quantum Ethereum world will involve many\napplications using many starks, and so I would advocate for an\naggregation protocol like\nthat described here to combine all of these STARKs into a single\nrecursive STARK to save space.\nStealth\naddresses and social recovery and multi-L2 wallets\n\nI have for a long time been a fan of social recovery wallets:\nwallets that have a multisig mechanism with keys shared between some\ncombination of institutions, your other devices and your friends, where\nsome supermajority of those keys can recover access to your account if\nyou lose your primary key.\n\nHowever, social recovery wallets do not mix nicely with stealth\naddresses: if you have to recover your account (meaning, change which\nprivate key controls it), you would also have to perform some step to\nchange the account verification logic of your N stealth wallets, and\nthis would require N transactions, at a high cost to fees, convenience\nand privacy.\n\nA similar concern exists with the interaction of social recovery and\na world of multiple layer-2 protocols: if you have an account on\nOptimism, and on Arbitrum, and on Starknet, and on Scroll, and on\nPolygon, and possibly some of these rollups have a dozen parallel\ninstances for scaling reasons and you have an account on each of those,\nthen changing keys may be a really complex operation.\n\nChanging the keys to many accounts across many chains\nis a huge effort.\n\nOne approach is to bite the bullet and accept that recoveries are\nrare and it's okay for them to be costly and painful. Perhaps you might\nhave some automated software transfer the assets out into new stealth\naddresses at random intervals over a two-week time span to reduce the\neffectiveness of time-based linking. But this is far from perfect.\nAnother approach is to secret-share the root key between the guardians\ninstead of using smart contract recovery. However, this removes the\nability to de-activate a guardian's power to help recover your\naccount, and so is long-run risky.\n\nA more sophisticated approach involves zero-knowledge proofs.\nConsider the ZKP-based scheme above, but modifying the logic as follows.\nInstead of the account holding k = hash(hash(x), c)\ndirectly, the account would hold a (hiding) commitment to the\nlocation of k on the chain. Spending from that\naccount would then require providing a zero-knowledge proof that (i) you\nknow the location on the chain that matches that commitment, and (ii)\nthe object in that location contains some value k (which\nyou're not revealing), and that you have some values x and\nc that satisfy k = hash(hash(x), c).\n\nThis allows many accounts, even across many layer-2 protocols, to be\ncontrolled by a single k value somewhere (on the base chain\nor on some layer-2), where changing that one value is enough to change\nthe ownership of all your accounts, all without revealing the link\nbetween your accounts and each other.\n\n## Conclusions\n\nBasic stealth addresses can be implemented fairly quickly today, and\ncould be a significant boost to practical user privacy on Ethereum. They\ndo require some work on the wallet side to support them. That said, it\nis my view that wallets should start moving toward a more natively\nmulti-address model (eg. creating a new address for each application you\ninteract with could be one option) for other privacy-related reasons as\nwell.\n\nHowever, stealth addresses do introduce some longer-term usability\nconcerns, such as difficulty of social recovery. It is probably okay to\nsimply accept these concerns for now, eg. by accepting that social\nrecovery will involve either a loss of privacy or a two-week delay to\nslowly release the recovery transactions to the various assets (which\ncould be handled by a third-party service). In the longer term, these\nproblems can be solved, but the stealth address ecosystem of the long\nterm is looking like one that would really heavily depend on\nzero-knowledge proofs.",
    "contentLength": 19577,
    "summary": "Stealth addresses let senders create one-time addresses that only recipients can control, providing privacy for ETH/NFT transfers without requiring recipient interaction, but face challenges like transaction fee payments.",
    "detailedSummary": {
      "theme": "Stealth addresses as a privacy solution for Ethereum that allows users to receive assets without revealing their identity publicly on-chain.",
      "summary": "Vitalik explores stealth addresses as a privacy-enhancing technology for Ethereum that addresses different privacy needs than existing solutions like Tornado Cash. While Tornado Cash focuses on hiding fungible token transfers, stealth addresses can provide privacy for NFTs, POAPs, and other unique assets by allowing recipients to generate unlinkable addresses for each transaction. The system works by having Bob generate a stealth meta-address that Alice can use to create a unique stealth address for sending assets, which only Bob can control but appears unconnected to his identity. Vitalik explains the underlying cryptography using elliptic curve techniques, where shared secrets and key blinding mechanisms enable this functionality without requiring interaction between sender and recipient. However, he identifies several significant implementation challenges that need solving for practical adoption, including the problem of paying transaction fees from stealth addresses that contain no ETH, the complexity of social recovery across multiple addresses, and the need for quantum-resistant alternatives as cryptographic threats evolve.",
      "takeaways": [
        "Stealth addresses provide a different type of privacy than Tornado Cash, focusing on hiding recipients rather than transfers, and can work with NFTs and unique assets",
        "The basic cryptographic implementation using elliptic curves is relatively straightforward and can be deployed today with existing wallet support",
        "Transaction fee payment from stealth addresses presents a major usability challenge that may require trusted aggregators or expensive ZK-SNARK solutions",
        "Social recovery wallets become significantly more complex with stealth addresses, potentially requiring costly multi-chain recovery operations",
        "Long-term solutions for stealth address usability problems will likely depend heavily on zero-knowledge proof technologies"
      ],
      "controversial": [
        "Vitalik suggests trusting specialized transaction aggregators for fee payment, which introduces centralization and regulatory risks",
        "The recommendation to accept that social recovery will be costly and privacy-compromising in the short term may be seen as inadequate for mainstream adoption"
      ]
    }
  },
  {
    "id": "general-2022-12-30-institutions",
    "title": "What even is an institution?",
    "date": "2022-12-30",
    "category": "society",
    "url": "https://vitalik.eth.limo/general/2022/12/30/institutions.html",
    "path": "general/2022/12/30/institutions.html",
    "content": "- \n\n- \n\n  \n    \n\n  \n  \n  Dark Mode Toggle\n  \n    \n\n  \n\n##  What even is an institution? \n\n 2022 Dec 30 \nSee all posts\n\n \n \n\n What even is an institution? \n\nSpecial thanks to Dennis Pourteaux and Tina Zhen for discussion\nthat led to this post.\n\nA recent alternative political compass proposed by Dennis Pourteaux\nproposes that the most important political divide of our present time is\nnot liberty vs authoritarianism or left vs right, but\nrather how we think about \"institutions\". Are the institutions that\nsociety runs on today good or bad, and is the solution to work\nincrementally to improve them, replace them with radically different\ninstitutions, or do away with institutions altogether?\n\nThis, however, raises a really important question: what even\nis an \"institution\" anyway?\n\nThe word \"institution\" in political discourse brings to mind things\nlike national governments, the New York Times, universities and\nmaybe the local public library. But the word also gets used to\ndescribe other kinds of things. The phrase \"the institution of marriage\"\nis common in English-language discourse, and gets over\ntwo million search results on Google. If you ask Google point-blank,\n\"is family an institution\", it answers yes.\n\nChatGPT agrees:\n\nIf we take ChatGPT's definition that \"a social institution is a\npattern of behaviors and norms that exist within a society and are\nthought to be essential to its functioning\" seriously, then the New York\nTimes is not an institution - no one argues that it's literally\nessential, and many people consider it to be actively harmful! And on\nthe other side, we can think of examples of things that maybe are\ninstitutions that Pourteaux's \"anti-institutionalists\" would\napprove of!\n\n- Twitter\n\n- The Bitcoin or Ethereum blockchains\n\n- The English language\n\n- Substack\n\n- Markets\n\n- Standards organizations dealing with international shipping\n\nThis leads us to two related, but also somewhat separate,\nquestions:\n\n- What is really the dividing line that makes some\nthings \"institutions\" in people's eyes and others not?\n\n- What kind of world do people who consider themselves\nanti-institutionalists actually want to see? And what should an\nanti-institutionalist in today's world be doing?\n\n## A survey experiment\n\nOver the past week, I made a series of\npolls\non Mastodon where I provided many examples of different objects,\npractices and social structures, and asked: is this an institution or\nnot? In some cases, I made different spins on the same concept to see\nthe effects of changing some specific variables. There were some\nfascinating results.\n\nHere are a few examples:\n\nAnd:\n\nAnd:\n\nAnd, of course:\n\nThere's more fun ones: NYT vs Russia\nToday vs Bitcoin\nMagazine, the solar\nsystem vs what if we\nstarted re-engineering it, prediction\nmarkets, various social customs,\nand a lot more.\n\nHere, we can already start to see some common factors. Marriage is\nmore institution-y than romantic relationships, likely because of its\nofficial stamp of recognition, and more mainstream relationship styles\nare more institution-y than less mainstream styles (a pattern that\nrepeats itself when comparing NYT vs Russia Today vs Bitcoin Magazine).\nSystems with clearly visible human beings making decisions are more\ninstitution-y than more impersonal algorithmic structures, even if their\noutputs are ultimately entirely a function of human-provided inputs.\n\nTo try to elucidate things further, I decided to do a more systematic\nanalysis.\n\n## What are some common factors?\n\nRobin Hanson recently\nmade a post in which he argued that:\n\nAt least on prestigious topics, most people want relevant\ninstitutions to take the following ideal form:\n\nMasses recognize elites, who oversee experts, who pick\ndetails.\n\nThis seemed to me to be an important and valuable insight, though in\na somewhat different direction: yes, that is the style of institution\nthat people find familiar and are not weirded out by (as they\nmight when they see many of the \"alternative institutions\" that Hanson\nlikes to\npropose), but it's also exactly the style of institutions that\nanti-institutionalists tend to most strongly rail against! Mark\nZuckerberg's very institution-y oversight\nboard certainly followed the \"masses recognize elites who oversee\nexperts\" template fairly well, but it did not really make a lot of\npeople happy.\n\nI decided to give this theory of institution-ness, along with some\nother theories, a test. I identified seven properties that seemed to me\npossible important characteristics of institutions, with the goal of\nidentifying which ones are most strongly correlated to people thinking\nof something as being an institution:\n\n- Does it have a \"masses recognize elites\"\npattern?\n\n- Does it have a \"elites oversee experts\"\npattern?\n\n- Is it mainstream?\n\n- Is it logically\ncentralized?\n\n- Does it involve interaction between people? (eg.\nintermittent fasting doesn't, as everyone just chooses whether or not to\ndo it separately, but a government does)\n\n- Does it have a specific structure that has\na lot of intentional design behind it? (eg.\ncorporations do, friendship doesn't)\n\n- Does it have roles that take on a life\nindependent of the individuals that fill them? (eg.\ndemocratically elected governments do, after all they even call the\nleader \"Mr.\u00a0President\", but a podcast which is named after its sole host\ndoes not at all)\n\nI went through the list and personally graded the 35\nmaybe-institutions from my polls on these categories. For example, Tesla\ngot:\n\n- 25% on \"masses recognize elites\" (because it's run\nby Elon Musk, who does in practice have a lot of recognition and support\nas a celebrity, but this isn't a deeply intrinsic feature of Tesla, Elon\nwon't get kicked out of Tesla if he loses legitimacy, etc)\n\n- 100% on \"elites oversee experts\" (all large\ncorporations follow this pattern)\n\n- 75% on \"is mainstream\" (almost everyone knows about\nit, lots of people have them, but it's not quite a New York\nTimes-level household name)\n\n- 100% on \"logical centralization\" (most things get\n100% on this score; as a counterexample, \"dating sites\" get 50% because\nthere are many dating sites and \"intermittent fasting\" gets 0%)\n\n- 100% on \"involves interaction between people\"\n(Tesla produces products that it sells to people, and it hires\nemployees, has investors, etc)\n\n- 75% on \"intentional structure\" (Tesla definitely\nhas a deep structure with shareholders, directors, management, etc, but\nthat structure isn't really part of its identity in the way\nthat, say, proof of stake consensus is for Ethereum or voting and\ncongress are for a government)\n\n- 50% for \"roles independent of individuals\" (while\nroles in companies are generally interchangeable, Tesla does get large\ngains from being part of the Elon-verse specifically)\n\nThe full data is here.\nI know that many people will have many disagreements over various\nindividual rankings I make, and readers could probably convince me that\na few of my scores are wrong; I am mainly hoping that I've included a\nsufficient number of diverse maybe-instiutions in the list that\nindividual disagreement or errors get roughly averaged out.\n\nHere's the table of correlations:\n\nMasses recognize elites\n\n0.491442156943094\n\nElites oversee experts\n\n0.697483431580409\n\nIs mainstream\n\n0.477135770662517\n\nLogical centralization\n\n0.406758324754985\n\nInteraction between people\n\n0.570201749796132\n\nIntelligently designed structure\n\n0.365640100778201\n\nRoles independent of individuals\n\n0.199412937985826\n\nBut as it turns out, the correlations are misleading. \"Interaction\nbetween people\" turns out to be an almost unquestionably\nnecessary property for something to have to be an institution.\nThe correlation of 0.57 kind of shows it, but it understates\nthe strength of the relationship:\n\nLiterally every thing that I labeled as clearly involving\ninteraction had a higher percentage of people considering it an\ninstitution than every thing I labeled as not involving\ninteraction. The single dot in the center is my hypothetical example of\nan island where people with odd-numbered birthdays are not allowed to\neat meat before 12:00; I didn't want to give it 100% because the\nnot-meat-eating is a private activity, but the question still strongly\nimplies some social or other pressure to follow the rule so it's also\nnot really 0%. This is a place where Spearman's\ncoefficient outperforms Pearson's, but rather than spurting out\nexotic numbers I'd rather just show the charts. Here are the other\nsix:\n\nThe most surprising finding for me is that \"roles independent\nof individuals\" is by far the weakest correlation. Twitter run\nby a democracy is the most institution-y of all, but Twitter run by a\npay-to-govern scheme is as institution-y as Twitter that's just run by\nElon directly. Roles being independent of individuals adds a guarantee\nof stability, but roles being independent of individuals in the\nwrong way feels too unfamiliar, or casual, or otherwise not\ninstitution-like. Dating sites are more independent of individuals than\nprofessional matchmaking agencies, and yet it's the matchmaking agencies\nthat are seen as more institution-like. Attempts at highly role-driven\nand mechanistic credibly-neutral media, (eg. this\ncontraption, which I actually think would be really cool) just feel\nalien - perhaps in a bad way, but also perhaps in a good way,\nif you find the institutions of today frustrating and you're open-minded\nabout possible alternatives.\n\nCorrelations with \"masses recognize elites\" and \"elites oversee\nexperts\" were high; higher for the second than the first, though perhaps\nHanson and I had different meanings in mind for \"recognize\". The\n\"intentional structure\" chart has an empty bottom-right corner but a\nfull top-left corner, suggesting that intentional structure is\nnecessary but not sufficient for something to be an\ninstitution.\n\nThat said, my main conclusion is probably that the term\n\"institution\" is a big mess. Rather than the term \"institution\"\nreferring to a single coherent cluster of concepts (as eg. \"high\nmodernism\" does), the term seems to have a number of\ndifferent definitions at play:\n\n- A structure that fits the familiar pattern of \"masses recognize\nelites who oversee experts\"\n\n- Any intentionally designed large-scale structure that mediates human\ninteraction (including things like financial markets, social media\nplatforms and dating sites)\n\n- Widely spread and standardized social customs in general\n\nI suspect that anti-institutionalists focus their suspicion on (1),\nand especially instances of (1) that have been captured by the wrong\ntribe. Whether a structure is personalistic or role-driven does\nnot seem to be very important to anti-institutionalists: both\npersonalities (\"Klaus\nSchwab\") and bureaucracies (\"woke academics\") are equally capable of\ncoming from the wrong tribe. Anti-institutionalists generally do not\noppose (3), and indeed in many cases want to see (3) replace (1) as much\nas possible.\n\nSupport for (2) probably maps closely to Pourteaux's\n\"techno-optimist\" vs \"techno-minimalist\" distinction.\nTechno-minimalists don't see things like Twitter, Substack, Bitcoin,\nEthereum, etc as part of the solution, though there are \"Bitcoin\nminimalists\" who see the Bitcoin blockchain as a narrow exception and\notherwise want to see a world where things like family decide more of\nthe outcomes. \"Techno-optimist anti-institutionalists\" are specifically\nengaged in a political project of either trying to replace (1) with the\nright kind of (2), or trying to reform (1) by introducing more elements\nof the right kind of (2).\nWhich\nway forward for anti-institutionalists or institutional reformers?\n\nIt would be wrong to ascribe too much intentional strategy to\nanti-institutionalists: anti-institutionalism is a movement that is much\nmore united in what is against than in support of any specific\nparticular alternative. But what is possible is to recognize\nthis pattern, and ask the question of which paths forward make sense for\nanti-institutionalists.\n\nFrom a language point of view, even using the word\n\"institution\" at all seems more likely to confuse than enlighten at this\npoint. There is a crucial difference between (i) a desire to\nreplace structures that contain enshrined elite roles with structures\nthat don't, (ii) a preference for small-scale and informal structures\nover large-scale and formal ones, (iii) a desire to simply swap the\ncurrent elites out for new elites, and (iv) a kind of social libertinist\nposition that individuals should be driven by their own whims and not by\nincentives created by other people. The word \"institution\" obscures that\ndivide, and probably focuses too much attention on what is being torn\ndown rather than what is to be built up in its place.\n\nDifferent anti-institutionalists have different goals in\nmind. Sure, the person on Twitter delivering that powerful incisive\ncriticism of the New York Times agrees with you on how society\nshould not be run, but are you sure they'll be your\nally when it comes time to decide how society should be\nrun?\n\nThe challenge with avoiding structures entirely is clear: prisoner's\ndilemmas exist and we need incentives. The challenge with small-scale\nand informal structures is often clear: economies of scale and gains\nfrom standardization - though sometimes there are other benefits from\ninformal approaches that are worth losing those gains. The challenge\nwith simply swapping the elites is clear: it has no path to socially\nscale into a cross-tribal consensus. If the goal is not to enshrine a\nnew set of elites forever, but for elites to permanently be high-churn\n(cf.\u00a0Balaji's founder vs\ninheritor dichotomy), that is more credibly neutral, but then it\nstarts getting closer to the territory of avoiding enshrined\nelites in general.\n\nCreating formal structures without enshrined elites is fascinating,\nnot least because it's under-explored: there's a strong case that\ninstitutions with enshrined elite roles might be an unfortunate\nhistorical necessity from when communication was more constrained, but\nmodern information technology (including the internet and also newer\nspookier stuff like zero-knowledge cryptography, blockchains and DAOs)\ncould rapidly expand our available options. That said, as Hanson points\nout, this path has its own fair share of challenges too.",
    "contentLength": 14273,
    "summary": "Dennis Pourteaux's political compass divides people by their stance on institutions, but surveys show \"institution\" is poorly defined and inconsistent.",
    "detailedSummary": {
      "theme": "Vitalik explores the ambiguous definition of 'institutions' and analyzes what anti-institutionalists actually oppose through empirical polling and correlation analysis.",
      "summary": "Vitalik examines Dennis Pourteaux's political compass that frames modern political divisions around attitudes toward institutions rather than traditional left-right or liberty-authoritarian axes. Through systematic polling on Mastodon, Vitalik tested whether various entities (from marriage to Bitcoin to the New York Times) are perceived as institutions, analyzing correlations with seven properties including elite recognition, centralization, and intentional design. Vitalik discovered that 'interaction between people' was the strongest predictor of something being seen as an institution, while 'roles independent of individuals' showed surprisingly weak correlation. The analysis reveals that 'institution' encompasses at least three distinct concepts: structures following 'masses recognize elites who oversee experts' patterns, intentionally designed large-scale interaction mediators, and widespread social customs. Vitalik argues that anti-institutionalists primarily oppose the first category, especially when captured by opposing tribes, while often supporting the third category and having mixed views on the second depending on their techno-optimism.",
      "takeaways": [
        "The term 'institution' is poorly defined and encompasses multiple distinct concepts, making political discourse around it confusing and imprecise",
        "Anti-institutionalists are more united in what they oppose than what they support, creating challenges for building alternative systems",
        "Modern technology, including blockchains and cryptography, may enable new institutional forms without enshrined elite roles that weren't possible in earlier eras",
        "Structures that involve human interaction are the strongest predictor of being perceived as institutions, while role independence from individuals matters less than expected",
        "Different types of anti-institutionalists have fundamentally different goals - from elite replacement to complete structural avoidance - despite appearing aligned in their criticisms"
      ],
      "controversial": [
        "The suggestion that traditional institutions with enshrined elite roles may be obsolete due to modern communication technology",
        "The implication that anti-institutionalist movements lack coherent positive visions and may fragment when moving from criticism to construction"
      ]
    }
  },
  {
    "id": "general-2022-12-06-gpt3",
    "title": "Updating my blog: a quick GPT chatbot coding experiment",
    "date": "2022-12-06",
    "category": "general",
    "url": "https://vitalik.eth.limo/general/2022/12/06/gpt3.html",
    "path": "general/2022/12/06/gpt3.html",
    "content": "- \n\n- \n\n  \n    \n\n  \n  \n  Dark Mode Toggle\n  \n    \n\n  \n\n##  Updating my blog: a quick GPT chatbot coding experiment \n\n 2022 Dec 06 \nSee all posts\n\n \n \n\n Updating my blog: a quick GPT chatbot coding experiment \n\nThe GPT chatbot has been all\nthe rage the last few days. Along with many important use cases like writing\nsong lyrics, acting as a language\nlearning buddy and coming up with convincing-sounding arguments for\narbitrary political opinions, one of the things that many people are\nexcited about is the possibility of using the chatbot to write code.\n\nIn a lot of cases, it can succeed and write some pretty good code\nespecially for common tasks. In cases that cover less well-trodden\nground, however, it can fail: witness its hilariously broken attempt to\nwrite\na PLONK verifier:\n\n(In case you want to know how to do it kinda-properly, here\nis a PLONK verifier written by me)\n\nBut how well do these tools actually perform in the average case? I\ndecided to take the GPT3 chatbot for a spin, and see if I could get it\nto solve a problem very relevant to me personally: changing the IPFS\nhash registered in my vitalik.eth ENS record, in order to\nmake the new article that\nI just released on my blog viewable through ENS.\n\nThe process of updating the ENS view of my blog normally consists of\ntwo steps: first, publish the updated contents to IPFS, and second,\nupdate my ENS record to contain the IPFS hash of the new contents. Fleek has automated the first part of this\nfor me for a long time: I just push the contents to Github, and Fleek\nuploads the new version to IPFS automatically. I have been told that I\ncould change the settings to give Fleek the power to also edit my ENS,\nbut here I want to be fully \"self-sovereign\" and not trust third\nparties, so I have not done this. Instead, so far, I have had to go to\nthe GUI at app.ens.domains, click\na few times, wait for a few loading screens to pass, and finally click\n\"ADD / EDIT RECORD\", change the CONTENT hash and click\n\"Confirm\". This is all a cumbersome process, and so today I finally\nthought that I would write a script in javascript to automate this all\ndown to a single piece of Javascript that I could just copy-paste into\nmy browser console in the future.\n\nThe task is simple: send an Ethereum transaction to the right address\nwith the right calldata to update the content hash record in the ENS\ncontract to equal the IPFS hash that Fleek gives to me. Yesterday, I did\nthis all manually (twice, once to publish and again to add some\ncorrections), and the IPFS hashes I got were:\n\nbafybeifvvseiarzdfoqadphxtfu5yjfgj3cr6x344qce4s4f7wqyf3zv4e\n\nbafybeieg6fhbjlhkzhbyfnmyid3ko5ogxp3mykdarsfyw66lmq6lq5z73m\n\nIf you click through to the top article in each one, you'll see the\ntwo different versions.\n\nThis hash format is often called a \"bafyhash\", because the hashes all\nbegin with \"bafy\". But there is a problem: the format of the hash that\nis saved in Ethereum is not a bafyhash. Here's the calldata of\nthe transaction that made one of the update operations:\n\nYes, I checked, that is not\nhexadecimalized ASCII.\n\nI do know that the IPFS content hash is the last two rows of the\ndata. How do I know? Well, I checked the two\ndifferent transactions\nI sent for my two updates, and I saw that the top row is the same and\nthe bottom two rows are different. Good enough.\n\nSo what do I do to convert from a bafyhash into a binary IPFS content\nhash? Well, let me try asking the GPT3 chatbot!\n\nNoooo!!!!!!!!!! Many issues. First, two things that are my fault:\n\n- I forgot to mention this, but I wanted Javascript, not python.\n\n- It uses external dependencies. I want my javascript copy-pasteable\ninto the console, so I don't want any libraries.\n\nThese are on me to specify, though, and in my next instruction to the\nchatbot I will. But now we get to the things that are its\nfault:\n\n- Bafyhashes are base 32, not base 58. There is a base-58\nformat for IPFS hashes, but those are called \"QM hashes\", not\n\"bafyhashes\".\n\n- By \"binary\" I didn't want literal ones and zeroes, I wanted the\nnormal binary format, a bytes or bytearray.\n\nThat said, at this part of the story I did not even realize that\nbafyhashes are base 32. I fixed the two issues that were my\nfault first:\n\nBAAAAAAAAAAAAAD, the AI trainer said sheepishly! The\natob function is for base\n64, not base 58.\n\nOK, let's keep going. A few rounds later...\n\nIt's hard to see what's going on at first, but it's incredibly wrong.\nBasically, instead of converting the whole string from base 58 to base\n16, it's converting each individual digit to base 16. Not what\nI want to do!\n\nGuess I'll have to tell it what strategy it should use:\n\nBetter! I soon start to realize that I don't need base 58, I need\nbase 32, and furthermore I need the lowercase version of base\n32. I also want the code wrapped in a function. For these simpler steps,\nit gets much more cooperative:\n\nAt this point, I try actually passing the bafyhashes I have into this\nfunction, and I get unrecognizably different outputs. Looks like I can't\njust assume this is generic base 32, and I have to poke into the\ndetails. Hmm, can I perhaps ask the GPT3 chatbot?\n\nOK, this is not helpful. Let me try to be more specific.\n\nThis is an.... interesting guess, but it's totally wrong. After this\npoint, I give up on the GPT3 for a while, and keep poking at the\ngenerated hex and the actual hex in python until I find similarities.\nEventually, I figure it out: I actually do convert both hexes to literal\nbinary, and search from a binary substring of one in the other. I\ndiscover that there is an offset of 2 bits.\n\nI just edit the code manually, compensating for the offset by\ndividing the bigint by 4:\n\nBecause I already know what to do, I also just code the part that\ngenerates the entire calldata myself:\n\nAnyway, then I switch to the next task: the portion of the Javascript\nthat actually sends a transaction. I go back to the GPT3.\n\nNOOOOO! I SAID NO LIBRARIES!!!!1!1!\n\nI tell it what to use directly:\n\nThis is more successful. Two errors though:\n\n- A from address actually is required.\n\n- You can't stick an integer into the gas field, you need\na hex value.\n\nAlso, post EIP-1559, there really isn't much point in hard-coding a\ngasPrice. From here, I do the rest of the work myself.\n\nfunction bafyToHex(bafyString) {\n  // Create a lookup table for the base32 alphabet\n  var alphabet = 'abcdefghijklmnopqrstuvwxyz234567';\n  var base = alphabet.length;\n  var lookupTable = {};\n  alphabet.split('').forEach(function(char, i) {\n    lookupTable[char] = i;\n  });\n\n  // Decode the base32-encoded string into a big integer\n  var bigInt = bafyString.split('').reduce(function(acc, curr) {\n    return acc * BigInt(base) + BigInt(lookupTable[curr]);\n  }, BigInt(0)) / BigInt(4);\n\n  // Convert the big integer into a hexadecimal string\n  var hexString = bigInt.toString(16);\n\n  return 'e30101701220' + hexString.slice(-64);\n}\n\nfunction bafyToCalldata(bafyString) {\n    return (\n        '0x304e6ade' +\n        'ee6c4522aab0003e8d14cd40a6af439055fd2577951148c14b6cea9a53475835' +\n        '0000000000000000000000000000000000000000000000000000000000000040' +\n        '0000000000000000000000000000000000000000000000000000000000000026' +\n        bafyToHex(bafyString) +\n        '0000000000000000000000000000000000000000000000000000'\n    )\n}\n\nasync function setBafyhash(bafyString) {\n  calldata = bafyToCalldata(bafyString);\n  const addr = (await window.ethereum.enable())[0];\n  // Set the \"to\" address for the transaction\n  const to = '0x4976fb03c32e5b8cfe2b6ccb31c09ba78ebaba41';\n\n  // Set the transaction options\n  const options = {\n    from: addr,\n    to: to,\n    data: calldata,\n    gas: \"0x040000\"\n  };\n  console.log(options);\n\n  // Send the transaction\n  window.ethereum.send('eth_sendTransaction', [options], function(error, result) {\n    if (error) {\n      console.error(error);\n    } else {\n      console.log(result);\n    }\n  });\n}\n\nI ask the GPT-3 some minor questions: how to declare an\nasync function, and what keyword to use in Twitter search\nto search only tweets that contain images (needed to write this post).\nIt answers both flawlessly: do async function functionName\nto declare an async function, and use filter:images to\nfilter for tweets that contain images.\n\n## Conclusions\n\nThe GPT-3 chatbot was helpful as a programming aid, but it also made\nplenty of mistakes. Ultimately, I was able to get past its mistakes\nquickly because I had lots of domain knowledge:\n\n- I know that it was unlikely that browsers would have a builtin for\nbase 58, which is a relatively niche format mostly used in the crypto\nworld, and so I immediately got suspicious of its attempt to suggest\natob\n\n- I could eventually recall that the hash being all-lowercase means\nit's base 32 and not base 58\n\n- I knew that the data in the Ethereum transaction had to encode the\nIPFS hash in some sensible way, which led me to eventually come\nup with the idea of checking bit offsets\n\n- I know that a simple \"correct\" way to convert between base A and\nbase B is to go through some abstract integer representation as an\nin-between, and that Javascript supported big integers.\n\n- I knew about window.ethereum.send\n\n- When I got the error that I was not allowed to put an integer into\nthe gas field, I knew immediately that it was supposed to\nbe hex.\n\nAt this point, AI is quite far from being a substitute for human\nprogrammers. In this particular case, it only sped me up by a little\nbit: I could have figured things out with Google eventually, and indeed\nin one or two places I did go back to googling. That said, it\ndid introduce me to some coding patterns I had not seen before, and it\nwrote the base converter faster than I would have on my own. For the\nboilerplate operation of writing the Javascript to send a simple\ntransaction, it did quite well.\n\nThat said, AI is improving quickly and I expect it to keep improving\nfurther and ironing out bugs like this over time.\n\nAddendum: while writing the part of this post that involved more\ncopy-paste than thinking, I put on my music playlist on shuffle. The\nfirst song that started playing was, coincidentally, Basshunter's Boten\nAnna (\"Anna The Bot\").",
    "contentLength": 10136,
    "summary": "Vitalik tested GPT-3's coding ability by having it help write JavaScript to automate updating his ENS blog hash, finding it useful but error-prone.",
    "detailedSummary": {
      "theme": "Vitalik experiments with GPT-3's code generation capabilities by using it to automate updating his ENS blog record, revealing both the potential and significant limitations of AI coding assistants.",
      "summary": "Vitalik documents his experience using GPT-3 to write JavaScript code for automatically updating his vitalik.eth ENS record with new IPFS hashes when publishing blog posts. Instead of manually using the ENS GUI interface, he wanted to create a copy-pasteable JavaScript function for his browser console. The task involved converting IPFS 'bafyhash' format to the binary format stored in Ethereum and generating the proper transaction calldata. Throughout the process, GPT-3 made numerous critical errors: confusing base32 with base58 encoding, suggesting unavailable browser functions like atob for base58, requiring external libraries despite Vitalik's request for standalone code, and making fundamental mistakes in base conversion logic. Vitalik had to repeatedly correct the AI and ultimately wrote significant portions of the code himself, using his domain expertise in cryptography, Ethereum transactions, and JavaScript to identify and fix the AI's mistakes.",
      "takeaways": [
        "GPT-3 can be helpful for generating boilerplate code and introducing new coding patterns, but makes frequent technical errors that require human expertise to identify and correct",
        "AI coding assistants are currently far from being substitutes for human programmers, especially for specialized tasks involving domain-specific knowledge",
        "The effectiveness of AI coding tools depends heavily on the user's existing technical knowledge to catch mistakes and provide proper guidance",
        "For tasks involving cryptocurrency, blockchain, and specialized encoding formats, GPT-3 demonstrated significant knowledge gaps and incorrect assumptions",
        "While AI sped up some aspects of the coding process, Vitalik notes it only provided marginal time savings compared to traditional methods like Google searches"
      ],
      "controversial": [
        "Vitalik's assertion that AI is 'quite far from being a substitute for human programmers' may be debated given the rapid pace of AI development",
        "His preference for remaining 'self-sovereign' by not trusting third parties like Fleek with ENS management reflects a philosophical stance that some might view as unnecessarily complex"
      ]
    }
  },
  {
    "id": "general-2022-12-05-excited",
    "title": "What in the Ethereum application ecosystem excites me",
    "date": "2022-12-05",
    "category": "applications",
    "url": "https://vitalik.eth.limo/general/2022/12/05/excited.html",
    "path": "general/2022/12/05/excited.html",
    "content": "- \n\n- \n\n  \n    \n\n  \n  \n  Dark Mode Toggle\n  \n    \n\n  \n\n##  What in the Ethereum application ecosystem excites me \n\n 2022 Dec 05 \nSee all posts\n\n \n \n\n What in the Ethereum application ecosystem excites me \n\nSpecial thanks to Matt Huang, Santi Siri and Tina Zhen for\nfeedback and review\n\nTen, five, or even two years ago, my opinions on what Ethereum and\nblockchains can do for the world were very abstract. \"This is a\ngeneral-purpose technology, like C++\", I would say; of course, it has\nspecific properties like decentralization, openness and censorship\nresistance, but beyond that it's just too early to say which specific\napplications are going to make the most sense.\n\nToday's world is no longer that world. Today, enough time has passed\nthat there are few ideas that are completely unexplored: if something\nsucceeds, it will probably be some version of something that\nhas already been discussed in blogs and forums and conferences on\nmultiple occasions. We've also come closer to identifying fundamental\nlimits of the space. Many DAOs have had a fair chance with an\nenthusiastic audience willing to participate in them despite the\ninconveniences and fees, and many have underperformed. Industrial\nsupply-chain applications have\nnot gone anywhere. Decentralized Amazon on the blockchain has not\nhappened. But it's also a world where we have seen genuine and growing\nadoption of a few key applications that are meeting people's real needs\n- and those are the applications that we need to focus on.\n\nHence my change in perspective: my excitement about Ethereum is now\nno longer based in the potential for undiscovered unknowns, but rather\nin a few specific categories of applications that are proving themselves\nalready, and are only getting stronger. What are these applications, and\nwhich applications am I no longer optimistic about? That is what this\npost will be about.\n1. Money: the\nfirst and still most important app\n\nWhen I first visited Argentina in December last year, one of the\nexperiences I remember well was walking around on Christmas Day, when\nalmost everything is closed, looking for a coffee shop. After passing by\nabout five closed ones, we finally found one that was open. When we\nwalked in, the owner recognized me, and immediately showed me that he\nhas ETH and other crypto-assets on his Binance account. We ordered tea\nand snacks, and we asked if we could pay in ETH. The coffee shop owner\nobliged, and showed me the QR code for his Binance deposit address, to\nwhich I sent about $20 of ETH from my Status wallet on my phone.\n\nThis was far from the most meaningful use of cryptocurrency that is\ntaking place in the country. Others are using it to save money, transfer\nmoney internationally, make payments for large and important\ntransactions, and much more. But even still, the fact that I randomly\nfound a coffee shop and it happened to accept cryptocurrency showed the\nsheer reach of adoption. Unlike wealthy countries like the United\nStates, where financial transactions are easy to make and 8% inflation\nis considered extreme, in Argentina\nand many other\ncountries\naround the world, links to global financial systems are more limited and\nextreme inflation is a reality\nevery day. Cryptocurrency often steps in as a lifeline.\n\nIn addition to Binance, there is also an increasing number\nof local exchanges, and you can see advertisements for them everywhere\nincluding at airports.\n\nThe one issue with my coffee transaction is that it did not really\nmake pragmatic sense. The fee was high, about a third of the value of\nthe transaction. The transaction took several minutes to confirm: I\nbelieve that at the time, Status did not yet support sending proper\nEIP-1559 transactions that more reliably confirm quickly. If, like many\nother Argentinian crypto users, I had simply had a Binance wallet, the\ntransfer would have been free and instant.\n\nA year later, however, the calculus is different. As a side effect of\nthe Merge, transactions get included significantly more quickly and the\nchain has become more stable, making it safer to accept transactions\nafter fewer confirmations. Scaling technology such as optimistic and ZK rollups is\nproceeding quickly. Social\nrecovery and multisig wallets are becoming more practical with account abstraction.\nThese trends will take years to play out as the technology develops, but\nprogress is already being made. At the same time, there is an\nimportant \"push factor\" driving interest in transacting on-chain: the\nFTX collapse, which has reminded everyone, Latin Americans\nincluded, that even the most trustworthy-seeming centralized services\nmay not be trustworthy after all.\nCryptocurrency in wealthy\ncountries\n\nIn wealthy countries, the more extreme use cases around surviving\nhigh inflation and doing basic financial activities at all usually do\nnot apply. But cryptocurrency still has significant value. As someone\nwho has used it to make donations (to quite normal organizations in many\ncountries), I can personally confirm that it is far more\nconvenient than traditional banking. It's also valuable for industries\nand activities at risk of being deplatformed by payment processors - a\ncategory which includes many\nindustries that are perfectly legal under most countries' laws.\n\nThere is also the important broader philosophical case for\ncryptocurrency as private money: the transition to a \"cashless society\"\nis being taken advantage of by many governments as an opportunity to\nintroduce levels of financial surveillance that would be unimaginable\n100 years ago. Cryptocurrency is the only thing\ncurrently being developed that can realistically combine the benefits of\ndigitalization with cash-like respect for personal privacy.\n\nBut in either case, cryptocurrency is far from perfect. Even with all\nthe technical, user experience and account safety problems solved, it\nremains a fact that cryptocurrency is volatile, and the volatility can\nmake it difficult to use for savings and business. For that reason, we\nhave...\n\n## Stablecoins\n\nThe value of stablecoins has been understood in the Ethereum\ncommunity for a long time. Quoting a\nblog post from 2014:\n\nOver the past eleven months, Bitcoin holders have lost about 67% of\ntheir wealth and quite often the price moves up or down by as much as\n25% in a single week. Seeing this concern, there is a growing interest\nin a simple question: can we get the best of both worlds? Can we have\nthe full decentralization that a cryptographic payment network offers,\nbut at the same time have a higher level of price stability, without\nsuch extreme upward and downward swings?\n\nAnd indeed, stablecoins are very popular among precisely those users\nwho are making pragmatic use of cryptocurrency today. That said, there\nis a reality that is not congenial to cypherpunk values today: the\nstablecoins that are most successful today are the centralized ones,\nmostly USDC, USDT and BUSD.\n\nTop cryptocurrency market caps, data from CoinGecko, 2022-11-30.\nThree of the top six are centralized stablecoins.\n\nStablecoins issued on-chain have many convenient properties: they are\nopen for use by anyone, they are resistant to the most large-scale and\nopaque forms of censorship (the issuer can blacklist and freeze\naddresses, but such blacklisting is transparent, and there are literal\ntransaction fee costs associated with freezing each address), and they\ninteract well with on-chain infrastructure (accounts, DEXes, etc). But\nit's not clear how long this state of affairs will last, and so there is\na need to keep working on other alternatives.\n\nI see the stablecoin design space as basically being split into three\ndifferent categories: centralized stablecoins,\nDAO-governed real-world-asset backed stablecoins and\ngovernance-minimized crypto-backed stablecoins.\n\nGovernance\nAdvantages\nDisadvantages\nExamples\n\nCentralized stablecoins\nTraditional legal entity\n\u2022 Maximum efficiency \n \u2022 Easy to understand\nVulnerable to risks of a single issuer and a single\njurisdiction\nUSDC, USDT, BUSD\n\nDAO-governed RWA-backed stablecoins\nDAO deciding on allowed collateral types and maximum per type\n\u2022 Adds resilience by diversifying issuers and jurisdictions \n \u2022\nStill somewhat capital efficient\nVulnerable to repeated issuer fraud or coordinated takedown\nDAI\n\nGovernance-minimized crypto-backed stablecoin\nPrice oracle only\n\u2022 Maximum resilience \n \u2022 No outside dependencies\n\u2022 High collateral requirements \n \u2022 Limited scale \n \u2022 Sometimes\nneeds negative interest rates\nRAI, \n LUSD\n\nFrom the user's perspective, the three types are arranged on\na tradeoff spectrum between efficiency and resilience. USDC\nworks today, and will almost certainly work tomorrow. But in the longer\nterm, its ongoing stability depends on the macroeconomic and political\nstability of the United States, a continued US regulatory environment\nthat supports making USDC available to everyone, and the trustworthiness\nof the issuing organization.\n\nRAI, on the other hand, can survive all of these risks, but it has a\nnegative interest rate: at the time of this writing, -6.7%. To make the system\nstable (so, not be vulnerable\nto collapse like LUNA), every holder of RAI must be matched by a\nholder of negative RAI (aka. a \"borrower\" or \"CDP holder\") who puts in\nETH as collateral. This rate could be improved with more people engaging\nin arbitrage, holding negative RAI and balancing it out with positive\nUSDC or even interest-bearing bank account deposits, but interest rates\non RAI will always be lower than in a functioning banking system, and\nthe possibility of negative rates, and the user experience headaches\nthat they imply, will always be there.\n\nThe RAI model is ultimately ideal for the more pessimistic lunarpunk\nworld: it avoids all connection to non-crypto financial systems, making\nit much more difficult to attack. Negative interest rates prevent it\nfrom being a convenient proxy for the dollar, but one way to adapt would\nbe to embrace the disconnection: a governance-minimized\nstablecoin could track some non-currency asset like a global average CPI\nindex, and advertise itself as representing abstract \"best-effort price\nstability\". This would also have lower inherent regulatory\nrisk, as such an asset would not be attempting to provide a \"digital\ndollar\" (or euro, or...).\n\nDAO-governed RWA-backed stablecoins, if they can be made to work\nwell, could be a happy medium. Such stablecoins could combine enough\nrobustness, censorship resistance, scale and economic practicality to\nsatisfy the needs of a large number of real-world crypto users. But\nmaking this work requires both real-world legal work to develop robust\nissuers, and a healthy dose of resilience-oriented DAO governance\nengineering.\n\nIn either case, any kind of stablecoin working well would be\na boon for many kinds of currency and savings applications that are\nalready concretely useful for millions of people today.\n\n## 2. Defi: keep it simple\n\nDecentralized finance is, in my view, a category that started off\nhonorable but limited, turned into somewhat of an overcapitalized\nmonster that relied on unsustainable forms of yield farming, and is now\nin the early stages of setting down into a stable medium, improving\nsecurity and refocusing on a few applications that are particularly\nvaluable. Decentralized stablecoins are, and probably forever will be,\nthe most important defi product, but there are a few others that have an\nimportant niche:\n\n- Prediction markets: these have been a niche but\nstable pillar of decentralized finance since the launch of Augur in\n2015. Since then, they have quietly been growing in adoption. Prediction\nmarkets showed their value\nand their limitations in the 2020 US election, and this year in\n2022, both crypto prediction markets like Polymarket and play-money markets\nlike Metaculus are\nbecoming more and more widely used. Prediction markets are valuable as\nan epistemic tool, and there is a genuine benefit from using\ncryptocurrency in making these markets more trustworthy and more\nglobally accessible. I expect prediction markets to not make extreme\nmultibillion-dollar splashes, but continue to steadily grow and become\nmore useful over time.\n\n- Other synthetic assets: the formula behind\nstablecoins can in principle be replicated to other real-world assets.\nInteresting natural candidates include major stock indices and real\nestate. The latter will take longer to get right due to the inherent\nheterogeneity and complexity of the space, but it could be valuable for\nprecisely the same reasons. The main question is whether or not someone\ncan create the right balance of decentralization and efficiency that\ngives users access to these assets at reasonable rates of return.\n\n- Glue layers for efficiently trading between other\nassets: if there are assets on-chain that people want to use,\nincluding ETH, centralized or decentralized stablecoins, more advanced\nsynthetic assets, or whatever else, there will be value in a layer that\nmakes it easy for users to trade between them. Some users may want to\nhold USDC and pay transaction fees in USDC. Others may hold some assets,\nbut want to be able to instantly convert to pay someone who wants to be\npaid in another asset. There is also space for using one asset as\ncollateral to take out loans of another asset, though such projects are\nmost likely to succeed and avoid leading to tears if they keep leverage\nvery limited (eg. not more than 2x).\n\n3. The identity\necosystem: ENS, SIWE, PoH, POAPs, SBTs\n\n\"Identity\" is a complicated concept that can mean many things. Some\nexamples include:\n\n- Basic authentication: simply proving that action A\n(eg. sending a transaction or logging into a website) was authorized by\nsome agent that has some identifier, such as an ETH address or a public\nkey, without attempting to say anything else about who or what the agent\nis.\n\n- Attestations: proving claims about an agent made by\nother agents (\"Bob attests that he knows Alice\", \"the government of\nCanada attests that Charlie is a citizen\")\n\n- Names: establishing consensus that a particular\nhuman-readable name can be used to refer to a particular agent.\n\n- Proof of\npersonhood: proving that an agent is human, and\nguaranteeing that each human can only obtain one identity through the\nproof of personhood system (this is often done with attestations, so\nit's not an entirely separate category, but it's a hugely important\nspecial case)\n\nFor a long time, I have been bullish on blockchain identity\nbut bearish on blockchain identity platforms. The use cases\nmentioned above are really important to many blockchain use cases, and\nblockchains are valuable for identity applications because of their\ninstitution-independent nature and the interoperability benefits that\nthey provide. But what will not work is an attempt to create a\ncentralized platform to achieve all of these tasks from scratch. What\nmore likely will work is an organic approach, with many projects working\non specific tasks that are individually valuable, and adding more and\nmore interoperability over time.\n\nAnd this is exactly what has happened since then. The Sign In With Ethereum (SIWE) standard\nallows users to log into (traditional) websites in much the same way\nthat you can use Google or Facebook accounts to log into websites today.\nThis is actually useful: it allows you to interact with a site without\ngiving Google or Facebook access to your private information or the\nability to take over or lock you out of your account. Techniques like social recovery could give\nusers account recovery options in case they forget their password that\nare much better than what centralized corporations offer today.\nSIWE is supported by many applications today, including Blockscan chat, the\nend-to-end-encrypted email and notes service Skiff, and various blockchain-based\nalternative social media projects.\n\nENS lets users have usernames: I have vitalik.eth. Proof\nof Humanity and other proof-of-personhood systems let users prove that\nthey are unique humans, which is useful in many applications including\nairdrops and governance. POAP (the\n\"proof of attendance protocol\", pronounced either\n\"pope\" or \"poe-app\" depending on whether you're a brave contrarian\nor a sheep) is a general-purpose protocol for issuing tokens that\nrepresent attestations: have you completed an educational course? Have\nyou attended an event? Have you met a particular person? POAPs could be\nused both as an ingredient in a proof-of-personhood protocol and as a\nway to try to determine whether or not someone is a member of a\nparticular community (valuable for governance or airdrops).\n\nAn NFC card that contains my ENS name, and allows you to receive a\nPOAP verifying that you've met me. I'm not sure I want\nto create any further incentive for people to bug me really hard to get\nmy POAP, but this seems fun and useful for other people.\n\nEach of these applications are useful individually. But what makes\nthem truly powerful is how well they compose with each other. When I log\non to Blockscan chat, I sign\nin with Ethereum. This means that I am immediately visible as\nvitalik.eth (my ENS name) to anyone I chat with. In the\nfuture, to fight spam, Blockscan chat could \"verify\" accounts by looking\nat on-chain activity or POAPs. The lowest tier would simply be to verify\nthat the account has sent or been the recipient in at least one on-chain\ntransaction (as that requires paying fees). A higher level of\nverification could involve checking for balances of specific tokens,\nownership of specific POAPs, a proof-of-personhood profile, or a\nmeta-aggregator like Gitcoin\nPassport.\n\nThe network effects of these different services combine to create an\necosystem that provides some very powerful options for users and\napplications. An Ethereum-based Twitter alternative (eg. Farcaster) could use POAPs and\nother proofs of on-chain activity to create a \"verification\" feature\nthat does not require conventional KYC, allowing anons to participate.\nSuch platforms could create rooms that are gated to members of a\nparticular community - or hybrid approaches where only community members\ncan speak but anyone can listen. The equivalent of Twitter polls could\nbe limited to particular communities.\n\nEqually importantly, there are much more pedestrian applications that\nare relevant to simply helping people make a living: verification\nthrough attestations can make it easier for people to prove that they\nare trustworthy to get rent, employment or loans.\n\nThe big future challenge for this ecosystem is\nprivacy. The status quo involves putting large amounts of\ninformation on-chain, which is something that is \"fine until it's not\",\nand eventually will become unpalatable if not outright risky to more and\nmore people. There are ways to solve this problem by combining on-chain\nand off-chain information and making heavy use of ZK-SNARKs,\nbut this is something that will actually need to be worked on; projects\nlike Sismo and HeyAnon are an early start. Scaling\nis also a challenge, but scaling can be solved generically with rollups\nand perhaps validiums. Privacy cannot, and must be worked on\nintentionally for each application.\n\n## 4. DAOs\n\n\"DAO\" is a powerful term that captures many of the hopes and dreams\nthat people have put into the crypto space to build more democratic,\nresilient and efficient forms of governance. It's also an incredibly\nbroad term whose meaning\nhas evolved a lot over the years. Most generally, a DAO is a smart\ncontract that is meant to represent a structure of ownership or control\nover some asset or process. But this structure could be anything, from\nthe lowly multisig to highly sophisticated multi-chamber governance\nmechanisms like those proposed for the Optimism\nCollective. Many of these structures work, and many others cannot,\nor at least are very mismatched to the goals that they are trying to\nachieve.\n\nThere are two questions to answer:\n\n- What kinds of governance structures make sense, and for what use\ncases?\n\n- Does it make sense to implement those structures as a DAO, or\nthrough regular incorporation and legal contracts?\n\nA particular subtlety is that the word \"decentralized\" is sometimes\nused to refer to both: a governance structure is decentralized\nif its decisions depend on decisions taken from a large group of\nparticipants, and an implementation of a governance structure\nis decentralized if it is built on a decentralized structure like a\nblockchain and is not dependent on any single nation-state legal\nsystem.\nDecentralization for\nrobustness\n\nOne way to think about the distinction is: decentralized\ngovernance structure protects against attackers on the inside, and a\ndecentralized implementation protects against powerful attackers on the\noutside (\"censorship resistance\").\n\nFirst, some examples:\n\nHigher need for protection from inside\nLower need for protection from inside\n\nHigher need for protection from outside\nStablecoins\nThe Pirate Bay, Sci-Hub\n\nLower need for protection from outside\nRegulated financial institutions\nRegular businesses\n\nThe Pirate Bay and\nSci-Hub are important case studies\nof something that is censorship-resistant, but does not need\ndecentralization. Sci-Hub is largely run by one person, and if some part\nof Sci-Hub infrastructure gets taken down, she can simply move it\nsomewhere else. The Sci-Hub URL has changed many times over the years.\nThe Pirate Bay is a hybrid: it relies on BitTorrent, which is\ndecentralized, but the Pirate Bay itself is a centralized convenience\nlayer on top.\n\nThe difference between these two examples and blockchain projects is\nthat they do not attempt to protect their users against the platform\nitself. If Sci-Hub or The Pirate Bay wanted to harm their users, the\nworst they could do is either serve bad results or shut down - either of\nwhich would only cause minor inconvenience until their users switch to\nother alternatives that would inevitably pop up in their absence. They\ncould also publish user IP addresses, but even if they did that the\ntotal harm to users would still be much lower than, say, stealing all\nthe users' funds.\n\nStablecoins are not like this. Stablecoins are trying to\ncreate stable credibly neutral\nglobal commercial infrastructure, and this demands both lack of\ndependence on a single centralized actor on the outside and\nprotection against attackers from the inside. If a stablecoin's\ngovernance is poorly designed, an attack on the governance could steal\nbillions of dollars from users.\n\nAt the time of this writing, MakerDAO has $7.8 billion in collateral, over 17x\nthe market cap of the profit-taking token, MKR. Hence, if\ngovernance was up to MKR holders with no safeguards, someone could buy\nup half the MKR, use that to manipulate the price oracles, and steal a\nlarge portion of the collateral for themselves. In fact, this actually\nhappened with a smaller stablecoin! It hasn't happened to MKR yet\nlargely because the MKR holdings are still fairly concentrated, with the\nmajority of the MKR held by a fairly small group that would not be\nwilling to sell because they believe in the project. This is a fine\nmodel to get a stablecoin started, but not a good one for the long term.\nHence, making decentralized stablecoins work long term requires\ninnovating in decentralized governance that does not have these kinds of\nflaws.\n\nTwo possible directions include:\n\n- Some kind of non-financialized\ngovernance, or perhaps a bicameral hybrid where decisions need to be\npassed not just by token holders but also by some other class of user\n(eg. the Optimism Citizens'\nHouse or stETH holders as in the Lido two-chamber\nproposal)\n\n- Intentional\nfriction, making it so that certain kinds of decisions can only take\neffect after a delay long enough that users can see that something is\ngoing wrong and escape the system.\n\nThere are many subtleties in making governance that effectively\noptimizes for robustness. If the system's robustness depends on pathways\nthat are only activated in extreme edge cases, the system may even want\nto intentionally test those pathways once in a while to make sure that\nthey work - much like the once-every-20-years rebuilding of Ise\nJingu. This aspect of decentralization for robustness continues to\nrequire more careful thought and development.\nDecentralization for\nefficiency\n\nDecentralization for efficiency is a different school of thought:\ndecentralized governance structure is valuable because it can\nincorporate opinions from more diverse voices at different scales, and\ndecentralized implementation is valuable because it can sometimes be\nmore efficient and lower cost than traditional legal-system-based\napproaches.\n\nThis implies a different style of decentralization.\nGovernance decentralized for robustness emphasizes having a large number\nof decision-makers to ensure alignment with a pre-set goal, and\nintentionally makes pivoting more difficult. Governance\ndecentralized for efficiency preserves the ability to act rapidly and\npivot if needed, but tries to move decisions away from the top to avoid\nthe organization becoming a sclerotic bureaucracy.\n\nPod-based governance in Ukraine DAO. This style of\ngovernance improves efficiency by maximizing autonomy.\n\nDecentralized implementations designed for robustness and\ndecentralized implementations designed for efficiency are in one way\nsimilar: they both just involve putting assets into smart contracts. But\ndecentralized implementations designed for efficiency are going to be\nmuch simpler: just a basic multisig will generally suffice.\n\nIt's worth noting that \"decentralizing for efficiency\" is a weak\nargument for large-scale projects in the same wealthy country. But it's\na stronger argument for very-small-scale projects, highly\ninternationalized projects, and projects located in countries with\ninefficient institutions and weak rule of law. Many applications of\n\"decentralizing for efficiency\" probably could also be done on\na central-bank-run chain run by a stable large country; I suspect that\nboth decentralized approaches and centralized approaches are good\nenough, and it's the path-dependent question of which one becomes viable\nfirst that will determine which approach dominates.\nDecentralization for\ninteroperability\n\nThis is a fairly boring class of reasons to decentralize, but it's\nstill important: it's easier and more secure for on-chain things\nto interact with other on-chain things, than with off-chain systems that\nwould inevitably require an (attackable) bridge layer.\n\nIf a large organization running on direct democracy holds 10,000 ETH\nin its reserves, that would be a decentralized governance decision, but\nit would not be a decentralized implementation: in practice, that\ncountry would have a few people managing the keys and that storage\nsystem could get attacked.\n\nThere is also a governance angle to this: if a system provides\nservices to other DAOs that are not capable of rapid change, it\nis better for that system to itself be incapable of rapid change, to\navoid \"rigidity mismatch\" where a system's dependencies break and that\nsystem's rigidity renders it unable to adapt to the break.\n\nThese three \"theories of decentralization\" can be put into a chart as\nfollows:\n\nWhy decentralize governance structure\nWhy decentralize implementation\n\nDecentralization for robustness\nDefense against inside threats (eg. SBF)\nDefense against outside threats, and censorship resistance\n\nDecentralization for efficiency\nGreater efficiency from accepting input from more voices and giving\nroom for autonomy\nSmart contracts often more convenient than legal systems\n\nDecentralization for interoperability\nTo be rigid enough to be safe to use by other rigid systems\nTo more easily interact with other decentralized things\n\nDecentralization\nand fancy new governance mechanisms\n\nOver the last few decades, we've seen the development of a number of\nfancy new governance mechanisms:\n\n- Quadratic\nvoting\n\n- Futarchy\n\n- Liquid\ndemocracy\n\n- Decentralized conversation tools like Pol.is\n\nThese ideas are an important part of the DAO story, and they can be\nvaluable for both robustness and efficiency. The case for\nquadratic voting relies on a mathematical argument that it makes the\nexactly correct tradeoff between giving space for stronger preferences\nto outcompete weaker but more popular preferences and not weighting\nstronger preferences (or wealthy actors) too much. But people\nwho have used it have found that it can improve robustness too. Newer\nideas, like pairwise\nmatching, intentionally sacrifice mathematically provable\noptimality for robustness in situations where the mathematical model's\nassumptions break.\n\nThese ideas, in addition to more \"traditional\" centuries-old ideas\naround multicameral architectures and intentional indirection and\ndelays, are going to be an important part of the story in making DAOs\nmore effective, though they will also find value in improving the\nefficiency of traditional organizations.\n\n## Case study: Gitcoin Grants\n\nWe can analyze the different styles of decentralization through an\ninteresting edge-case: Gitcoin Grants. Should Gitcoin Grants be an\non-chain DAO, or should it just be a centralized org?\n\nHere are some possible arguments for Gitcoin Grants to be a DAO:\n\n- It holds and deals with cryptocurrency, because most of its users\nand funders are Ethereum users\n\n- Secure quadratic funding is best done on-chain (see next section on\nblockchain voting, and implementation of\non-chain QF here), so you reduce security risks if the result of the\nvote feeds into the system directly\n\n- It deals with communities all around the world, and so benefits from\nbeing credibly\nneutral and not centered around a single country.\n\n- It benefits from being able to give its users confidence that it\nwill still be around in five years, so that public goods funders can\nstart projects now and hope to be rewarded later.\n\nThese arguments lean toward decentralization for robustness and\ndecentralization for interoperability of the superstructure, though the\nindividual quadratic funding rounds are more in the \"decentralization\nfor efficiency\" school of thought (the theory behind Gitcoin Grants is\nthat quadratic funding is a more efficient way to fund public\ngoods).\n\nIf the robustness and interoperability arguments did not\napply, then it probably would have been better to simply run Gitcoin\nGrants as a regular company. But they do apply, and so Gitcoin Grants\nbeing a DAO makes sense.\n\nThere are plenty of other examples of this kind of argument applying,\nboth for DAOs that people increasingly rely on for their day-to-day\nlives, and for \"meta-DAOs\" that provide services to other\nDAOs:\n\n- Proof of humanity\n\n- Kleros\n\n- Chainlink\n\n- Stablecoins\n\n- Blockchain layer 2 protocol governance\n\nI don't know enough about all of these systems to testify that they\nall do optimize for decentralization-for-robustness enough to\nsatisfy my standards, but hopefully it should be obvious by now that\nthey should.\n\nThe main thing that does not work well are DAOs that\nrequire pivoting ability that is in conflict with robustness, and that\ndo not have a sufficient case to \"decentralize for efficiency\".\nLarge-scale companies that mainly interface with US users would be one\nexample. When making a DAO, the first thing is to determine whether or\nnot it is worth it to structure the project as a DAO, and the second\nthing is to determine whether it's targeting robustness or efficiency:\nif the former, deep thought into governance design is also\nrequired, and if the latter, then either it's innovating on governance\nvia mechanisms like quadratic funding, or it should just be a\nmultisig.\n\n## 5. Hybrid applications\n\nThere are many applications that are not entirely on-chain, but that\ntake advantage of both blockchains and other systems to improve their\ntrust models.\n\nVoting is an excellent\nexample. High assurances of censorship resistance, auditability and\nprivacy are all required, and systems like MACI\neffectively combine blockchains, ZK-SNARKs and a limited centralized (or\nM-of-N) layer for scalability and coercion resistance to achieve all of\nthese guarantees. Votes are published to the blockchain, so users have a\nway independent of the voting system to ensure that their votes get\nincluded. But votes are encrypted, preserving privacy, and a\nZK-SNARK-based solution is used to ensure that the final result is the\ncorrect computation of the votes.\n\nDiagram of how MACI works, combining together blockchains\nfor censorship resistance, encryption for privacy, and ZK-SNARKs to\nensure the result is correct without compromising on the other\ngoals.\n\nVoting in existing national elections is already a high-assurance\nprocess, and it will take a long time before countries and citizens are\ncomfortable with the security assurances of any electronic ways to vote,\nblockchain or otherwise. But technology like this can be\nvaluable very soon in two other places:\n\n- Increasing the assurance of voting processes that already happen\nelectronically today (eg. social media votes, polls, petitions)\n\n- Creating new forms of voting that allow citizens or members of\ngroups to give rapid feedback, and baking high assurance into those from\nthe start\n\nGoing beyond voting, there is an entire field of potential \"auditable\ncentralized services\" that could be well-served by some form of hybrid\noff-chain validium\narchitecture. The easiest example of this is proof of solvency for\nexchanges, but there are plenty of other possible examples:\n\n- Government registries\n\n- Corporate accounting\n\n- Games (see Dark Forest for an\nexample)\n\n- Supply chain applications\n\n- Tracking access authorization\n\n- ...\n\nAs we go further down the list, we get to use cases that are lower\nand lower value, but it is important to remember that these use cases\nare also quite low cost. Validiums do not require publishing everything\non-chain. Rather, they can be simple wrappers around existing pieces of\nsoftware that maintain a Merkle root (or other commitment) of the\ndatabase and occasionally publish the root on-chain along with a SNARK\nproving that it was updated correctly. This is a strict improvement over\nexisting systems, because it opens the door for cross-institutional\nproofs and public auditing.\n\n## So how do we get there?\n\nMany of these applications are being built today, though many of\nthese applications are seeing only limited usage because of the\nlimitations of present-day technology. Blockchains are not scalable,\ntransactions until recently took a fairly long time to reliably get\nincluded on the chain, and present-day wallets give users an\nuncomfortable choice between low convenience and low security. In the\nlonger term, many of these applications will need to overcome the\nspecter of privacy issues.\n\nThese are all problems that can be solved, and there is a strong\ndrive to solve them. The FTX collapse has shown many people the\nimportance of truly decentralized solutions to holding funds, and the\nrise of ERC-4337\nand account abstraction wallets gives us an opportunity to create such\nalternatives. Rollup technology is rapidly progressing to solve\nscalability, and transactions already get included much more quickly\non-chain than they did three years ago.\n\nBut what is also important is to be intentional about the application\necosystem itself. Many of the more stable and boring applications do not\nget built because there is less excitement and less short-term profit to\nbe earned around them: the LUNA market cap got to over $30 billion,\nwhile stablecoins striving for robustness and simplicity often get\nlargely ignored for years. Non-financial applications often have no hope\nof earning $30 billion because they do not have a token at all. But it\nis these applications that will be most valuable for the ecosystem in\nthe long term, and that will bring the most lasting value to both their\nusers and those who build and support them.",
    "contentLength": 35787,
    "summary": "Vitalik highlights money/stablecoins, identity, and DeFi as Ethereum's most promising apps proving real-world value today.",
    "detailedSummary": {
      "theme": "Vitalik analyzes which Ethereum applications have proven their value in practice, focusing on money/stablecoins, simplified DeFi, identity systems, DAOs, and hybrid applications while advocating for sustainable development over speculative token projects.",
      "summary": "Vitalik reflects on how his perspective on Ethereum has evolved from viewing it as a general-purpose technology with unlimited potential to focusing on specific applications that have demonstrated real-world utility. He identifies five key areas that excite him: cryptocurrency as money (particularly valuable in countries with high inflation and financial instability), stablecoins (with different tradeoffs between centralized, DAO-governed, and governance-minimized approaches), simplified DeFi applications like prediction markets and synthetic assets, identity infrastructure combining ENS, SIWE, proof of personhood and POAPs, and DAOs designed for specific purposes like robustness or efficiency rather than general governance. Vitalik emphasizes the importance of building sustainable, boring applications that provide genuine utility rather than chasing speculative gains. He argues that while many ambitious blockchain applications have failed to materialize (like decentralized Amazon or supply chain solutions), the applications that have found product-market fit are becoming increasingly sophisticated and interconnected, creating powerful network effects that benefit users worldwide.",
      "takeaways": [
        "Cryptocurrency has found genuine adoption as money, especially in countries with high inflation and limited financial infrastructure, though technical challenges around fees and confirmation times are being addressed",
        "Stablecoins exist on a spectrum from centralized (efficient but risky) to governance-minimized crypto-backed (resilient but with negative interest rates), with DAO-governed RWA-backed stablecoins potentially offering a middle ground",
        "DeFi should focus on simple, sustainable applications like prediction markets and basic synthetic assets rather than complex yield farming schemes that proved unsustainable",
        "Identity infrastructure is succeeding through composable, specialized services (ENS, SIWE, POAPs, proof of personhood) rather than centralized identity platforms, though privacy remains a major challenge",
        "DAOs work best when designed for specific purposes - either robustness against attacks, efficiency improvements, or interoperability - rather than as general-purpose governance structures"
      ],
      "controversial": [
        "Vitalik's dismissal of many DAO experiments as 'underperforming' may be seen as premature given the nascent stage of decentralized governance",
        "His preference for 'boring' applications over innovative tokenomics could be viewed as stifling experimentation in the space",
        "The argument that governance-minimized stablecoins with negative interest rates are preferable to centralized ones may be disputed by those prioritizing user experience and adoption"
      ]
    }
  },
  {
    "id": "general-2022-11-19-proof_of_solvency",
    "title": "Having a safe CEX: proof of solvency and beyond",
    "date": "2022-11-19",
    "category": "math",
    "url": "https://vitalik.eth.limo/general/2022/11/19/proof_of_solvency.html",
    "path": "general/2022/11/19/proof_of_solvency.html",
    "content": "- \n\n- \n\n  \n    \n\n  \n  \n  Dark Mode Toggle\n  \n    \n\n  \n\n##  Having a safe CEX: proof of solvency and beyond \n\n 2022 Nov 19 \nSee all posts\n\n \n \n\n Having a safe CEX: proof of solvency and beyond \n\nSpecial thanks to Balaji Srinivasan, and Coinbase, Kraken and\nBinance staff for discussion.\n\nEvery time a major centralized exchange blows up, a common question\nthat comes up is whether or not we can use cryptographic techniques to\nsolve the problem. Rather than relying solely on \"fiat\" methods like\ngovernment licenses, auditors and examining the corporate governance and\nthe backgrounds of the individuals running the exchange, exchanges could\ncreate cryptographic proofs that show that the funds they hold\non-chain are enough to cover their liabilities to their users.\n\nEven more ambitiously, an exchange could build a system where it\ncan't withdraw a depositor's funds at all without their consent.\nPotentially, we could explore the entire spectrum between the \"don't be\nevil\" aspiring-good-guy CEX and the \"can't be evil\", but for-now\ninefficient and privacy-leaking, on-chain DEX. This post will get into\nthe history of attempts to move exchanges one or two steps closer to\ntrustlessness, the limitations of these techniques, and some newer and\nmore powerful ideas that rely on ZK-SNARKs and other advanced\ntechnologies.\nBalance\nlists and Merkle trees: old-school proof-of-solvency\n\nThe earliest attempts by exchanges to try to cryptographically prove\nthat they are not cheating their users go back quite far. In 2011,\nthen-largest Bitcoin exchange MtGox proved that they had funds by\nsending a\ntransaction that moved 424242\nBTC to a pre-announced address. In 2013, discussions\nstarted on how to solve the other side of the problem: proving the\ntotal size of customers' deposits. If you prove that customers' deposits\nequal X (\"proof of liabilities\"), and prove ownership\nof the private keys of X coins (\"proof of assets\"),\nthen you have a proof of solvency: you've proven the\nexchange has the funds to pay back all of its depositors.\n\nThe simplest way to prove deposits is to simply publish a list of\n(username, balance) pairs. Each user can check that their\nbalance is included in the list, and anyone can check the full list to\nsee that (i) every balance is non-negative, and (ii) the total sum is\nthe claimed amount. Of course, this breaks privacy, so we can change the\nscheme a little bit: publish a list of\n(hash(username, salt), balance) pairs, and send each user\nprivately their salt\nvalue. But even this leaks balances, and it leaks the pattern of changes\nin balances. The desire to preserve privacy brings\nus to the next\ninvention: the Merkle tree technique.\n\nGreen: Charlie's node. Blue: nodes Charlie will receive as part of\nhis proof. Yellow: root node, publicly shown to everyone.\n\nThe Merkle tree technique consists of putting the table of customers'\nbalances into a Merkle sum tree. In a Merkle sum tree,\neach node is a (balance, hash) pair. The bottom-layer leaf\nnodes represent the balances and salted username hashes of individual\ncustomers. In each higher-layer node, the balance is the sum of the two\nbalances below, and the hash is the hash of the two nodes below. A\nMerkle sum proof, like a Merkle proof, is a \"branch\" of the tree,\nconsisting of the sister nodes along the path from a leaf to the\nroot.\n\nThe exchange would send each user a Merkle sum proof of their\nbalance. The user would then have a guarantee that their balance is\ncorrectly included as part of the total. A simple example code\nimplementation can be found here.\n\n# The function for computing a parent node given two child nodes\ndef combine_tree_nodes(L, R):\n    L_hash, L_balance = L\n    R_hash, R_balance = R\n    assert L_balance >= 0 and R_balance >= 0\n    new_node_hash = hash(\n        L_hash + L_balance.to_bytes(32, 'big') +\n        R_hash + R_balance.to_bytes(32, 'big')\n    )\n    return (new_node_hash, L_balance + R_balance)\n\n# Builds a full Merkle tree. Stored in flattened form where\n# node i is the parent of nodes 2i and 2i+1\ndef build_merkle_sum_tree(user_table: \"List[(username, salt, balance)]\"):\n    tree_size = get_next_power_of_2(len(user_table))\n    tree = (\n        [None] * tree_size +\n        [userdata_to_leaf(*user) for user in user_table] +\n        [EMPTY_LEAF for _ in range(tree_size - len(user_table))]\n    )\n    for i in range(tree_size - 1, 0, -1):\n        tree[i] = combine_tree_nodes(tree[i*2], tree[i*2+1])\n    return tree\n\n# Root of a tree is stored at index 1 in the flattened form\ndef get_root(tree):\n    return tree[1]\n\n# Gets a proof for a node at a particular index\ndef get_proof(tree, index):\n    branch_length = log2(len(tree)) - 1\n    # ^ = bitwise xor, x ^ 1 = sister node of x\n    index_in_tree = index + len(tree) // 2\n    return [tree[(index_in_tree // 2**i) ^ 1] for i in range(branch_length)]\n\n# Verifies a proof (duh)\ndef verify_proof(username, salt, balance, index, user_table_size, root, proof):\n    leaf = userdata_to_leaf(username, salt, balance)\n    branch_length = log2(get_next_power_of_2(user_table_size)) - 1\n    for i in range(branch_length):\n        if index & (2**i):\n            leaf = combine_tree_nodes(proof[i], leaf)\n        else:\n            leaf = combine_tree_nodes(leaf, proof[i])\n    return leaf == root\n\nPrivacy leakage in this design is much lower than with a fully public\nlist, and it can be decreased further by shuffling the branches each\ntime a root is published, but some privacy leakage is still there:\nCharlie learns that someone has a balance of 164 ETH,\nsome two users have balances that add up to 70 ETH, etc. An\nattacker that controls many accounts could still potentially learn a\nsignificant amount about the exchange's users.\n\nOne important subtlety of the scheme is the possibility of\nnegative balances: what if an exchange that has 1390 ETH of\ncustomer balances but only 890 ETH in reserves tries to make up the\ndifference by adding a -500 ETH balance under a fake account somewhere\nin the tree? It turns out that this possibility does not break the\nscheme, though this is the reason why we specifically need a Merkle sum\ntree and not a regular Merkle tree. Suppose that Henry is the fake\naccount controlled by the exchange, and the exchange puts -500 ETH\nthere:\n\nGreta's proof verification would fail: the exchange would have to\ngive her Henry's -500 ETH node, which she would reject as invalid. Eve\nand Fred's proof verification would also fail, because the\nintermediate node above Henry has -230 total ETH, and so is also\ninvalid! To get away with the theft, the exchange would have to hope\nthat nobody in the entire right half of the tree checks their balance\nproof.\n\nIf the exchange can identify 500 ETH worth of users that they are\nconfident will either not bother to check the proof, or will not be\nbelieved when they complain that they never received a proof, they could\nget away with the theft. But then the exchange could also just exclude\nthose users from the tree and have the same effect. Hence, the Merkle\ntree technique is basically as good as a proof-of-liabilities scheme can\nbe, if only achieving a proof of liabilities is the goal. But its\nprivacy properties are still not ideal. You can go a little bit further\nby using Merkle trees in more clever ways, like making\neach satoshi or wei a separate leaf, but ultimately with more modern\ntech there are even better ways to do it.\nImproving\nprivacy and robustness with ZK-SNARKs\n\nZK-SNARKs are a powerful technology. ZK-SNARKs may be to cryptography\nwhat transformers\nare to AI: a general-purpose technology that is so powerful that it will\ncompletely steamroll a whole bunch of application-specific techniques\nfor a whole bunch of problems that were developed in the decades prior.\nAnd so, of course, we can use ZK-SNARKs to greatly simplify and improve\nprivacy in proof-of-liabilities protocols.\n\nThe simplest thing that we can do is put all users' deposits into a\nMerkle tree (or, even simpler, a KZG\ncommitment), and use a ZK-SNARK to prove that all balances in the\ntree are non-negative and add up to some claimed value. If we add a\nlayer of hashing for privacy, the Merkle branch (or KZG proof) given to\neach user would reveal nothing about the balance of any other\nuser.\n\nUsing KZG commitments is one way to avoid privacy leakage, as there\nis no need to provide \"sister nodes\" as proofs, and a simple ZK-SNARK\ncan be used to prove the sum of the balances and that each balance is\nnon-negative.\n\nWe can prove the sum and non-negativity of balances in the above KZG\nwith a special-purpose ZK-SNARK. Here is one simple example way to do\nthis. We introduce an auxiliary polynomial \\(I(x)\\), which \"builds up the bits\" of each\nbalance (we assume for the sake of example that balances are under \\(2^{15}\\)) and where every 16th position\ntracks a running total with an offset so that it sums to zero only if\nthe actual total matches the declared total. If \\(z\\) is an order-128 root of unity, we might\nprove the equations:\n\n\\(I(z^{16x}) = 0\\)\n\n\\(I(z^{16x + 14}) =\nP(\\omega^{2x+1})\\)\n\n\\(I(z^{i}) - 2*I(z^{i-1}) \\in \\{0, 1\\}\\ \\\nif\\ \\ i\\ \\ mod\\ 16 \\not \\in \\{0, 15\\}\\)\n\n\\(I(z^{16*x + 15}) = I(z^{16*x-1}) +\nI(z^{16*x+14}) - \\frac{the\\ declared\\ total}{user\\ count}\\)\n\nThe first values of a valid setting for \\(I(x)\\) would be 0 0 0 0\n0 0 0 0 0 0 1 2 5 10 20 -165\n0 0 0 0 0 0 0 0 0 1 3 6\n12 25 50 -300 ...\n\nSee here\nand here\nin my post on ZK-SNARKs\nfor further explanation of how to convert equations like these into a\npolynomial check and then into a ZK-SNARK. This isn't an optimal\nprotocol, but it does show how these days these kinds of cryptographic\nproofs are not that spooky!\n\nWith only a few extra equations, constraint systems like this can be\nadapted to more complex settings. For example, in a leverage trading\nsystem, an individual users having negative balances is acceptable but\nonly if they have enough other assets to cover the funds with some\ncollateralization margin. A SNARK could be used to prove this more\ncomplicated constraint, reassuring users that the exchange is not\nrisking their funds by secretly\nexempting other users from the rules.\n\nIn the longer-term future, this kind of ZK proof of liabilities could\nperhaps be used not just for customer deposits at exchanges, but for\nlending more broadly. Anyone taking out a loan would put a record into a\npolynomial or a tree containing that loan, and the root of that\nstructure would get published on-chain. This would let anyone seeking a\nloan ZK-prove to the lender that they have not yet taken out too many\nother loans. Eventually, legal innovation could even make loans that\nhave been committed to in this way higher-priority than loans that have\nnot. This leads us in exactly the same direction as one of the ideas\nthat was discussed in the \"Decentralized\nSociety: Finding Web3's Soul\" paper: a general notion of negative\nreputation or encumberments on-chain through some form of \"soulbound\ntokens\".\n\n## Proof of assets\n\nThe simplest version of proof of assets is the protocol that we saw\nabove: to prove that you hold X coins, you simply move X coins around at\nsome pre-agreed time or in a transaction where the data field contains\nthe words \"these funds belong to Binance\". To avoid paying transaction\nfees, you could sign an off-chain message instead; both Bitcoin and Ethereum\nhave standards for off-chain\nsigned messages.\n\nThere are two practical problems with this simple proof-of-assets\ntechnique:\n\n- Dealing with cold storage\n\n- Collateral dual-use\n\nFor safety reasons, most exchanges keep the great majority of\ncustomer funds in \"cold storage\": on offline computers, where\ntransactions need to be signed and carried over onto the internet\nmanually. Literal air-gapping is common: a cold storage setup that I\nused to use for personal funds involved a permanently offline computer\ngenerating a QR code containing the signed transaction, which I would\nscan from my phone. Because of the high values at stake, the security\nprotocols used by exchanges are crazier still, and often involve using\nmulti-party computation between several devices to further reduce the\nchance of a hack against a single device compromising a key. Given this\nkind of setup, making even a single extra message to prove control of an\naddress is an expensive operation!\n\nThere are several paths that an exchange can take:\n\n- Keep a few public long-term-use addresses. The\nexchange would generate a few addresses, publish a proof of each address\nonce to prove ownership, and then use those addresses\nrepeatedly. This is by far the simplest option, though it does add some\nconstraints in how to preserve security and privacy.\n\n- Have many addresses, prove a few randomly. The\nexchange would have many addresses, perhaps even using each address only\nonce and retiring it after a single transaction. In this case, the\nexchange may have a protocol where from time to time a few addresses get\nrandomly selected and must be \"opened\" to prove ownership. Some\nexchanges already do something like this with an auditor, but in\nprinciple this technique could be turned into a fully automated\nprocedure.\n\n- More complicated ZKP options. For example, an\nexchange could set all of its addresses to be 1-of-2 multisigs, where\none of the keys is different per address, and the other is a blinded\nversion of some \"grand\" emergency backup key stored in some complicated\nbut very high-security way, eg. a 12-of-16 multisig. To preserve privacy\nand avoid revealing the entire set of its addresses, the exchange could\neven run a zero knowledge proof over the blockchain where it proves the\ntotal balance of all addresses on chain that have this format.\n\nThe other major issue is guarding against collateral dual-use.\nShuttling collateral back and forth between each other to do proof of\nreserves is something that exchanges could\neasily do, and would allow them to pretend to be solvent when they\nactually are not. Ideally, proof of solvency would be done in real time,\nwith a proof that updates after every block. If this is impractical, the\nnext best thing would be to coordinate on a fixed schedule between the\ndifferent exchanges, eg. proving reserves at 1400 UTC every Tuesday.\n\nOne final issue is: can you do proof-of-assets on\nfiat? Exchanges don't just hold cryptocurrency, they also hold\nfiat currency within the banking system. Here, the answer is: yes, but\nsuch a procedure would inevitably rely on \"fiat\" trust models: the bank\nitself can attest to balances, auditors can attest to balance sheets,\netc. Given that fiat is not cryptographically verifiable, this is the\nbest that can be done within that framework, but it's still worth\ndoing.\n\nAn alternative approach would be to cleanly separate between one\nentity that runs the exchange and deals with asset-backed stablecoins\nlike USDC, and another entity (USDC itself) that handles the cash-in and\ncash-out process for moving between crypto and traditional banking\nsystems. Because the \"liabilities\" of USDC are just on-chain ERC20\ntokens, proof of liabilities comes \"for free\" and only proof of assets\nis required.\nPlasma and\nvalidiums: can we make CEXes non-custodial?\n\nSuppose that we want to go further: we don't want to just prove that\nthe exchange has the funds to pay back its users. Rather, we\nwant to prevent the exchange from stealing users' funds\ncompletely.\n\nThe first major attempt at this was Plasma, a\nscaling solution that was popular in Ethereum research circles in 2017\nand 2018. Plasma works by splitting up the balance into a set of\nindividual \"coins\", where each coin is assigned an index and lives in a\nparticular position in the Merkle tree of a Plasma block. Making a valid\ntransfer of a coin requires putting a transaction into the correct\nposition of a tree whose root gets published on-chain.\n\nOversimplified diagram of one version of Plasma. Coins are held in a\nsmart contract that enforces the rules of the Plasma protocol at\nwithdrawal time.\n\nOmiseGo attempted to make a decentralized exchange based on this\nprotocol, but since then they have pivoted to other ideas - as has, for\nthat matter, Plasma Group itself, which is now the optimistic EVM rollup\nproject Optimism.\n\nIt's not worth looking at the technical limitations of Plasma as\nconceived in 2018 (eg. proving\ncoin defragmentation) as some kind of morality tale about the whole\nconcept. Since the peak of Plasma discourse in 2018, ZK-SNARKs have\nbecome much more viable for scaling-related use cases, and as we have\nsaid above, ZK-SNARKs change everything.\n\nThe more modern version of the Plasma idea is what Starkware calls a\nvalidium:\nbasically the same as a ZK-rollup, except where data is held off-chain.\nThis construction could be used for a lot of use cases, conceivably\nanything where a centralized server needs to run some code and prove\nthat it's executing code correctly. In a validium, the operator\nhas no way to steal funds, though depending on the details of\nthe implementation some quantity of user funds could get stuck\nif the operator disappears.\n\nThis is all really good: far from CEX vs DEX being a binary, it turns\nout that there is a whole spectrum of options, including various forms\nof hybrid centralization where you gain some benefits like efficiency\nbut still have a lot of cryptographic guardrails preventing the\ncentralized operator from engaging in most forms of abuses.\n\nBut it's worth getting to the fundamental issue with the right half\nof this design space: dealing with user errors. By far\nthe most important type of error is: what if a user forgets their\npassword, loses their devices, gets hacked, or otherwise loses access to\ntheir account?\n\nExchanges can solve this problem: first e-mail recovery, and if even\nthat fails, more complicated forms of recovery through KYC. But to be\nable to solve such problems, the exchange needs to actually have control\nover the coins. In order to have the ability to recover user accounts'\nfunds for good reasons, exchanges need to have power that could also be\nused to steal user accounts' funds for bad reasons. This is an\nunavoidable tradeoff.\n\nThe ideal long-term solution is to rely on self-custody, in a future\nwhere users have easy access to technologies such as multisig and social recovery\nwallets to help deal with emergency situations. But in the short\nterm, there are two clear alternatives that have clearly distinct\ncosts and benefits:\n\nOption\nExchange-side risk\nUser-side risk\n\nCustodial exchange (eg. Coinbase today)\nUser funds may be lost if there is a problem on the exchange\nside\nExchange can help recover account\n\nNon-custodial exchange (eg. Uniswap today)\nUser can withdraw even if exchange acts maliciously\nUser funds may be lost if user screws up\n\nAnother important issue is cross-chain support: exchanges need to\nsupport many different chains, and systems like Plasma and validiums\nwould need to have code written in different languages to support\ndifferent platforms, and cannot be implemented at all on others (notably\nBitcoin) in their current form. In the long-term future, this can\nhopefully be fixed with technological upgrades and standardization; in\nthe short term, however, it's another argument in favor of custodial\nexchanges remaining custodial for now.\nConclusions: the\nfuture of better exchanges\n\nIn the short term, there are two clear \"classes\" of exchanges:\ncustodial exchanges and non-custodial exchanges. Today, the latter\ncategory is just DEXes such as Uniswap, and in the future we may also\nsee cryptographically \"constrained\" CEXes where user funds are held in\nsomething like a validium smart contract. We may also see half-custodial\nexchanges where we trust them with fiat but not cryptocurrency.\n\nBoth types of exchanges will continue to exist, and the easiest\nbackwards-compatible way to improve the safety of custodial exchanges is\nto add proof of reserve. This consists of a combination of proof of\nassets and proof of liabilities. There are technical challenges in\nmaking good protocols for both, but we can and should go as far as\npossible to make headway in both, and open-source the software and\nprocesses as much as possible so that all exchanges can benefit.\n\nIn the longer-term future, my hope is that we move closer and closer\nto all exchanges being non-custodial, at least on the crypto side.\nWallet recovery would exist, and there may need to be highly centralized\nrecovery options for new users dealing with small amounts, as well as\ninstitutions that require such arrangements for legal reasons, but this\ncan be done at the wallet layer rather than within the exchange itself.\nOn the fiat side, movement between the traditional banking system and\nthe crypto ecosystem could be done via cash in / cash out processes\nnative to asset-backed stablecoins such as USDC. However, it will still\ntake a while before we can fully get there.",
    "contentLength": 20792,
    "summary": "Crypto exchanges can use Merkle sum trees or ZK-SNARKs to cryptographically prove they hold enough funds to cover customer deposits.",
    "detailedSummary": {
      "theme": "Vitalik explores cryptographic methods to make centralized exchanges safer through proof of solvency techniques and discusses the spectrum between custodial and non-custodial exchanges.",
      "summary": "Vitalik examines how cryptographic techniques can address the recurring problem of centralized exchange failures by implementing proof of solvency mechanisms. He traces the evolution from early methods like simple balance lists and Merkle trees to more sophisticated ZK-SNARK-based approaches that better preserve privacy while proving exchanges have sufficient funds to cover user deposits. Vitalik discusses both proof of assets (proving the exchange controls claimed funds) and proof of liabilities (proving the total amount owed to users), highlighting technical challenges like cold storage management and preventing collateral dual-use between exchanges. He also explores non-custodial alternatives like Plasma and validiums that could prevent exchanges from stealing funds entirely, but notes the fundamental tradeoff between security and user experience, particularly regarding account recovery when users lose access to their credentials. Vitalik concludes that while both custodial and non-custodial exchanges will coexist in the short term, the goal should be moving toward non-custodial solutions as wallet recovery technologies mature, with proof of reserves serving as an important interim improvement for custodial exchanges.",
      "takeaways": [
        "Proof of solvency combines proof of assets and proof of liabilities to cryptographically demonstrate an exchange can pay back all depositors",
        "ZK-SNARKs represent a major technological advancement that can improve privacy and robustness in proof systems compared to older Merkle tree approaches",
        "There exists a spectrum between fully custodial and fully non-custodial exchanges, with hybrid solutions like validiums offering cryptographic constraints on operators",
        "The fundamental tradeoff between exchange security and user experience centers on account recovery - custodial exchanges can help users recover lost accounts but also have power to steal funds",
        "Long-term vision involves moving toward non-custodial exchanges with improved wallet recovery technologies, while using proof of reserves as a near-term safety improvement for custodial exchanges"
      ],
      "controversial": [
        "The assertion that users should ultimately be responsible for self-custody despite the risks of losing funds through user error",
        "The suggestion that ZK-SNARKs are analogous to transformers in AI as a general-purpose technology that will 'steamroll' existing cryptographic techniques"
      ]
    }
  },
  {
    "id": "general-2022-10-28-revenue_evil",
    "title": "The Revenue-Evil Curve: a different way to think about prioritizing public goods funding",
    "date": "2022-10-28",
    "category": "governance",
    "url": "https://vitalik.eth.limo/general/2022/10/28/revenue_evil.html",
    "path": "general/2022/10/28/revenue_evil.html",
    "content": "- \n\n- \n\n  \n    \n\n  \n  \n  Dark Mode Toggle\n  \n    \n\n  \n\n##  The Revenue-Evil Curve: a different way to think about prioritizing public goods funding \n\n 2022 Oct 28 \nSee all posts\n\n \n \n\n The Revenue-Evil Curve: a different way to think about prioritizing public goods funding \n\nSpecial thanks to Karl Floersch, Hasu and Tina Zhen for feedback\nand review.\n\nPublic goods are an incredibly important topic in any large-scale\necosystem, but they are also one that is often surprisingly tricky to\ndefine. There is an economist definition of public goods - goods that\nare non-excludable and non-rivalrous, two technical terms that taken\ntogether mean that it's difficult to provide them through private\nproperty and market-based means. There is a layman's definition of\npublic good: \"anything that is good for the public\". And there is a\ndemocracy enthusiast's definition of public good, which includes\nconnotations of public participation in decision-making.\n\nBut more importantly, when the abstract category of non-excludable\nnon-rivalrous public goods interacts with the real world, in almost any\nspecific case there are all kinds of subtle edge cases that need to be\ntreated differently. A park is a public good. But what if you add a $5\nentrance fee? What if you fund it by auctioning off the right to have a\nstatue of the winner in the park's central square? What if it's\nmaintained by a semi-altruistic billionaire that enjoys the park for\npersonal use, and designs the park around their personal use, but still\nleaves it open for anyone to visit?\n\nThis post will attempt to provide a different way of analyzing\n\"hybrid\" goods on the spectrum between private and public: the\nrevenue-evil curve. We ask the question: what are the tradeoffs\nof different ways to monetize a given project, and how much good can be\ndone by adding external subsidies to remove the pressure to monetize?\nThis is far from a universal framework: it assumes a \"mixed-economy\"\nsetting in a single monolithic \"community\" with a commercial market\ncombined with subsidies from a central funder. But it can still tell us\na lot about how to approach funding public goods in crypto communities,\ncountries and many other real-world contexts today.\nThe\ntraditional framework: excludability and rivalrousness\n\nLet us start off by understanding how the usual economist lens views\nwhich projects are private vs public goods. Consider the following\nexamples:\n\n- Alice owns 1000 ETH, and wants to sell it on the market.\n\n- Bob runs an airline, and sells tickets for a flight.\n\n- Charlie builds a bridge, and charges a toll to pay for it.\n\n- David makes and releases a podcast.\n\n- Eve makes and releases a song.\n\n- Fred invents a new and better cryptographic algorithm for making\nzero knowledge proofs.\n\nWe put these situations on a chart with two axes:\n\n- Rivalrousness: to what extent does one person\nenjoying the good reduce another person's ability to enjoy it?\n\n- Excludability: how difficult is it to prevent\nspecific individuals, eg. those who do not pay, from enjoying the\ngood?\n\nSuch a chart might look like this:\n\n- Alice's ETH is completely excludable (she has total power to choose\nwho gets her coins), and crypto coins are rivalrous (if one person owns\na particular coin, no one else owns that same coin)\n\n- Bob's plane tickets are excludable, but a tiny bit less rivalrous:\nthere's a chance the plane won't be full.\n\n- Charlie's bridge is a bit less excludable than plane tickets,\nbecause adding a gate to verify payment of tolls takes extra effort (so\nCharlie can exclude but it's costly, both to him and to users), and its\nrivalrousness depends on whether the road is congested or not.\n\n- David's podcast and Eve's song are not rivalrous: one person\nlistening to it does not interfere with another person doing the same.\nThey're a little bit excludable, because you can make a paywall but\npeople can circumvent the paywall.\n\n- And Fred's cryptographic algorithm is close to not excludable at\nall: it needs to be open-source for people to trust it, and if Fred\ntries to patent it, the target user base (open-source-loving crypto\nusers) may well refuse to use the algorithm and even cancel him for\nit.\n\nThis is all a good and important analysis. Excludability tells us\nwhether or not you can fund the project by charging a toll as a business\nmodel, and rivalrousness tells us whether exclusion is a tragic waste or\nif it's just an unavoidable property of the good in question that if one\nperson gets it another does not. But if we look at some of the examples\ncarefully, especially the digital examples, we start to see that it\nmisses a very important issue: there are many business models\navailable other than exclusion, and those business models have\ntradeoffs too.\n\nConsider one particular case: David's podcast versus Eve's song. In\npractice, a huge number of podcasts are released mostly or completely\nfreely, but songs are more often gated with licensing and copyright\nrestrictions. To see why, we need only look at how these podcasts are\nfunded: sponsorships. Podcast hosts typically find a few sponsors, and\ntalk about the sponsors briefly at the start or middle of each episode.\nSponsoring songs is harder: you can't suddenly start talking about how\nawesome Athletic Greens* are in the middle of a\nlove song, because come on, it kills the vibe, man!\n\nCan we get beyond focusing solely on exclusion, and talk about\nmonetization and the harms of different monetization strategies more\ngenerally? Indeed we can, and this is exactly what the revenue/evil\ncurve is about.\nThe revenue-evil curve,\ndefined\n\nThe revenue-evil curve of a product is a two-dimensional curve that\nplots the answer to the following question:\n\nHow much harm would the product's creator have to inflict on\ntheir potential users and the wider community to earn $N of revenue to\npay for building the product?\n\nThe word \"evil\" here is absolutely not meant to imply that no\nquantity of evil is acceptable, and that if you can't fund a project\nwithout committing evil you should not do it at all. Many projects make\nhard tradeoffs that hurt their customers and community in order to\nensure sustainable funding, and often the value of the project existing\nat all greatly outweighs these harms. But nevertheless, the goal is to\nhighlight that there is a tragic aspect to many monetization\nschemes, and public goods funding can provide value by giving existing\nprojects a financial cushion that enables them to avoid such\nsacrifices.\n\nHere is a rough attempt at plotting the revenue-evil curves of our\nsix examples above:\n\n- For Alice, selling her ETH at market price is actually the\nmost compassionate thing she could do. If she sells more\ncheaply, she will almost certainly create an on-chain gas war, trader\nHFT war, or other similarly value-destructive financial conflict between\neveryone trying to claim her coins the fastest. Selling above market\nprice is not even an option: no one would buy.\n\n- For Bob, the socially-optimal price to sell at is the highest price\nat which all tickets get sold out. If Bob sells below that price,\ntickets will sell out quickly and some people will not be able to get\nseats at all even if they really need them (underpricing may have a few\ncountervailing benefits by giving opportunities to poor people, but it\nis far from the most\nefficient way to achieve that goal). Bob could also sell\nabove market price and potentially earn a higher profit at the\ncost of selling fewer seats and (from the god's-eye perspective)\nneedlessly excluding people.\n\n- If Charlie's bridge and the road leading to it are uncongested,\ncharging any toll at all imposes a burden and needlessly excludes\ndrivers. If they are congested, low tolls help by reducing congestion\nand high tolls needlessly exclude people.\n\n- David's podcast can monetize to some extent without hurting\nlisteners much by adding advertisements from sponsors. If pressure to\nmonetize increases, David would have to adopt more and more intrusive\nforms of advertising, and truly maxing out on revenue would require\npaywalling the podcast, a high cost to potential listeners.\n\n- Eve is in the same position as David, but with fewer low-harm\noptions (perhaps selling an NFT?). Especially in Eve's case, paywalling\nmay well require actively participating in the legal apparatus of\ncopyright enforcement and suing infringers, which carries further\nharms.\n\n- Fred has even fewer monetization options. He could patent it, or\npotentially do exotic things like auction off the right to choose\nparameters so that hardware manufacturers that favor particular values\nwould bid on it. All options are high-cost.\n\nWhat we see here is that there are actually many kinds of \"evil\" on\nthe revenue-evil curve:\n\n- Traditional economic\ndeadweight loss from exclusion: if a product is priced\nabove marginal cost, mutually beneficial transactions that could have\ntaken place do not take place\n\n- Race conditions: congestion, shortages and other\ncosts from products being too cheap.\n\n- \"Polluting\" the product in ways that make it\nappealing to a sponsor, but is harmful to a (maybe small, maybe large)\ndegree to listeners.\n\n- Engaging in offensive actions through the legal\nsystem, which increases everyone's fear and need to spend money\non lawyers, and has all kinds of hard-to-predict secondary chilling\neffects. This is particularly severe in the case of patenting.\n\n- Sacrificing on principles highly valued by the\nusers, the community and even the people working on the project\nitself.\n\nIn many cases, this evil is very context-dependent. Patenting is both\nextremely harmful and ideologically offensive within the crypto space\nand software more broadly, but this is less true in industries building\nphysical goods: in physical goods industries, most people who\nrealistically can create a derivative work of something patented are\ngoing to be large and well-organized enough to negotiate for a license,\nand capital costs mean that the need for monetization is much higher and\nhence maintaining purity is harder. To what extent advertisements are\nharmful depends on the advertiser and the audience: if the podcaster\nunderstands the audience very well, ads can even be helpful!\nWhether or not the possibility to \"exclude\" even exists depends on\nproperty rights.\n\nBut by talking about committing evil for the sake of earning revenue\nin general terms, we gain the ability to compare these situations\nagainst each other.\nWhat\ndoes the revenue-evil curve tell us about funding prioritization?\n\nNow, let's get back to the key question of why we care about what is\na public good and what is not: funding prioritization. If we have a\nlimited pool of capital that is dedicated to helping a community\nprosper, which things should we direct funding to? The revenue-evil\ncurve graphic gives us a simple starting point for an answer:\ndirect funds toward those projects where the slope of the\nrevenue-evil curve is the steepest.\n\nWe should focus on projects where each $1 of subsidies, by reducing\nthe pressure to monetize, most greatly reduces the evil that is\nunfortunately required to make the project possible. This gives us\nroughly this ranking:\n\n- Top of the line are \"pure\" public goods, because\noften there aren't any ways to monetize them at all, or if there are,\nthe economic or moral costs of trying to monetize are extremely\nhigh.\n\n- Second priority is \"naturally\" public but monetizable\ngoods that can be funded through commercial channels by\ntweaking them a bit, like songs or sponsorships to a podcast.\n\n- Third priority is non-commodity-like private goods\nwhere social welfare is already optimized by charging a fee,\nbut where profit margins are high or more generally there are\nopportunities to \"pollute\" the product to increase revenue, eg. by\nkeeping accompanying software closed-source or refusing to use\nstandards, and subsidies could be used to push such projects to make\nmore pro-social choices on the margin.\n\nNotice that the excludability and rivalrousness framework usually\noutputs similar answers: focus on non-excludable and non-rivalrous goods\nfirst, excludable goods but non-rivalrous second, and excludable and\npartially rivalrous goods last - and excludable and rivalrous goods\nnever (if you have capital left over, it's better to just give it out as\na UBI). There is a rough approximate mapping between revenue/evil curves\nand excludability and rivalrousness: higher excludability means lower\nslope of the revenue/evil curve, and rivalrousness tells us whether the\nbottom of the revenue/evil curve is zero or nonzero. But the\nrevenue/evil curve is a much more general tool, which allows us to talk\nabout tradeoffs of monetization strategies that go far beyond\nexclusion.\n\nOne practical example of how this framework can be used to analyze\ndecision-making is Wikimedia donations. I personally have never donated\nto Wikimedia, because I've always thought that they could and should\nfund themselves without relying on limited public-goods-funding capital\nby just adding a few advertisements, and this would be only a small cost\nto their user experience and neutrality. Wikipedia admins, however,\ndisagree; they even have a\nwiki page listing their arguments why they disagree.\n\nWe can understand this disagreement as a dispute over revenue-evil\ncurves: I think Wikimedia's revenue-evil curve has a low slope (\"ads are\nnot that bad\"), and therefore they are low priority for my charity\ndollars; some other people think their revenue-evil curve has a high\nslope, and therefore they are high priority for their charity\ndollars.\nRevenue-evil\ncurves are an intellectual tool, NOT a good direct mechanism\n\nOne important conclusion that it is important NOT to take from this\nidea is that we should try to use revenue-evil curves directly as a way\nof prioritizing individual projects. There are severe constraints on our\nability to do this because of limits to monitoring.\n\nIf this framework is widely used, projects would have an incentive to\nmisrepresent their revenue-evil curves. Anyone charging a toll would\nhave an incentive to come up with clever arguments to try to show that\nthe world would be much better if the toll could be 20% lower, but\nbecause they're desperately under-budget, they just can't lower the toll\nwithout subsidies. Projects would have an incentive to be more\nevil in the short term, to attract subsidies that help them become less\nevil.\n\nFor these reasons, it is probably best to use the framework not as a\nway to allocate decisions directly, but to identify general principles\nfor what kinds of projects to prioritize funding for. For example, the\nframework can be a valid way to determine how to prioritize whole\nindustries or whole categories of goods. It can help you answer\nquestions like: if a company is producing a public good, or is making\npro-social but financially costly choices in the design of a\nnot-quite-public good, do they deserve subsidies for that? But even\nhere, it's better to treat revenue-evil curves as a mental tool, rather\nthan attempting to precisely measure them and use them to make\nindividual decisions.\n\n## Conclusions\n\nExcludability and rivalrousness are important dimensions of a good,\nthat have really important consequences for its ability to monetize\nitself, and for answering the question of how much harm can be averted\nby funding it out of some public pot. But especially once more complex\nprojects enter the fray, these two dimensions quickly start to become\ninsufficient for determining how to prioritize funding. Most things are\nnot pure public goods: they are some hybrid in the middle, and there are\nmany dimensions on which they could become more or less public that do\nnot easily map to \"exclusion\".\n\nLooking at the revenue-evil curve of a project gives us another way\nof measuring the statistic that really matters: how much harm can be\naverted by relieving a project of one dollar of monetization pressure?\nSometimes, the gains from relieving monetization pressure are decisive:\nthere just is no way to fund certain kinds of things through commercial\nchannels, until you can find one single user that benefits from them\nenough to fund them unilaterally. Other times, commercial funding\noptions exist, but have harmful side effects. Sometimes these effects\nare smaller, sometimes they are greater. Sometimes a small piece of an\nindividual project has a clear tradeoff between pro-social choices and\nincreasing monetization. And, still other times, projects just fund\nthemselves, and there is no need to subsidize them - or at least,\nuncertainties and hidden information make it too hard to create a\nsubsidy schedule that does more good than harm. It's always better to\nprioritize funding in order of greatest gains to smallest; and how far\nyou can go depends on how much funding you have.\n\n * I did not accept sponsorship money from\nAthletic Greens. But the\npodcaster Lex Fridman did. And no, I did not accept sponsorship money\nfrom Lex Fridman either. But maybe someone else did. Whatevs man, as\nlong as we can keep getting podcasts funded so they can be\nfree-to-listen without annoying people too much, it's all good, you\nknow?",
    "contentLength": 17120,
    "summary": "The post proposes a \"revenue-evil curve\" framework to analyze public goods by plotting how much harm creators must inflict to earn funding versus seeking external subsidies instead.",
    "detailedSummary": {
      "theme": "Vitalik proposes a new framework called the 'revenue-evil curve' to better prioritize public goods funding by analyzing the tradeoffs between monetization strategies and their harmful effects on communities.",
      "summary": "Vitalik argues that the traditional economic framework of categorizing goods by excludability and rivalrousness is insufficient for real-world funding decisions, especially in complex digital projects. He introduces the 'revenue-evil curve' concept, which plots how much harm a project creator must inflict on users and communities to earn different levels of revenue. This framework reveals that there are many types of 'evil' beyond simple exclusion, including deadweight economic loss, product pollution through advertising, offensive legal actions like patents, and compromising community values. Vitalik suggests that funding should prioritize projects where subsidies can most effectively reduce harmful monetization pressures - typically pure public goods first, followed by naturally public but monetizable goods, then private goods with high profit margins or opportunities for harmful modifications. He emphasizes this should be used as an intellectual tool for identifying general principles rather than a direct mechanism for individual project funding decisions, due to monitoring limitations and potential gaming by projects seeking subsidies.",
      "takeaways": [
        "Traditional excludability/rivalrousness frameworks miss important nuances in how projects can monetize and the various harms these strategies can cause",
        "The revenue-evil curve measures how much harm must be inflicted to earn different revenue levels, providing a more comprehensive view of monetization tradeoffs",
        "Funding should prioritize projects where each dollar of subsidy most greatly reduces harmful monetization pressure, typically favoring pure public goods first",
        "Many forms of 'evil' exist beyond exclusion, including advertising pollution, patent enforcement, congestion from underpricing, and compromising community values",
        "This framework should be used as a mental tool for general principles rather than precise measurement for individual funding decisions, to avoid gaming and misrepresentation"
      ],
      "controversial": [
        "Vitalik's stance that Wikipedia should fund itself through advertisements rather than donations, disagreeing with Wikimedia's position on maintaining neutrality",
        "The characterization of various business practices as 'evil' may be seen as overly judgmental of legitimate monetization strategies"
      ]
    }
  },
  {
    "id": "general-2022-09-20-daos",
    "title": "DAOs are not corporations: where decentralization in autonomous organizations matters",
    "date": "2022-09-20",
    "category": "governance",
    "url": "https://vitalik.eth.limo/general/2022/09/20/daos.html",
    "path": "general/2022/09/20/daos.html",
    "content": "- \n\n- \n\n  \n    \n\n  \n  \n  Dark Mode Toggle\n  \n    \n\n  \n\n##  DAOs are not corporations: where decentralization in autonomous organizations matters \n\n 2022 Sep 20 \nSee all posts\n\n \n \n\n DAOs are not corporations: where decentralization in autonomous organizations matters \n\nSpecial thanks to Karl Floersch and Tina Zhen for feedback and\nreview on earlier versions of this article.\n\nRecently, there has been a lot of discourse\naround the idea\nthat highly decentralized DAOs do not\nwork, and DAO governance should\nstart to more\nclosely resemble that of traditional\ncorporations in order to remain competitive. The argument is always\nsimilar: highly decentralized governance is inefficient, and traditional\ncorporate governance structures with boards, CEOs and the like evolved\nover hundreds of years to optimize for the goal of making good decisions\nand delivering value to shareholders in a changing world. DAO idealists\nare naive to assume that egalitarian ideals of decentralization can\noutperform this, when attempts to do this in the traditional corporate\nsector have had marginal success at best.\n\nThis post will argue why this position is often wrong, and offer a\ndifferent and more detailed perspective about where different kinds of\ndecentralization are important. In particular, I will focus on\nthree types of situations where decentralization is\nimportant:\n\n- Decentralization for making better decisions in concave\nenvironments, where pluralism and even naive forms of\ncompromise are on average likely to outperform the kinds of coherency\nand focus that come from centralization.\n\n- Decentralization for censorship resistance:\napplications that need to continue functioning while resisting attacks\nfrom powerful external actors.\n\n- Decentralization as credible fairness: applications\nwhere DAOs are taking on nation-state-like functions like basic\ninfrastructure provision, and so traits like predictability, robustness\nand neutrality are valued above efficiency.\n\nCentralization\nis convex, decentralization is concave\n\nSee the original post: ../../../2020/11/08/concave.html\n\nOne way to categorize decisions that need to be made is to look at\nwhether they are convex or concave. In\na choice between A and B, we would first look not at the question of A\nvs B itself, but instead at a higher-order question: would you rather\ntake a compromise between A and B or a coin flip? In\nexpected utility terms, we can express this distinction using a\ngraph:\n\nIf a decision is concave, we would prefer a compromise, and if it's\nconvex, we would prefer a coin flip. Often, we can answer the\nhigher-order question of whether a compromise or a coin flip is better\nmuch more easily than we can answer the first-order question of A vs B\nitself.\n\nExamples of convex decisions include:\n\n- Pandemic response: a 100% travel ban may work at\nkeeping a virus out, a 0% travel ban won't stop viruses but at least\ndoesn't inconvenience people, but a 50% or 90% travel ban is the worst\nof both worlds.\n\n- Military strategy: attacking on front A may make\nsense, attacking on front B may make sense, but splitting your army in\nhalf and attacking at both just means the enemy can easily deal with the two\nhalves one by one\n\n- Technology choices in crypto protocols: using\ntechnology A may make sense, using technology B may make sense, but some\nhybrid between the two often just leads to needless complexity and even\nadds risks of the two interfering with each\nother.\n\nExamples of concave decisions include:\n\n- Judicial decisions: an average between two\nindependently chosen judgements is probably more likely to be fair, and\nless likely to be completely ridiculous, than a random choice of one of\nthe two judgements.\n\n- Public goods funding: usually, giving $X to each of\ntwo promising projects is more effective than giving $2X to one and\nnothing to the other. Having any money at all gives a much bigger boost\nto a project's ability to achieve its mission than going from $X to $2X\ndoes.\n\n- Tax rates: because of quadratic\ndeadweight loss mechanics, a tax rate of X% is often only a\nquarter as harmful as a tax rate of 2X%, and at the same time\nmore than half as good at raising revenue. Hence, moderate\ntaxes are better than a coin flip between low/no taxes and high\ntaxes.\n\nWhen decisions are convex, decentralizing the process of making that\ndecision can easily lead to confusion and low-quality compromises. When\ndecisions are concave, on the other hand, relying on the wisdom of the\ncrowds can give better answers. In these cases, DAO-like\nstructures with large amounts of diverse input going into\ndecision-making can make a lot of sense. And indeed, people who see the\nworld as a more concave place in general are more likely to see\na need for decentralization in a wider variety of contexts.\nShould VitaDAO and\nUkraine DAO be DAOs?\n\nMany of the more recent DAOs differ from earlier DAOs, like MakerDAO,\nin that whereas the earlier DAOs are organized around providing\ninfrastructure, the newer DAOs are organized around performing\nvarious tasks around a particular theme. VitaDAO is a DAO funding early-stage\nlongevity research, and UkraineDAO\nis a DAO organizing and funding efforts related to helping Ukrainian\nvictims of war and supporting the Ukrainian defense effort. Does it make\nsense for these to be DAOs?\n\nThis is a nuanced question, and we can get a view of one possible\nanswer by understanding the internal workings of UkraineDAO itself.\nTypical DAOs tend to \"decentralize\" by gathering large amounts of\ncapital into a single pool and using token-holder voting to fund each\nallocation. UkraineDAO, on the other hand, works by splitting its\nfunctions up into many\npods, where each pod works as independently as possible. A top layer\nof governance can create new pods (in principle, governance can also\nfund pods, though so far funding has only gone to external\nUkraine-related organizations), but once a pod is made and endowed with\nresources, it functions largely on its own. Internally, individual pods\ndo have leaders and function in a more centralized way, though they\nstill try to respect an ethos of personal autonomy.\n\nOne natural question that one might ask is: isn't this kind\nof \"DAO\" just rebranding the traditional concept of multi-layer\nhierarchy? I would say this depends on the implementation: it's\ncertainly possible to take this template and turn it into something that\nfeels authoritarian in the same way stereotypical large corporations do,\nbut it's also possible to use the template in a very different way.\n\nTwo things that can help ensure that an organization built this way\nwill actually turn out to be meaningfully decentralized include:\n\n- A truly high level of autonomy for pods, where the\npods accept resources from the core and are occasionally checked for\nalignment and competence if they want to keep getting those resources,\nbut otherwise act entirely on their own and don't \"take orders\" from the\ncore.\n\n- Highly decentralized and diverse core governance.\nThis does not require a\n\"governance token\", but it does require broader and more diverse\nparticipation in the core. Normally, broad and diverse participation is\na large tax on efficiency. But if (1) is satisfied, so pods are highly\nautonomous and the core needs to make fewer decisions, the effects of\ntop-level governance being less efficient become smaller.\n\nNow, how does this fit into the \"convex vs concave\" framework? Here,\nthe answer is roughly as follows: the (more decentralized) top\nlevel is concave, the (more centralized within each pod) bottom level is\nconvex. Giving a pod $X is generally better than a coin flip\nbetween giving it $0 and giving it $2X, and there isn't a large loss\nfrom having compromises or \"inconsistent\" philosophies guiding different\ndecisions. But within each individual pod, having a clear opinionated\nperspective guiding decisions and being able to insist on many choices\nthat have synergies with each other is much more important.\nDecentralization and\ncensorship resistance\n\nThe most often publicly cited reason for decentralization in crypto\nis censorship resistance: a DAO or protocol needs to be able to function\nand defend itself despite external attack, including from large\ncorporate or even state actors. This has already been publicly\ntalked about at length, and so deserves less elaboration, but there\nare still some important nuances.\n\nTwo of the most successful censorship-resistant services that large\nnumbers of people use today are The\nPirate Bay and Sci-Hub. The Pirate\nBay is a hybrid system: it's a search engine for BitTorrent, which is a\nhighly decentralized network, but the search engine itself is\ncentralized. It has a small core team that is dedicated to keeping it\nrunning, and it defends itself with the mole's strategy in whack-a-mole:\nwhen the hammer comes down, move out of the way and re-appear somewhere\nelse. The Pirate Bay and Sci-Hub have both frequently changed domain\nnames, relied on arbitrage between different jurisdictions, and used all\nkinds of other techniques. This strategy is centralized, but it has\nallowed them both to be successful both at defense and at\nproduct-improvement agility.\n\nDAOs do not act like The Pirate Bay and Sci-Hub; DAOs act like\nBitTorrent. And there is a reason why BitTorrent does\nneed to be decentralized: it requires not just censorship resistance,\nbut also long-term investment and reliability. If BitTorrent\ngot shut down once a year and required all its seeders and users to\nswitch to a new provider, the network would quickly degrade in quality.\nCensorship resistance-demanding DAOs should also be in the same\ncategory: they should be providing a service that isn't just evading\npermanent censorship, but also evading mere instability and disruption.\nMakerDAO (and the Reflexer DAO\nwhich manages RAI) are excellent examples of this. A DAO running a\ndecentralized search engine probably does not: you can just build a\nregular search engine and use Sci-Hub-style techniques to ensure its\nsurvival.\nDecentralization as\ncredible fairness\n\nSometimes, DAOs' primary concern is not a need to resist\nnation states, but rather a need to take on some of the\nfunctions of nation states. This often involves tasks that can be\ndescribed as \"maintaining basic infrastructure\". Because governments\nhave less ability to oversee DAOs, DAOs need to be structured to take on\na greater ability to oversee themselves. And this requires\ndecentralization.\n\nOf course, it's not actually possible to come anywhere\nclose to eliminating hierarchy and inequality of information and\ndecision-making power in its entirety etc etc etc, but what if we can\nget even 30% of the way there?\n\nConsider three motivating examples: algorithmic stablecoins, the Kleros court, and the Optimism\nretroactive funding mechanism.\n\n- An algorithmic stablecoin DAO is a system that uses\non-chain financial contracts to create a crypto-asset whose price tracks\nsome stable index, often but not necessarily the US dollar.\n\n- Kleros is a \"decentralized\ncourt\": a DAO whose function is to give rulings on\narbitration questions such as \"is this Github commit an acceptable\nsubmission to this on-chain bounty?\"\n\n- Optimism's retroactive funding mechanism is a\ncomponent of the Optimism DAO\nwhich retroactively rewards projects that have provided value to the\nEthereum and Optimism ecosystems.\n\nIn all three cases, there is a need to make subjective judgements,\nwhich cannot be done automatically through a piece of on-chain code. In\nthe first case, the goal is simply to get reasonably accurate\nmeasurements of some price index. If the stablecoin tracks the US\ndollar, then you just need the ETH/USD price. If hyperinflation or some\nother reason to abandon the US dollar arises, the stablecoin DAO might\nneed to manage a trustworthy on-chain CPI calculation. Kleros is all\nabout making unavoidably subjective judgements on any arbitrary question\nthat is submitted to it, including whether or not submitted questions\nshould be rejected\nfor being \"unethical\". Optimism's retroactive funding is tasked with\none of the most open-ended subjective questions at all: what projects\nhave done work that is the most useful to the Ethereum and Optimism\necosystems?\n\nAll three cases have an unavoidable need for \"governance\", and pretty\nrobust governance too. In all cases, governance being attackable, from\nthe outside or the inside, can easily lead to very big problems.\nFinally, the governance doesn't just need to be robust, it\nneeds to credibly convince a large and untrusting public that\nit is robust.\nThe\nalgorithmic stablecoin's Achilles heel: the oracle\n\nAlgorithmic stablecoins depend on oracles. In order for an on-chain\nsmart contract to know whether to target the value of DAI to 0.005 ETH\nor 0.0005 ETH, it needs some mechanism to learn the\n(external-to-the-chain) piece of information of what the ETH/USD price\nis. And in fact, this \"oracle\" is the primary place at which an\nalgorithmic stablecoin can be attacked.\n\nThis leads to a security conundrum: an algorithmic stablecoin cannot\nsafely hold more collateral, and therefore cannot issue more units, than\nthe market cap of its speculative token (eg. MKR, FLX...), because if it\ndoes, then it becomes profitable to buy up half the speculative token\nsupply, use those tokens to control the oracle, and steal funds from\nusers by feeding bad oracle values and liquidating them.\n\nHere is a possible alternative design for a stablecoin oracle: add\na layer of indirection. Quoting the ethresear.ch post:\n\nWe set up a contract where there are 13 \"providers\"; the answer to a\nquery is the median of the answer returned by these providers. Every\nweek, there is a vote, where the oracle token holders can replace one of\nthe providers ...\n\nThe security model is simple: if you trust the voting mechanism, you\ncan trust the oracle output, unless 7 providers get corrupted at the\nsame time. If you trust the current set of oracle providers, you can\ntrust the output for at least the next six weeks, even if you completely\ndo not trust the voting mechanism. Hence, if the voting mechanism gets\ncorrupted, there will be able time for participants in any applications\nthat depend on the oracle to make an orderly exit.\n\nNotice the very un-corporate-like nature of this proposal. It\ninvolves taking away the governance's ability to act quickly,\nand intentionally spreading out oracle responsibility across a large\nnumber of participants. This is valuable for two reasons. First, it\nmakes it harder for outsiders to attack the oracle, and for new coin\nholders to quickly take over control of the oracle. Second, it makes it\nharder for the oracle participants themselves to collude to\nattack the system. It also mitigates oracle extractable value,\nwhere a single provider might intentionally delay publishing to\npersonally profit from a liquidation (in a multi-provider system, if one\nprovider doesn't immediately publish, others soon will).\n\n## Fairness in Kleros\n\nThe \"decentralized court\" system Kleros is a really valuable and\nimportant piece of infrastructure for the Ethereum ecosystem: Proof of Humanity uses it,\nvarious \"smart contract bug insurance\" products use it, and many other\nprojects plug into it as some kind of \"adjudication of last resort\".\n\nRecently, there have been some public concerns about whether or not\nthe platform's decision-making is fair. Some participants have made\ncases, trying to claim a payout from decentralized smart contract\ninsurance platforms that they argue they deserve. Perhaps the most\nfamous of these cases is Mizu's\nreport on case #1170. The case blew up from being a minor language\nintepretation dispute into a broader scandal because of the accusation\nthat insiders to Kleros itself were making a coordinated effort to throw\na large number of tokens to pushing the decision in the direction they\nwanted. A participant to the debate writes:\n\nThe incentives-based decision-making process of the court ... is by all\nappearances being corrupted by a single dev with a very large (25%)\nstake in the courts.\n\nOf course, this is but one side of one issue in a broader debate, and\nit's up to the Kleros community to figure out who is right or wrong and\nhow to respond. But zooming out from the question of this individual\ncase, what is important here is the the extent to which the entire\nvalue proposition of something like Kleros depends on it being able\nto convince the public that it is strongly protected against this kind\nof centralized manipulation. For something like Kleros to be trusted, it\nseems necessary that there should not be a single individual with a 25%\nstake in a high-level court. Whether through a more widely distributed\ntoken supply, or through more use of non-token-driven governance, a more\ncredibly decentralized form of governance could help Kleros avoid such\nconcerns entirely.\n\n## Optimism retro funding\n\nOptimism's retroactive\nfounding round 1 results were chosen by a quadratic vote among 24\n\"badge holders\". Round 2 will likely use a larger number of badge\nholders, and the eventual goal is to move to a system where a much larger body\nof citizens control retro funding allocation, likely through some\nmultilayered mechanism involving sortition, subcommittees and/or\ndelegation.\n\nThere have been some internal debates about whether to have more vs\nfewer citizens: should \"citizen\" really mean something closer\nto \"senator\", an expert contributor who deeply understands the Optimism\necosystem, should it be a position given out to just about\nanyone who has significantly participated in the Optimism\necosystem, or somewhere in between? My personal stance on this\nissue has always been in the direction of more citizens, solving\ngovernance inefficiency issues with second-layer delegation instead of\nadding enshrined centralization into the governance protocol. One key\nreason for my position is the potential for insider trading and\nself-dealing issues.\n\nThe Optimism retroactive funding mechanism has always been intended\nto be coupled with a prospective speculation ecosystem:\npublic-goods projects that need funding now could sell \"project\ntokens\", and anyone who buys project tokens becomes eligible for a large\nretroactively-funded compensation later. But this mechanism working well\ndepends crucially on the retroactive funding part working correctly, and\nis very vulnerable to the retroactive funding mechanism\nbecoming corrupted. Some example attacks:\n\n- If some group of people has decided how they will vote on some\nproject, they can buy up (or if overpriced, short) its project token\nahead of releasing the decision.\n\n- If some group of people knows that they will later adjudicate on\nsome specific project, they can buy up the project token early and then\nintentionally vote in its favor even if the project does not actually\ndeserve funding.\n\n- Funding deciders can accept bribes from projects.\n\nThere are typically three ways of dealing with these types of\ncorruption and insider trading issues:\n\n- Retroactively punish malicious deciders.\n\n- Proactively filter for higher-quality deciders.\n\n- Add more deciders.\n\nThe corporate world typically focuses on the first two, using\nfinancial surveillance and judicious penalties for the first and\nin-person interviews and background checks for the second. The\ndecentralized world has less access to such tools: project tokens are\nlikely to be tradeable anonymously, DAOs have at best limited recourse\nto external judicial systems, and the remote and online nature of the\nprojects and the desire for global inclusivity makes it harder to do\nbackground checks and informal in-person \"smell tests\" for character.\nHence, the decentralized world needs to put more weight on the third\ntechnique: distribute decision-making power among more\ndeciders, so that each individual decider has less power, and so\ncollusions are more likely to be whistleblown on and revealed.\nShould\nDAOs learn more from corporate governance or political science?\n\nCurtis Yarvin, an American philosopher whose primary \"big idea\" is\nthat corporations are much more effective and optimized than governments\nand so we should improve governments by making them look more like\ncorporations (eg. by moving away from democracy and closer to monarchy),\nrecently wrote an article expressing his\nthoughts on how DAO governance should be designed. Not surprisingly,\nhis answer involves borrowing ideas from governance of traditional\ncorporations. From his introduction:\n\nInstead the basic design of the Anglo-American limited-liability\njoint-stock company has remained roughly unchanged since the start of\nthe Industrial Revolution\u2014which, a contrarian historian might argue,\nmight actually have been a Corporate Revolution. If the joint-stock\ndesign is not perfectly optimal, we can expect it to be nearly\noptimal.\n\nWhile there is a categorical difference between these two types of\norganizations\u2014we could call them first-order (sovereign) and\nsecond-order (contractual) organizations\u2014it seems that society in the\ncurrent year has very effective second-order organizations, but not very\neffective first-order organizations.\n\nTherefore, we probably know more about second-order organizations.\nSo, when designing a DAO, we should start from corporate governance, not\npolitical science.\n\nYarvin's post is very correct in identifying the key difference\nbetween \"first-order\" (sovereign) and \"second-order\" (contractual)\norganizations - in fact, that exact distinction is precisely the topic\nof the section in my own post above on credible fairness. However,\nYarvin's post makes a big, and surprising, mistake immediately after, by\nimmediately pivoting to saying that corporate governance is the better\nstarting point for how DAOs should operate. The mistake is surprising\nbecause the logic of the situation seems to almost directly imply the\nexact opposite conclusion. Because DAOs do not have a sovereign\nabove them, and are often explicitly in the business of providing\nservices (like currency and arbitration) that are typically reserved for\nsovereigns, it is precisely the design of sovereigns (political\nscience), and not the design of corporate governance, that DAOs have\nmore to learn from.\n\nTo Yarvin's credit, the second part of his post does\nadvocate an \"hourglass\" model that combines a decentralized alignment\nand accountability layer and a centralized management and execution\nlayer, but this is already an admission that DAO design needs to learn\nat least as much from first-order orgs as from second-order orgs.\n\nSovereigns are inefficient and corporations are efficient for the\nsame reason why number theory can prove very many things but abstract group theory can\nprove much fewer things: corporations fail less and accomplish\nmore because they can make more assumptions and have more powerful tools\nto work with. Corporations can count on their local sovereign\nto stand up to defend them if the need arises, as well as to provide an\nexternal legal system they can lean on to stabilize their incentive\nstructure. In a sovereign, on the other hand, the biggest challenge is\noften what to do when the incentive structure is under attack and/or at\nrisk of collapsing entirely, with no external leviathan standing ready\nto support it.\n\nPerhaps the greatest problem in the design of successful governance\nsystems for sovereigns is what Samo Burja calls\n\"the succession problem\": how to ensure continuity as the system\ntransitions from being run by one group of humans to another group as\nthe first group retires. Corporations, Burja writes, often just don't\nsolve the problem at all:\n\nSilicon Valley enthuses over \"disruption\" because we have become so\nused to the succession problem remaining unsolved within discrete\ninstitutions such as companies.\n\nDAOs will need to solve the succession problem eventually (in fact,\ngiven the sheer frequency of the \"get rich and retire\" pattern among\ncrypto early adopters, some DAOs have to deal with succession issues\nalready). Monarchies and corporate-like forms often have a hard\ntime solving the succession problem, because the institutional structure\ngets deeply tied up with the habits of one specific person, and it\neither proves difficult to hand off, or there is a very-high-stakes\nstruggle over whom to hand it off to. More decentralized political forms\nlike democracy have at least a theory of how smooth transitions can\nhappen. Hence, I would argue that for this reason too, DAOs have more to\nlearn from the more liberal and democratic schools of political science\nthan they do from the governance of corporations.\n\nOf course, DAOs will in some cases have to accomplish specific\ncomplicated tasks, and some use of corporate-like forms for\naccomplishing those tasks may well be a good idea. Additionally, DAOs\nneed to handle unexpected uncertainty. A system that was intended to\nfunction in a stable and unchanging way around one set of assumptions,\nwhen faced with an extreme and unexpected change to those circumstances,\ndoes need some kind of brave leader to coordinate a response. A\nprototypical example of the latter is stablecoins handling a US dollar\ncollapse: what happens when a stablecoin DAO that evolved around the\nassumption that it's just trying to track the US dollar suddenly faces a\nworld where the US dollar is no longer a viable thing to be tracking,\nand a rapid switch to some kind of CPI is needed?\n\nStylized diagram of the internal experience of the RAI ecosystem\ngoing through an unexpected transition to a CPI-based regime if the USD\nceases to be a viable reference asset.\n\nHere, corporate governance-inspired approaches may seem better,\nbecause they offer a ready-made pattern for responding to such a\nproblem: the founder organizes a pivot. But as it turns out, the history\nof political systems also offers a pattern well-suited to this\nsituation, and one that covers the question of how to go back to a\ndecentralized mode when the crisis is over: the Roman Republic custom of\nelecting a\ndictator for a temporary term to respond to a crisis.\n\nRealistically, we probably only need a small number of DAOs\nthat look more like constructs from political science than something out\nof corporate governance. But those are the really important\nones. A stablecoin does not need to be efficient; it must first\nand foremost be stable and decentralized. A decentralized court is\nsimilar. A system that directs funding for a particular cause - whether\nOptimism retroactive funding, VitaDAO, UkraineDAO or something else - is\noptimizing for a much more complicated purpose than profit maximization,\nand so an alignment solution other than shareholder profit is needed to\nmake sure it keeps using the funds for the purpose that was\nintended.\n\nBy far the greatest number of organizations, even in a crypto world,\nare going to be \"contractual\" second-order organizations that\nultimately lean on these first-order giants for support, and for these\norganizations, much simpler and leader-driven forms of governance\nemphasizing agility are often going to make sense. But this should not\ndistract from the fact that the ecosystem would not survive without some\nnon-corporate decentralized forms keeping the whole thing\nstable.",
    "contentLength": 27155,
    "summary": "DAOs work best for concave decisions (where compromise/averaging improves outcomes), censorship-resistant services, and fair infrastructure provision.",
    "detailedSummary": {
      "theme": "Vitalik argues that DAOs should not imitate corporate governance structures, but instead should embrace decentralization in specific contexts where it provides unique advantages over traditional centralized approaches.",
      "summary": "Vitalik challenges the common criticism that decentralized autonomous organizations (DAOs) are inefficient and should adopt corporate-style governance. He argues that decentralization is valuable in three key scenarios: making better decisions in 'concave' environments where compromise outperforms focused centralization, achieving censorship resistance against powerful external actors, and providing credible fairness in infrastructure-like services. Vitalik uses the mathematical concept of convex versus concave decisions to illustrate when centralization (convex decisions like military strategy) versus decentralization (concave decisions like judicial rulings or public goods funding) is more effective. He examines real examples like algorithmic stablecoins, Kleros court system, and Optimism's retroactive funding to demonstrate how decentralization provides crucial benefits like attack resistance and public trust. Vitalik concludes that while most organizations may benefit from corporate-style efficiency, the crypto ecosystem requires some truly decentralized 'first-order' organizations that function more like political sovereigns than corporations to provide stable foundational infrastructure.",
      "takeaways": [
        "Decentralization works best for 'concave' decisions where compromise solutions are superior to winner-take-all approaches, while centralization suits 'convex' decisions requiring focused execution",
        "DAOs providing infrastructure services need decentralization for censorship resistance and credible neutrality, not just efficiency optimization",
        "Algorithmic stablecoins, decentralized courts, and funding mechanisms require distributed governance to prevent insider manipulation and maintain public trust",
        "DAOs should learn more from political science than corporate governance since they often function as sovereign entities without external oversight",
        "A hybrid approach with decentralized oversight and centralized execution pods can balance the benefits of both governance models"
      ],
      "controversial": [
        "Vitalik's direct criticism of Curtis Yarvin's philosophy that corporations are superior to governments and his argument for the opposite in DAO contexts",
        "The suggestion that some level of inefficiency in DAOs is acceptable and even beneficial compared to corporate efficiency",
        "The implicit criticism of existing DAO governance structures, particularly calling out specific issues in Kleros court system governance"
      ]
    }
  },
  {
    "id": "general-2022-09-17-layer_3",
    "title": "What kind of layer 3s make sense?",
    "date": "2022-09-17",
    "category": "applications",
    "url": "https://vitalik.eth.limo/general/2022/09/17/layer_3.html",
    "path": "general/2022/09/17/layer_3.html",
    "content": "- \n\n- \n\n  \n    \n\n  \n  \n  Dark Mode Toggle\n  \n    \n\n  \n\n##  What kind of layer 3s make sense? \n\n 2022 Sep 17 \nSee all posts\n\n \n \n\n What kind of layer 3s make sense? \n\nSpecial thanks to Georgios Konstantopoulos, Karl Floersch and the\nStarkware team for feedback and review.\n\nOne topic that often re-emerges in layer-2 scaling discussions is the\nconcept of \"layer 3s\". If we can build a layer 2 protocol that anchors\ninto layer 1 for security and adds scalability on top, then surely we\ncan scale even more by building a layer 3 protocol that anchors\ninto layer 2 for security and adds even more scalability on top\nof that?\n\nA simple version of this idea goes: if you have a scheme that can\ngive you quadratic scaling, can you stack the scheme on top of itself\nand get exponential scaling? Ideas like this include my\n2015 scalability paper, the multi-layer scaling ideas\nin the Plasma paper, and many more. Unfortunately, such simple\nconceptions of layer 3s rarely quite work out that easily. There's\nalways something in the design that's just not stackable, and can only\ngive you a scalability boost once - limits to data availability,\nreliance on L1 bandwidth for emergency withdrawals, or many other\nissues.\n\nNewer ideas around layer 3s, such as the framework\nproposed by Starkware, are more sophisticated: they aren't just\nstacking the same thing on top of itself, they're assigning the second\nlayer and the third layer different purposes. Some form of this approach\nmay well be a good idea - if it's done in the right way. This post will\nget into some of the details of what might and might not make sense to\ndo in a triple-layered architecture.\nWhy\nyou can't just keep scaling by stacking rollups on top of rollups\n\nRollups (see my longer article on them here) are a scaling\ntechnology that combines different techniques to address the two main\nscaling bottlenecks of running a blockchain: computation and\ndata. Computation is addressed by either fraud proofs or SNARKs, which rely on a very\nsmall number of actors to process and verify each block, requiring\neveryone else to perform only a tiny amount of computation to check that\nthe proving process was done correctly. These schemes, especially\nSNARKs, can scale almost without limit; you really can just keep making\n\"a SNARK of many SNARKs\" to scale even more computation down to\na single proof.\n\nData is different. Rollups use a collection\nof compression tricks to reduce the amount of data that a\ntransaction needs to store on-chain: a simple currency transfer\ndecreases from ~100 to ~16 bytes, an ERC20 transfer in an EVM-compatible\nchain from\n~180 to ~23 bytes, and a privacy-preserving ZK-SNARK transaction\ncould be compressed from ~600 to ~80 bytes. About 8x compression in all\ncases. But rollups still need to make data available on-chain in a\nmedium that users are guaranteed to be able to access and verify, so\nthat users can independently compute the state of the rollup and join as\nprovers if existing provers go offline. Data can be compressed once, but\nit cannot be compressed again - if it can, then there's generally a way\nto put the logic of the second compressor into the first, and get the\nsame benefit by compressing once. Hence, \"rollups on top of rollups\" are\nnot something that can actually provide large gains in scalability -\nthough, as we will see below, such a pattern can serve other\npurposes.\nSo what's the \"sane\"\nversion of layer 3s?\n\nWell, let's look at what Starkware, in their post\non layer 3s, advocates. Starkware is made up of very smart\ncryptographers who are actually sane, and so if they are advocating for\nlayer 3s, their version will be much more sophisticated than \"if rollups\ncompress data 8x, then obviously rollups on top of rollups will\ncompress data 64x\".\n\nHere's a diagram from Starkware's post:\n\nA few quotes:\n\nAn example of such an ecosystem is depicted in Diagram 1. Its L3s\ninclude:\n\n- A StarkNet with Validium data availability, e.g., for general use by\napplications with extreme sensitivity to pricing.\n\n- App-specific StarkNet systems customized for better application\nperformance, e.g., by employing designated storage structures or data\navailability compression.\n\n- StarkEx systems (such as those serving dYdX, Sorare, Immutable, and\nDeversiFi) with Validium or Rollup data availability, immediately\nbringing battle-tested scalability benefits to StarkNet.\n\n- Privacy StarkNet instances (in this example also as an L4) to allow\nprivacy-preserving transactions without including them in public\nStarkNets.\n\nWe can compress the article down into three visions of what \"L3s\" are\nfor:\n\n- L2 is for scaling, L3 is for customized functionality, for\nexample privacy. In this vision there is no attempt to provide\n\"scalability squared\"; rather, there is one layer of the stack that\nhelps applications scale, and then separate layers for customized\nfunctionality needs of different use cases.\n\n- L2 is for general-purpose scaling, L3 is for customized\nscaling. Customized scaling might come in different forms:\nspecialized applications that use something other than the EVM to do\ntheir computation, rollups whose data compression is optimized around\ndata formats for specific applications (including separating \"data\" from\n\"proofs\" and replacing proofs with a single SNARK per block entirely),\netc.\n\n- L2 is for trustless scaling (rollups), L3 is for\nweakly-trusted scaling (validiums). Validiums\nare systems that use SNARKs to verify computation, but leave data\navailability up to a trusted third party or committee. Validiums are in\nmy view highly underrated: in particular, many \"enterprise blockchain\"\napplications may well actually be best served by a centralized server\nthat runs a validium prover and regularly commits hashes to chain.\nValidiums have a lower grade of security than rollups, but can be vastly\ncheaper.\n\nAll three of these visions are, in my view, fundamentally reasonable.\nThe idea that specialized data compression requires its own platform is\nprobably the weakest of the claims - it's quite easy to design a layer 2\nwith a general-purpose base-layer compression scheme that users can\nautomatically extend with application-specific sub-compressors - but\notherwise the use cases are all sound. But this still leaves open one\nlarge question: is a three-layer structure the right way to\naccomplish these goals? What's the point of validiums, and privacy\nsystems, and customized environments, anchoring into layer 2 instead of\njust anchoring into layer 1? The answer to this question turns out to be\nquite complicated.\n\nWhich one is actually better?\n\nDoes\ndepositing and withdrawing become cheaper and easier within a layer 2's\nsub-tree?\n\nOne possible argument for the three-layer model over the two-layer\nmodel is: a three-layer model allows an entire sub-ecosystem to exist\nwithin a single rollup, which allows cross-domain operations within that\necosystem to happen very cheaply, without needing to go through the\nexpensive layer 1.\n\nBut as it turns out, you can do deposits and withdrawals cheaply even\nbetween two layer 2s (or even layer 3s) that commit to the same layer 1!\nThe key realization is that tokens and other assets do not have\nto be issued in the root chain. That is, you can have an ERC20\ntoken on Arbitrum, create a wrapper of it on Optimism, and move back and\nforth between the two without any L1 transactions!\n\nLet us examine how such a system works. There are two smart\ncontracts: the base contract on Arbitrum, and the wrapper\ntoken contract on Optimism. To move from Arbitrum to Optimism, you\nwould send your tokens to the base contract, which would generate a\nreceipt. Once Arbitrum finalizes, you can take a Merkle proof of that\nreceipt, rooted in L1 state, and send it into the wrapper token contract\non Optimism, which verifies it and issues you a wrapper token. To move\ntokens back, you do the same thing in reverse.\n\nEven though the Merkle path needed to prove the deposit on\nArbitrum goes through the L1 state, Optimism only needs to read the L1\nstate root to process the deposit - no L1 transactions required. Note\nthat because data on rollups is the scarcest resource, a practical\nimplementation of such a scheme would use a SNARK or a KZG proof, rather\nthan a Merkle proof directly, to save space.\n\nSuch a scheme has one key weakness compared to tokens rooted on L1,\nat least on optimistic rollups: depositing also requires waiting the\nfraud proof window. If a token is rooted on L1, withdrawing from\nArbitrum or Optimism back to L1 takes a week delay, but depositing is\ninstant. In this scheme, however, both depositing and withdrawing take a\nweek delay. That said, it's not clear that a three-layer architecture on\noptimistic rollups is better: there's a lot of technical complexity in\nensuring that a fraud proof game happening inside a system that itself\nruns on a fraud proof game is safe.\n\nFortunately, neither of these issues will be a problem on ZK rollups.\nZK rollups do not require a week-long waiting window for security\nreasons, but they do still require a shorter window (perhaps 12 hours\nwith first-generation technology) for two other reasons. First,\nparticularly the more complex general-purpose ZK-EVM rollups need a longer\namount of time to cover non-parallelizable compute time of proving a\nblock. Second, there is the economic consideration of needing to submit\nproofs rarely to minimize the fixed costs associated with proof\ntransactions. Next-gen ZK-EVM technology, including specialized\nhardware, will solve the first problem, and better-architected batch\nverification can solve the second problem. And it's precisely the issue\nof optimizing and batching proof submission that we will get into\nnext.\nRollups\nand validiums have a confirmation time vs fixed cost tradeoff. Layer 3s\ncan help fix this. But what else can?\n\nThe cost of a rollup per transaction is cheap: it's just\n16-60 bytes of data, depending on the application. But rollups also have\nto pay a high fixed cost every time they submit a batch of\ntransactions to chain: 21000\nL1 gas per batch for optimistic rollups, and more than 400,000 gas\nfor ZK rollups (millions of gas if you want something quantum-safe that\nonly uses STARKs).\n\nOf course, rollups could simply choose to wait until there's 10\nmillion gas worth of L2 transactions to submit a batch, but this would\ngive them very long batch intervals, forcing users to wait much longer\nuntil they get a high-security confirmation. Hence, they have a\ntradeoff: long batch intervals and optimum costs, or shorter batch\nintervals and greatly increased costs.\n\nTo give us some concrete numbers, let us consider a ZK rollup that\nhas 600,000 gas per-batch costs and processes fully optimized ERC20\ntransfers (23 bytes), which cost 368 gas per transaction. Suppose that\nthis rollup is in early to mid stages of adoption, and is averaging 5\nTPS. We can compute gas per transaction vs batch intervals:\n\nBatch interval\nGas per tx (= tx cost + batch cost / (TPS * batch interval))\n\n12s (one per Ethereum block)\n10368\n\n1 min\n2368\n\n10 min\n568\n\n1 h\n401\n\nIf we're entering a world with lots of customized validiums and\napplication-specific environments, then many of them will do much less\nthan 5 TPS. Hence, tradeoffs between confirmation time and cost start to\nbecome a very big deal. And indeed, the \"layer 3\" paradigm does solve\nthis! A ZK rollup inside a ZK rollup, even implemented naively, would\nhave fixed costs of only ~8,000 layer-1 gas (500 bytes for the proof).\nThis changes the table above to:\n\nBatch interval\nGas per tx (= tx cost + batch cost / (TPS * batch interval))\n\n12s (one per Ethereum block)\n501\n\n1 min\n394\n\n10 min\n370\n\n1 h\n368\n\nProblem basically solved. So are layer 3s good? Maybe. But it's worth\nnoticing that there is a different approach to solving this problem,\ninspired by ERC\n4337 aggregate verification.\n\nThe strategy is as follows. Today, each ZK rollup or validium accepts\na state root if it receives a proof proving that \\(S_{new} = STF(S_{old}, D)\\): the new state\nroot must be the result of correctly processing the transaction data or\nstate deltas on top of the old state root. In this new scheme, the ZK\nrollup would accept a message from a batch verifier contract\nthat says that it has verified a proof of a batch of statements, where\neach of those statements is of the form \\(S_{new} = STF(S_{old}, D)\\). This batch\nproof could be constructed via a recursive SNARK scheme or Halo aggregation.\n\nThis would be an open protocol: any ZK-rollup could join, and any\nbatch prover could aggregate proofs from any compatible ZK-rollup, and\nwould get compensated by the aggregator with a transaction fee. The\nbatch handler contract would verify the proof once, and then pass off a\nmessage to each rollup with the \\((S_{old},\nS_{new}, D)\\) triple for that rollup; the fact that the triple\ncame from the batch handler contract would be evidence that the\ntransition is valid.\n\nThe cost per rollup in this scheme could be close to 8000 if it's\nwell-optimized: 5000 for a state write adding the new update, 1280 for\nthe old and new root, and an extra 1720 for miscellaneous data juggling.\nHence, it would give us the same savings. Starkware actually has\nsomething like this already, called SHARP,\nthough it is not (yet) a permissionless open protocol.\n\nOne response to this style of approach might be: but isn't this\nactually just another layer 3 scheme? Instead of\nbase layer <- rollup <- validium, you have\nbase layer <- batch mechanism <- rollup or validium.\nFrom some philosophical architectural standpoint, this may be true. But\nthere is an important difference: instead of the middle layer being a\ncomplicated full EVM system, the middle layer is a simplified and highly\nspecialized object, and so it is more likely to be secure, it is more\nlikely to be built at all without needing yet another specialized token,\nand it is more likely to be governance-minimized and not change over\ntime.\nConclusion: what even is a\n\"layer\"?\n\nA three-layer scaling architecture that consists of stacking the\nsame scaling scheme on top of itself generally does not work well.\nRollups on top of rollups, where the two layers of rollups use the same\ntechnology, certainly do not. A three-layer architecture where the\nsecond layer and third layer have different purposes, however,\ncan work. Validiums on top of rollups do make sense, even if they're not\ncertain to be the long-term best way of doing things.\n\nOnce we start getting into the details of what kind of\narchitecture makes sense, however, we get into the philosophical\nquestion: what is a \"layer\" and what is not? The\nbase layer <- batch mechanism <- rollup or validium\npattern does the same job as a\nbase layer <- rollup <- rollup or validium pattern.\nBut in terms of how it works, a proof aggregation layer looks\nmore like ERC-4337\nthan like a rollup. Typically, we don't refer to ERC-4337 as a \"layer\n2\". Similarly, we don't refer to Tornado Cash as a \"layer 2\" - and so if\nwe were to be consistent, we would not refer to a privacy-focused\nsub-system that lives on top of a layer 2 as a layer 3. So there is an\nunresolved semantics debate of what deserves the title of \"layer\" in the\nfirst place.\n\nThere are many possible schools of thought on this. My personal\npreference would be to keep the term \"layer 2\" restricted to things with\nthe following properties:\n\n- Their purpose is to increase scalability\n\n- They follow the \"blockchain within a blockchain\" pattern: they have\ntheir own mechanism for processing transactions and their own internal\nstate\n\n- They inherit the full security of the Ethereum chain\n\nSo, optimistic rollups and ZK rollups are layer 2s, but validiums,\nproof aggregation schemes, ERC 4337, on-chain privacy systems and\nSolidity are something else. It may make sense to call some of them\nlayer 3s, but probably not all of them; in any case, it seems premature\nto settle definitions while the architecture of the multi-rollup\necosystem is far from set in stone and most of the discussion is\nhappening only in theory.\n\nThat said, the language debate is less important than the technical\nquestion of which constructions actually make the most sense. There is\nclearly an important role to be played by \"layers\" of some kind that\nserve non-scaling needs like privacy, and there is clearly is an\nimportant function of proof aggregation that needs to be filled\nsomehow, and preferably by an open protocol. But at the same\ntime, there are good technical reasons to make the intermediary layers\nthat connect user-facing environments to the layer 1 as simple as\npossible; the \"glue layer\" being an EVM rollup is probably not the right\napproach in many cases. I suspect more sophisticated (and simpler)\nconstructions such as those described in this post will start to have a\nbigger role to play as the layer 2 scaling ecosystem matures.",
    "contentLength": 16802,
    "summary": "Starkware proposes sophisticated layer 3s that assign different purposes to L2 (scaling) and L3 (custom functionality/privacy/validiums).",
    "detailedSummary": {
      "theme": "Vitalik explores which types of layer 3 blockchain architectures make technical sense, arguing against simple stacking approaches while supporting specialized multi-layer designs.",
      "summary": "Vitalik begins by explaining why naive approaches to layer 3s - simply stacking the same scaling technology on top of itself - don't work, particularly because data compression cannot be meaningfully compressed again even though computation can scale through recursive proofs. He then examines Starkware's more sophisticated approach, which assigns different purposes to each layer rather than just stacking identical systems. Vitalik identifies three reasonable visions for layer 3s: using L3 for customized functionality like privacy while L2 handles scaling; using L3 for customized scaling while L2 provides general-purpose scaling; and using L3 for weakly-trusted scaling (validiums) while L2 handles trustless scaling (rollups). He analyzes the tradeoffs between confirmation time and fixed costs in rollups, showing how layer 3s can help solve this problem, while also proposing alternative solutions like proof aggregation protocols that might achieve similar benefits with simpler architectures.",
      "takeaways": [
        "Simply stacking rollups on top of rollups using the same technology doesn't provide meaningful scaling benefits because data can only be compressed once",
        "Layer 3s make sense when each layer serves different purposes - such as L2 for scaling and L3 for customized functionality like privacy or specialized applications",
        "Cross-layer token transfers can be done cheaply between different L2s without requiring L1 transactions through wrapper token contracts and Merkle proofs",
        "Layer 3s can solve the confirmation time vs fixed cost tradeoff that rollups face, but proof aggregation protocols might achieve similar benefits with simpler architecture",
        "The definition of what constitutes a 'layer' remains philosophically unclear, with Vitalik suggesting L2s should be scalability-focused, follow a 'blockchain within blockchain' pattern, and inherit full Ethereum security"
      ],
      "controversial": [
        "Vitalik's restrictive definition of what qualifies as 'layer 2' excludes validiums, which some in the community consider legitimate L2 solutions",
        "His suggestion that proof aggregation schemes like SHARP might be better than traditional layer 3 architectures could be seen as dismissive of current L3 development efforts"
      ]
    }
  },
  {
    "id": "general-2022-09-09-ens",
    "title": "Should there be demand-based recurring fees on ENS domains?",
    "date": "2022-09-09",
    "category": "applications",
    "url": "https://vitalik.eth.limo/general/2022/09/09/ens.html",
    "path": "general/2022/09/09/ens.html",
    "content": "- \n\n- \n\n  \n    \n\n  \n  \n  Dark Mode Toggle\n  \n    \n\n  \n\n##  Should there be demand-based recurring fees on ENS domains? \n\n 2022 Sep 09 \nSee all posts\n\n \n \n\n Should there be demand-based recurring fees on ENS domains? \n\nSpecial thanks to Lars Doucet, Glen Weyl and Nick Johnson for\ndiscussion and feedback on various topics.\n\nENS domains today are cheap. Very cheap. The cost to register and\nmaintain a five-letter domain name is only $5\nper year. This sounds reasonable from the perspective of one person\ntrying to register a single domain, but it looks very different when you\nlook at the situation globally: when ENS was younger, someone could have\nregistered all 8938 five-letter words in the Scrabble\nwordlist (which includes exotic stuff like \"BURRS\", \"FLUYT\" and\n\"ZORIL\") and pre-paid their ownership for a hundred years, all for the\nprice of a\ndozen lambos. And in fact, many people did: today, almost all of\nthose five-letter words are already taken, many by squatters waiting for\nsomeone to buy the domain from them at a much higher price. A random\nscrape of OpenSea shows that about 40% of all these domains are for sale\nor have been sold on that platform alone.\n\nThe question worth asking is: is this really the best way to allocate\ndomains? By selling off these domains so cheaply, ENS DAO is almost\ncertainly gathering far less revenue than it could, which limits its\nability to act to improve the ecosystem. The status quo is also bad for\nfairness: being able to buy up all the domains cheaply was great for\npeople in 2017, is okay in 2022, but the consequences may severely\nhandicap the system in 2050. And given that buying a five-letter-word\ndomain in practice costs anywhere from 0.1 to 500\nETH, the notionally cheap registration prices are not actually\nproviding cost savings to users. In fact, there are deep\neconomic reasons\nto believe that reliance on secondary markets makes domains\nmore expensive than a well-designed in-protocol mechanism.\n\nCould we allocate ongoing ownership of domains in a better way? Is\nthere a way to raise more revenue for ENS DAO, do a better job\nof ensuring domains go to those who can make best use of them, and at\nthe same time preserve the credible neutrality and the accessible very\nstrong guarantees of long-term ownership that make ENS valuable?\nProblem\n1: there is a fundamental tradeoff between strength of property rights\nand fairness\n\nSuppose that there are \\(N\\)\n\"high-value names\" (eg. five-letter words in the Scrabble dictionary,\nbut could be any similar category). Suppose that each year, users grab\nup \\(k\\) names, and some portion \\(p\\) of them get grabbed by someone who's\nirrationally stubborn and not willing to give them up (\\(p\\) could be really low, it just needs to\nbe greater than zero). Then, after \\(\\frac{N}{k * p}\\) years, no one will be\nable to get a high-value name again.\n\nThis is a two-line mathematical theorem, and it feels too simple to\nbe saying anything important. But it actually gets at a crucial truth:\ntime-unlimited allocation of a finite resource is incompatible with\nfairness across long time horizons. This is true for land; it's the\nreason why there have been so\nmany land reforms throughout history, and it's a big part of why\nmany advocate for land taxes\ntoday. It's also true for domains, though the problem in the\ntraditional domain space has been temporarily alleviated by a \"forced\ndilution\" of early .com holders in the form of a mass introduction of\n.io, .me, .network and many other domains. ENS has\nsoft-committed to not add new TLDs to avoid polluting the global\nnamespace and rupturing its chances of eventual integration with\nmainstream DNS, so such a dilution is not an option.\n\nFortunately, ENS charges not just a one-time fee to register a\ndomain, but also a recurring annual fee to maintain it. Not all\ndecentralized domain name systems had the foresight to implement this;\nUnstoppable Domains did not, and even goes so far as to proudly\nadvertise its preference for short-term consumer appeal over long-term\nsustainability (\"No renewal\nfees ever!\"). The recurring fees in ENS and traditional DNS are a\nhealthy mitigation to the worst excesses of a truly unlimited\npay-once-own-forever model: at the very least, the recurring fees mean\nthat no one will be able to accidentally lock down a domain forever\nthrough forgetfulness or carelessness. But it may not be enough. It's\nstill possible to spend $500 to lock down an ENS domain for an entire\ncentury, and there are certainly some types of domains that are in high\nenough demand that this is vastly underpriced.\nProblem\n2: speculators do not actually create efficient markets\n\nOnce we admit that a first-come-first-serve model with low fixed fees\nhas these problems, a common counterargument is to say: yes, many of the\nnames will get bought up by speculators, but speculation is natural and\ngood. It is a free market mechanism, where speculators who actually want\nto maximize their profit are motivated to resell the domain in such a\nway that it goes to whoever can make the best use of the domain, and\ntheir outsized returns are just compensation for this service.\n\nBut as it turns out, there has been academic research on this topic,\nand it is not actually true that profit-maximizing auctioneers maximize\nsocial welfare! Quoting Myerson\n1981:\n\nBy announcing a reservation price of 50, the seller risks a\nprobability \\((1 / 2^n)\\) of keeping\nthe object even though some bidder is willing to pay more than \\(t_0\\) for it; but the seller also increases\nhis expected revenue, because he can command a higher price when the\nobject is sold.\n\nThus the optimal auction may not be ex-post efficient. To see more\nclearly why this can happen, consider the example in the above\nparagraph, for the case when \\(n = 1\\)\n... Ex post efficiency would require that the bidder must always get the\nobject, as long as his value estimate is positive. But then the bidder\nwould never admit to more than an infinitesimal value estimate, since\nany positive bid would win the object ... In fact the seller's optimal\npolicy is to refuse to sell the object for less than 50.\n\nTranslated into diagram form:\n\nMaximizing revenue for the seller almost always requires accepting\nsome probability of never selling the domain at all, leaving it unused\noutright. One important nuance in the argument is that\nseller-revenue-maximizing auctions are at their most inefficient when\nthere is one possible buyer (or at least, one buyer with a valuation far\nabove the others), and the inefficiency decreases quickly once there are\nmany competing potential buyers. But for a large class of domains, the\nfirst category is precisely the situation they are in. Domains that are\nsimply some person, project or company's name, for example, have one\nnatural buyer: that person or project. And so if a speculator buys up\nsuch a name, they will of course set the price high, accepting a large\nchance of never coming to a deal to maximize their revenue in the case\nwhere a deal does arise.\n\nHence, we cannot say that speculators grabbing a large portion of\ndomain allocation revenues is merely just compensation for them ensuring\nthat the market is efficient. On the contrary, speculators can easily\nmake the market worse than a well-designed mechanism in the\nprotocol that encourages domains to be directly available for sale at\nfair prices.\nOne\ncheer for stricter property rights: stability of domain ownership has\npositive externalities\n\nThe monopoly problems of overly-strict property rights on\nnon-fungible assets have been known for a long time. Resolving this\nissue in a market-based way was the original goal of Harberger\ntaxes: require the owner of each covered asset to set a price at\nwhich they are willing to sell it to anyone else, and charge an annual\nfee based on that price. For example, one could charge 0.5% of the sale\nprice every year. Holders would be incentivized to leave the asset\navailable for purchase at prices that are reasonable, \"lazy\" holders who\nrefuse to sell would lose money every year, and hoarding assets without\nusing them would in many cases become economically infeasible\noutright.\n\nBut the risk of being forced to sell something at any time can have\nlarge economic and psychological costs, and it's for this reason that\nadvocates of Harberger taxes generally focus on industrial property\napplications where the market participants are sophisticated. Where do\ndomains fall on the spectrum? Let us consider the costs of a business\ngetting \"relocated\", in three separate cases: a data center, a\nrestaurant, and an ENS name.\n\nData center\nRestaurant\nENS name\n\nConfusion from people expecting old location\nAn employee comes to the old location, and unexpectedly finds it\nclosed.\nAn employee or a customer comes to the old location, and\nunexpectedly finds it closed.\nSomeone sends a big chunk of money to the wrong address.\n\nLoss of location-specific long-term investment\nLow\nThe restaurant will probably lose many long-term customers for whom\nthe new location is too far away\nThe owner spent years building a brand around the old name that\ncannot easily carry over.\n\nAs it turns out, domains do not hold up very well. Domain name owners\nare often not sophisticated, the costs of switching domain names are\noften high, and negative externalities of a name-change gone wrong can\nbe large. The highest-value owner of coinbase.eth may not\nbe Coinbase; it could just as easily be a scammer who would grab up the\ndomain and then immediately make a fake charity or ICO claiming it's run\nby Coinbase and ask people to send that address their money. For\nthese reasons, Harberger taxing domains is not a great\nidea.\nAlternative\nsolution 1: demand-based recurring pricing\n\nMaintaining ownership over an ENS domain today requires paying a\nrecurring fee. For most domains, this is a simple and very low $5 per\nyear. The only exceptions are four-letter domains ($160 per year) and\nthree-letter domains ($640 per year). But what if instead, we make the\nfee somehow depend on the actual level of market demand for the\ndomain?\n\nThis would not be a Harberger-like scheme where you have to make the\ndomain available for immediate sale at a particular price.\nRather, the initiative in the price-setting procedure would fall on the\nbidders. Anyone could bid on a particular domain, and if they keep an\nopen bid for a sufficiently long period of time (eg. 4 weeks), the\ndomain's valuation rises to that level. The annual fee on the domain\nwould be proportional to the valuation (eg. it might be set to 0.5% of\nthe valuation). If there are no bids, the fee might decay at a constant\nrate.\n\nWhen a bidder sends their bid amount into a smart contract to place a\nbid, the owner has two options: they could either accept the bid, or\nthey could reject, though they may have to start paying a higher price.\nIf a bidder bids a value higher than the actual value of the domain, the\nowner could sell to them, costing the bidder a huge amount of money.\n\nThis property is important, because it means that \"griefing\"\ndomain holders is risky and expensive, and may even end up benefiting\nthe victim. If you own a domain, and a powerful actor wants to\nharass or censor you, they could try to make a very high bid for that\ndomain to greatly increase your annual fee. But if they do this, you\ncould simply sell to them and collect the massive payout.\n\nThis already provides much more stability and is more noob-friendly\nthan a Harberger tax. Domain owners don't need to constantly worry\nwhether or not they're setting prices too low. Rather, they can simply\nsit back and pay the annual fee, and if someone offers to bid they can\ntake 4 weeks to make a decision and either sell the domain or continue\nholding it and accept the higher fee. But even this probably does not\nprovide quite enough stability. To go even further, we need a compromise\non the compromise.\nAlternative\nsolution 2: capped demand-based recurring pricing\n\nWe can modify the above scheme to offer even stronger guarantees to\ndomain-name holders. Specifically, we can try to offer the following\nproperty:\n\nStrong time-bound ownership guarantee: for any fixed\nnumber of years, it's always possible to compute a fixed amount of money\nthat you can pre-pay to unconditionally guarantee ownership for at\nleast that number of years.\n\nIn math language, there must be some function \\(y = f(n)\\) such that if you pay \\(y\\) dollars (or ETH), you get a hard\nguarantee that you will be able to hold on to the domain for at least\n\\(n\\) years, no matter what happens.\n\\(f\\) may also depend on other\nfactors, such as what happened to the domain previously, as long as\nthose factors are known at the time the transaction to register or\nextend a domain is made. Note that the maximum annual fee after\n\\(n\\) years would be the derivative\n\\(f'(n)\\).\n\nThe new price after a bid would be capped at the implied maximum\nannual fee. For example, if \\(f(n) =\n\\frac{1}{2}n^2\\), so \\(f'(n) =\nn\\), and you get a bid of $5 after 7 years, the annual fee would\nrise to $5, but if you get a bid of $10 after 7 years, the annual fee\nwould only rise to $7. If no bids that raise the fee to the max are made\nfor some length of time (eg. a full year), \\(n\\) resets. If a bid is made and rejected,\n\\(n\\) resets.\n\nAnd of course, we have a highly subjective criterion that \\(f(n)\\) must be \"reasonable\". We can create\ncompromise proposals by trying different shapes for \\(f\\):\n\nType\n\\(f(n)\\) (\\(p_0\\) = price of last sale or last rejected\nbid, or $1 if most recent event is a reset)\nIn plain English\nTotal cost to guarantee holding for >= 10 years\nTotal cost to guarantee holding for >= 100 years\n\nExponential fee growth\n\\(f(n) = \\int_0^n p_0 *\n1.1^n\\)\nThe fee can grow by a maximum of 10% per year (with\ncompounding).\n$836\n$7.22m\n\nLinear fee growth\n\\(f(n) = p_0 * n +\n\\frac{15}{2}n^2\\)\nThe annual fee can grow by a maximum of $15 per year.\n$1250\n$80k\n\nCapped annual fee\n\\(f(n) = 640 * n\\)\nThe annual fee cannot exceed $640 per year. That is, a domain in\nhigh demand can start to cost as much as a three-letter domain, but not\nmore.\n$6400\n$64k\n\nOr in chart form:\n\nNote that the amounts in the table are only the theoretical\nmaximums needed to guarantee holding a domain for that number of\nyears. In practice, almost no domains would have bidders willing to bid\nvery high amounts, and so holders of almost all domains would end up\npaying much less than the maximum.\n\nOne fascinating property of the \"capped annual fee\" approach\nis that there are versions of it that are strictly more\nfavorable to existing domain-name holders than the status\nquo. In particular, we could imagine a system where a domain\nthat gets no bids does not have to pay any annual fee, and a\nbid could increase the annual fee to a maximum of $5 per year.\n\nDemand from external bids clearly provides some signal about\nhow valuable a domain is (and therefore, to what extent an owner is\nexcluding others by maintaining control over it). Hence,\nregardless of your views on what level of fees should be\nrequired to maintain a domain, I would argue that you should find\nsome parameter choice for demand-based fees\nappealing.\n\nI will still make my case for why some superlinear \\(f(n)\\), a max annual fee that goes up over\ntime, is a good idea. First, paying more for longer-term security is a\ncommon feature throughout the economy. Fixed-rate mortgages usually have\nhigher\ninterest rates than variable-rate mortgages. You can get\nhigher interest by providing deposits that are locked up for longer\nperiods of time; this is compensation the bank pays you for\nproviding longer-term security to the bank. Similarly,\nlonger-term government bonds typically have higher yields.\nSecond, the annual fee should be able to eventually adjust to\nwhatever the market value of the domain is; we just don't want that to\nhappen too quickly.\n\nSuperlinear \\(f(n)\\) values still\nmake hard guarantees of ownership reasonably accessible over pretty long\ntimescales: with the linear-fee-growth formula \\(f(n) = p_0 * n + \\frac{15}{2}n^2\\), for\nonly $6000 ($120 per year) you could ensure ownership of the domain for\n25 years, and you would almost certainly pay much less. The ideal of\n\"register and forget\" for censorship-resistant services would still be\nvery much available.\n\n## From here to there\n\nWeakening property norms, and increasing fees, is psychologically\nvery unappealing to many people. This is true even when these fees make\nclear economic sense, and even when you can redirect fee revenue into a\nUBI and mathematically show that the majority of people would\neconomically net-benefit from your proposal. Cities have a hard time adding\ncongestion pricing, even when it's painfully clear that the only two\nchoices are paying congestion fees in dollars and paying congestion fees\nin wasted time and\nweakened mental health driving in painfully slow traffic. Land value taxes, despite being in\nmany ways one\nof the most effective and least harmful taxes out there, have a hard\ntime getting adopted. Unstoppable Domains's loud and proud advertisement\nof \"no renewal fees ever\" is in my view very short-sighted, but it's\nclearly at least somewhat effective. So how could I possibly\nthink that we have any chance of adding fees and conditions to domain\nname ownership?\n\nThe crypto space is not going to solve deep challenges in human\npolitical psychology that humanity has failed at for centuries. But we\ndo not have to. I see two possible answers that do have some\nrealistic hope for success:\n\n- \n\nDemocratic legitimacy: come up with a compromise proposal\nthat really is a sufficient compromise that it makes enough people\nhappy, and perhaps even makes some existing domain name holders\n(not just potential domain name holders) better off\nthan they are today.\n\nFor example, we could implement demand-based annual fees (eg. setting\nthe annual fee to 0.5% of the highest bid) with a fee cap of $640 per\nyear for domains up to eight letters long, and $5 per year for longer\ndomains, and let domain holders pay nothing if no one makes a bid. Many\naverage users would save money under such a proposal.\n\n- \n\nMarket legitimacy: avoid the need to get legitimacy to\noverturn people's expectations in the existing system by instead\ncreating a new system (or sub-system).\n\nIn traditional DNS, this could be done just by creating a new TLD\nthat would be as convenient as existing TLDs. In ENS, there is a stated\ndesire to stick to .eth only to avoid conflicting with the\nexisting domain name system. And using existing subdomains doesn't quite\nwork: foo.bar.eth is much less nice than\nfoo.eth. One possible middle route is for the ENS DAO to\nhand off single-letter domain names solely to projects that run\nsome other kind of credibly-neutral marketplace for their subdomains, as\nlong as they hand over at least 50% of the revenue to the ENS DAO.\n\nFor example, perhaps x.eth could use one of my proposed\npricing schemes for its subdomains, and t.eth could\nimplement a mechanism where ENS DAO has the right to forcibly transfer\nsubdomains for anti-fraud and trademark reasons. foo.x.eth\njust barely looks good enough to be sort-of a substitute for\nfoo.eth; it will have to do.\n\nIf making changes to ENS domain pricing itself are off the table,\nthen the market-based approach of explicitly encouraging marketplaces\nwith different rules in subdomains should be strongly considered.\n\nTo me, the crypto space is not just about coins, and I admit my\nattraction to ENS does not center around some notion of\nunconditional and infinitely strict property-like ownership\nover domains. Rather, my interest in the space lies more in credible\nneutrality, and property rights that are strongly protected\nparticularly against politically motivated censorship and arbitrary and\ntargeted interference by powerful actors. That said, a high degree of\nguarantee of ownership is nevertheless very important for a domain name\nsystem to be able to function.\n\nThe hybrid proposals I suggest above are my attempt at preserving\ntotal credible neutrality, continuing to provide a high degree of\nownership guarantee, but at the same time increasing the cost of domain\nsquatting, raising more revenue for the ENS DAO to be able to work on\nimportant public goods, and improving the chances that people\nwho do not have the domain they want already will be able to get\none.",
    "contentLength": 20317,
    "summary": "ENS domains are underpriced at $5/year, enabling squatting; author proposes demand-based recurring fees instead of fixed pricing.",
    "detailedSummary": {
      "theme": "Vitalik proposes demand-based recurring fees for ENS domains to address domain squatting, improve fairness, and increase revenue for the ENS DAO while maintaining ownership guarantees.",
      "summary": "Vitalik argues that ENS domains are currently severely underpriced at $5 per year for five-letter domains, leading to widespread domain squatting where speculators buy up valuable names cheaply and resell them at much higher prices on secondary markets. He identifies two core problems: the fundamental tradeoff between strong property rights and fairness over time, and the fact that profit-maximizing speculators don't actually create efficient markets but often leave domains unused to maximize revenue. Vitalik proposes alternative solutions including demand-based recurring pricing where anyone can bid on a domain, and if they maintain the bid for 4 weeks, the annual fee rises proportionally to that valuation. He also suggests a 'capped demand-based recurring pricing' system that provides stronger ownership guarantees by allowing domain holders to pre-pay fixed amounts to guarantee ownership for specific time periods, with maximum annual fees that grow over time but remain predictable.",
      "takeaways": [
        "Current ENS domain pricing at $5/year enables massive domain squatting, with someone able to register all 8,938 five-letter Scrabble words for the cost of a dozen expensive cars",
        "Time-unlimited allocation of finite resources like domains is mathematically incompatible with fairness across long time horizons, as stubborn holders will eventually make valuable names permanently unavailable",
        "Profit-maximizing speculators don't create efficient markets but often accept high probability of never selling domains to maximize revenue when deals do occur",
        "Demand-based recurring fees could address squatting while maintaining ownership stability by letting bidders set valuations and requiring owners to either sell or pay higher annual fees based on bid prices",
        "Implementation faces psychological and political challenges around weakening property norms, but could work through democratic legitimacy with compromise proposals or market legitimacy through new subdomain systems"
      ],
      "controversial": [
        "Proposing to weaken existing property rights and increase fees for current ENS domain holders who bought under different expectations",
        "Arguing that speculators harm market efficiency rather than improve it, contradicting common free-market assumptions",
        "Suggesting that strong unlimited property rights are fundamentally incompatible with long-term fairness"
      ]
    }
  },
  {
    "id": "general-2022-08-04-zkevm",
    "title": "The different types of ZK-EVMs",
    "date": "2022-08-04",
    "category": "protocol",
    "url": "https://vitalik.eth.limo/general/2022/08/04/zkevm.html",
    "path": "general/2022/08/04/zkevm.html",
    "content": "- \n\n- \n\n  \n    \n\n  \n  \n  Dark Mode Toggle\n  \n    \n\n  \n\n##  The different types of ZK-EVMs \n\n 2022 Aug 04 \nSee all posts\n\n \n \n\n The different types of ZK-EVMs \n\nSpecial thanks to the PSE, Polygon Hermez, Zksync, Scroll, Matter\nLabs and Starkware teams for discussion and review.\n\nThere have been many \"ZK-EVM\" projects making flashy announcements\nrecently. Polygon\nopen-sourced their ZK-EVM project, ZKSync\nreleased their plans for ZKSync 2.0, and the relative newcomer Scroll\nannounced their ZK-EVM recently. There is also the ongoing effort from\nthe Privacy\nand Scaling Explorations team, Nicolas\nLiochon et al's team, an alpha\ncompiler from the EVM to Starkware's ZK-friendly language Cairo, and certainly at least a\nfew others I have missed.\n\nThe core goal of all of these projects is the same: to use ZK-SNARK technology to make\ncryptographic proofs of execution of Ethereum-like transactions, either\nto make it much easier to verify the Ethereum chain itself or to build\nZK-rollups that are (close\nto) equivalent to what Ethereum provides but are much more scalable. But\nthere are subtle differences between these projects, and what tradeoffs\nthey are making between practicality and speed. This post will attempt\nto describe a taxonomy of different \"types\" of EVM equivalence, and what\nare the benefits and costs of trying to achieve each type.\n\n## Overview (in chart form)\n\nType 1 (fully\nEthereum-equivalent)\n\nType 1 ZK-EVMs strive to be fully and uncompromisingly\nEthereum-equivalent. They do not change any part of the Ethereum system\nto make it easier to generate proofs. They do not replace hashes, state\ntrees, transaction trees, precompiles or any other in-consensus logic,\nno matter how peripheral.\nAdvantage: perfect compatibility\n\nThe goal is to be able to verify Ethereum blocks as they are today -\nor at least, verify the execution-layer\nside (so, beacon chain consensus logic is not included, but all the\ntransaction execution and smart contract and account logic is\nincluded).\n\nType 1 ZK-EVMs are what we ultimately need make the Ethereum layer 1\nitself more scalable. In the long term, modifications to Ethereum tested\nout in Type 2 or Type 3 ZK-EVMs might be introduced into Ethereum\nproper, but such a re-architecting comes with its own complexities.\n\nType 1 ZK-EVMs are also ideal for rollups, because they allow rollups\nto re-use a lot of infrastructure. For example, Ethereum execution\nclients can be used as-is to generate and process rollup blocks (or at\nleast, they can be once withdrawals\nare implemented and that functionality can be re-used to support ETH\nbeing deposited into the rollup), so tooling such as block explorers,\nblock production, etc is very easy to re-use.\nDisadvantage: prover time\n\nEthereum was not originally designed around ZK-friendliness, so there\nare many parts of the Ethereum protocol that take a large\namount of computation to ZK-prove. Type 1 aims to replicate Ethereum\nexactly, and so it has no way of mitigating these inefficiencies. At\npresent, proofs for Ethereum blocks take many hours to produce. This can\nbe mitigated either by clever engineering to massively parallelize the\nprover or in the longer term by ZK-SNARK ASICs.\nWho's building\nit?\n\nThe ZK-EVM\nCommunity Edition (bootstrapped by community contributors including\nPrivacy and\nScaling Explorations, the Scroll team, Taiko and others) is a Tier 1 ZK-EVM.\n\n## Type 2 (fully EVM-equivalent)\n\nType 2 ZK-EVMs strive to be exactly EVM-equivalent, but not quite\nEthereum-equivalent. That is, they look exactly like Ethereum \"from\nwithin\", but they have some differences on the outside, particularly in\ndata structures like the block structure and state\ntree.\n\nThe goal is to be fully compatible with existing applications, but\nmake some minor modifications to Ethereum to make development easier and\nto make proof generation faster.\nAdvantage: perfect equivalence at the VM\nlevel\n\nType 2 ZK-EVMs make changes to data structures that hold things like\nthe Ethereum state. Fortunately, these are structures that the EVM\nitself cannot access directly, and so applications that work on Ethereum\nwould almost always still work on a Type 2 ZK-EVM rollup. You would not\nbe able to use Ethereum execution clients as-is, but you could use them\nwith some modifications, and you would still be able to use EVM\ndebugging tools and most other developer infrastructure.\n\nThere are a small number of exceptions. One incompatibility arises\nfor applications that verify Merkle proofs of historical Ethereum\nblocks to verify claims about historical transactions, receipts or\nstate (eg. bridges sometimes do this). A ZK-EVM that replaces Keccak\nwith a different hash function would break these proofs. However, I\nusually recommend against building applications this way anyway, because\nfuture Ethereum changes (eg. Verkle\ntrees) will break such applications even on Ethereum itself. A\nbetter alternative would be for Ethereum itself to add future-proof\nhistory access precompiles.\nDisadvantage: improved but still slow prover\ntime\n\nType 2 ZK-EVMs provide faster prover times than Type 1 mainly by\nremoving parts of the Ethereum stack that rely on needlessly complicated\nand ZK-unfriendly cryptography. Particularly, they might change\nEthereum's Keccak and RLP-based Merkle Patricia tree and perhaps the\nblock and receipt structures. Type 2 ZK-EVMs might instead use a\ndifferent hash function, eg. Poseidon. Another natural\nmodification is modifying the state tree to store the code hash and\nkeccak, removing the need to verify hashes to process the\nEXTCODEHASH and EXTCODECOPY opcodes.\n\nThese modifications significantly improve prover times, but they do\nnot solve every problem. The slowness from having to prove the EVM\nas-is, with all of the inefficiencies and ZK-unfriendliness inherent to\nthe EVM, still remains. One simple example of this is memory: because an\nMLOAD can read any 32 bytes, including \"unaligned\" chunks\n(where the start and end are not multiples of 32), an MLOAD can't simply\nbe interpreted as reading one chunk; rather, it might require reading\ntwo consecutive chunks and performing bit operations to combine the\nresult.\nWho's building\nit?\n\nScroll's\nZK-EVM project is building toward a Type 2 ZK-EVM, as is Polygon\nHermez. That said, neither project is quite there yet; in\nparticular, a lot of the more complicated precompiles have not yet been\nimplemented. Hence, at the moment both projects are better considered Type 3.\nType 2.5\n(EVM-equivalent, except for gas costs)\n\nOne way to significantly improve\nworst-case prover times is to greatly increase the gas costs of\nspecific operations in the EVM that are very difficult to ZK-prove. This\nmight involve precompiles, the KECCAK opcode, and possibly specific\npatterns of calling contracts or accessing memory or storage or\nreverting.\n\nChanging gas costs may reduce developer\ntooling compatibility and break a few applications, but it's\ngenerally considered less risky than \"deeper\" EVM changes. Developers\nshould take care to not require more gas in a transaction than fits into\na block, to never make calls with hard-coded amounts of gas (this has\nalready been standard advice for developers for a long time).\n\nAn alternative way to manage resource constraints is to simply set\nhard limits on the number of times each operation can be called. This is\neasier to implement in circuits, but plays much less nicely with EVM\nsecurity assumptions. I would call this approach Type 3 rather than Type\n2.5.\nType 3 (almost\nEVM-equivalent)\n\nType 3 ZK-EVMs are almost EVM-equivalent, but make a few\nsacrifices to exact equivalence to further improve prover times and make\nthe EVM easier to develop.\nAdvantage: easier to build, and faster prover\ntimes\n\nType 3 ZK-EVMs might remove a few features that are exceptionally\nhard to implement in a ZK-EVM implementation. Precompiles\nare often at the top of the list here;. Additionally, Type 3 ZK-EVMs\nsometimes also have minor differences in how they treat contract code,\nmemory or stack.\nDisadvantage: more incompatibility\n\nThe goal of a Type 3 ZK-EVM is to be compatible with most\napplications, and require only minimal re-writing for the rest. That\nsaid, there will be some applications that would need to be rewritten\neither because they use pre-compiles that the Type 3 ZK-EVM removes or\nbecause of subtle dependencies on edge cases that the VMs treat\ndifferently.\nWho's building\nit?\n\nScroll and Polygon are both Type 3 in their current forms, though\nthey're expected to improve compatibility over time. Polygon has a\nunique design where they are ZK-verifying their own internal language\ncalled zkASM,\nand they interpret ZK-EVM code using the zkASM implementation. Despite\nthis implementation detail, I would still call this a genuine Type 3\nZK-EVM; it can still verify EVM code, it just uses some different\ninternal logic to do it.\n\nToday, no ZK-EVM team wants to be a Type 3; Type 3 is simply\na transitional stage until the complicated work of adding precompiles is\nfinished and the project can move to Type 2.5. In the future, however,\nType 1 or Type 2 ZK-EVMs may become Type 3 ZK-EVMs voluntarily, by\nadding in new ZK-SNARK-friendly precompiles that provide\nfunctionality for developers with low prover times and gas costs.\nType 4\n(high-level-language equivalent)\n\nA Type 4 system works by taking smart contract source code written in\na high-level language (eg. Solidity, Vyper, or some\nintermediate that both compile to) and compiling that to some\nlanguage that is explicitly designed to be ZK-SNARK-friendly.\nAdvantage: very fast prover times\n\nThere is a lot of overhead that you can avoid by not\nZK-proving all the different parts of each EVM execution step, and\nstarting from the higher-level code directly.\n\nI'm only describing this advantage with one sentence in this post\n(compared to a big bullet point list below for compatibility-related\ndisadvantages), but that should not be interpreted as a value judgement!\nCompiling from high-level languages directly really can greatly reduce\ncosts and help decentralization by making it easier to be a prover.\nDisadvantage: more incompatibility\n\nA \"normal\" application written in Vyper or Solidity can be compiled\ndown and it would \"just work\", but there are some important ways in\nwhich very many applications are not \"normal\":\n\n- Contracts may not have the same addresses in a Type\n4 system as they do in the EVM, because CREATE2 contract addresses\ndepend on the exact bytecode. This breaks applications that rely on\nnot-yet-deployed \"counterfactual contracts\", ERC-4337 wallets, EIP-2470 singletons\nand many other applications.\n\n- Handwritten EVM bytecode is more difficult to use.\nMany applications use handwritten EVM bytecode in some parts for\nefficiency. Type 4 systems may not support it, though there are ways to\nimplement limited EVM bytecode support to satisfy these use cases\nwithout going through the effort of becoming a full-on Type 3\nZK-EVM.\n\n- Lots of debugging infrastructure cannot be carried\nover, because such infrastructure runs over the EVM bytecode.\nThat said, this disadvantage is mitigated by the greater access\nto debugging infrastructure from \"traditional\" high-level or\nintermediate languages (eg. LLVM).\n\nDevelopers should be mindful of these issues.\nWho's building\nit?\n\nZKSync\nis a Type 4 system, though it may add compatibility for EVM bytecode\nover time. Nethermind's Warp project is\nbuilding a compiler from Solidity to Starkware's Cairo, which will turn\nStarkNet into a de-facto Type 4 system.\n\n## The future of ZK-EVM types\n\nThe types are not unambiguously \"better\" or \"worse\" than other types.\nRather, they are different points on the tradeoff space: lower-numbered\ntypes are more compatible with existing infrastructure but slower, and\nhigher-numbered types are less compatible with existing infrastructure\nbut faster. In general, it's healthy for the space that all of these\ntypes are being explored.\n\nAdditionally, ZK-EVM projects can easily start at higher-numbered\ntypes and jump to lower-numbered types (or vice versa) over time. For\nexample:\n\n- A ZK-EVM could start as Type 3, deciding not to include some\nfeatures that are especially hard to ZK-prove. Later, they can add those\nfeatures over time, and move to Type 2.\n\n- A ZK-EVM could start as Type 2, and later become a hybrid Type 2 /\nType 1 ZK-EVM, by providing the possibility of operating either in full\nEthereum compatibility mode or with a modified state tree that can be\nproven faster. Scroll is considering moving in this direction.\n\n- What starts off as a Type 4 system could become Type 3 over time by\nadding the ability to process EVM code later on (though developers would\nstill be encouraged to compile direct from high-level languages to\nreduce fees and prover times)\n\n- A Type 2 or Type 3 ZK-EVM can become a Type 1 ZK-EVM if Ethereum\nitself adopts its modifications in an effort to become more\nZK-friendly.\n\n- A Type 1 or Type 2 ZK-EVM can become a Type 3 ZK-EVM by adding a\nprecompile for verifying code in a very ZK-SNARK-friendly language. This\nwould give developers a choice between Ethereum compatibility and speed.\nThis would be Type 3, because it breaks perfect EVM equivalence, but for\npractical intents and purposes it would have a lot of the benefits of\nType 1 and 2. The main downside might be that some developer tooling\nwould not understand the ZK-EVM's custom precompiles, though this could\nbe fixed: developer tools could add universal precompile support by\nsupporting a config format that includes an EVM code equivalent\nimplementation of the precompile.\n\nPersonally, my hope is that everything becomes Type 1 over time,\nthrough a combination of improvements in ZK-EVMs and improvements to\nEthereum itself to make it more ZK-SNARK-friendly. In such a future, we\nwould have multiple ZK-EVM implementations which could be used both for\nZK rollups and to verify the Ethereum chain itself. Theoretically, there\nis no need for Ethereum to standardize on a single ZK-EVM implementation\nfor L1 use; different clients could use different proofs, so we continue\nto benefit from code redundancy.\n\nHowever, it is going to take quite some time until we get to such a\nfuture. In the meantime, we are going to see a lot of innovation in the\ndifferent paths to scaling Ethereum and Ethereum-based ZK-rollups.",
    "contentLength": 14285,
    "summary": "Vitalik categorizes ZK-EVMs into 4 types based on Ethereum compatibility vs proving speed tradeoffs, from fully compatible Type 1 to high-level Type 4.",
    "detailedSummary": {
      "theme": "Vitalik provides a comprehensive taxonomy of different types of ZK-EVM (Zero-Knowledge Ethereum Virtual Machine) implementations, categorizing them by their level of Ethereum compatibility versus proof generation speed tradeoffs.",
      "summary": "Vitalik categorizes ZK-EVMs into five types based on their compatibility with Ethereum and proof generation efficiency. Type 1 (fully Ethereum-equivalent) offers perfect compatibility but slow proof times, while Type 4 (high-level-language equivalent) provides fast proofs but breaks compatibility with existing infrastructure. Type 2 maintains EVM equivalence with minor modifications, Type 2.5 adjusts gas costs for difficult operations, and Type 3 makes selective sacrifices for better performance. Each approach represents different points on the tradeoff spectrum between compatibility and speed. Vitalik notes that projects can transition between types over time and that different teams are exploring various approaches - from Scroll and Polygon working on Type 2/3 implementations to ZKSync pursuing Type 4. His long-term vision is for everything to converge toward Type 1 through improvements in both ZK-EVM technology and Ethereum's ZK-friendliness, enabling multiple ZK-EVM implementations that could be used for both rollups and verifying the Ethereum chain itself.",
      "takeaways": [
        "ZK-EVMs exist on a spectrum from perfect Ethereum compatibility (Type 1) to optimized high-level language compilation (Type 4), with each type offering different tradeoffs between compatibility and proof generation speed",
        "Type 1 ZK-EVMs are ideal for scaling Ethereum Layer 1 and rollups due to perfect compatibility, but currently require many hours to generate proofs due to Ethereum's ZK-unfriendly design",
        "Projects can transition between types over time, starting with faster but less compatible implementations and gradually improving compatibility, or vice versa",
        "Multiple teams are pursuing different approaches: Privacy and Scaling Explorations building Type 1, Scroll and Polygon working toward Type 2, and ZKSync implementing Type 4",
        "Vitalik's long-term vision is convergence toward Type 1 implementations through both ZK-EVM improvements and making Ethereum itself more ZK-SNARK-friendly, potentially enabling multiple ZK-EVM clients for Ethereum Layer 1"
      ],
      "controversial": [
        "The suggestion that Ethereum itself should adopt modifications tested in Type 2 or Type 3 ZK-EVMs could be seen as controversial given the complexity and risks of re-architecting core Ethereum systems",
        "The recommendation against building applications that verify Merkle proofs of historical Ethereum blocks, despite this being a common practice for bridges and other protocols"
      ]
    }
  },
  {
    "id": "general-2022-07-13-networkstates",
    "title": "What do I think about network states?",
    "date": "2022-07-13",
    "category": "society",
    "url": "https://vitalik.eth.limo/general/2022/07/13/networkstates.html",
    "path": "general/2022/07/13/networkstates.html",
    "content": "- \n\n- \n\n  \n    \n\n  \n  \n  Dark Mode Toggle\n  \n    \n\n  \n\n##  What do I think about network states? \n\n 2022 Jul 13 \nSee all posts\n\n \n \n\n What do I think about network states? \n\nOn July 4, Balaji Srinivasan released the first version of his\nlong-awaited new book describing his vision for \"network\nstates\": communities organized around a particular vision of\nhow to run their own society that start off as online clubs, but then\nbuild up more and more of a presence over time and eventually become\nlarge enough to seek political autonomy or even diplomatic\nrecognition.\n\nNetwork states can be viewed as an attempt at an ideological\nsuccessor to libertarianism: Balaji repeatedly praises The\nSovereign Individual (see my mini-review here)\nas important reading and inspiration, but also departs from its thinking\nin key ways, centering in his new work many non-individualistic and\nnon-monetary aspects of social relations like morals and community.\nNetwork states can also be viewed as an attempt to sketch out a possible\nbroader political narrative for the crypto space. Rather than staying in\ntheir own corner of the internet disconnected from the wider world,\nblockchains could serve as a centerpiece for a new way of organizing\nlarge chunks of human society.\n\nThese are high promises. Can network states live up to them? Do\nnetwork states actually provide enough benefits to be worth getting\nexcited about? Regardless of the merits of network states, does it\nactually make sense to tie the idea together with blockchains and\ncryptocurrency? And on the other hand, is there anything crucially\nimportant that this vision of the world misses? This post represents my\nattempt to try to understand these questions.\n\n## Table of contents\n\n- What is a network state?\n\n- So what\nkinds of network states could we build?\n\n- What is\nBalaji's megapolitical case for network states?\n\n- Do\nyou have to like Balaji's megapolitics to like network states?\n\n- What\ndoes cryptocurrency have to do with network states?\n\n- What aspects of\nBalaji's vision do I like?\n\n- What\naspects of Balaji's vision do I take issue with?\n\n- Non-Balajian network\nstates\n\n- Is there a middle way?\n\n## What is a network state?\n\nBalaji helpfully gives multiple short definitions of what a network\nstate is. First, his definition in one sentence:\n\nA network state is a highly aligned online community with a capacity\nfor collective action that crowdfunds territory around the world and\neventually gains diplomatic recognition from pre-existing states.\n\nThis so far seems uncontroversial. Create a new internet community\nonline, once it grows big enough materialize it offline, and eventually\ntry to negotiate for some kind of status. Someone of almost any\npolitical ideology could find some form of network state under\nthis definition that they could get behind. But now, we get to his\ndefinition in a longer sentence:\n\nA network state is a social network with a moral innovation, a sense\nof national consciousness, a recognized founder, a capacity for\ncollective action, an in-person level of civility, an integrated\ncryptocurrency, a consensual government limited by a social smart\ncontract, an archipelago of crowdfunded physical territories, a virtual\ncapital, and an on-chain census that proves a large enough population,\nincome, and real-estate footprint to attain a measure of diplomatic\nrecognition.\n\nHere, the concept starts to get opinionated: we're not just talking\nabout the general concept of online communities that have collective\nagency and eventually try to materialize on land, we're talking about a\nspecific Balajian vision of what network states should look\nlike. It's completely possible to support network states in general, but\nhave disagreements with the Balajian view of what properties network\nstates should have. If you're not already a \"crypto convert\", it's hard\nto see why an \"integrated cryptocurrency\" is such a fundamental part of\nthe network state concept, for example - though Balaji does later on in\nthe book defend his choices.\n\nFinally, Balaji expands on this conception of a Balajian network\nstate in longer-form, first in \"a\nthousand words\" (apparently, Balajian network states use base 8, as the actual\nword count is exactly \\(512 = 8^3\\))\nand then an\nessay, and at the very end of the book a whole\nchapter.\n\nAnd, of course,\nan\nimage.\n\nOne key point that Balaji stresses across many chapters and\npages is the\nunavoidable moral ingredient required for any successful new\ncommunity. As Balaji writes:\n\nThe quick answer comes from Paul Johnson at the 11:00 mark of this talk, where he notes\nthat early America's religious colonies succeeded at a higher rate than\nits for-profit colonies, because the former had a purpose. The slightly\nlonger answer is that in a startup society, you're not asking people to\nbuy a product (which is an economic, individualistic pitch) but to join\na community (which is a cultural, collective pitch).\n\nThe commitment\nparadox of religious communes is key here: counterintuitively, it's\nthe religious communes that demand the most of their members\nthat are the most long-lasting.\n\nThis is where Balajism explicitly diverges from the more traditional\nneoliberal-capitalist ideal of the defanged, apolitical and passion-free\nconsumerist \"last\nman\". Unlike the strawman libertarian, Balaji does not believe that\neverything can \"merely be a consumer product\". Rather, he stresses\ngreatly the importance of social norms for cohesion, and a literally\nreligious attachment to the values that make a particular network state\ndistinct from the world outside. As Balaji says in\nthis podcast at 18:20, most current libertarian attempts at\nmicronations are like \"Zionism without Judaism\", and this is a key part\nof why they fail.\n\nThis recognition is not a new one. Indeed, it's at the core of\nAntonio Garcia Martinez's criticism of Balaji's earlier\nsovereign-individual ideas (see this\npodcast at ~27:00), praising the tenacity of Cuban exiles in Miami\nwho \"perhaps irrationally, said this is our new homeland, this is our\nlast stand\". And in Fukuyama's The End of History:\n\nThis city, like any city, has foreign enemies and needs to be\ndefended from outside attack. It therefore needs a class of guardians\nwho are courageous and public-spirited, who are willing to sacrifice\ntheir material desires and wants for the sake of the common good.\nSocrates does not believe that courage and public-spiritedness can arise\nout of a calculation of enlightened self-interest. Rather, they must be\nrooted in thymos, in the just pride of the guardian class in themselves\nand in their own city, and their potentially irrational anger against\nthose who threaten it.\n\nBalaji's argument in The Network State, as I am\ninterpreting it, is as follows. While we do need political collectives\nbound not just by economic interest but also by moral force, we don't\nneed to stick with the specific political collectives we have\ntoday, which are highly flawed and increasingly unrepresentative of\npeople's values. Rather, we can, and should, create new and better\ncollectives - and his seven-step\nprogram tells us how.\nSo what kinds of\nnetwork states could we build?\n\nBalaji outlines a few ideas for network states, which I will condense\ninto two key directions: lifestyle immersion and\npro-tech regulatory innovation.\n\nBalaji's go-to example for lifestyle immersion is a network state\norganized around health:\n\nNext, let's do an example which requires a network archipelago (with\na physical footprint) but not a full network state (with diplomatic\nrecognition). This is Keto Kosher, the sugar-free\nsociety.\n\nStart with a history of the horrible USDA Food Pyramid, the\ngrain-heavy monstrosity that gave cover to the corporate sugarification\nof the globe and the obesity epidemic. ... Organize a community online\nthat crowdfunds properties around the world, like apartment buildings\nand gyms, and perhaps eventually even culdesacs and small towns. You\nmight take an extreme sugar teeotaler approach, literally banning\nprocessed foods and sugar at the border, thereby implementing a kind of\n\"Keto Kosher\".\n\nYou can imagine variants of this startup society that are like\n\"Carnivory Communities\" or \"Paleo People\". These would be competing\nstartup societies in the same broad area, iterations on a theme. If\nsuccessful, such a society might not stop at sugar. It could get into\nsetting cultural defaults for fitness and exercise. Or perhaps it could\nbulk purchase continuous glucose meters for all members, or orders of\nmetformin.\n\nThis, strictly speaking, does not require any diplomatic recognition\nor even political autonomy - though perhaps, in the longer-term future,\nsuch enclaves could negotiate for lower health insurance fees and\nmedicare taxes for their members. What does require autonomy? Well, how\nabout a free zone for medical innovation?\n\nNow let's do a more difficult example, which will require a full\nnetwork state with diplomatic recognition. This is the medical\nsovereignty zone, the FDA-free society.\n\nYou begin your startup society with Henninger's history of FDA-caused\ndrug lag\nand Tabarrok's history of FDA interference with so-called \"off\nlabel\" prescription. You point out how many millions were killed by\nits policies, hand out t-shirts like ACT-UP did, show Dallas Buyers Club\nto all prospective residents, and make clear to all new members why your\ncause of medical sovereignty is righteous ...\n\nFor the case of doing it outside the US, your startup society would\nride behind, say, the support of the Malta's FDA for a new biomedical\nregime. For the case of doing it within the US, you'd need a governor\nwho'd declare a sanctuary state for biomedicine. That is, just like a\nsanctuary city declares that it won't enforce federal immigration law, a\nsanctuary state for biomedicine would not enforce FDA writ.\n\nOne can think up of many more examples for both categories. One could\nhave a zone where it's okay to walk around naked, both securing your\nlegal right to do so and helping you feel comfortable by\ncreating an environment where many other people are naked too.\nAlternatively, you could have a zone where everyone can only wear basic\nplain-colored clothing, to discourage what's perceived as a zero-sum\nstatus competition of expending huge effort to look better than everyone\nelse. One could have an intentional community zone for cryptocurrency\nusers, requiring every store to accept it and demanding an NFT to get in\nthe zone at all. Or one could build an enclave that legalizes radical\nexperiments in transit and drone delivery, accepting higher risks to\npersonal safety in exchange for the privilege of participating in a\ntechnological frontier that will hopefully set examples for the world as\na whole.\n\nWhat is common about all of these examples is the value of having a\nphysical region, at least of a few hectares, where the network state's\nunique rules are enforced. Sure, you could individually insist on only\neating at healthy restaurants, and research each restaurant carefully\nbefore you go there. But it's just so much easier to have a\ndefined plot of land where you have an assurance that anywhere you go\nwithin that plot of land will meet your standards. Of course, you could\nlobby your local government to tighten health and safety regulations.\nBut if you do that, you risk friction with people who have radically\ndifferent preferences on tradeoffs, and you risk shutting\npoor people out of the economy. A network state offers a moderate\napproach.\nWhat is\nBalaji's megapolitical case for network states?\n\nOne of the curious features of the book that a reader will notice\nalmost immediately is that it sometimes feels like two books in one:\nsometimes, it's a book about the concept of network states, and at other\ntimes it's an exposition of Balaji's grand megapolitical theory.\n\nBalaji's grand megapolitical theory is pretty out-there and fun in a\nbunch of ways. Near the beginning of the book, he entices readers with\ntidbits like... ok fine, I'll just quote:\n\n- Germany sent\nVladimir\nLenin into Russia, potentially as part of a\nstrategy\nto destabilize their then-rival in war. Antony Sutton's books document\nhow some\nWall\nStreet bankers apparently funded the Russian Revolution (and how\nother Wall Street bankers\nfunded\nthe Nazis years later). Leon Trotsky spent\ntime\nin New York prior to the revolution, and propagandistic reporting\nfrom Americans like\nJohn\nReed aided Lenin and Trotsky in their revolution. Indeed, Reed was\nso useful to the Soviets \u2014 and so misleading as to the nature of the\nrevolution \u2014 that he was\nburied\nat the base of the Kremlin Wall. Surprise: the Russian Revolution\nwasn't done wholly by Russians, but had significant foreign involvement\nfrom Germans and Americans.\n\n- The Ochs-Sulzberger family, which owns The New York Times Company,\nowned\nslaves but didn't report that fact in their 1619\ncoverage.\n\n- New York Times correspondent Walter\nDuranty won a Pulitzer\nPrize for helping the Soviet Union starve Ukraine into submission,\n90 years before the Times decided to instead \"stand\nwith Ukraine\".\n\nYou can find a bunch more juicy examples in the chapter titled,\nappropriately, \"If\nthe News is Fake, Imagine History\". These examples seem haphazard,\nand indeed, to some extent they are so intentionally: the goal is first\nand foremost to shock the reader out of their existing world model so\nthey can start downloading Balaji's own.\n\nBut pretty soon, Balaji's examples do start to point to some\nparticular themes: a deep dislike of the \"woke\" US left, exemplified by\nthe New York Times, a combination of strong discomfort with the Chinese\nCommunist Party's authoritarianism with an understanding of why the CCP\noften justifiably fears the United States, and an appreciation of the\nlove of freedom of the US right (exemplified by Bitcoin maximalists)\ncombined with a dislike of their hostility toward cooperation and\norder.\n\nNext, we get Balaji's overview of the\npolitical realignments in recent history, and finally we get to his\ncore model of politics in the present day: NYT, CCP, BTC.\n\nTeam NYT basically runs the US, and its total lack of competence\nmeans that the US is collapsing. Team BTC (meaning, both actual Bitcoin\nmaximalists and US rightists in general) has some positive values, but\ntheir outright hostility to collective action and order means that they\nare incapable of building anything. Team CCP can build, but they are\nbuilding a dystopian surveillance state that much of the world would not\nwant to live in. And all three teams are waaay too nationalist: they\nview things from the perspective of their own country, and ignore or\nexploit everyone else. Even when the teams are internationalist in\ntheory, their specific ways of interpreting their values make them\nunpalatable outside of a small part of the world.\n\nNetwork states, in Balaji's view, are a \"de-centralized\ncenter\" that could create a better alternative. They combine the\nlove of freedom of team BTC with the moral energy of team NYT and the\norganization of team CCP, and give us the best benefits of all three\n(plus a level of international appeal greater than any of the\nthree) and avoid the worst parts.\n\nThis is Balajian megapolitics in a nutshell. It is not trying to\njustify network states using some abstract theory (eg. some Dunbar's\nnumber or concentrated-incentive argument that the optimal size of a\npolitical body is actually in the low tens of thousands). Rather, it is\nan argument that situates network states as a response to the particular\npolitical situation of the world at its current place and time.\n\nBalaji's helical theory of history: yes, there are cycles,\nbut there is also ongoing progress. Right now, we're at the part of the\ncycle where we need to help the sclerotic old order die, but also seed a\nnew and better one.\n\nDo\nyou have to agree with Balaji's megapolitics to like network\nstates?\n\nMany aspects of Balajian megapolitics will not be convincing to many\nreaders. If you believe that \"wokeness\" is an important movement that\nprotects the vulnerable, you may not appreciate the almost off-handed\ndismissal that it is basically just a mask for a professional elite's\nwill-to-power. If you are worried about the plight of smaller countries\nsuch as Ukraine who are threatened by aggressive neighbors and\ndesperately need outside support, you will not be convinced by Balaji's\nplea that \"it may instead be best for countries to rearm, and take\non their own defense\".\n\nI do think that you can support network states while disagreeing with\nsome of Balaji's reasoning for them (and vice versa). But first, I\nshould explain why I think Balaji feels that his view of the\nproblem and his view of the solution are connected. Balaji has been\npassionate about roughly the same problem for a long time; you can see a\nsimilar narrative outline of defeating US institutional sclerosis\nthrough a technological and exit-driven approach in his speech on \"the\nultimate exit\" from 2013. Network states are the latest iteration of\nhis proposed solution.\n\nThere are a few reasons why talking about the problem is\nimportant:\n\n- To show that network states are the only way to protect\nfreedom and capitalism, one must show why the US cannot. If the\nUS, or the \"democratic liberal order\", is just fine, then there is no\nneed for alternatives; we should just double down on global coordination\nand rule of law. But if the US is in an irreversible decline, and its\nrivals are ascending, then things look quite different. Network states\ncan \"maintain liberal values in an illiberal world\"; hegemony thinking\nthat assumes \"the good guys are in charge\" cannot.\n\n- Many of Balaji's intended readers are not in the US, and a\nworld of network states would inherently be globally distributed - and\nthat includes lots of people who are suspicious of\nAmerica. Balaji himself is Indian, and has a large Indian fan\nbase. Many people in India, and elsewhere, view the US not as a\n\"guardian of the liberal world order\", but as something much more\nhypocritical at best and sinister at worst. Balaji wants to make it\nclear that you do not have to be pro-American to be a liberal (or at\nleast a Balaji-liberal).\n\n- Many parts of US left-leaning media are increasingly hostile\nto both cryptocurrency and the tech sector. Balaji expects that\nthe \"authoritarian left\" parts of \"team NYT\" will be hostile to network\nstates, and he explains this by pointing out that the media are not\nangels and their attacks are often self-interested.\n\nBut this is not the only way of looking at the broader picture. What\nif you do believe in the importance of role of social justice\nvalues, the New York Times, or America? What if you value governance\ninnovation, but have more moderate views on politics? Then, there are\ntwo ways you could look at the issue:\n\n- Network states as a synergistic strategy, or at least as a\nbackup. Anything that happens in US politics in terms of\nimproving equality, for example, only benefits the ~4% of the\nworld's population that lives in the United States. The First Amendment\ndoes not apply outside US borders. The governance of many wealthy\ncountries is sclerotic, and we do need some way to try\nmore governance innovation. Network states could fill in the gaps.\nCountries like the United States could host network states that attract\npeople from all over the world. Successful network states could even\nserve as a policy model for countries to adopt. Alternatively, what\nif the Republicans win and secure a decades-long majority in 2024,\nor the United States breaks down? You want there to be an\nalternative.\n\n- Exit to network states as a distraction, or even a\nthreat. If everyone's first instinct when faced with a large\nproblem within their country is to exit to an enclave elsewhere, there\nwill be no one left to protect and maintain the countries themselves.\nGlobal infrastructure that ultimately network states depend on will\nsuffer.\n\nBoth perspectives are compatible with a lot of disagreement with\nBalajian megapolitics. Hence, to argue for or against Balajian network\nstates, we will ultimately have to talk about network states. My own\nview is friendly to network states, though with a lot of caveats and\ndifferent ideas about how network states could work.\nWhat\ndoes cryptocurrency have to do with network states?\n\nThere are two kinds of alignment here: there is the\nspiritual alignment, the idea that \"Bitcoin\nbecomes the flag of technology\", and there is the practical\nalignment, the specific ways in which network states could use\nblockchains and cryptographic tokens. In general, I agree with both of\nthese arguments - though I think Balaji's book could do much more to\nspell them out more explicitly.\n\n## The spiritual alignment\n\nCryptocurrency in 2022 is a key standard-bearer for internationalist\nliberal values that are difficult to find in any other social force that\nstill stands strong today. Blockchains and cryptocurrencies are\ninherently global. Most Ethereum developers are outside the US, living\nin far-flung places like Europe, Taiwan and Australia. NFTs have given\nunique opportunities to artists\nin Africa and elsewhere\nin the Global South. Argentinians punch above their weight in\nprojects like Proof of Humanity, Kleros and Nomic Labs.\n\nBlockchain communities continue to stand for openness, freedom,\ncensorship resistance and credible\nneutrality, at a time where many geopolitical actors are\nincreasingly only serving their own interests. This enhances their\ninternational appeal further: you don't have to love US hegemony to love\nblockchains and the values that they stand for. And this all makes\nblockchains an ideal spiritual companion for the network state vision\nthat Balaji wants to see.\n\n## The practical alignment\n\nBut spiritual alignment means little without practical use value for\nblockchains to go along with it. Balaji gives plenty of blockchain use\ncases. One of Balaji's favorite concepts is the idea of the blockchain\nas a \"ledger of record\": people can timestamp events on-chain, creating\na global provable log of humanity's \"microhistory\". He continues with\nother examples:\n\n- Zero-knowledge\ntechnology like ZCash, Ironfish, and Tornado Cash allow on-chain attestation\nof exactly what people want to make public and nothing more.\n\n- Naming systems like the Ethereum Name\nService (ENS) and Solana Name\nService (SNS) attach identity to on-chain transactions.\n\n- Incorporation systems allow the on-chain representation of corporate\nabstractions above the level of a mere transaction, like financial\nstatements or even full programmable company-equivalents like DAOs.\n\n- Cryptocredentials,\nNon-Fungible\nTokens (NFTs), Non-Transferable\nFungibles (NTFs), and Soulbounds allow the\nrepresentation of non-financial data on chain, like diplomas or\nendorsements.\n\nBut how does this all relate to network states? I could go into\nspecific examples in the vein of crypto cities: issuing\ntokens, issuing CityDAO-style\ncitizen NFTs, combining blockchains with zero-knowledge cryptography to\ndo secure privacy-preserving\nvoting, and a lot more. Blockchains are the\nLego of crypto-finance and crypto-governance: they are a very\neffective tool for implementing transparent in-protocol rules to govern\ncommon resources, assets and incentives.\n\nBut we also need to go a level deeper. Blockchains and\nnetwork states have the shared property that they are both trying to\n\"create a new root\". A corporation is not a root: if there is a\ndispute inside a corporation, it ultimately gets resolved by a national\ncourt system. Blockchains and network states, on the other hand,\nare trying to be new roots. This does not mean some absolute\n\"na na no one can catch me\" ideal of sovereignty that is perhaps only\ntruly accessible to the ~5 countries that have highly self-sufficient\nnational economies and/or nuclear weapons. Individual blockchain\nparticipants are of course vulnerable to national regulation, and\nenclaves of network states even more so. But blockchains are the only\ninfrastructure system that at least attempts to do ultimate\ndispute resolution at the non-state level (either through on-chain smart\ncontract logic or through the freedom\nto fork). This makes them an ideal base infrastructure for network\nstates.\nWhat aspects of\nBalaji's vision do I like?\n\nGiven that a purist \"private\nproperty rights only\" libertarianism inevitably runs into large\nproblems like its inability to fund public goods, any successful\npro-freedom program in the 21st century has to be a hybrid containing at\nleast one Big Compromise Idea that solves at least 80% of the problems,\nso that independent individual initiative can take care of the rest.\nThis could be some stringent measures against economic power and wealth\nconcentration (maybe charge annual Harberger\ntaxes on everything), it could be an 85% Georgist\nland tax, it could be a UBI, it could be mandating that sufficiently\nlarge companies become democratic internally, or one of any other\nproposals. Not all of these work, but you need something that\ndrastic to have any shot at all.\n\nGenerally, I am used to the Big Compromise Idea being a leftist one:\nsome form of equality and democracy. Balaji, on the other hand, has Big\nCompromise Ideas that feel more rightist: local communities with shared\nvalues, loyalty, religion, physical environments structured to encourage\npersonal discipline (\"keto kosher\") and hard work. These values are\nimplemented in a very libertarian and tech-forward way, organizing not\naround land, history, ethnicity and country, but around the cloud and\npersonal choice, but they are rightist values nonetheless. This style of\nthinking is foreign to me, but I find it fascinating, and important.\nStereotypical \"wealthy\nwhite liberals\" ignore this at their peril: these more \"traditional\"\nvalues are actually quite popular even among some\nethnic minorities in the United States, and even more so in places\nlike Africa and India, which is exactly where Balaji is trying to build\nup his base.\nBut\nwhat about this particular baizuo that's currently writing this review?\nDo network states actually interest me?\n\nThe \"Keto Kosher\" health-focused lifestyle immersion network state is\ncertainly one that I would want to live in. Sure, I could just spend\ntime in cities with lots of healthy stuff that I can seek out\nintentionally, but a concentrated physical environment makes it so much\neasier. Even the motivational aspect of being around other people who\nshare a similar goal sounds very appealing.\n\nBut the truly interesting stuff is the governance innovation: using\nnetwork states to organize in ways that would actually not be possible\nunder existing regulations. There are three ways that you can interpret\nthe underlying goal here:\n\n- Creating new regulatory environments that let their\nresidents have different priorities from the priorities\npreferred by the mainstream: for example, the \"anyone can walk around\nnaked\" zone, or a zone that implements different tradeoffs between\nsafety and convenience, or a zone that legalizes more psychoactive\nsubstances.\n\n- Creating new regulatory institutions that might be more\nefficient at serving the same priorities as the status quo. For\nexample, instead of improving environmental friendliness by regulating\nspecific behaviors, you could just have a Pigovian tax.\nInstead of requiring licenses and regulatory pre-approval for many\nactions, you could require mandatory\nliability insurance. You could use quadratic voting for\ngovernance and quadratic funding to fund local public goods.\n\n- Pushing against regulatory conservatism in general,\nby increasing the chance that there's some jurisdiction that\nwill let you do any particular thing. Institutionalized bioethics, for\nexample, is a notoriously conservative enterprise, where 20 people dead\nin a medical experiment gone wrong is a tragedy, but 200000 people dead\nfrom life-saving\nmedicines and vaccines not being approved quickly enough is a\nstatistic. Allowing people to opt into network states that accept higher\nlevels of risk could be a successful strategy for pushing against\nthis.\n\nIn general, I see value in all three. A large-scale\ninstitutionalization of [1] could make the word simultaneously more free\nwhile making people comfortable with higher levels of restriction of\ncertain things, because they know that if they want to do something\ndisallowed there are other zones they could go to do it. More generally,\nI think there is an important idea hidden in [1]: while the\n\"social technology\" community has come up with many good ideas around\nbetter governance, and many good ideas around better public discussion, there is a\nmissing emphasis on better social technology for\nsorting. We don't just want to take existing maps of\nsocial connections as given and find better ways to come to consensus\nwithin them. We also want to reform the webs of social connections\nthemselves, and put people closer to other people that are more\ncompatible with them to better allow different ways of life to maintain\ntheir own distinctiveness.\n\n[2] is exciting because it fixes a major problem in politics: unlike\nstartups, where the early stage of the process looks somewhat like a\nmini version of the later stage, in politics the early stage is a public\ndiscourse game that often selects for very different things than what\nactually work in practice. If governance ideas are regularly implemented\nin network states, then we would move from an\nextrovert-privileging \"talker liberalism\" to a more balanced \"doer\nliberalism\" where ideas rise and fall based on how well they actually do\non a small scale. We could even combine [1] and [2]: have a\nzone for people who want to automatically participate in a new\ngovernance experiment every year as a lifestyle.\n\n[3] is of course a more complicated moral question: whether you view\nparalysis and creep toward de-facto authoritarian global government as a\nbigger problem or someone inventing an evil technology that dooms us all\nas a bigger problem. I'm generally in the first camp; I am concerned\nabout the prospect of both\nthe West and China settling into a kind of low-growth conservatism,\nI love how imperfect coordination between nation states limits\nthe enforceability of things like global copyright law, and I'm\nconcerned about the possibility that, with future surveillance\ntechnology, the world as a whole will enter a highly self-enforcing but\nterrible political equilibrium that it cannot get out of. But there are\nspecific areas (cough cough, unfriendly\nAI risk) where I am in the risk-averse camp ... but here we're already\ngetting into the second part of my reaction.\nWhat\naspects of Balaji's vision do I take issue with?\n\nThere are four aspects that I am worried about the most:\n\n- The \"founder\" thing - why do network states need a recognized\nfounder to be so central?\n\n- What if network states end up only serving the wealthy?\n\n- \"Exit\" alone is not sufficient to stabilize global politics. So if\nexit is everyone's first choice, what happens?\n\n- What about global negative externalities more generally?\n\n## The \"founder\" thing\n\nThroughout the book, Balaji is insistent on the importance of\n\"founders\" in a network state (or rather, a startup society:\nyou found a startup society, and become a network state if you are\nsuccessful enough to get diplomatic recognition). Balaji explicitly\ndescribes startup society founders as being \"moral entrepreneurs\":\n\nThese presentations are similar to startup pitch decks. But as the\nfounder of a startup society, you aren't a technology entrepreneur\ntelling investors why this new innovation is better, faster, and\ncheaper. You are a moral entrepreneur telling potential future citizens\nabout a better way of life, about a single thing that the broader world\nhas gotten wrong that your community is setting right.\n\nFounders crystallize moral intuitions and learnings\nfrom history into a concrete philosophy, and people whose moral\nintuitions are compatible with that philosophy coalesce around the\nproject. This is all very reasonable at an early stage - though it is\ndefinitely not the only approach for how a startup society\ncould emerge. But what happens at later stages? Mark Zuckerberg being\nthe centralized founder of facebook the startup was perhaps necessary.\nBut Mark Zuckerberg being in charge of a multibillion-dollar (in fact,\nmultibillion-user) company is something quite different. Or,\nfor that matter, what about Balaji's nemesis: the fifth-generation\nhereditary white Ochs-Sulzberger dynasty running the New York Times?\n\nSmall things being centralized is great, extremely large things being\ncentralized is terrifying. And given the reality of network effects, the\nfreedom to exit again is not sufficient. In my view, the problem of how\nto settle into something other than founder control is important, and\nBalaji spends too little effort on it. \"Recognized founder\" is baked\ninto the definition of what a Balajian network state is, but a roadmap\ntoward wider participation in governance is not. It should be.\nWhat about everyone who\nis not wealthy?\n\nOver the last few years, we've seen many instances of governments\naround the world becoming explicitly more open to \"tech talent\". There\nare 42\ncountries offering digital nomad visas, there is a French\ntech visa, a similar\nprogram in Singapore, golden visas for Taiwan, a\nprogram for Dubai,\nand many others. This is all great for skilled professionals and rich\npeople. Multimillionaires fleeing China's tech crackdowns and covid\nlockdowns (or, for that matter, moral disagreements with China's other\npolicies) can often escape the\nworld's systemic discrimination against Chinese and other\nlow-income-country citizens by spending a few hundred thousand\ndollars on buying\nanother passport. But what about regular people? What about the Rohingya minority\nfacing extreme conditions in Myanmar, most of whom do not have a way\nto enter the US or Europe, much less buy another passport?\n\nHere, we see a potential tragedy of the network state concept. On the\none hand, I can really see how exit can be the most viable strategy for\nglobal human rights protection in the twenty first century. What do you\ndo if another country is oppressing an ethnic minority? You could do\nnothing. You could sanction them (often ineffective\nand ruinous\nto the very people you're trying to help). You could try to invade (same\ncriticism but even worse). Exit is a more humane option. People\nsuffering human rights atrocities could just pack up and leave for\nfriendlier pastures, and coordinating to do it in a group would mean\nthat they could leave without sacrificing the communities they depend on\nfor friendship and economic livelihood. And if you're wrong and the\ngovernment you're criticizing is actually not that oppressive, then\npeople won't leave and all is fine, no starvation or bombs required.\nThis is all beautiful and good. Except... the whole thing breaks down\nbecause when the people try to exit, nobody is there to take them.\n\nWhat is the answer? Honestly, I don't see one. One point in favor of\nnetwork states is that they could be based in poor countries,\nand attract wealthy people from abroad who would then help the local\neconomy. But this does nothing for people in poor countries who want\nto get out. Good old-fashioned political action within existing\nstates to liberalize immigration laws seems like the only option.\n\n## Nowhere to run\n\nIn the wake of Russia's invasion of Ukraine on Feb 24, Noah Smith\nwrote an important\npost on the moral clarity that the invasion should bring to our\nthought. A particularly striking section is titled \"nowhere to run\".\nQuoting:\n\nBut while exit works on a local level \u2014 if San Francisco is too\ndysfunctional, you can probably move to Austin or another tech town \u2014 it\nsimply won't work at the level of nations. In fact, it never really did\n\u2014 rich crypto guys who moved to countries like Singapore or territories\nlike Puerto Rico still depended crucially on the infrastructure and\ninstitutions of highly functional states. But Russia is making it even\nclearer that this strategy is doomed, because eventually there is\nnowhere to run. Unlike in previous eras, the arm of the great powers is\nlong enough to reach anywhere in the world.\n\nIf the U.S. collapses, you can't just move to Singapore, because in a\nfew years you'll be bowing to your new Chinese masters. If the U.S.\ncollapses, you can't just move to Estonia, because in a few years\n(months?) you'll be bowing to your new Russian masters. And those\nmasters will have extremely little incentive to allow you to remain a\nfree individual with your personal fortune intact ... Thus it is very very\nimportant to every libertarian that the U.S. not collapse.\n\nOne possible counter-argument is: sure, if Ukraine was full of people\nwhose first instinct was exit, Ukraine would have collapsed. But if\nRussia was also more exit-oriented, everyone in Russia would\nhave pulled out of the country within a week of the invasion. Putin\nwould be left standing alone in the fields of the Luhansk oblast facing\nZelensky a hundred meters away, and when Putin shouts his demand for\nsurrender, Zelensky would reply: \"you and what army\"? (Zelensky would of\ncourse win a fair one-on-one fight)\n\nBut things could go a different way. The risk is that exitocracy\nbecomes recognized as the primary way you do the \"freedom\"\nthing, and societies that value freedom will become exitocratic,\nbut centralized states will censor and suppress these impulses, adopt a\nmilitaristic attitude of national unconditional loyalty, and run\nroughshod over everyone else.\nSo what about those\nnegative externalities?\n\nIf we have a hundred much-less-regulated innovation labs everywhere\naround the world, this could lead to a world where harmful things are\nmore difficult to prevent. This raises a question: does\nbelieving in Balajism require believing in a world where\nnegative externalities are not too big a deal? Such a\nviewpoint would be the opposite of the Vulnerable\nWorld Hypothesis (VWH), which suggests that are technology\nprogresses, it gets easier and easier for one or a few crazy people to\nkill millions, and global authoritarian surveillance might be\nrequired to prevent extreme suffering or even extinction.\n\nOne way out might be to focus on self-defense technology. Sure, in a\nnetwork state world, we could not feasibly ban gain-of-function\nresearch, but we could use network states to help the world along a path\nto adopting really good HEPA air\nfiltering, far-UVC\nlight, early detection infrastructure and a very rapid\nvaccine development and deployment pipeline that could defeat not only\ncovid, but far worse viruses too. This\n80,000 hours episode outlines the bull case for bioweapons being a\nsolvable problem. But this is not a universal solution for all\ntechnological risks: at the very least, there is no self-defense against\na super-intelligent unfriendly AI that kills us all.\n\nSelf-defense technology is good, and is probably an undervalued\nfunding focus area. But it's not realistic to rely on that alone.\nTransnational cooperation to, for example, ban\nslaughterbots, would be required. And so we do want a world where,\neven if network states have more sovereignty than intentional\ncommunities today, their sovereignty is not absolute.\n\n## Non-Balajian network states\n\nReading The Network State reminded me of a different book\nthat I read ten years ago: David de Ugarte's Phyles: Economic Democracy\nin the Twenty First Century. Phyles talks about\nsimilar ideas of transnational communities organized around values, but\nit has a much more left-leaning emphasis: it assumes that these\ncommunities will be democratic, inspired by a combination of 2000s-era\nonline communities and nineteenth and twentieth-century ideas of\ncooperatives and workplace democracy.\n\nWe can see the differences most clearly by looking at de Ugarte's\ntheory of formation. Since I've already spent a lot of time quoting\nBalaji, I'll give de Ugarte a fair hearing with a longer quote:\n\nThe very blogosphere is an ocean of identities and conversation in\nperpetual cross-breeding and change from among which the great social\ndigestion periodically distils stable groups with their own contexts and\nspecific knowledge.\n\nThese conversational communities which crystallise, after a certain\npoint in their development, play the main roles in what we call digital\nZionism: they start to precipitate into reality, to generate mutual\nknowledge among their members, which makes them more identitarially\nimportant to them than the traditional imaginaries of the imagined\ncommunities to which they are supposed to belong (nation, class,\ncongregation, etc.) as if it were a real community (group of friends,\nfamily, guild, etc.)\n\nSome of these conversational networks, identitarian and dense, start\nto generate their own economic metabolism, and with it a distinct demos\n\u2013 maybe several demoi \u2013 which takes the nurturing of the autonomy of the\ncommunity itself as its own goal. These are what we call Neo-Venetianist\nnetworks. Born in the blogosphere, they are heirs to the hacker work\nethic, and move in the conceptual world, which tends to the economic\ndemocracy which we spoke about in the first part of this book.\n\nUnlike traditional cooperativism, as they do not spring from real\nproximity-based communities, their local ties do not generate identity.\nIn the Indianos' foundation, for instance, there are residents in two\ncountries and three autonomous regions, who started out with two\ncompanies founded hundreds of kilometres away from each other.\n\nWe see some very Balajian ideas: shared collective identities, but\nformed around values rather than geography, that start off as discussion\ncommunities in the cloud but then materialize into taking over large\nportions of economic life. De Ugarte even uses the exact same metaphor\n(\"digital Zionism\") that Balaji does!\n\nBut we also see a key difference: there is no single founder. Rather\nthan a startup society being formed by an act of a single individual\ncombining together intuitions and strands of thought into a coherent\nformally documented philosophy, a phyle starts off as a conversational\nnetwork in the blogosphere, and then directly turns into a group that\ndoes more and more over time - all while keeping its democratic and\nhorizontal nature. The whole process is much more organic, and not at\nall guided by a single person's intention.\n\nOf course, the immediate challenge that I can see is the incentive\nissues inherent to such structures. One way to perhaps unfairly\nsummarize both Phyles and The Network State is that\nThe Network State seeks to use 2010s-era blockchains as a model\nfor how to reorganize human society, and Phyles seeks to use\n2000s-era open source software communities and blogs as a model for how\nto reorganize human society. Open source has the failure mode of not\nenough incentives, cryptocurrency has the failure mode of\nexcessive and overly concentrated incentives. But what this\ndoes suggest is that some kind of middle way should be possible.\n\n## Is there a middle way?\n\nMy judgement so far is that network states are great, but they are\nfar from being a viable Big Compromise Idea that can actually plug all\nthe holes needed to build the kind of world I and most of my readers\nwould want to see in the 21st century. Ultimately, I do think that we\nneed to bring in more democracy and large-scale-coordination oriented\nBig Compromise Ideas of some kind to make network states truly\nsuccessful.\n\nHere are some significant adjustments to Balajism that I would\nendorse:\nFounder\nto start is okay (though not the only way), but we really need a\nbaked-in roadmap to exit-to-community\n\nMany founders want to eventually retire or start something\nnew (see: basically half of every crypto project), and we need to\nprevent network states from collapsing or sliding into mediocrity when\nthat happens. Part of this process is some kind of constitutional\nexit-to-community guarantee: as the network state\nenters higher tiers of maturity and scale, more input from community\nmembers is taken into account automatically.\n\nProspera attempted something like this. As Scott Alexander summarizes:\n\nOnce Pr\u00f3spera has 100,000 residents (so realistically a long time\nfrom now, if the experiment is very successful), they can hold a\nreferendum where 51% majority can change anything about the charter,\nincluding kicking HPI out entirely and becoming a direct democracy, or\nrejoining the rest of Honduras, or anything\n\nBut I would favor something even more participatory than the\nresidents having an all-or-nothing nuclear option to kick the government\nout.\n\nAnother part of this process, and one that I've recognized in the\nprocess of Ethereum's growth, is explicitly encouraging broader\nparticipation in the moral and philosophical development of the\ncommunity. Ethereum has its Vitalik, but it also has its Polynya: an internet anon who\nhas recently entered the scene unsolicited and started providing\nhigh-quality thinking on rollups and scaling technology. How will your\nstartup society recruit its first ten Polynyas?\nNetwork\nstates should be run by something that's not coin-driven governance\n\nCoin-driven governance is plutocratic and vulnerable to attacks; I\nhave written about this many times, but it's worth\nrepeating. Ideas like Optimism's soulbound and\none-per-person citizen NFTs\nare key here. Balaji already acknowledges the need for non-fungibility\n(he supports\ncoin lockups), but we should go further and more explicit in\nsupporting governance that's not just shareholder-driven. This will also\nhave the beneficial side effect that more democratic governance is more\nlikely to be aligned with the outside world.\nNetwork\nstates commit to making themselves friendly through outside\nrepresentation in governance\n\nOne of the fascinating and under-discussed ideas from the rationalist\nand friendly-AI community is functional\ndecision theory. This is a complicated concept, but the\npowerful core idea is that AIs could coordinate better than humans,\nsolving prisoner's dilemmas where humans often fail, by making\nverifiable public commitments about their source code. An AI could\nrewrite itself to have a module that prevents it from cheating other AIs\nthat have a similar module. Such AIs would all cooperate with each other\nin\nprisoner's dilemmas.\n\nAs I pointed\nout years ago, DAOs could potentially do the same thing. They could\nhave governance mechanisms that are explicitly more charitable toward\nother DAOs that have a similar mechanism. Network states would be run by\nDAOs, and this would apply to network states too. They could even commit\nto governance mechanisms that promise to take wider public interests\ninto account (eg. 20% of the votes could go to a randomly selected set\nof residents of the host city or country), without the burden of having\nto follow specific complicated regulations of how they should\ntake those interests into account. A world where network states do such\na thing, and where countries adopt policies that are explicitly more\nfriendly to network states that do it, could be a better one.\n\n## Conclusion\n\nI want to see startup societies along these kinds of visions exist. I\nwant to see immersive lifestyle experiments around healthy living. I\nwant to see crazy governance experiments where public goods are funded\nby quadratic funding, and all zoning laws are replaced by a system where\nevery building's property tax floats between zero and five percent per\nyear based on what percentage of nearby residents express approval or\ndisapproval in a real-time blockchain and ZKP-based voting\nsystem. And I want to see more technological experiments that accept\nhigher levels of risk, if the people taking those risks consent to it.\nAnd I think blockchain-based tokens, identity and reputation systems and\nDAOs could be a great fit.\n\nAt the same time, I worry that the network state vision in its\ncurrent form risks only satisfying these needs for those wealthy enough\nto move and desirable enough to attract, and many people lower down the\nsocioeconomic ladder will be left in the dust. What can be said in\nnetwork states' favor is their internationalism: we even have the\nAfrica-focused Afropolitan.\nInequalities between countries are responsible\nfor two thirds of global inequality and inequalities within\ncountries are only one third. But that still leaves a lot of people in\nall countries that this vision doesn't do much for. So we need something\nelse too - for the global poor, for Ukrainians that want to keep their\ncountry and not just squeeze into Poland for a decade until Poland gets\ninvaded too, and everyone else that's not in a position to move to a\nnetwork state tomorrow or get accepted by one.\n\nNetwork states, with some modifications that push for more democratic\ngovernance and positive relationships with the communities that surround\nthem, plus some other way to help everyone else? That is a\nvision that I can get behind.",
    "contentLength": 48900,
    "summary": "Vitalik reviews Balaji Srinivasan's \"network states\" book, finding merit in values-based online communities seeking political autonomy but criticizing excessive crypto integration.",
    "detailedSummary": {
      "theme": "Vitalik's nuanced analysis of Balaji Srinivasan's network states concept, exploring both its potential benefits and significant limitations as a new form of governance and social organization.",
      "summary": "Vitalik examines Balaji Srinivasan's vision of network states - online communities that eventually acquire physical territory and diplomatic recognition - finding both promise and problems in the concept. He appreciates the potential for governance innovation, lifestyle experimentation (like health-focused communities), and regulatory alternatives that could benefit from blockchain technology's global, neutral infrastructure. However, Vitalik raises serious concerns about the emphasis on centralized founders, the risk of serving only wealthy elites while leaving behind those who can't afford to exit their current situations, and the danger that excessive focus on 'exit' strategies could undermine collective defense against authoritarian threats. He argues that while exit-based solutions like network states have merit, they cannot replace the need for traditional political engagement and international cooperation, especially regarding global challenges like preventing dangerous technologies or defending against aggressive nation-states. Vitalik concludes by proposing a 'middle way' that incorporates more democratic governance structures, broader community participation beyond founders, and explicit commitments to positive relationships with surrounding communities, while maintaining that network states alone cannot solve global inequality and political problems.",
      "takeaways": [
        "Network states could enable valuable governance experiments and lifestyle communities, particularly when combined with blockchain technology for transparent, programmable rules",
        "The current network state vision risks primarily serving wealthy individuals while leaving behind those who cannot afford to exit their current situations",
        "Pure 'exit' strategies are insufficient for addressing global challenges like defending against authoritarian regimes or preventing dangerous technological developments",
        "Vitalik proposes reforms including transition from founder control to community governance, non-plutocratic decision-making systems, and explicit commitments to benefit surrounding communities",
        "Network states should be viewed as complementary to, not replacements for, traditional political engagement and international cooperation"
      ],
      "controversial": [
        "Vitalik's critique of exit-focused strategies potentially conflicting with libertarian principles of individual choice and mobility",
        "His argument that network states might primarily serve the wealthy could be seen as dismissive of their potential to create new opportunities",
        "The suggestion that network states should include 'outside representation in governance' may be viewed as compromising their autonomy",
        "His skepticism about pure market-based solutions to collective action problems challenges some libertarian assumptions"
      ]
    }
  },
  {
    "id": "general-2022-06-20-backpack",
    "title": "My 40-liter backpack travel guide",
    "date": "2022-06-20",
    "category": "general",
    "url": "https://vitalik.eth.limo/general/2022/06/20/backpack.html",
    "path": "general/2022/06/20/backpack.html",
    "content": "- \n\n- \n\n  \n    \n\n  \n  \n  Dark Mode Toggle\n  \n    \n\n  \n\n##  My 40-liter backpack travel guide \n\n 2022 Jun 20 \nSee all posts\n\n \n \n\n My 40-liter backpack travel guide \n\nSpecial thanks to Liam Horne for feedback and review. I received\nno money from and have never even met any of the companies making the\nstuff I'm shilling here (with the sole exception of Unisocks); this is\nall just an honest listing of what works for me today.\n\nI have lived as a nomad for the last nine years, taking 360 flights\ntravelling over 1.5 million kilometers (assuming flight paths are\nstraight, ignoring layovers) during that time. During this time, I've\nconsiderably optimized the luggage I carry along with me: from a\n60-liter shoulder bag with a separate laptop bag, to a 60-liter shoulder\nbag that can contain the laptop bag, and now to a 40-liter packpage that\ncan contain the laptop bag along with all the supplies I need to live my\nlife.\n\nThe purpose of this post will be to go through the contents, as well\nas some of the tips that I've learned for how you too can optimize your\ntravel life and never have to wait at a luggage counter again. There is\nno obligation to follow this guide in its entirety; if you have\nimportant needs that differ from mine, you can still get a lot of the\nbenefits by going a hybrid route, and I will talk about these options\ntoo.\n\nThis guide is focused on my own experiences; plenty of other people\nhave made their own guides and you should look at them too. /r/onebag is an excellent\nsubreddit for this.\n\nThe backpack, with the various sub-bags laid out separately. Yes,\nthis all fits in the backpack, and without that much effort to pack and\nunpack.\n\nAs a point of high-level organization, notice the bag-inside-a-bag\nstructure. I have a T-shirt bag, an underwear bag, a sock bag, a\ntoiletries bag, a dirty-laundry bag, a medicine bag, a laptop bag, and\nvarious small bags inside the inner compartment of my backpack, which\nall fit into a 40-liter\nHynes Eagle backpack. This structure makes it easy to keep things\norganized.\nIt's like\nfrugality, but for cm3 instead of dollars\n\nThe general principle that you are trying to follow is that you're\ntrying to stay within a \"budget\" while still making sure you have\neverything that you need - much like normal financial planning of the\ntype that almost everyone, with the important exception of crypto\nparticipants during bull runs, is used to dealing with. A key difference\nhere is that instead of optimizing for dollars, you're optimizing for\ncubic centimeters. Of course, none of the things that I\nrecommend here are going to be particularly hard on your dollars either,\nbut minimizing cm3 is the primary objective.\n\nWhat do I mean by this? Well, I mean getting items like this:\n\nElectric shaver. About 5cm long and 2.5cm wide at the top.\nNo charger or handle is required: it's USBC pluggable, your phone is the\ncharger and handle. Buy on\nAmazon here (told you it's not hard on your\ndollars!)\n\nAnd this:\n\nCharger for mobile phone and laptop (can charge both at the\nsame time)! About 5x5x2.5 cm. Buy here.\n\nAnd there's more. Electric toothbrushes are normally known for being\nwide and bulky. But they don't have to be! Here\nis an electric toothbrush that is rechargeable, USBC-friendly (so no\nextra charging equipment required), only slightly wider than a regular\ntoothbrush, and costs about $30, plus a couple dollars every few months\nfor replacement brush heads. For connecting to various different\ncontinents' plugs, you can either use any\nregular reasonably small universal adapter, or get the Zendure\nPassport III which combines a universal adapter with a charger, so\nyou can plug in USBC cables to charge your laptop and multiple other\ndevices directly (!!).\n\nAs you might have noticed, a key ingredient in making this\nwork is to be a USBC maximalist. You should strive to ensure that every\nsingle thing you buy is USBC-friendly. Your laptop, your phone,\nyour toothbrush, everything. This ensures that you don't need to carry\nany extra equipment beyond one charger and 1-2 charging cables. In the\nlast ~3 years, it has become much easier to live the USBC maximalist\nlife; enjoy it!\n\n## Be a Uniqlo maximalist\n\nFor clothing, you have to navigate a tough tradeoff between price,\ncm3 and the clothing looking reasonably good. Fortunately,\nmany of the more modern brands do a great job of fulfilling all three at\nthe same time! My current strategy is to be a Uniqlo maximalist:\naltogether, about 70% of the clothing items in my bag are from\nUniqlo.\n\nThis includes:\n\n- 8 T-shirts, of which 6 are this\ntype from Uniqlo\n\n- 8 pairs of underwear, mostly various Uniqlo products\n\n- 8 socks, of which none are Uniqlo (I'm less confident about what to\ndo with socks than with other clothing items, more on this later)\n\n- Heat-tech tights,\nfrom Uniqlo\n\n- Heat-tech sweater, from Uniqlo\n\n- Packable jacket, from Uniqlo\n\n- Shorts that also double as a swimsuit, from.... ok fine, it's also\nUniqlo.\n\nThere are other stores that can give you often equally good products,\nbut Uniqlo is easily accessible in many (though not all) of the regions\nI visit and does a good job, so I usually just start and stop there.\n\n## Socks\n\nSocks are a complicated balancing act between multiple desired\ntraits:\n\n- Low cm3\n\n- Easy to put on\n\n- Warm (when needed)\n\n- Comfortable\n\nThe ideal scenario is if you find low-cut\nor ankle socks comfortable to wear, and you never go to cold\nclimates. These are very low on cm3, so you can just buy\nthose and be happy. But this doesn't work for me: I sometimes visit cold\nareas, I don't find ankle socks comfortable and prefer something a bit\nlonger, and I need to be comfortable for my long runs. Furthermore, my\nlarge foot size means that Uniqlo's one-size-fits-all approach does not\nwork well for me: though I can put the socks on, it often takes a long\ntime to do so (especially after a shower), and the socks rip often.\n\nSo I've been exploring various brands to try to find a solution\n(recently trying CEP and DarnTough).\nI generally try to find socks that cover the ankle but don't go much\nhigher than that, and I have one pair of long ones for when I go to the\nsnowier places. My sock bag is currently larger than my underwear bag,\nand only a bit smaller than my T-shirt bag: both a sign of the challenge\nof finding good socks, and a testament to Uniqlo's amazing Airism\nT-shirts. Once you do find a pair of socks that you like, ideally you\nshould just buy many copies of the same type. This removes the effort of\nsearching for a matching pair in your bag, and it ensures that if one of\nyour socks rips you don't have to choose between losing the whole pair\nand wearing mismatched socks.\n\nFor shoes, you probably want to limit yourself to at most two: some\nheavier shoes that you can just wear, and some very cm3-light\nalternative, such as flip-flops.\n\n## Layers\n\nThere is a key mathematical reason why dressing in layers is a good\nidea: it lets you cover many possible temperature ranges with fewer\nclothing items.\n\nTemperature (\u00b0C)\n\nClothing\n\n20\u00b0\n\n13\u00b0\n\n + \n\n7\u00b0\n\n + \n\n0\u00b0\n\n +  +\n\nYou want to keep the T-shirt on in all cases, to protect the other\nlayers from getting dirty. But aside from that, the general rule is: if\nyou choose N clothing items, with levels of warmness spread out across\npowers of two, then you can be comfortable in \\(2^N\\) different temperature ranges by\nbinary-encoding the expected temperature in the clothing you wear. For\nnot-so-cold climates, two layers (sweater and jacket) are fine. For a\nmore universal range of climates you'll want three layers: light\nsweater, heavy sweater and heavy jacket, which can cover \\(2^3 = 8\\) different temperature ranges all\nthe way from summer to Siberian winter (of course, heavy winter jackets\nare not easily packable, so you may have to just wear it when you get on\nthe plane).\n\nThis layering principle applies not just to upper-wear, but also\npants. I have a pair of thin pants plus Uniqlo tights, and I can wear\nthe thin pants alone in warmer climates and put the Uniqlo tights under\nthem in colder climates. The tights also double as pyjamas.\n\n## My miscellaneous stuff\n\nThe internet constantly yells at me for not having a good microphone.\nI solved this problem by getting a portable microphone!\n\nMy workstation, using the Apogee HypeMIC\ntravel microphone (unfortunately micro-USB, not USBC). A toilet paper\nroll works great as a stand, but I've also found that having a stand is\nnot really necessary and you can just let the microphone lie down beside\nyour laptop.\n\nNext, my laptop stand. Laptop stands are great for\nimproving your posture. I have two recommendations for laptop stands,\none medium-effective but very light on cm3, and one very\neffective but heavier on cm3.\n\n- The lighter one: Majextand\n\n- The more powerful one: Nexstand\n\nNexstand is the one in the picture above. Majextand is the one glued\nto the bottom of my laptop now:\n\nI have used both, and recommend both. In addition to this I also have\nanother piece of laptop gear: a 20000\nmAh laptop-friendly power bank. This adds even more to my laptop's\nalready decent battery life, and makes it generally easy to live on the\nroad.\n\nNow, my medicine bag:\n\nThis contains a combination of various life-extension medicines\n(metformin, ashwagandha, and some vitamins), and covid defense gear: a\nCO2 meter (CO2 concentration minus 420 roughly gives you how much\nhuman-breathed-out air you're breathing in, so it's a good proxy for\nvirus risk), masks, antigen tests and fluvoxamine. The tests were a free\ncare package from the Singapore government, and they happened to be\nexcellent on cm3 so I carry them around. Covid defense and\nlife extension are both fields where the science is rapidly evolving, so\ndon't blindly follow this static list; follow the science yourself or\nlisten to the latest advice of an expert that you do trust. Air filters\nand far-UVC (especially 222 nm) lamps are also promising covid defense\noptions, and portable versions exist for both.\n\nAt this particular time I don't happen to have a first aid kit with\nme, but in general it's also recommended; plenty of good travel options\nexist, eg. this.\n\nFinally, mobile data. Generally, you want to make\nsure you have a phone that supports eSIM. These days, more and more\nphones do. Wherever you go, you can buy an eSIM for that place online. I\npersonally use Airalo, but there\nare many options. If you are lazy, you can also just use Google Fi, though in my\nexperience Google Fi's quality and reliability of service tends to be\nfairly mediocre.\n\n## Have some fun!\n\nNot everything that you have needs to be designed around\ncm3 minimization. For me personally, I have four items that\nare not particularly cm3 optimized but that I still really\nenjoy having around.\n\nMy laptop bag, bought in an outdoor market in Zambia.\n\nUnisocks.\n\nSweatpants for indoor use, that are either fox-themed or Shiba\nInu-themed depending on whom you ask.\n\nGloves (phone-friendly): I bought the left one for $4 in Mong Kok\nand the right one for $5 in Chinatown, Toronto back in 2016. By\ncoincidence, I lost different ones from each pair, so the remaining two\nmatch. I keep them around as a reminder of the time when money was much\nmore scarce for me.\n\nThe more you save space on the boring stuff, the more you can leave\nsome space for a few special items that can bring the most joy to your\nlife.\n\n## How to stay sane as a nomad\n\nMany people find the nomad lifestyle to be disorienting, and report\nfeeling comfort from having a \"permanent base\". I find myself not really\nhaving these feelings: I do feel disorientation when I change locations\nmore than once every ~7 days, but as long as I'm in the same place for\nlonger than that, I acclimate and it \"feels like home\". I can't tell how\nmuch of this is my unique difficult-to-replicate personality traits, and\nhow much can be done by anyone. In general, some tips that I recommend\nare:\n\n- Plan ahead: make sure you know where you'll be at\nleast a few days in advance, and know where you're going to go when you\nland. This reduces feelings of uncertainty.\n\n- Have some other regular routine: for me, it's as\nsimple as having a piece of dark chocolate and a cup of tea every\nmorning (I prefer Bigelow\ngreen tea decaf, specifically the 40-packs, both because it's the\nmost delicious decaf green tea I've tried and because it's packaged in a\nfour-teabag-per-bigger-bag format that makes it very convenient and at\nthe same time cm3-friendly). Having some part of\nyour lifestyle the same every day helps me feel grounded. The more\ndigital your life is, the more you get this \"for free\" because you're\nstaring into the same computer no matter what physical location you're\nin, though this does come at the cost of nomadding potentially providing\nfewer benefits.\n\n- Your nomadding should be embedded in some\ncommunity: if you're just being a lowest-common-denominator\ntourist, you're doing it wrong. Find people in the places you visit who\nhave some key common interest (for me, of course, it's blockchains).\nMake friends in different cities. This helps you learn about the places\nyou visit and gives you an understanding of the local culture in a way\nthat \"ooh look at the 800 year old statue of the emperor\" never will.\nFinally, find other nomad friends, and make sure to intersect with them\nregularly. If home can't be a single place, home can be the people you\njump places with.\n\n- Have some semi-regular bases: you don't have to\nkeep visiting a completely new location every time. Visiting a place\nthat you have seen before reduces mental effort and adds to the feeling\nof regularity, and having places that you visit frequently gives you\nopportunities to put stuff down, and is important if you want your\nfriendships and local cultural connections to actually develop.\n\n## How to compromise\n\nNot everyone can survive with just the items I have. You might have\nsome need for heavier clothing that cannot fit inside one backpack. You\nmight be a big nerd in some physical-stuff-dependent field: I know life\nextension nerds, covid defense nerds, and many more. You might really\nlove your three monitors and keyboard. You might have children.\n\nThe 40-liter backpack is in my opinion a truly ideal size if you\ncan manage it: 40 liters lets you carry a week's worth of\nstuff, and generally all of life's basic necessities, and it's at the\nsame time very carry-friendly: I have never had it rejected from\ncarry-on in all the flights on many kinds of airplane that I have taken\nit, and when needed I can just barely stuff it under the seat in front\nof me in a way that looks legit to staff. Once you start going lower\nthan 40 liters, the disadvantages start stacking up and exceeding the\nmarginal upsides. But if 40 liters is not enough for you, there are two\nnatural fallback options:\n\n- A larger-than-40 liter backpack. You can find 50 liter\nbackpacks, 60 liter\nbackpacks or even\nlarger (I highly recommend backpacks over shoulder bags for carrying\nfriendliness). But the higher you go, the more tiring it is to carry,\nthe more risk there is on your spine, and the more you incur the risk\nthat you'll have a difficult situation bringing it as a carry-on on the\nplane and might even have to check it.\n\n- Backpack plus mini-suitcase. There are plenty of carry-on\nsuitcases that you can buy. You can often make it onto a plane with\na backpack and a mini-suitcase. This depends on you: you may\nfind this to be an easier-to-carry option than a really big backpack.\nThat said, there is sometimes a risk that you'll have a hard time\ncarrying it on (eg. if the plane is very full) and occasionally you'll\nhave to check something.\n\nEither option can get you up to a respectable 80 liters, and still\npreserve a lot of the benefits of the 40-liter backpack\nlifestyle. Backpack plus mini-suitcase generally seems to be more\npopular than the big backpack route. It's up to you to decide which\ntradeoffs to take, and where your personal values lie!",
    "contentLength": 15927,
    "summary": "Vitalik shares his optimized 40-liter backpack packing system after 9 years of nomadic travel, emphasizing USBC devices & Uniqlo clothing.",
    "detailedSummary": {
      "theme": "A comprehensive guide to minimalist nomadic travel with a 40-liter backpack, covering gear optimization, lifestyle tips, and practical advice for long-term digital nomadism.",
      "summary": "Vitalik shares his refined travel system developed over nine years of nomadic living, during which he took 360 flights and traveled over 1.5 million kilometers. His approach centers on optimizing for cubic centimeters rather than cost, using a 'bag-inside-a-bag' organizational structure with a 40-liter backpack that contains specialized sub-bags for different items. Vitalik advocates for becoming a 'USB-C maximalist' to minimize charging equipment and a 'Uniqlo maximalist' for clothing, emphasizing the mathematical advantage of layering clothes to cover multiple temperature ranges efficiently. Beyond gear recommendations, Vitalik provides psychological advice for nomadic living, including the importance of planning ahead, maintaining routines, embedding in communities with shared interests, and establishing semi-regular bases. He acknowledges that his system won't work for everyone and offers compromise solutions like larger backpacks or backpack-plus-mini-suitcase combinations for those needing more space.",
      "takeaways": [
        "Optimize travel gear for cubic centimeters rather than cost, treating space like a budget constraint",
        "Use USB-C compatible devices exclusively to minimize charging equipment and cables needed",
        "Organize belongings in a bag-inside-a-bag structure with specialized sub-bags for different item categories",
        "Layer clothing strategically to cover multiple temperature ranges with fewer items using mathematical principles",
        "Maintain psychological well-being while nomadic through planning, routines, community connections, and semi-regular bases"
      ],
      "controversial": [
        "Vitalik's advocacy for specific life-extension medicines like metformin and ashwagandha without medical disclaimers",
        "His COVID defense strategy including fluvoxamine, which may not align with all medical recommendations"
      ]
    }
  },
  {
    "id": "general-2022-06-15-using_snarks",
    "title": "Some ways to use ZK-SNARKs for privacy",
    "date": "2022-06-15",
    "category": "math",
    "url": "https://vitalik.eth.limo/general/2022/06/15/using_snarks.html",
    "path": "general/2022/06/15/using_snarks.html",
    "content": "- \n\n- \n\n  \n    \n\n  \n  \n  Dark Mode Toggle\n  \n    \n\n  \n\n##  Some ways to use ZK-SNARKs for privacy \n\n 2022 Jun 15 \nSee all posts\n\n \n \n\n Some ways to use ZK-SNARKs for privacy \n\nSpecial thanks to Barry Whitehat and Gubsheep for feedback and\nreview.\n\nZK-SNARKs are a powerful cryptographic tool, and an increasingly\nimportant part of the applications that people are building both in the\nblockchain space and beyond. But they are complicated, both in terms of\nhow they work, and in terms of how you can use\nthem.\n\nMy previous post explaining\nZK-SNARKs focused on the first question, attempting to explain the\nmath behind ZK-SNARKs in a way that's reasonably understandable but\nstill theoretically complete. This post will focus on the second\nquestion: how do ZK-SNARKs fit into existing applications, what are some\nexamples of what they can do, what can't they do, and what are some\ngeneral guidelines for figuring out whether or not ZK-SNARKing some\nparticular application is possible?\n\nIn particular, this post focuses on applications of ZK-SNARKs\nfor preserving privacy.\n\n## What does a ZK-SNARK do?\n\nSuppose that you have a public input \\(x\\), a private input \\(w\\), and a (public) function \\(f(x, w) \\rightarrow \\{True, False\\}\\) that\nperforms some kind of verification on the inputs. With a ZK-SNARK, you\ncan prove that you know an \\(w\\) such\nthat \\(f(x, w) = True\\) for some given\n\\(f\\) and \\(x\\), without revealing what \\(w\\) is. Additionally, the verifier can\nverify the proof much faster than it would take for them to compute\n\\(f(x, w)\\) themselves, even if they\nknow \\(w\\).\n\nThis gives the ZK-SNARK its two properties: privacy\nand scalability. As mentioned above, in this post our\nexamples will focus on privacy.\n\n## Proof of membership\n\nSuppose that you have an Ethereum wallet, and you want to prove that\nthis wallet has a proof-of-humanity registration, without revealing\nwhich registered human you are. We can mathematically describe\nthe function as follows:\n\n- The private input (\\(w\\)): your address \\(A\\), and the private key \\(k\\) to your address\n\n- The public input (\\(x\\)): the set of all addresses with\nverified proof-of-humanity profiles \\(\\{H_1\n... H_n\\}\\)\n\n- The verification function \\(f(x, w)\\):\n\n- Interpret \\(w\\) as the pair \\((A, k)\\), and \\(x\\) as the list of valid profiles \\(\\{H_1 ... H_n\\}\\)\n\n- Verify that \\(A\\) is one of the\naddresses in \\(\\{H_1 ... H_n\\}\\)\n\n- Verify that \\(privtoaddr(k) =\nA\\)\n\n- Return \\(True\\) if both\nverifications pass, \\(False\\) if either\nverification fails\n\nThe prover generates their address \\(A\\) and the associated key \\(k\\), and provides \\(w = (A, k)\\) as the private input to \\(f\\). They take the public input, the\ncurrent set of verified proof-of-humanity profiles \\(\\{H_1 ... H_n\\}\\), from the chain. They run\nthe ZK-SNARK proving algorithm, which (assuming the inputs are correct)\ngenerates the proof. The prover sends the proof to the verifier and they\nprovide the block height at which they obtained the list of verified\nprofiles.\n\nThe verifier also reads the chain, gets the list \\(\\{H_1 ... H_n\\}\\) at the height that the\nprover specified, and checks the proof. If the check passes, the\nverifier is convinced that the prover has some verified\nproof-of-humanity profile.\n\nBefore we move on to more complicated examples, I highly\nrecommend you go over the above example until you understand every bit\nof what is going on.\nMaking the\nproof-of-membership more efficient\n\nOne weakness in the above proof system is that the verifier needs to\nknow the whole set of profiles \\(\\{H_1 ...\nH_n\\}\\), and they need to spend \\(O(n)\\) time \"inputting\" this set into the\nZK-SNARK mechanism.\n\nWe can solve this by instead passing in as a public input an on-chain\nMerkle root containing all profiles (this could just be the\nstate root). We add another private input, a Merkle proof \\(M\\) proving that the prover's account \\(A\\) is in the relevant part of the\ntree.\n\nAdvanced readers: A very new and more efficient alternative to\nMerkle proofs for ZK-proving membership is Caulk. In the future, some\nof these use cases may migrate to Caulk-like schemes.\n\n## ZK-SNARKs for coins\n\nProjects like Zcash and Tornado.cash allow you to have\nprivacy-preserving currency. Now, you might think that you can\ntake the \"ZK proof-of-humanity\" above, but instead of proving access of\na proof-of-humanity profile, use it to prove access to a coin.\nBut we have a problem: we have to simultaneously solve privacy and\nthe double spending problem. That is, it should not be possible to\nspend the coin twice.\n\nHere's how we solve this. Anyone who has a coin has a private secret\n\\(s\\). They locally compute the \"leaf\"\n\\(L = hash(s, 1)\\), which gets\npublished on-chain and becomes part of the state, and \\(N = hash(s, 2)\\), which we call the\nnullifier. The state gets stored in a Merkle tree.\n\nTo spend a coin, the sender must make a ZK-SNARK where:\n\n- The public input contains a nullifier \\(N\\), the current or recent Merkle root\n\\(R\\), and a new leaf \\(L'\\) (the intent is that recipient has\na secret \\(s'\\), and passes to the\nsender \\(L' = hash(s',\n1)\\))\n\n- The private input contains a secret \\(s\\), a leaf \\(L\\) and a Merkle branch \\(M\\)\n\n- The verification function checks that:\n\n- \\(M\\) is a valid Merkle branch\nproving that \\(L\\) is a leaf in a tree\nwith root \\(R\\), where \\(R\\) is the current Merkle root of the\nstate\n\n- \\(hash(s, 1) = L\\)\n\n- \\(hash(s, 2) = N\\)\n\nThe transaction contains the nullifier \\(N\\) and the new leaf \\(L'\\). We don't actually prove anything\nabout \\(L'\\), but we \"mix it in\" to\nthe proof to prevent \\(L'\\) from\nbeing modified by third parties when the transaction is in-flight.\n\nTo verify the transaction, the chain checks the ZK-SNARK, and\nadditionally checks that \\(N\\) has not\nbeen used in a previous spending transaction. If the transaction\nsucceeds, \\(N\\) is added to the spent\nnullifier set, so that it cannot be spent again. \\(L'\\) is added to the Merkle tree.\n\nWhat is going on here? We are using a zk-SNARK to relate two values,\n\\(L\\) (which goes on-chain when a coin\nis created) and \\(N\\) (which goes\non-chain when a coin is spent), without revealing which \\(L\\) is connected to which \\(N\\). The connection between \\(L\\) and \\(N\\) can only be discovered if you know the\nsecret \\(s\\) that generates both. Each\ncoin that gets created can only be spent once (because for each \\(L\\) there is only one valid corresponding\n\\(N\\)), but which coin is\nbeing spent at a particular time is kept hidden.\n\nThis is also an important primitive to understand. Many of\nthe mechanisms we describe below will be based on a very similar\n\"privately spend only once\" gadget, though for different\npurposes.\nCoins with arbitrary\nbalances\n\nThe above can easily be extended to coins of arbitrary balances. We\nkeep the concept of \"coins\", except each coin has a (private) balance\nattached. One simple way to do this is have the chain store for each\ncoin not just the leaf \\(L\\) but also\nan encrypted balance.\n\nEach transaction would consume two coins and create two new\ncoins, and it would add two (leaf, encrypted balance) pairs to the\nstate. The ZK-SNARK would also check that the sum of the balances coming\nin equals the sum of the balances going out, and that the two output\nbalances are both non-negative.\n\n## ZK anti-denial-of-service\n\nAn interesting anti-denial-of-service\ngadget. Suppose that you have some on-chain identity that is non-trivial\nto create: it could be a proof-of-humanity profile, it could be a\nvalidator with 32 ETH, or it could just be an account that has a nonzero\nETH balance. We could create a more DoS resistant peer-to-peer network\nby only accepting a message if it comes with a proof that the message's\nsender has such a profile. Every profile would be allowed to send up to\n1000 messages per hour, and a sender's profile would be removed from the\nlist if the sender cheats. But how do we make this\nprivacy-preserving?\n\nFirst, the setup. Let \\(k\\) be the\nprivate key of a user; \\(A =\nprivtoaddr(k)\\) is the corresponding address. The list of valid\naddresses is public (eg. it's a registry on-chain). So far this is\nsimilar to the proof-of-humanity example: you have to prove that you\nhave the private key to one address without revealing which one. But\nhere, we don't just want a proof that you're in the list. We want a\nprotocol that lets you prove you're in the list but prevents you\nfrom making too many proofs. And so we need to do some more\nwork.\n\nWe'll divide up time into epochs; each epoch lasts 3.6 seconds (so,\n1000 epochs per hour). Our goal will be to allow each user to send only\none message per epoch; if the user sends two messages in the\nsame epoch, they will get caught. To allow users to send occasional\nbursts of messages, they are allowed to use epochs in the recent past,\nso if some user has 500 unused epochs they can use those epochs to send\n500 messages all at once.\n\n## The protocol\n\nWe'll start with a simple version: we use nullifiers. A user\ngenerates a nullifier with \\(N = hash(k,\ne)\\), where \\(k\\) is their key\nand \\(e\\) is the epoch number, and\npublishes it along with the message \\(m\\). The ZK-SNARK once again mixes in \\(hash(m)\\) without verifying anything about\n\\(m\\), so that the proof is bound to a\nsingle message. If a user makes two proofs bound to two different\nmessages with the same nullifier, they can get caught.\n\nNow, we'll move on to the more complex version. Instead of just\nmaking it easy to prove if someone used the same epoch twice, this next\nprotocol will actually reveal their private key in that case.\nOur core technique will rely on the \"two points make a line\" trick: if\nyou reveal one point on a line, you've revealed little, but if you\nreveal two points on a line, you've revealed the whole line.\n\nFor each epoch \\(e\\), we take the\nline \\(L_e(x) = hash(k, e) * x + k\\).\nThe slope of the line is \\(hash(k,\ne)\\), and the y-intercept is \\(k\\); neither is known to the public. To\nmake a certificate for a message \\(m\\), the sender provides \\(y = L_e(hash(m)) =\\) \\(hash(k, e) * hash(m) + k\\), along with a\nZK-SNARK proving that \\(y\\) was\ncomputed correctly.\n\nTo recap, the ZK-SNARK here is as follows:\n\n- Public input:\n\n- \\(\\{A_1 ... A_n\\}\\), the list of\nvalid accounts\n\n- \\(m\\), the message that the\ncertificate is verifying\n\n- \\(e\\), the epoch number used for\nthe certificate\n\n- \\(y\\), the line function\nevaluation\n\n- Private input:\n\n- \\(k\\), your private key\n\n- Verification function:\n\n- Check that \\(privtoaddr(k)\\) is in\n\\(\\{A_1 ... A_n\\}\\)\n\n- Check that \\(y = hash(k, e) * hash(m) +\nk\\)\n\nBut what if someone uses a single epoch twice? That means they\npublished two values \\(m_1\\) and \\(m_2\\) and the corresponding certificate\nvalues \\(y_1 = hash(k, e) * hash(m_1) +\nk\\) and \\(y_2 = hash(k, e) * hash(m_2)\n+ k\\). We can use the two points to recover the line, and hence\nthe y-intercept (which is the private key):\n\n\\(k = y_1 - hash(m_1) * \\frac{y_2 -\ny_1}{hash(m_2) - hash(m_1)}\\)\n\nSo if someone reuses an epoch, they leak out their private key for\neveryone to see. Depending on the circumstance, this could imply stolen\nfunds, a slashed validator, or simply the private key getting\nbroadcasted and included into a smart contract, at which point the\ncorresponding address would get removed from the set.\n\nWhat have we accomplished here? A viable off-chain, anonymous\nanti-denial-of-service system useful for systems like blockchain\npeer-to-peer networks, chat applications, etc, without requiring any\nproof of work. The RLN\n(rate limiting nullifier) project is currently building essentially\nthis idea, though with minor modifications (namely, they do\nboth the nullifier and the two-points-on-a-line technique,\nusing the nullifier to make it easier to catch double-use of an\nepoch).\n\n## ZK negative reputation\n\nSuppose that we want to build 0chan, an internet\nforum which provides full anonymity like 4chan (so you don't even have\npersistent names), but has a reputation system to encourage more quality\ncontent. This could be a system where some moderation DAO can flag posts\nas violating the rules of the system and institutes a\nthree-strikes-and-you're-out mechanism, it could be users being able to\nupvote and downvote posts; there are lots of configurations.\n\nThe reputation system could support positive or negative reputation;\nhowever, supporting negative reputation requires extra infrastructure to\nrequire the user to take into account all reputation messages\nin their proof, even the negative ones. It's this harder use case, which\nis similar to what is being implemented with Unirep Social, that\nwe'll focus on.\n\n## Chaining posts: the basics\n\nAnyone can make a post by publishing a message on-chain that contains\nthe post, and a ZK-SNARK proving that either (i) you own some scarce\nexternal identity, eg. proof-of-humanity, that entitles you to create an\naccount, or (ii) that you made some specific previous post.\nSpecifically, the ZK-SNARK is as follows:\n\n- Public inputs:\n\n- The nullifier \\(N\\)\n\n- A recent blockchain state root \\(R\\)\n\n- The post contents (\"mixed in\" to the proof to bind it to the post,\nbut we don't do any computation on it)\n\n- Private inputs:\n\n- Your private key \\(k\\)\n\n- Either an external identity (with address \\(A\\)), or the nullifier \\(N_{prev}\\) used by the previous post\n\n- A Merkle proof \\(M\\) proving\ninclusion of \\(A\\) or \\(N_{prev}\\) on-chain\n\n- The number \\(i\\) of posts that you\nhave previously made with this account\n\n- Verification function:\n\n- Check that \\(M\\) is a valid Merkle\nbranch proving that (either \\(A\\) or\n\\(N_{prev}\\), whichever is provided) is\na leaf in a tree with root \\(R\\)\n\n- Check that \\(N = enc(i, k)\\), where\n\\(enc\\) is an encryption function (eg.\nAES)\n\n- If \\(i = 0\\), check that \\(A = privtoaddr(k)\\), otherwise check that\n\\(N_{prev} = enc(i-1, k)\\)\n\nIn addition to verifying the proof, the chain also checks that (i)\n\\(R\\) actually is a recent state root,\nand (ii) the nullifier \\(N\\) has not\nyet been used. So far, this is like the privacy-preserving coin\nintroduced earlier, but we add a procedure for \"minting\" a new account,\nand we remove the ability to \"send\" your account to a different key -\ninstead, all nullifiers are generated using your original key.\n\nWe use \\(enc\\) instead of \\(hash\\) here to make the nullifiers\nreversible: if you have \\(k\\), you can\ndecrypt any specific nullifier you see on-chain and if the result is a\nvalid index and not random junk (eg. we could just check \\(dec(N) < 2^{64}\\)), then you know that\nnullifier was generated using \\(k\\).\n\n## Adding reputation\n\nReputation in this scheme is on-chain and in the clear: some smart\ncontract has a method addReputation, which takes as input\n(i) the nullifier published along with the post, and (ii) the number of\nreputation units to add and subtract.\n\nWe extend the on-chain data stored per post: instead of just storing\nthe nullifier \\(N\\), we store \\(\\{N, \\bar{h}, \\bar{u}\\}\\), where:\n\n- \\(\\bar{h} = hash(h, r)\\) where\n\\(h\\) is the block height of the state\nroot that was referenced in the proof\n\n- \\(\\bar{u} = hash(u, r)\\) where\n\\(u\\) is the account's reputation score\n(0 for a fresh account)\n\n\\(r\\) here is simply a random value,\nadded to prevent \\(h\\) and \\(u\\) from being uncovered by brute-force\nsearch (in cryptography jargon, adding \\(r\\) makes the hash a hiding\ncommitment).\n\nSuppose that a post uses a root \\(R\\) and stores \\(\\{N, \\bar{h}, \\bar{u}\\}\\). In the proof, it\nlinks to a previous post, with stored data \\(\\{N_{prev}, \\bar{h}_{prev},\n\\bar{u}_{prev}\\}\\). The post's proof is also required to walk\nover all the reputation entries that have been published between \\(h_{prev}\\) and \\(h\\). For each nullifier \\(N\\), the verification function would\ndecrypt \\(N\\) using the user's key\n\\(k\\), and if the decryption outputs a\nvalid index it would apply the reputation update. If the sum of all\nreputation updates is \\(\\delta\\), the\nproof would finally check \\(u = u_{prev} +\n\\delta\\).\n\nIf we want a \"three strikes and you're out\" rule, the ZK-SNARK would\nalso check \\(u > -3\\). If we want a\nrule where a post can get a special \"high-reputation poster\" flag if the\nposter has \\(\\ge 100\\) rep, we can\naccommodate that by adding \"is \\(u \\ge\n100\\)?\" as a public input. Many kinds of such rules can be\naccommodated.\n\nTo increase the scalability of the scheme, we could split it up into\ntwo kinds of messages: posts and reputation update\nacknowledgements (RCAs). A post would be off-chain, though it would\nbe required to point to an RCA made in the past week. RCAs would be\non-chain, and an RCA would walk through all the reputation updates since\nthat poster's previous RCA. This way, the on-chain load is reduced to\none transaction per poster per week plus one transaction per reputation\nmessage (a very low level if reputation updates are rare, eg. they're\nonly used for moderation actions or perhaps \"post of the day\" style\nprizes).\nHolding centralized\nparties accountable\n\nSometimes, you need to build a scheme that has a central \"operator\"\nof some kind. This could be for many reasons: sometimes it's for\nscalability, and sometimes it's for privacy - specifically, the privacy\nof data held by the operator.\n\nThe MACI\ncoercion-resistant voting system, for example, requires voters to submit\ntheir votes on-chain encrypted to a secret key held by a central\noperator. The operator would decrypt all the votes on-chain, count them\nup, and reveal the final result, along with a ZK-SNARK proving that they\ndid everything correctly. This extra complexity is necessary to ensure a\nstrong privacy property (called coercion-resistance):\nthat users cannot prove to others how they voted even if they wanted\nto.\n\nThanks to blockchains and ZK-SNARKs, the amount of trust in the\noperator can be kept very low. A malicious operator could still break\ncoercion resistance, but because votes are published on the blockchain,\nthe operator cannot cheat by censoring votes, and because the operator\nmust provide a ZK-SNARK, they cannot cheat by mis-calculating the\nresult.\n\n## Combining ZK-SNARKs with MPC\n\nA more advanced use of ZK-SNARKs involves making proofs over\ncomputations where the inputs are split between two or more parties, and\nwe don't want each party to learn the other parties' inputs. You can\nsatisfy the privacy requirement with garbled circuits in the\n2-party case, and more complicated multi-party computation protocols in\nthe N-party case. ZK-SNARKs can be combined with these protocols to do\nverifiable multi-party computation.\n\nThis could enable more advanced reputation systems where multiple\nparticipants can perform joint computations over their private inputs,\nit could enable privacy-preserving but authenticated data markets, and\nmany other applications. That said, note that the math for doing this\nefficiently is still relatively in its infancy.\n\n## What can't we make private?\n\nZK-SNARKs are generally very effective for creating systems where\nusers have private state. But ZK-SNARKs cannot hold private\nstate that nobody knows. To make a proof about a piece\nof information, the prover has to know that piece of information in\ncleartext.\n\nA simple example of what can't (easily) be made private is Uniswap.\nIn Uniswap, there is a single logically-central\n\"thing\", the market maker account, which belongs to no one, and every\nsingle trade on Uniswap is trading against the market maker account. You\ncan't hide the state of the market maker account, because then someone\nwould have to hold the state in cleartext to make proofs, and their\nactive involvement would be necessary in every single transaction.\n\nYou could make a centrally-operated, but safe and private,\nUniswap with ZK-SNARKed garbled circuits, but it's not clear that the\nbenefits of doing this are worth the costs. There may not even be any\nreal benefit: the contract would need to be able to tell users what the\nprices of the assets are, and the block-by-block changes in the prices\ntell a lot about what the trading activity is.\n\nBlockchains can make state information global, ZK-SNARKs can\nmake state information private, but we don't really have any\ngood way to make state information global and private at the\nsame time.\n\nEdit: you can use multi-party computation to implement shared\nprivate state. But this requires an honest-majority threshold\nassumption, and one that's likely unstable in practice because (unlike\neg. with 51% attacks) a malicious majority could collude to break the\nprivacy without ever being detected.\nPutting the primitives\ntogether\n\nIn the sections above, we've seen some examples that are powerful and\nuseful tools by themselves, but they are also intended to serve as\nbuilding blocks in other applications. Nullifiers, for example, are\nimportant for currency, but it turns out that they pop up again and\nagain in all kinds of use cases.\n\nThe \"forced chaining\" technique used in the negative reputation\nsection is very broadly applicable. It's effective for many applications\nwhere users have complex \"profiles\" that change in complex ways over\ntime, and you want to force the users to follow the rules of the system\nwhile preserving privacy so no one sees which user is performing which\naction. Users could even be required to have entire private Merkle trees\nrepresenting their internal \"state\". The \"commitment pool\" gadget proposed in this\npost could be built with ZK-SNARKs. And if some application can't be\nentirely on-chain and must have a centralized operator, the exact same\ntechniques can be used to keep the operator honest too.\n\nZK-SNARKs are a really powerful tool for combining together the\nbenefits of accountability and privacy. They do have their limits,\nthough in some cases clever application design can work around those\nlimits. I expect to see many more applications using ZK-SNARKs, and\neventually applications combining ZK-SNARKs with other forms of\ncryptography, to be built in the years to come.",
    "contentLength": 21823,
    "summary": "ZK-SNARKs enable privacy in applications like anonymous proof-of-humanity, private cryptocurrencies, and DoS-resistant networks.",
    "detailedSummary": {
      "theme": "Vitalik explores practical applications of ZK-SNARKs for preserving privacy in blockchain and cryptographic systems, demonstrating how they can enable anonymous verification while maintaining security and preventing abuse.",
      "summary": "Vitalik begins by explaining that ZK-SNARKs allow users to prove they know a private input that satisfies a public verification function without revealing what that private input is, providing both privacy and scalability benefits. He walks through several concrete examples, starting with proof of membership systems (like proving you have a proof-of-humanity profile without revealing which one), then progressing to privacy-preserving currencies that solve the double-spending problem through nullifiers - unique identifiers that prevent coins from being spent twice while hiding which specific coin is being spent. Vitalik also covers more advanced applications including ZK anti-denial-of-service systems that use clever cryptographic techniques like 'two points make a line' to punish users who exceed rate limits by exposing their private keys, and negative reputation systems for anonymous forums that chain posts together while tracking reputation scores. He concludes by discussing the limitations of ZK-SNARKs, particularly noting that they cannot create truly global and private state simultaneously since someone must always know the private information in cleartext to generate proofs, though he suggests these tools will become increasingly important building blocks for future privacy-preserving applications.",
      "takeaways": [
        "ZK-SNARKs enable proof of membership in groups without revealing specific identity, useful for anonymous verification systems",
        "Privacy-preserving currencies use nullifiers to solve double-spending while hiding transaction linkability",
        "Anti-DoS systems can use ZK-SNARKs with cryptographic tricks to automatically punish rate limit violations by exposing violators' private keys",
        "Chained posting systems can enable anonymous forums with reputation tracking by forcing users to acknowledge all reputation changes in their proofs",
        "ZK-SNARKs cannot make state information both global and private simultaneously since someone must always know the private data to generate proofs"
      ],
      "controversial": [
        "The 'two points make a line' technique that automatically exposes private keys of users who violate rate limits could be seen as overly punitive",
        "The reliance on centralized operators in some systems like MACI voting, even with ZK-SNARK accountability measures, may be viewed as undermining decentralization principles"
      ]
    }
  },
  {
    "id": "general-2022-06-12-nonfin",
    "title": "Where to use a blockchain in non-financial applications?",
    "date": "2022-06-12",
    "category": "applications",
    "url": "https://vitalik.eth.limo/general/2022/06/12/nonfin.html",
    "path": "general/2022/06/12/nonfin.html",
    "content": "- \n\n- \n\n  \n    \n\n  \n  \n  Dark Mode Toggle\n  \n    \n\n  \n\n##  Where to use a blockchain in non-financial applications? \n\n 2022 Jun 12 \nSee all posts\n\n \n \n\n Where to use a blockchain in non-financial applications? \n\nSpecial thanks to Shrey Jain and Puja Ohlhaver for substantial\nfeedback and review\n\nRecently, there has been a growing amount of interest in using\nblockchains for not-just-financial applications. This is a trend that I\nhave been\nstrongly in favor of, for\nvarious reasons. In the last month,\nPuja Ohlhaver, Glen Weyl and I collaborated on a paper\ndescribing a more detailed vision for what could be done with a richer\necosystem of soulbound tokens making claims describing various kinds of\nrelationships. This has led to some discussion, particularly focused on\nwhether or not it makes any sense to use a blockchain in a decentralized\nidentity ecosystem:\n\n- Kate Sills argues for\noff-chain signed claims\n\n- Puja Ohlhaver responds\nto Kate Sills\n\n- Evin McMullen and myself have a\npodcast debating on-chain vs off-chain attestations\n\n- Kevin Yu writes\na technical overview bringing up the on-chain versus off-chain\nquestion\n\n- Molly White argues a pessimistic\ncase against self-sovereign identity\n\n- Shrey Jain makes\na meta-thread containing the above and many other Twitter\ndiscussions\n\nIt's worth zooming out and asking a broader question: where does it\nmake sense, in general, to use a blockchain in non-financial\napplications? Should we move toward a world where even decentralized\nchat apps work by every message being an on-chain transaction containing\nthe encrypted message? Or, alternatively, are blockchains only good for\nfinance (say, because network effects mean that money has a unique need\nfor a \"global view\"), with all other applications better done using\ncentralized or more local systems?\n\nMy own view tends to be, like with blockchain voting, far from\nthe \"blockchain everywhere\" viewpoint, but also far from a \"blockchain\nminimalist\". I see the value of blockchains in many situations,\nsometimes for really important goals like trust and censorship\nresistance but sometimes purely for convenience. This post will attempt\nto describe some types of situations where blockchains might be useful,\nespecially in the context of identity, and where they are not.\nThis post is not a complete list and intentionally leaves many\nthings out. The goal is rather to elucidate some common\ncategories.\nUser account key changes\nand recovery\n\nOne of the biggest challenges in a cryptographic account system is\nthe issue of key changes. This can happen in a few cases:\n\n- You're worried that your current key might get lost or stolen, and\nyou want to switch to a different key\n\n- You want to switch to a different cryptographic\nalgorithm (eg. because you're worried quantum computers will\ncome soon and you want to upgrade to post-quantum)\n\n- Your key got lost, and you want to regain access to\nyour account\n\n- Your key got stolen, and you want to regain\nexclusive access to your account (and you don't want the thief to be\nable to do the same)\n\n[1] and [2] are relatively simple in that they can be done in a fully\nself-sovereign way: you control key X, you want to switch to\nkey Y, so you publish a message signed with X saying \"Authenticate me\nwith Y from now on\", and everyone accepts that.\n\nBut notice that even for these simpler key change scenarios,\nyou can't just use cryptography. Consider the\nfollowing sequence of events:\n\n- You are worried that key A might get stolen, so you sign a message\nwith A saying \"I use B now\"\n\n- A year later, a hacker actually does steal key A. They sign a\nmessage saying with A saying \"I use C now\", where C is their own\nkey\n\nFrom the point of view of someone coming in later who just receives\nthese two messages, they see that A is no longer used, but they don't\nknow whether \"replace A with B\" or \"replace A with C\" has higher\npriority.\n\nThis is equivalent to the famous double-spend\nproblem in designing decentralized currencies, except instead of the\ngoal being to prevent a previous owner of a coin from being able to send\nit again, here the goal is to prevent the previous key controlling an\naccount from being able to change the key. Just like creating a\ndecentralized currency, doing account management in a decentralized way\nrequires something like a blockchain. A blockchain can\ntimestamp the key change messages, providing common knowledge over\nwhether B or C came first.\n\n[3] and [4] are harder. In general, my own preferred solution is multisig and social recovery\nwallets, where a group of friends, family members and other contacts\ncan transfer control of your account to a new key if it gets lost or\nstolen. For critical operations (eg. transferring large quantities of\nfunds, or signing an important contract), participation of this group\ncan also be required.\n\nBut this too requires a blockchain. Social recovery using secret\nsharing is possible, but it is more difficult in practice: if you no\nlonger trust some of your contacts, or if they want to change their own\nkeys, you have no way to revoke access without changing your key\nyourself. And so we're back to requiring some form of on-chain\nrecord.\n\nOne subtle but important idea in the\nDeSoc paper is that to preserve non-transferability, social recovery\n(or \"community recovery\") of profiles might actually need to be\nmandatory. That is, even if you sell your account, you can\nalways use community recovery to get the account back. This would solve\nproblems like not-actually-reputable drivers buying\nverified accounts on ride sharing platforms. That said, this is a\nspeculative idea and does not have to be fully implemented to get the\nother benefits of blockchain-based identity and reputation systems.\n\nNote that so far this is a limited use-case of blockchains:\nit's totally okay to have accounts on-chain but do everything else\noff-chain. There's a place for these kinds of hybrid visions;\nSign-in With Ethereum is good simple\nexample of how this could be done in practice.\nModifying and revoking\nattestations\n\nAlice goes to Example College and gets a degree in example studies.\nShe gets a digital record certifying this, signed with Example College's\nkeys. Unfortunately, six months later, Example College discovers that\nAlice had committed a large amount of plagiarism, and revokes her\ndegree. But Alice continues to use her old digital record to go around\nclaiming to various people and institutions that she has a degree.\nPotentially, the attestation could even carry permissions - for\nexample, the right to log in to the college's online forum - and Alice\nmight try to inappropriately access that too. How do we prevent\nthis?\n\nThe \"blockchain maximalist\" approach would be to make the degree an\non-chain NFT, so Example College can then issue an on-chain transaction\nto revoke the NFT. But perhaps this is needlessly expensive: issuance is\ncommon, revocation is rare, and we don't want to require Example College\nto issue transactions and pay fees for every issuance if they don't have\nto. So instead we can go with a hybrid solution: make initial\ndegree an off-chain signed message, and do revocations\non-chain. This is the approach that OpenCerts uses.\n\nThe fully off-chain solution, and the one advocated by many\noff-chain verifiable credentials proponents, is that Example College\nruns a server where they publish a full list of their revocations (to\nimprove privacy, each attestation can come with an attached nonce and\nthe revocation list can just be a list of nonces).\n\nFor a college, running a server is not a large burden. But for any\nsmaller organization or individual, managing \"yet another server script\"\nand making sure it stays online is a significant burden for IT people.\nIf we tell people to \"just use a server\" out of\nblockchain-phobia, then the likely outcome is that everyone outsources\nthe task to a centralized provider. Better to keep the system\ndecentralized and just use a blockchain - especially now that rollups,\nsharding and other techniques are finally starting to come online to\nmake the cost of a blockchain cheaper and cheaper.\n\n## Negative reputation\n\nAnother important area where off-chain signatures do not suffice is\nnegative reputation - that is, attestations where the\nperson or organization that you're making attestations about might not\nwant you to see them. I'm using \"negative reputation\" here as a\ntechnical term: the most obvious motivating use case is attestations\nsaying bad things about someone, like a bad review or a report that\nsomeone acted abusively in some context, but there are also use cases\nwhere \"negative\" attestations don't imply bad behavior - for example,\ntaking out a loan and wanting to prove that you have not taken out too\nmany other loans at the same time.\n\nWith off-chain claims, you can do positive reputation,\nbecause it's in the interest of the recipient of a claim to show it to\nappear more reputable (or make a ZK-proof about it), but you can't do\nnegative reputation, because someone can always choose to only\nshow the claims that make them look good and leave out all the\nothers.\n\nHere, making attestations on-chain actually does fix things. To\nprotect privacy, we can add encryption and zero knowledge proofs: an\nattestation can just be an on-chain record with data encrypted to the\nrecipient's public key, and users could prove lack of negative\nreputation by running a zero knowledge proof that walks over the entire\nhistory of records on chain. The proofs being on-chain and the\nverification process being blockchain-aware makes it easy to verify that\nthe proof actually did walk over the whole history and did not\nskip any records. To make this computationally feasible, a user could\nuse incrementally verifiable\ncomputation (eg. Halo)\nto maintain and prove a tree of records that were encrypted to them, and\nthen reveal parts of the tree when needed.\n\nNegative reputation and revoking attestations are in some sense\nequivalent problems: you can revoke an attestation by adding another\nnegative-reputation attestation saying \"this other attestation doesn't\ncount anymore\", and you can implement negative reputation with\nrevocation by piggybacking on positive reputation: Alice's degree at\nExample College could be revoked and replaced with a degree saying\n\"Alice got a degree in example studies, but she took out a loan\".\nIs negative reputation a\ngood idea?\n\nOne critique of negative reputation that we sometimes hear is: but\nisn't negative reputation a dystopian scheme of \"scarlet\nletters\", and shouldn't we try our best to do things with positive\nreputation instead?\n\nHere, while I support the goal of avoiding unlimited\nnegative reputation, I disagree with the idea of avoiding it entirely.\nNegative reputation is important for many use cases. Uncollateralized\nlending, which is highly valuable for improving capital efficiency\nwithin the blockchain space and outside, clearly benefits from it. Unirep Social shows\na proof-of-concept social media platform that combines a high level of\nanonymity with a privacy-preserving negative reputation system to limit\nabuse.\n\nSometimes, negative reputation can be empowering and positive\nreputation can be exclusionary. An online forum where every unique human gets the\nright to post until they get too many \"strikes\" for misbehavior is more\negalitarian than a forum that requires some kind of \"proof of good\ncharacter\" to be admitted and allowed to speak in the first place.\nMarginalized people whose lives are mostly \"outside the system\", even if\nthey actually are of good character, would have a hard time getting such\nproofs.\n\nReaders of the strong civil-libertarian persuasion may also want to\nconsider the case of an anonymous reputation system for clients of sex\nworkers: you want to protect privacy, but you also might want a system\nwhere if a client mistreats a sex worker, they get a \"black mark\" that\nencourages other workers to be more careful or stay away. In this way,\nnegative reputation that's hard to hide can actually empower the\nvulnerable and protect safety. The point here is not to defend some\nspecific scheme for negative reputation; rather, it's to show\nthat there's very real value that negative reputation unlocks, and a\nsuccessful system needs to support it somehow.\n\nNegative reputation does not have to be unlimited negative\nreputation: I would argue that it should always be possible to create a\nnew profile at some cost (perhaps sacrificing a lot or all of your\nexisting positive reputation). There is a balance between too\nlittle accountability and too much accountability. But having\nsome technology that makes negative reputation possible in the\nfirst place is a prerequisite for unlocking this design space.\n\n## Committing to scarcity\n\nAnother example of where blockchains are valuable is issuing\nattestations that have a provably limited quantity. If I want to make an\nendorsement for someone (eg. one might imagine a company looking for\njobs or a government visa program looking at such endorsements), the\nthird party looking at the endorsement would want to know whether I'm\ncareful with endorsements or if I give them off to pretty much whoever\nis friends with me and asks nicely.\n\nThe ideal solution to this problem would be to make endorsements\npublic, so that endorsements become incentive-aligned: if I endorse\nsomeone who turns out to do something wrong, everyone can discount my\nendorsements in the future. But often, we also want to preserve privacy.\nSo instead what I could do is publish hashes of each endorsement\non-chain, so that anyone can see how many I have given out.\n\nAn even more effective usecase is many-at-a-time issuance: if an\nartists wants to issue N copies of a \"limited-edition\" NFT, they could\npublish on-chain a single hash containing the Merkle root of\nthe NFTs that they are issuing. The single issuance prevents them from\nissuing more after the fact, and you can publish the number (eg. 100)\nsignifying the quantity limit along with the Merkle root, signifying\nthat only the leftmost 100 Merkle branches are valid.\n\nBy publishing a single Merkle root and max count on-chain, you can\ncommit issue a limited quantity of attestations. In this example, there\nare only five possible valid Merkle branches that could satisfy the\nproof check. Astute readers may notice a conceptual similarity to Plasma\nchains.\n\n## Common knowledge\n\nOne of the powerful properties of blockchains is that they create\ncommon\nknowledge: if I publish something on-chain, then Alice can see\nit, Alice can see that Bob can see it, Charlie can see that Alice can\nsee that Bob can see it, and so on.\n\nCommon knowledge is often important for coordination. For example, a\ngroup of people might want to speak out about an issue, but only feel\ncomfortable doing so if there's enough of them speaking out at the same\ntime that they have safety in numbers. One possible way to do this is\nfor one person to start a \"commitment pool\" around a particular\nstatement, and invite others to publish hashes (which are private at\nfirst) denoting their agreement. Only if enough people participate\nwithin some period of time, all participants would be required to have\ntheir next on-chain message publicly reveal their position.\n\nA design like this could be accomplished with a combination of zero\nknowledge proofs and blockchains (it could be done without blockchains,\nbut that requires either witness encryption,\nwhich is not yet available, or trusted\nhardware, which has deeply problematic security assumptions). There\nis a large design space around these kinds of ideas that is very\nunderexplored today, but could easily start to grow once the ecosystem\naround blockchains and cryptographic tools grows further.\nInteroperability\nwith other blockchain applications\n\nThis is an easy one: some things should be on-chain to better\ninteroperate with other on-chain applications. Proof of humanity being\nan on-chain NFT makes it easier for projects to automatically airdrop or\ngive governance rights to accounts that have proof of humanity profiles.\nOracle data being on-chain makes it easier for defi projects to read. In\nall of these cases, the blockchain does not remove the need for trust,\nthough it can house structures like DAOs that manage the trust. But the\nmain value that being on-chain provides is simply being in the same\nplace as the stuff that you're interacting with, which needs a\nblockchain for other reasons.\n\nSure, you could run an oracle off-chain and require the data\nto be imported only when it needs to be read, but in many cases that\nwould actually be more expensive, and needlessly impose\ncomplexity and costs on developers.\n\n## Open-source metrics\n\nOne key goal of the Decentralized\nSociety paper is the idea that it should be possible to make\ncalculations over the graph of attestations. A really important one\nis measuring decentralization and diversity. For\nexample, many people seem\nto agree\nthat an ideal voting mechanism would somehow keep diversity in mind,\ngiving greater weight to projects that are supported not just by the\nlargest number of coins or even humans, but by the\nlargest number of truly distinct perspectives.\n\nQuadratic funding as implemented in Gitcoin Grants also includes some\nexplicitly\ndiversity-favoring logic to mitigate attacks.\n\nAnother natural place where measurements and scores are going to be\nvaluable is reputation systems. This already exists in\na centralized form with ratings, but it can be done in a much more\ndecentralized way where the algorithm is transparent while at the same\ntime preserving more user privacy.\n\nAside from tightly-coupled use cases like this, where attempts to\nmeasure to what extent some set of people is connected and feed that\ndirectly into a mechanism, there's also broader use case of helping a\ncommunity understand itself. In the case of measuring decentralization,\nthis might be a matter of identifying areas where concentration is\ngetting too high, which might require a response. In all of these cases,\nrunning computerized algorithms over large bodies of attestations and\ncommitments and doing actually important things with the outputs is\ngoing to be unavoidable.\nWe\nshould not try to abolish quantified metrics, we should try to\nmake better ones\n\nKate Sills expressed her skepticism of the goal of making\ncalculations over reputation, an argument that applies both for public\nanalytics and for individuals ZK-proving over their reputation (as in\nUnirep Social):\n\nThe process of evaluating a claim is very subjective and\ncontext-dependent. People will naturally disagree about the\ntrustworthiness of other people, and trust depends on the context ...\n[because of this] we should be extremely skeptical of any proposal to\n\"calculate over\" claims to get objective results.\n\nI this case, I agree with the importance of subjectivity and context,\nbut I would disagree with the more expansive claim that avoiding\ncalculations around reputation entirely is the right goal to be aiming\ntowards. Pure individualized analysis does not scale far beyond Dunbar's\nnumber, and any complex society that is attempting to support\nlarge-scale cooperation has to rely on aggregations and simplifications\nto some extent.\n\nThat said, I would argue that an open-participation ecosystem of\nattestations (as opposed to the centralized one we have today) can get\nus the best of both worlds by opening up space for better\nmetrics. Here are some principles that such designs could follow:\n\n- Inter-subjectivity: eg. a reputation should not be\na single global score; instead, it should be a more subjective\ncalculation involving the person or entity being evaluated but also the\nviewer checking the score, and potentially even other aspects of the\nlocal context.\n\n- Credible\nneutrality: the scheme should clearly not leave room for\npowerful elites to constantly manipulate it in their own favor. Some\npossible ways to achieve this are maximum transparency\nand infrequent change of the algorithm.\n\n- Openness: the ability to make meaningful inputs,\nand to audit other people's outputs by running the check yourself,\nshould be open to anyone, and not just restricted to a small number of\npowerful groups.\n\nIf we don't create good large-scale aggregates of social data, then\nwe risk ceding market share to opaque and centralized social credit\nscores instead.\n\nNot all data should be on-chain, but making some data public\nin a common-knowledge way can help increase a community's legibility to\nitself without creating data-access disparities that could be abused to\ncentralize control.\n\n## As a data store\n\nThis is the really controversial use case, even among those who\naccept most of the others. There is a common viewpoint in the blockchain\nspace that blockchains should only be used in those cases where they are\ntruly needed and unavoidable, and everywhere else we should use other\ntools.\n\nThis attitude makes sense in a world where transaction fees are very\nexpensive, and blockchains are uniquely incredibly inefficient. But it\nmakes less sense in a world where blockchains have rollups and sharding\nand transaction fees have dropped down to a few cents, and the\ndifference in redundancy between a blockchain and non-blockchain\ndecentralized storage might only be 100x.\n\nEven in such a world, it would not make sense to store all\ndata on-chain. But small text records? Absolutely. Why? Because\nblockchains are just a really convenient place to store\nstuff. I maintain a copy of this blog on IPFS. But uploading to\nIPFS often takes an hour, it requires centralized gateways for users to\naccess it with anything close to website levels of latency, and\noccasionally files drop off and no longer become visible. Dumping the\nentire blog on-chain, on the other hand, would solve that problem\ncompletely. Of course, the blog is too big to actually be\ndumped on-chain, even post-sharding, but the same principle applies to\nsmaller records.\n\nSome examples of small cases where putting data on-chain just to\nstore it may be the right decision include:\n\n- Augmented\nsecret sharing: splitting your password into N\npieces where any M = N-R of the pieces can recover the\npassword, but in a way where you can choose the contents of all\nN of the pieces. For example, the pieces could all be\nhashes of passwords, secrets generated through some other tool, or\nanswers to security questions. This is done by publishing an extra\nR pieces (which are random-looking) on-chain, and doing\nN-of-(N+R) secret sharing on the whole\nset.\n\n- ENS optimization. ENS could be made more\nefficient by combining all records into a single hash, only publishing\nthe hash on-chain, and requiring anyone accessing the data to get the\nfull data off of IPFS. But this would significantly increase complexity,\nand add yet another software dependency. And so ENS keeps data on-chain\neven if it is longer than 32 bytes.\n\n- Social metadata - data connected to your account\n(eg. for sign-in-with-Ethereum\npurposes) that you want to be public and that is very short in length.\nThis is generally not true for larger data like profile pictures (though\nif the picture happens to be a small SVG\nfile it could be!), but it is true for text records.\n\n- Attestations and access permissions. Especially if\nthe data being stored is less than a few hundred bytes long, it might be\nmore convenient to store the data on-chain than put the hash on-chain\nand the data off-chain.\n\nIn a lot of these cases, the tradeoff isn't just cost but also\nprivacy in those edge cases where keys or cryptography break. Sometimes,\nprivacy is only somewhat important, and the occasional loss of privacy\nfrom leaked keys or the faraway specter of quantum computing revealing\neverything in 30 years is less important than having a very high degree\nof certainty that the data will remain accessible. After all, off-chain\ndata stored in your \"data wallet\" can get hacked too.\n\nBut sometimes, data is particularly sensitive, and that can be\nanother argument against putting it on-chain, and keeping it stored\nlocally as a second layer of defense. But note that in those cases, that\nprivacy need is an argument not just against blockchains, but against\nall decentralized storage.\n\n## Conclusions\n\nOut of the above list, the two I am personally by far the most\nconfident about are interoperability with other blockchain\napplications and account management. The first\nis on-chain already, and the second is relatively cheap (need to use the\nchain once per user, and not once per action), the case for it is clear,\nand there really isn't a good non-blockchain-based solution.\n\nNegative reputation and revocations\nare also important, though they are still relatively early-stage use\ncases. A lot can be done with reputation by relying on off-chain\npositive reputation only, but I expect that the case for revocation and\nnegative reputation will become more clear over time. I expect there to\nbe attempts to do it with centralized servers, but over time it should\nbecome clear that blockchains are the only way to avoid a hard choice\nbetween inconvenience and centralization.\n\nBlockchains as data stores for short text records\nmay be marginal or may be significant, but I do expect at least some of\nthat kind of usage to keep happening. Blockchains really are just\nincredibly convenient for cheap and reliable data retrieval, where data\ncontinues to be retrievable whether the application has two users or two\nmillion. Open-source metrics are still a very\nearly-stage idea, and it remains to see just how much can be done and\nmade open without it becoming exploitable (as eg. online reviews, social\nmedia karma and the like get exploited all the time). And common\nknowledge games require convincing people to accept entirely\nnew workflows for socially important things, so of course that is an\nearly-stage idea too.\n\nI have a large degree of uncertainty in exactly what level of\nnon-financial blockchain usage in each of these categories makes sense,\nbut it seems clear that blockchains as an enabling tool in these areas\nshould not be dismissed.",
    "contentLength": 26009,
    "summary": "The blog argues blockchains are useful for non-financial apps like identity systems when you need timestamped records, revocations, or negative reputation.",
    "detailedSummary": {
      "theme": "Vitalik explores where blockchains provide genuine value in non-financial applications, rejecting both blockchain maximalism and minimalism while identifying specific use cases where blockchain's unique properties solve real problems.",
      "summary": "Vitalik argues that blockchains have legitimate uses beyond finance, but rejects the 'blockchain everywhere' approach in favor of selective application where blockchain properties genuinely solve problems. He identifies several key areas where blockchains provide unique value: account management and key recovery (solving the double-spend-like problem of conflicting key changes), attestation revocation (allowing modification of credentials without requiring constant server maintenance), and negative reputation systems (enabling comprehensive reputation tracking that can't be gamed by selective disclosure). Vitalik also discusses how blockchains enable commitment to scarcity, create common knowledge for coordination, facilitate interoperability with other blockchain applications, and support open-source metrics for measuring decentralization and diversity. While acknowledging that not all data should be on-chain, he argues that as transaction costs decrease through rollups and sharding, blockchains become increasingly practical as convenient data stores for small records. He emphasizes that the goal isn't to eliminate quantified metrics but to create better, more transparent ones that avoid the centralization and opacity of current systems.",
      "takeaways": [
        "Blockchains solve critical account management problems by providing timestamping and ordering for key changes, similar to how they solve double-spending in currencies",
        "Negative reputation systems require blockchain infrastructure because users can selectively hide unfavorable off-chain attestations, but cannot hide on-chain records",
        "As transaction costs decrease through scaling solutions, blockchains become viable for storing small data records due to their convenience and reliability",
        "The choice isn't between blockchain maximalism and minimalism, but strategic deployment where blockchain properties provide genuine advantages over alternatives",
        "Open and transparent reputation metrics are preferable to eliminating quantified systems entirely, as complex societies require aggregations for large-scale cooperation"
      ],
      "controversial": [
        "Using blockchains as data stores for convenience rather than necessity may be seen as wasteful by blockchain minimalists",
        "Mandatory social recovery systems that prevent account transfers could be viewed as overly paternalistic",
        "Negative reputation systems raise privacy concerns and could create 'scarlet letter' scenarios despite Vitalik's arguments for their necessity",
        "The push for quantified, algorithmic reputation systems may be seen as dehumanizing compared to purely subjective, context-dependent trust assessments"
      ]
    }
  },
  {
    "id": "general-2022-05-25-stable",
    "title": "Two thought experiments to evaluate automated stablecoins",
    "date": "2022-05-25",
    "category": "applications",
    "url": "https://vitalik.eth.limo/general/2022/05/25/stable.html",
    "path": "general/2022/05/25/stable.html",
    "content": "- \n\n- \n\n  \n    \n\n  \n  \n  Dark Mode Toggle\n  \n    \n\n  \n\n##  Two thought experiments to evaluate automated stablecoins \n\n 2022 May 25 \nSee all posts\n\n \n \n\n Two thought experiments to evaluate automated stablecoins \n\nSpecial thanks to Dan Robinson, Hayden Adams and Dankrad Feist\nfor feedback and review.\n\nThe recent LUNA crash, which led to tens of billions of dollars of\nlosses, has led to a storm\nof criticism\nof \"algorithmic\nstablecoins\" as a category,\nwith many considering\nthem to be a \"fundamentally\nflawed product\". The greater level of scrutiny on defi financial\nmechanisms, especially those that try very hard to optimize for \"capital\nefficiency\", is highly welcome. The greater acknowledgement that present\nperformance is no guarantee of future returns (or even future\nlack-of-total-collapse) is even more welcome. Where the sentiment goes\nvery wrong, however, is in painting all automated pure-crypto\nstablecoins with the same brush, and dismissing the entire category.\n\nWhile there are plenty of automated stablecoin designs that are\nfundamentally flawed and doomed to collapse eventually, and plenty more\nthat can survive theoretically but are highly risky, there are also many\nstablecoins that are highly robust in theory, and have survived extreme\ntests of crypto market conditions in practice. Hence, what we need\nis not stablecoin boosterism or stablecoin doomerism, but\nrather a return to principles-based thinking. So what\nare some good principles for evaluating whether or not a\nparticular automated stablecoin is a truly stable one? For me, the test\nthat I start from is asking how the stablecoin responds to two thought\nexperiments.\n\nClick here to skip straight to the\nthought experiments.\nReminder: what is an\nautomated stablecoin?\n\nFor the purposes of this post, an automated stablecoin is a system\nthat has the following properties:\n\n- It issues a stablecoin, which attempts to\ntarget a particular price index.\nUsually, the target is 1 USD, but there are other options too. There is\nsome targeting mechanism that continuously works to\npush the price toward the index if it veers away in either direction.\nThis makes ETH and BTC not stablecoins (duh).\n\n- The targeting mechanism is completely\ndecentralized, and free of protocol-level dependencies on\nspecific trusted actors. Particularly, it must not rely on asset\ncustodians. This makes USDT and USDC not automated\nstablecoins.\n\nIn practice, (2) means that the targeting mechanism must be some kind\nof smart contract which manages some reserve of crypto-assets, and uses\nthose crypto-assets to prop up the price if it drops.\n\n## How does Terra work?\n\nTerra-style\nstablecoins (roughly the same family as seignorage\nshares, though many implementation details differ) work by having a\npair of two coins, which we'll call a stablecoin and a\nvolatile-coin or volcoin (in Terra, UST is the\nstablecoin and LUNA is the volcoin). The stablecoin retains stability\nusing a simple mechanism:\n\n- If the price of the stablecoin exceeds the target, the system\nauctions off new stablecoins (and uses the revenue to burn volcoins)\nuntil the price returns to the target\n\n- If the price of the stablecoin drops below the target, the system\nbuys back and burns stablecoins (issuing new volcoins to fund the burn)\nuntil the price returns to the target\n\nNow what is the price of the volcoin? The volcoin's value could be\npurely speculative, backed by an assumption of greater stablecoin demand\nin the future (which would require burning volcoins to issue).\nAlternatively, the value could come from fees: either trading fees on\nstablecoin <-> volcoin exchange, or holding fees charged per year\nto stablecoin holders, or both. But in all cases, the price of\nthe volcoin comes from the expectation of future activity in the\nsystem.\n\n## How does RAI work?\n\nIn this post I'm focusing on RAI rather than DAI because RAI better\nexemplifies the pure \"ideal type\" of a collateralized automated\nstablecoin, backed by ETH only. DAI is a hybrid system backed by both\ncentralized and decentralized collateral, which is a reasonable choice\nfor their product but it does make analysis trickier.\n\nIn RAI, there are two main categories of participants (there's also\nholders of FLX, the speculative token, but they play a less important\nrole):\n\n- A RAI holder holds RAI, the stablecoin of the RAI\nsystem.\n\n- A RAI lender deposits some ETH into a smart\ncontract object called a \"safe\". They can then withdraw\nRAI up to the value of \\(\\frac{2}{3}\\)\nof that ETH (eg. if 1 ETH = 100 RAI, then if you deposit 10 ETH you can\nwithdraw up to \\(10 * 100 * \\frac{2}{3}\n\\approx 667\\) RAI). A lender can recover the ETH in the same if\nthey pay back their RAI debt.\n\nThere are two main reasons to become a RAI lender:\n\n- To go long on ETH: if you deposit 10 ETH and\nwithdraw 500 RAI in the above example, you end up with a position worth\n500 RAI but with 10 ETH of exposure, so it goes up/down by 2% for every\n1% change in the ETH price.\n\n- Arbitrage if you find a fiat-denominated investment\nthat goes up faster than RAI, you can borrow RAI, put the funds into\nthat investment, and earn a profit on the difference.\n\nIf the ETH price drops, and a safe no longer has enough collateral\n(meaning, the RAI debt is now more than \\(\\frac{2}{3}\\) times the value of the ETH\ndeposited), a liquidation event takes place. The safe\ngets auctioned off for anyone else to buy by putting up more\ncollateral.\n\nThe other main mechanism to understand is redemption rate\nadjustment. In RAI, the target isn't a fixed quantity of USD;\ninstead, it moves up or down, and the rate at which it moves up or down\nadjusts in response to market conditions:\n\n- If the price of RAI is above the target, the redemption rate\ndecreases, reducing the incentive to hold RAI and increasing\nthe incentive to hold negative RAI by being a lender. This pushes the\nprice back down.\n\n- If the price of RAI is below the target, the redemption rate\nincreases, increasing the incentive to hold RAI and reducing\nthe incentive to hold negative RAI by being a lender. This pushes the\nprice back up.\n\nThought\nexperiment 1: can the stablecoin, even in theory, safely \"wind down\" to\nzero users?\n\nIn the non-crypto real world, nothing lasts forever. Companies shut\ndown all the time, either because they never manage to find enough users\nin the first place, or because once-strong demand for their product is\nno longer there, or because they get displaced by a superior competitor.\nSometimes, there are partial collapses, declines from mainstream status\nto niche status (eg. MySpace). Such things have to happen to make room\nfor new products. But in the non-crypto world, when a product shuts down\nor declines, customers generally don't get hurt all that much.\nThere are certainly some cases of people falling through the cracks, but\non the whole shutdowns are orderly and the problem is manageable.\n\nBut what about automated stablecoins? What happens if we look at a\nstablecoin from the bold and radical perspective that the system's\nability to avoid collapsing and losing huge amounts of user funds should\nnot depend on a constant influx of new users? Let's see and\nfind out!\n\n## Can Terra wind down?\n\nIn Terra, the price of the volcoin (LUNA) comes from the expectation\nof fees from future activity in the system. So what happens if\nexpected future activity drops to near-zero? The market cap of\nthe volcoin drops until it becomes quite small relative to the\nstablecoin. At that point, the system becomes extremely\nfragile: only a small downward shock to demand for the\nstablecoin could lead to the targeting mechanism printing lots of\nvolcoins, which causes the volcoin to hyperinflate, at which point the\nstablecoin too loses its value.\n\nThe system's collapse can even become a self-fulfilling\nprophecy: if it seems like a collapse is likely, this reduces\nthe expectation of future fees that is the basis of the value of the\nvolcoin, pushing the volcoin's market cap down, making the system even\nmore fragile and potentially triggering that very collapse - exactly as\nwe saw happen with Terra in May.\n\n LUNA price, May 8-12 \n\n UST price, May 8-12 \n\nFirst, the volcoin price drops. Then, the stablecoin starts to\nshake. The system attempts to shore up stablecoin demand by issuing more\nvolcoins. With confidence in the system low, there are few buyers, so\nthe volcoin price rapidly falls. Finally, once the volcoin price is\nnear-zero, the stablecoin too collapses.\n\nIn principle, if demand decreases extremely slowly, the volcoin's\nexpected future fees and hence its market cap could still be large\nrelative to the stablecoin, and so the system would continue to be\nstable at every step of its decline. But this kind of successful\nslowly-decreasing managed decline is very unlikely. What's more likely\nis a rapid drop in interest followed by a bang.\n\nSafe wind-down: at every step, there's enough expected future\nrevenue to justify enough volcoin market cap to keep the stablecoin safe\nat its current level.\n\nUnsafe wind-down: at some point, there's not enough expected\nfuture revenue to justify enough volcoin market cap to keep the\nstablecoin safe. Collapse is likely.\n\n## Can RAI wind down?\n\nRAI's security depends on an asset external to the RAI system\n(ETH), so RAI has a much easier time safely winding down. If\nthe decline in demand is unbalanced (so, either demand for holding drops\nfaster or demand for lending drops faster), the redemption rate will\nadjust to equalize the two. The lenders are holding a leveraged position\nin ETH, not FLX, so there's no risk of a positive-feedback loop where\nreduced confidence in RAI causes demand for lending to also\ndecrease.\n\nIf, in the extreme case, all demand for holding RAI disappears\nsimultaneously except for one holder, the redemption rate would\nskyrocket until eventually every lender's safe gets liquidated. The\nsingle remaining holder would be able to buy the safe in the liquidation\nauction, use their RAI to immediately clear its debt, and withdraw the\nETH. This gives them the opportunity to get a fair price for their RAI,\npaid for out of the ETH in the safe.\n\nAnother extreme case worth examining is where RAI becomes the\nprimary appliation on Ethereum. In this case, a reduction in\nexpected future demand for RAI would crater the price of ETH. In the\nextreme case, a cascade of liquidations is possible, leading to a messy\ncollapse of the system. But RAI is far more robust against this\npossibility than a Terra-style system.\nThought\nexperiment 2: what happens if you try to peg the stablecoin to an index\nthat goes up 20% per year?\n\nCurrently, stablecoins tend to be pegged to the US dollar. RAI stands\nout as a slight exception, because its peg adjusts up or down due to the\nredemption rate and the peg started at 3.14 USD instead of 1 USD (the\nexact starting value was a concession to being normie-friendly, as a\ntrue math nerd would have chosen tau =\n6.28 USD instead). But they do not have to be. You can have a\nstablecoin pegged to a basket of assets, a consumer price index, or some\narbitrarily complex formula (\"a quantity of value sufficient to buy\n{global average CO2 concentration minus 375} hectares of land in the\nforests of Yakutia\"). As long as you can find an oracle to prove the\nindex, and people to participate on all sides of the market, you can\nmake such a stablecoin work.\n\nAs a thought experiment to evaluate sustainability, let's imagine a\nstablecoin with a particular index: a quantity of US dollars that grows\nby 20% per year. In math language, the index is \\(1.2^{(t - t_0)}\\) USD, where \\(t\\) is the current time in years and \\(t_0\\) is the time when the system launched.\nAn even more fun alternative is \\(1.04^{\\frac{1}{2}*(t - t_0)^2}\\) USD, so it\nstarts off acting like a regular USD-denominated stablecoin, but the\nUSD-denominated return rate keeps increasing by 4% every year.\n\nObviously, there is no genuine investment that can get anywhere close\nto 20% returns per year, and there is definitely no genuine\ninvestment that can keep increasing its return rate by 4% per year\nforever. But what happens if you try?\n\nI will claim that there's basically two ways for a\nstablecoin that tries to track such an index to turn out:\n\n- It charges some kind of negative interest rate on holders that\nequilibrates to basically cancel out the USD-denominated growth rate\nbuilt in to the index.\n\n- It turns into a Ponzi, giving stablecoin holders amazing returns for\nsome time until one day it suddenly collapses with a bang.\n\nIt should be pretty easy to understand why RAI does (1) and LUNA does\n(2), and so RAI is better than LUNA. But this also shows a deeper and\nmore important fact about stablecoins: for a collateralized\nautomated stablecoin to be sustainable, it has to somehow\ncontain the possibility of implementing a negative interest\nrate. A version of RAI programmatically prevented from\nimplementing negative interest rates (which is what the earlier single-collateral\nDAI basically was) would also turn into a Ponzi if tethered to a\nrapidly-appreciating price index.\n\nEven outside of crazy hypotheticals where you build a stablecoin to\ntrack a Ponzi index, the stablecoin must somehow be able to\nrespond to situations where even at a zero interest rate, demand for\nholding exceeds demand for borrowing. If you don't, the price rises\nabove the peg, and the stablecoin becomes vulnerable to price movements\nin both directions that are quite unpredictable.\n\nNegative interest rates can be done in two ways:\n\n- RAI-style, having a floating target that can drop over time if the\nredemption rate is negative\n\n- Actually having balances decrease over time\n\nOption (1) has the user-experience flaw that the stablecoin no longer\ncleanly tracks \"1 USD\". Option (2) has the developer-experience flaw\nthat developers aren't used to dealing with assets where receiving N\ncoins does not unconditionally mean that you can later send N coins. But\nchoosing one of the two seems unavoidable - unless you go the MakerDAO\nroute of being a hybrid stablecoin that uses both pure\ncryptoassets and centralized assets like USDC as collateral.\n\n## What can we learn?\n\nIn general, the crypto space needs to move away from the attitude\nthat it's okay to achieve safety by relying on endless growth. It's\ncertainly not acceptable to maintain that attitude by saying that \"the\nfiat world works in the same way\", because the fiat world is not\nattempting to offer anyone returns that go up much faster than the\nregular economy, outside of isolated cases that certainly should be\ncriticized with the same ferocity.\n\nInstead, while we certainly should hope for growth, we should\nevaluate how safe systems are by looking at their steady state, and even\nthe pessimistic state of how they would fare under extreme conditions\nand ultimately whether or not they can safely wind down. If a system\npasses this test, that does not mean it's safe; it could still be\nfragile for other reasons (eg. insufficient collateral ratios), or have\nbugs or governance\nvulnerabilities. But steady-state and extreme-case soundness should\nalways be one of the first things that we check for.",
    "contentLength": 15103,
    "summary": "Vitalik proposes two thought experiments to test stablecoin robustness after Terra's collapse, favoring ETH-backed systems over speculative ones.",
    "detailedSummary": {
      "theme": "Vitalik proposes two thought experiments to evaluate the sustainability and robustness of automated stablecoins, using Terra (LUNA/UST) and RAI as contrasting examples.",
      "summary": "In response to the Terra LUNA collapse, Vitalik argues against dismissing all automated stablecoins and instead advocates for principles-based evaluation using two key thought experiments. The first examines whether a stablecoin can safely 'wind down' to zero users without catastrophic losses, while the second tests what happens when pegging to an index with 20% annual returns. Vitalik demonstrates that Terra-style systems fail both tests because they rely on expected future growth and fees, making them vulnerable to death spirals when confidence drops. In contrast, RAI-style collateralized systems pass these tests because they're backed by external assets (ETH) and can implement negative interest rates through floating targets or declining balances. Vitalik concludes that sustainable stablecoins must be able to handle steady-state conditions and extreme scenarios without depending on endless growth, and should incorporate mechanisms for negative interest rates to maintain stability across all market conditions.",
      "takeaways": [
        "Not all automated stablecoins are fundamentally flawed - the category requires nuanced analysis rather than blanket dismissal",
        "Terra-style stablecoins are inherently fragile because their volcoin value depends on expectations of future system activity, creating vulnerability to death spirals",
        "RAI-style collateralized stablecoins are more robust because they're backed by external assets and can safely wind down without catastrophic losses",
        "Sustainable stablecoins must be able to implement negative interest rates to handle situations where holding demand exceeds borrowing demand",
        "The crypto space should evaluate system safety based on steady-state conditions and extreme scenarios rather than assuming endless growth"
      ],
      "controversial": [
        "Vitalik's dismissal of growth-dependent models may be seen as overly conservative by those who believe sustainable growth models are possible",
        "The argument that negative interest rates are necessary for stablecoin sustainability challenges conventional expectations about money and stores of value"
      ]
    }
  },
  {
    "id": "general-2022-04-01-maximalist",
    "title": "In Defense of Bitcoin Maximalism",
    "date": "2022-04-01",
    "category": "society",
    "url": "https://vitalik.eth.limo/general/2022/04/01/maximalist.html",
    "path": "general/2022/04/01/maximalist.html",
    "content": "- \n\n- \n\n  \n    \n\n  \n  \n  Dark Mode Toggle\n  \n    \n\n  \n\n##  In Defense of Bitcoin Maximalism \n\n 2022 Apr 01 \nSee all posts\n\n \n \n\n In Defense of Bitcoin Maximalism \n\nWe've been hearing for years that the future is blockchain,\nnot Bitcoin. The future of the world won't be one major\ncryptocurrency, or even a few, but many cryptocurrencies - and the\nwinning ones will have strong leadership under one central roof to adapt\nrapidly to users' needs for scale. Bitcoin is a boomer\ncoin,\nand Ethereum is soon to follow; it will be newer and more energetic\nassets that attract the new waves of mass users who don't care about\nweird libertarian ideology or \"self-sovereign verification\", are turned\noff by toxicity and anti-government mentality, and just want blockchain\ndefi and games that are fast and work.\n\nBut what if this narrative is all wrong, and the ideas, habits and\npractices of Bitcoin maximalism are in fact pretty close to correct?\nWhat if Bitcoin is far more than an outdated pet rock tied to a network\neffect? What if Bitcoin maximalists actually deeply understand that they\nare operating in a very hostile and uncertain world where there are\nthings that need to be fought for, and their actions, personalities and\nopinions on protocol design deeply reflect that fact? What if we live in\na world of honest cryptocurrencies (of which there are very\nfew) and grifter cryptocurrencies (of which there are very\nmany), and a healthy dose of intolerance is in fact necessary\nto prevent the former from sliding into the latter? That is the argument\nthat this post will make.\nWe\nlive in a dangerous world, and protecting freedom is serious\nbusiness\n\nHopefully, this is much more obvious now than it was six weeks ago,\nwhen many people still seriously thought that Vladimir Putin is a\nmisunderstood and kindly character who is merely trying to protect\nRussia and save Western Civilization from the gaypocalypse. But it's\nstill worth repeating. We live in a dangerous world, where there\nare plenty of bad-faith actors who do not listen to compassion and\nreason.\n\nA blockchain is at its core a security technology - a technology that\nis fundamentally all about protecting people and helping them survive in\nsuch an unfriendly world. It is, like the Phial of\nGaladriel, \"a light to you in dark places, when all other lights go\nout\". It is not a low-cost light, or a fluorescent hippie\nenergy-efficient light, or a high-performance light. It is a light that\nsacrifices on all of those dimensions to optimize for one thing\nand one thing only: to be a light that does when it needs to do when\nyou're facing the toughest challenge of your life and there is a friggin\ntwenty foot spider staring at you in the face.\n\nSource: https://www.blackgate.com/2014/12/23/frodo-baggins-lady-galadriel-and-the-games-of-the-mighty/\n\nBlockchains are being used every day by unbanked and underbanked\npeople, by activists, by sex workers, by refugees, and by many other\ngroups either who are uninteresting for profit-seeking centralized\nfinancial institutions to serve, or who have enemies that don't\nwant them to be served. They are used as a primary lifeline by\nmany people to make their payments and store their savings.\n\nAnd to that end, public blockchains sacrifice a lot for\nsecurity:\n\n- Blockchains require each transaction to be independently verified\nthousands of times to be accepted.\n\n- Unlike centralized systems that confirm transactions in a few\nhundred milliseconds, blockchains require users to wait anywhere from 10\nseconds to 10 minutes to get a confirmation.\n\n- Blockchains require users to be fully in charge of authenticating\nthemselves: if you lose your key, you lose your coins.\n\n- Blockchains sacrifice privacy, requiring even crazier and more expensive\ntechnology to get that privacy back.\n\nWhat are all of these sacrifices for? To create a system that\ncan survive in an unfriendly world, and actually do the job of being \"a\nlight in dark places, when all other lights go out\".\n\nExcellent at that task requires two key ingredients: (i) a\nrobust and defensible technology stack and (ii) a\nrobust and defensible culture. The key property to have\nin a robust and defensible technology stack is a focus on\nsimplicity and deep mathematical purity: a 1 MB block\nsize, a 21 million coin limit, and a simple Nakamoto consensus proof of\nwork mechanism that even a high school student can understand. The\nprotocol design must be easy to justify decades and centuries down the\nline; the technology and parameter choices must be a work of\nart.\n\nThe second ingredient is the culture of uncompromising,\nsteadfast minimalism. This must be a culture that can stand unyieldingly\nin defending itself against corporate and government actors trying to\nco-opt the ecosystem from outside, as well as bad actors inside\nthe crypto space trying to exploit it for personal profit, of which there\nare many.\n\nNow, what do Bitcoin and Ethereum culture actually look like? Well,\nlet's ask Kevin Pham:\n\nDon't believe this is representative? Well, let's ask Kevin Pham\nagain:\n\nNow, you might say, this is just Ethereum people having fun, and at\nthe end of the day they understand what they have to do and what they\nare dealing with. But do they? Let's look at the kinds of people that\nVitalik Buterin, the founder of Ethereum, hangs out with:\n\nVitalik hangs out with elite tech CEOs in Beijing, China.\n\nVitalik meets Vladimir Putin in Russia.\n\nVitalik meets Nir Bakrat, mayor of Jerusalem.\n\nVitalik shakes hands with Argentinian former president Mauricio\nMacri.\n\nVitalik gives a friendly hello to Eric Schmidt, former CEO of Google\nand advisor to US Department of Defense.\n\nVitalik has his first of many meetings with Audrey Tang, digital\nminister of Taiwan.\n\nAnd this is only a small selection. The immediate question that\nanyone looking at this should ask is: what the hell is the point of\npublicly meeting with all these people? Some of these people are\nvery decent entrepreneurs and politicians, but others are actively\ninvolved in serious human rights abuses that Vitalik certainly does not\nsupport. Does Vitalik not realize just how much some of these people are\ngeopolitically at each other's throats?\n\nNow, maybe he is just an idealistic person who believes in talking to\npeople to help bring about world peace, and a follower of Frederick\nDouglass's dictum to \"unite with anybody to do right and with nobody\nto do wrong\". But there's also a simpler hypothesis: Vitalik is a\nhippy-happy globetrotting pleasure and status-seeker, and he deeply\nenjoys meeting and feeling respected by people who are important.\nAnd it's not just Vitalik; companies like Consensys are totally happy to\npartner\nwith Saudi Arabia, and the ecosystem as a whole keeps trying to look\nto mainstream figures for validation.\n\nNow ask yourself the question: when the time comes, actually\nimportant things are happening on the blockchain - actually\nimportant things that offend people who are powerful - which\necosystem would be more willing to put its foot down and refuse to\ncensor them no matter how much pressure is applied on them to do so? The\necosystem with globe-trotting nomads who really really care about being\neveryone's friend, or the ecosystem with people who take pictures of\nthemslves with an AR15 and an axe as a side hobby?\nCurrency\nis not \"just the first app\". It's by far the most successful one.\n\nMany people of the \"blockchain, not Bitcoin\" persuasion argue that\ncryptocurrency is the first application of blockchains, but it's a very\nboring one, and the true potential of blockchains lies in bigger and\nmore exciting things. Let's go through the list of applications in the Ethereum\nwhitepaper:\n\n- Issuing tokens\n\n- Financial derivatives\n\n- Stablecoins\n\n- Identity and reputation systems\n\n- Decentralized file storage\n\n- Decentralized autonomous organizations (DAOs)\n\n- Peer-to-peer gambling\n\n- Prediction markets\n\nMany of these categories have applications that have launched and\nthat have at least some users. That said, cryptocurrency people\nreally value empowering under-banked people in the \"Global South\". Which\nof these applications actually have lots of users in the Global\nSouth?\n\nAs it turns out, by far the most successful one is storing wealth and\npayments. 3%\nof Argentinians own cryptocurrency, as do 6% of Nigerians\nand 12% of\npeople in Ukraine. By far the biggest instance of a government using\nblockchains to accomplish something useful today is cryptocurrency donations to the\ngovernment of Ukraine, which have raised more\nthan $100 million if you include donations to non-governmental\nUkraine-related efforts.\n\nWhat other application has anywhere close to that level of actual,\nreal adoption today? Perhaps the closest is ENS. DAOs are real and growing, but\ntoday far too many of them are appealing to wealthy rich-country people\nwhose main interest is having fun and using cartoon-character profiles\nto satisfy their first-world need for self-expression, and not build\nschools and hospitals and solve other real world problems.\n\nThus, we can see the two sides pretty clearly: team \"blockchain\",\nprivileged people in wealthy countries who love to virtue-signal about\n\"moving beyond money and capitalism\" and can't help being excited about\n\"decentralized governance experimentation\" as a hobby, and team\n\"Bitcoin\", a highly diverse group of both rich and poor people in many\ncountries around the world including the Global South, who are actually\nusing the capitalist tool of free self-sovereign money to provide real\nvalue to human beings today.\nFocusing\nexclusively on being money makes for better money\n\nA common misconception about why Bitcoin does not support \"richly\nstateful\" smart contracts goes as follows. Bitcoin really really values\nbeing simple, and particularly having low technical complexity, to\nreduce the chance that something will go wrong. As a result, it doesn't\nwant to add the more complicated features and opcodes that are necessary\nto be able to support more complicated smart contracts in Ethereum.\n\nThis misconception is, of course, wrong. In fact, there are\nplenty of ways to add rich statefulness into Bitcoin; search\nfor the word \"covenants\" in\nBitcoin chat archives to see many proposals being discussed. And many of\nthese proposals are surprisingly simple. The reason why covenants have\nnot been added is not that Bitcoin developers see the value in\nrich statefulness but find even a little bit more protocol complexity\nintolerable. Rather, it's because Bitcoin developers are worried\nabout the risks of the systemic complexity that\nrich statefulness being possible would introduce into the ecosystem!\n\nA recent paper by\nBitcoin researchers describes some ways to introduce covenants to\nadd some degree of rich statefulness to Bitcoin.\n\nEthereum's battle\nwith miner-extractable value (MEV) is an excellent example of this\nproblem appearing in practice. It's very easy in Ethereum to build\napplications where the next person to interact with some contract gets a\nsubstantial reward, causing transactors and miners to fight over it, and\ncontributing greatly to network centralization risk and requiring\ncomplicated workarounds. In Bitcoin, building such systemically\nrisky applications is hard, in large part because Bitcoin lacks rich\nstatefulness and focuses on the simple (and MEV-free) use case of\njust being money.\n\nSystemic contagion can happen in non-technical ways too. Bitcoin just\nbeing money means that Bitcoin requires relatively few developers,\nhelping to reduce the risk that developers will start demanding to\nprint themselves free money to build new protocol features. Bitcoin\njust being money reduces pressure for core developers to keep adding\nfeatures to \"keep up with the competition\" and \"serve developers'\nneeds\".\n\nIn so many ways, systemic effects are real, and it's just not\npossible for a currency to \"enable\" an ecosystem of highly complex and\nrisky decentralized applications without that complexity biting it back\nsomehow. Bitcoin makes the safe choice. If Ethereum continues\nits layer-2-centric approach, ETH-the-currency may gain some\ndistance from the application ecosystem that it's enabling and thereby\nget some protection. So-called high-performance layer-1 platforms, on\nthe other hand, stand no chance.\nIn\ngeneral, the earliest projects in an industry are the most\n\"genuine\"\n\nMany industries and fields follow a similar pattern. First, some new\nexciting technology either gets invented, or gets a big leap of\nimprovement to the point where it's actually usable for something. At\nthe beginning, the technology is still clunky, it is too risky for\nalmost anyone to touch as an investment, and there is no \"social proof\"\nthat people can use it to become successful. As a result, the first\npeople involved are going to be the idealists, tech geeks and others who\nare genuinely excited about the technology and its potential to improve\nsociety.\n\nOnce the technology proves itself enough, however, the normies come\nin - an event that in internet culture is often called Eternal\nSeptember. And these are not just regular kindly normies who want to\nfeel part of something exciting, but business normies, wearing\nsuits, who start scouring the ecosystem wolf-eyed for ways to\nmake money - with armies of venture capitalists just as eager to make\ntheir own money supporting them from the sidelines. In the extreme\ncases, outright grifters come in, creating blockchains with no\nredeeming social or technical value which are basically borderline\nscams. But the reality is that the line from \"altruistic idealist\" and\n\"grifter\" is really a spectrum. And the longer an ecosystem keeps going,\nthe harder it is for any new project on the altruistic side of the\nspectrum to get going.\n\nOne noisy proxy for the blockchain industry's slow replacement of\nphilosophical and idealistic values with short-term profit-seeking\nvalues is the larger and larger size of premines: the allocations that\ndevelopers of a cryptocurrency give to themselves.\n\nSource for insider allocations: Messari.\n\nWhich blockchain communities deeply value self-sovereignty, privacy\nand decentralization, and are making to get big sacrifices to get it?\nAnd which blockchain communities are just trying to pump up their market\ncaps and make money for founders and investors? The above chart should\nmake it pretty clear.\n\n## Intolerance is good\n\nThe above makes it clear why Bitcoin's status as the first\ncryptocurrency gives it unique advantages that are extremely difficult\nfor any cryptocurrency created within the last five years to replicate.\nBut now we get to the biggest objection against Bitcoin maximalist\nculture: why is it so toxic?\n\nThe case for Bitcoin toxicity stems from Conquest's\nsecond law. In Robert Conquest's original formulation, the law says\nthat \"any organization not explicitly and constitutionally\nright-wing will sooner or later become left-wing\". But really,\nthis is just a special case of a much more general pattern, and one that\nin the modern age of relentlessly homogenizing and conformist social\nmedia is more relevant than ever:\n\nIf you want to retain an identity that is different from the\nmainstream, then you need a really strong culture that actively resists\nand fights assimilation into the mainstream every time it tries to\nassert its hegemony.\n\nBlockchains are, as I mentioned above, very fundamentally and\nexplicitly a counterculture movement that is trying to create and\npreserve something different from the mainstream. At a time when the\nworld is splitting up into great power blocs that actively suppress\nsocial and economic interaction between them, blockchains are one of the\nvery few things that can remain global. At a time when more and more\npeople are reaching for censorship to defeat their short-term enemies,\nblockchains steadfastly continue to censor nothing.\n\nThe only correct way to respond to \"reasonable adults\" trying to\ntell you that to \"become mainstream\" you have to compromise on your\n\"extreme\" values. Because once you compromise once, you can't\nstop.\n\nBlockchain communities also have to fight bad actors on the\ninside. Bad actors include:\n\n- Scammers, who make and sell projects that are\nultimately valueless (or worse, actively harmful) but cling to the\n\"crypto\" and \"decentralization\" brand (as well as highly abstract ideas\nof humanism and friendship) for legitimacy.\n\n- Collaborationists, who publicly and loudly\nvirtue-signal about working together with governments and actively try\nto convince\ngovernments to use coercive force against their competitors.\n\n- Corporatists, who try to use their resources to\ntake over the development of blockchains, and often push for protocol\nchanges that enable centralization.\n\nOne could stand against all of these actors with a smiling\nface, politely telling the world why they \"disagree with their\npriorities\". But this is unrealistic: the bad actors will try hard to\nembed themselves into your community, and at that point it becomes\npsychologically hard to criticize them with the sufficient level of\nscorn that they truly require: the people you're criticizing are\nfriends of your friends. And so any culture that values agreeableness\nwill simply fold before the challenge, and let scammers roam freely\nthrough the wallets of innocent newbies.\n\nWhat kind of culture won't fold? A culture that is willing\nand eager to tell both scammers on the inside and powerful opponents on\nthe outside to go\nthe way of the Russian warship.\nWeird crusades\nagainst seed oils are good\n\nOne powerful bonding tool to help a community maintain internal\ncohesion around its distinctive values, and avoid falling into the\nmorass that is the mainstream, is weird beliefs and crusades that are in\na similar spirit, even if not directly related, to the core mission.\nIdeally, these crusades should be at least partially correct,\npoking at a genuine blind spot or inconsistency of mainstream\nvalues.\n\nThe Bitcoin community is good at this. Their most recent crusade is a\nwar\nagainst seed oils, oils derived from vegetable seeds high\nin omega-6 fatty acids that are harmful\nto human health.\n\nThis Bitcoiner crusade gets treated skeptically when reviewed\nin the media, but the media treats the topic much\nmore favorably when \"respectable\" tech firms are tackling it. The\ncrusade helps to remind Bitcoiners that the mainstream media is\nfundamentally tribal and hypocritical, and so the media's shrill\nattempts to slander\ncryptocurrency as being primarily for money laundering and terrorism\nshould be treated with the same level of scorn.\n\n## Be a maximalist\n\nMaximalism is often derided in the media as both a dangerous toxic\nright-wing cult, and as a paper tiger that will disappear as soon as\nsome other cryptocurrency comes in and takes over Bitcoin's supreme\nnetwork effect. But the reality is that none of the arguments\nfor maximalism that I describe above depend at all on network\neffects. Network effects really are logarithmic, not quadratic:\nonce a cryptocurrency is \"big enough\", it has enough liquidity to\nfunction and multi-cryptocurrency payment processors will easily add it\nto their collection. But the claim that Bitcoin is an outdated pet rock\nand its value derives entirely from a walking-zombie network\neffect that just needs a little push to collapse is similarly completely\nwrong.\n\nCrypto-assets like Bitcoin have real cultural and structural\nadvantages that make them powerful assets worth holding and using.\nBitcoin is an excellent example of the category, though it's certainly\nnot the only one; other honorable cryptocurrencies do exist, and\nmaximalists have been willing to support and use them. Maximalism is not\njust Bitcoin-for-the-sake-of-Bitcoin; rather, it's a very genuine\nrealization that most other cryptoassets are scams, and a culture of\nintolerance is unavoidable and necessary to protect newbies and make\nsure at least one corner of that space continues to be a corner worth\nliving in.\n\nIt's better to mislead ten newbies into avoiding an\ninvestment that turns out good than it is to allow a single newbie to\nget bankrupted by a grifter.\n\nIt's better to make your protocol too simple and fail to\nserve ten low-value short-attention-span gambling applications than it\nis to make it too complex and fail to serve the central sound money use\ncase that underpins everything else.\n\nAnd it's better to offend millions by standing aggressively\nfor what you believe in than it is to try to keep everyone happy and end\nup standing for nothing.\n\nBe brave. Fight for your values. Be a\nmaximalist.",
    "contentLength": 20511,
    "summary": "Vitalik argues Bitcoin maximalism is correct because blockchains need robust, simple technology and uncompromising culture to survive hostile actors.",
    "detailedSummary": {
      "theme": "Vitalik argues that Bitcoin maximalism's seemingly toxic culture and uncompromising stance are actually necessary defensive mechanisms to protect cryptocurrency's core values from co-optation and maintain its function as a security technology for the vulnerable.",
      "summary": "Vitalik makes a contrarian case defending Bitcoin maximalism by arguing that cryptocurrencies operate in a hostile world where they serve as critical infrastructure for the unbanked, activists, and other vulnerable populations who need robust, censorship-resistant money. He contends that Bitcoin's success stems from its focus on simplicity, mathematical purity, and being solely money rather than a platform for complex applications. Vitalik criticizes his own involvement with world leaders and the Ethereum ecosystem's pursuit of mainstream validation, suggesting this makes it more susceptible to compromise when faced with pressure to censor or adapt to government demands.\n\nVitalik argues that Bitcoin maximalists' 'toxic' culture is actually a necessary defense mechanism against scammers, collaborationists, and corporatists who would corrupt the movement from within. He suggests that maintaining a counterculture identity requires active resistance to mainstream assimilation, and that agreeable cultures simply fold under pressure from bad actors. The post concludes by advocating for maximalist principles: better to be overly protective of newbies, overly simple in protocol design, and offensively principled rather than risk compromising core values for broader appeal.",
      "takeaways": [
        "Cryptocurrencies function as security technology for vulnerable populations, requiring sacrifice of convenience and efficiency for robustness and censorship resistance",
        "Bitcoin's focus solely on being money, rather than supporting complex applications, makes it more secure and less prone to systemic risks like MEV",
        "Early blockchain projects tend to be more genuine and idealistic compared to later ventures driven primarily by profit motives",
        "Toxic maximalist culture serves as a necessary immune system against scammers, government co-optation, and mission drift",
        "Maintaining counterculture values requires active resistance to mainstream assimilation and a willingness to be uncompromising on core principles"
      ],
      "controversial": [
        "Vitalik's harsh criticism of his own past meetings with world leaders and suggestion that such engagement compromises Ethereum's integrity",
        "The argument that 'intolerance is good' and that toxic behavior is necessary to maintain community values",
        "Claims that most cryptocurrencies besides Bitcoin are scams and that the ecosystem is overrun with grifters",
        "The assertion that Bitcoin maximalists are better positioned to resist censorship than Ethereum developers due to their more confrontational culture"
      ]
    }
  },
  {
    "id": "general-2022-03-29-road",
    "title": "The roads not taken",
    "date": "2022-03-29",
    "category": "general",
    "url": "https://vitalik.eth.limo/general/2022/03/29/road.html",
    "path": "general/2022/03/29/road.html",
    "content": "- \n\n- \n\n  \n    \n\n  \n  \n  Dark Mode Toggle\n  \n    \n\n  \n\n##  The roads not taken \n\n 2022 Mar 29 \nSee all posts\n\n \n \n\n The roads not taken \n\nThe Ethereum protocol development community has made a lot of\ndecisions in the early stages of Ethereum that have had a large impact\non the project's trajectory. In some cases, Ethereum developers made\nconscious decisions to improve in some place where we thought that\nBitcoin erred. In other places, we were creating something new entirely,\nand we simply had to come up with something to fill in a blank\n- but there were many somethings to choose from. And in still other\nplaces, we had a tradeoff between something more complex and something\nsimpler. Sometimes, we chose the simpler thing, but sometimes, we chose\nthe more complex thing too.\n\nThis post will look at some of these forks-in-the-road as I remember\nthem. Many of these features were seriously discussed within core\ndevelopment circles; others were barely considered at all but perhaps\nreally should have been. But even still, it's worth looking at what a\ndifferent Ethereum might have looked like, and what we can learn from\nthis going forward.\nShould\nwe have gone with a much simpler version of proof of stake?\n\nThe Gasper proof of\nstake that Ethereum is very soon going to merge to is a complex system,\nbut a very powerful system. Some of its properties include:\n\n- Very strong single-block confirmations - as soon as\na transaction gets included in a block, usually\nwithin a few seconds that block gets solidified to the point that it\ncannot be reverted unless either a large fraction of nodes are dishonest\nor there is extreme network latency.\n\n- Economic finality - once a block gets\nfinalized, it cannot be reverted without the attacker having to\nlose millions of ETH to being slashed.\n\n- Very predictable rewards - validators reliably earn\nrewards every epoch (6.4 minutes), reducing incentives to pool\n\n- Support for very high validator count - unlike most\nother chains with the above features, the Ethereum beacon chain supports\nhundreds of thousands of validators (eg. Tendermint offers even\nfaster finality than Ethereum, but it only\nsupports a few hundred validators)\n\nBut making a system that has these properties is hard. It\ntook years\nof research, years of failed\nexperiments,\nand generally took a huge amount of effort. And the final output was\npretty complex.\n\nIf our researchers did not have to worry so much about consensus and\nhad more brain cycles to spare, then maybe, just maybe, rollups could have been\ninvented in 2016. This brings us to a question: should we really\nhave had such high standards for our proof of stake, when even a much\nsimpler and weaker version of proof of stake would have been a large\nimprovement over the proof of work status quo?\n\nMany have the misconception that proof of stake is inherently complex,\nbut in reality there are plenty of proof of stake algorithms that are\nalmost as simple as Nakamoto PoW. NXT\nproof of stake existed since 2013 and would have been a natural\ncandidate; it had issues but those issues could easily have been\npatched, and we could have had a reasonably well-working proof of stake\nfrom 2017, or even from the beginning. The reason why Gasper is more\ncomplex than these algorithms is simply that it tries to accomplish\nmuch more than they do. But if we had been more modest at the\nbeginning, we could have focused on achieving a more limited set of\nobjectives first.\n\nProof of stake from the beginning would in my opinion have been a\nmistake; PoW was helpful in expanding the initial issuance distribution and\nmaking Ethereum accessible, as well as encouraging a hobbyist community.\nBut switching to a simpler proof of stake in 2017, or even 2020, could\nhave led to much less environmental damage (and anti-crypto mentality as\na result of environmental damage) and a lot more research talent being\nfree to think about scaling. Would we have had to spend a lot of\nresources on making a better proof of stake eventually? Yes. But it's\nincreasingly looking like we'll end up\ndoing that anyway.\nThe de-complexification of\nsharding\n\nEthereum sharding has been on a very consistent trajectory of\nbecoming less and less complex since the ideas started being worked\non in 2014. First, we had complex sharding with built-in execution\nand cross-shard transactions. Then, we simplified the protocol by moving\nmore responsibilities to the user (eg. in a cross-shard transaction, the\nuser would have to separately pay for gas on both shards). Then, we\nswitched to the rollup-centric\nroadmap where, from the protocol's point of view, shards are just\nblobs of data. Finally, with danksharding, the\nshard fee markets are merged into one, and the final design just looks\nlike a non-sharded chain but where some data availability sampling magic\nhappens behind the scenes to make sharded verification happen.\n\nSharding in 2015\n\nSharding in 2022\n\nBut what if we had gone the opposite path? Well, there actually are\nEthereum researchers who heavily explored\na much more sophisticated sharding system: shards would be chains, there\nwould be fork choice rules where child chains depend on parent chains,\ncross-shard messages would get routed by the protocol, validators would\nbe rotated between shards, and even applications would get automatically\nload-balanced between shards!\n\nThe problem with that approach: those forms of sharding are largely\njust ideas and mathematical models, whereas Danksharding is a complete\nand almost-ready-for-implementation spec.\nHence, given Ethereum's circumstances and constraints, the\nsimplification and de-ambitionization of sharding was, in my opinion,\nabsolutely the right move. That said, the more ambitious research also\nhas a very important role to play: it identifies promising research\ndirections, even the very complex ideas often have \"reasonably simple\"\nversions of those ideas that still provide a lot of benefits, and\nthere's a good chance that it will significantly influence Ethereum's\nprotocol development (or even layer-2 protocols) over the years to\ncome.\nMore or less features in the\nEVM?\n\nRealistically, the specification of the EVM was basically, with the\nexception of security auditing, viable for launch by mid-2014. However,\nover the next few months we continued actively exploring new features\nthat we felt might be really important for a decentralized application\nblockchain. Some did not go in, others did.\n\n- We considered adding\na POST opcode, but decided against it. The\nPOST opcode would have made an asynchronous call,\nthat would get executed after the rest of the transaction finishes.\n\n- We considered adding an\nALARM opcode, but decided against it.\nALARM would have functioned like POST, except\nexecuting the asynchronous call in some future block, allowing contracts\nto schedule operations.\n\n- We added logs,\nwhich allow contracts to output records that do not touch the state, but\ncould be interpreted by dapp interfaces and wallets. Notably, we also\nconsidered making ETH transfers emit a log, but decided against\nit - the rationale being that \"people will soon switch to smart\ncontract wallets anyway\".\n\n- We considered expanding SSTORE to support byte\narrays, but decided against it, because of concerns about\ncomplexity and safety.\n\n- We added precompiles,\ncontracts which execute specialized cryptographic operations with native\nimplementations at a much cheaper gas cost than can be done in the\nEVM.\n\n- In the months right after launch, state rent was considered again and again, but was\nnever included. It was just too complicated. Today, there are\nmuch better state\nexpiry schemes being actively explored, though stateless\nverification and proposer/builder\nseparation mean that it is now a much lower priority.\n\nLooking at this today, most of the decisions to not add more\nfeatures have proven to be very good decisions. There was no obvious\nreason to add a POST opcode. An ALARM opcode\nis actually very difficult to implement safely: what happens if everyone\nin blocks 1...99999 sets an ALARM to execute a lot of code at\nblock 100000? Will that block take hours to process? Will some scheduled\noperations get pushed back to later blocks? But if that happens, then\nwhat guarantees is ALARM even preserving?\nSSTORE for byte arrays is difficult to do safely, and would\nhave greatly expanded worst-case witness sizes.\n\nThe state rent issue is more challenging: had we actually implemented\nsome kind of state rent from day 1, we would not have had a smart\ncontract ecosystem evolve around a normalized assumption of persistent\nstate. Ethereum would have been harder to build for, but it could have\nbeen more scalable and sustainable. At the same time, the state expiry\nschemes we had back then really were much worse than what we\nhave now. Sometimes, good ideas just take years to arrive at and\nthere is no better way around that.\nAlternative paths for\nLOG\n\nLOG could have been done differently in two different\nways:\n\n- We could have made ETH transfers auto-issue a\nLOG. This would have saved a lot of\neffort and software bug issues for exchanges and many other users, and\nwould have accelerated everyone relying on logs that would have\nironically helped smart contract wallet adoption.\n\n- We could have not bothered with a LOG\nopcode at all, and instead made it an ERC: there would\nbe a standard contract that has a function submitLog and\nuses the technique\nfrom the Ethereum deposit contract to compute a Merkle root of all\nlogs in that block. Either EIP-2929 or\nblock-scoped storage (equivalent to TSTORE but cleared\nafter the block) would have made this cheap.\n\nWe strongly considered (1), but rejected it. The main reason was\nsimplicity: it's easier for logs to just come from the\nLOG opcode. We also (very wrongly!) expected most users to\nquickly migrate to smart contract wallets, which could have logged\ntransfers explicitly using the opcode.\n\n- was not considered, but in retrospect it was always an option. The\nmain downside of (2) would have been the lack of a Bloom filter\nmechanism for quickly scanning for logs. But as it turns out, the Bloom\nfilter mechanism is too slow to be user-friendly for dapps anyway, and\nso these days more and more people use TheGraph for querying\nanyway.\n\nOn the whole, it seems very possible that either one of\nthese approaches would have been superior to the status quo. Keeping\nLOG outside the protocol would have kept things simpler,\nbut if it was inside the protocol auto-logging all ETH transfers would\nhave made it more useful.\n\nToday, I would probably favor the eventual abolition of the\nLOG opcode from the EVM.\nWhat if the EVM\nwas something totally different?\n\nThere were two natural very different paths that the EVM could have\ntaken:\n\n- Make the EVM be a higher-level language, with\nbuilt-in constructs for variables, if-statements, loops, etc.\n\n- Make the EVM be a copy of some existing VM (LLVM,\nWASM, etc)\n\nThe first path was never really considered. The attraction of this\npath is that it could have made compilers simpler, and allowed more\ndevelopers to code in EVM directly. It could have also made ZK-EVM\nconstructions simpler. The weakness of the path is that it would have\nmade EVM code structurally more complicated: instead of being a\nsimple list of opcodes in a row, it would have been a more complicated\ndata structure that would have had to be stored somehow. That said,\nthere was a missed opportunity for a best-of-both-worlds: some EVM\nchanges could have given us a lot of those benefits while keeping the\nbasic EVM structure roughly as is: ban dynamic jumps\nand add some opcodes designed to support subroutines (see also: EIP-2315), allow\nmemory access only on 32-byte word boundaries, etc.\n\nThe second path was suggested many times, and rejected many times.\nThe usual argument for it is that it would allow programs to compile\nfrom existing languages (C, Rust, etc) into the EVM. The argument\nagainst has always been that given Ethereum's unique constraints it\nwould not actually provide any benefits:\n\n- Existing compilers from high-level languages tend to not care about\ntotal code size, whereas blockchain code must optimize heavily to cut\ndown every byte of code size\n\n- We need multiple implementations of the VM with a hard requirement\nthat two implementations never process the same code\ndifferently. Security-auditing and verifying this on code that we did\nnot write would be much harder.\n\n- If the VM specification changes, Ethereum would have to either\nalways update along with it or fall more and more out-of-sync.\n\nHence, there probably was never a viable path for the EVM that's\nradically different from what we have today, though there are\nlots of smaller details (jumps, 64 vs 256 bit, etc) that could have led\nto much better outcomes if they were done differently.\nShould\nthe ETH supply have been distributed differently?\n\nThe current ETH supply is approximately represented by this chart from Etherscan:\n\nAbout half of the ETH that exists today was sold in an open public ether\nsale, where anyone could send BTC to a standardized bitcoin address,\nand the initial ETH supply distribution was computed by an open-source\nscript that scans the Bitcoin blockchain for transactions going to\nthat address. Most of the remainder was mined. The slice at the bottom,\nthe 12M ETH marked \"other\", was the \"premine\" - a piece distributed\nbetween the Ethereum Foundation and ~100 early contributors to the\nEthereum protocol.\n\nThere are two main criticisms of this process:\n\n- The premine, as well as the fact that the Ethereum\nFoundation received the sale funds, is not credibly\nneutral. A few recipient addresses were hand-picked through\na closed process, and the Ethereum Foundation had to be trusted to not\ntake out loans to recycle funds received furing the sale back into the\nsale to give itself more ETH (we did not, and no one seriously claims\nthat we have, but even the requirement to be trusted at all offends\nsome).\n\n- The premine over-rewarded very early contributors, and left\ntoo little for later contributors. 75% of the premine went to\nrewarding contributors for their work before launch, and post-launch the\nEthereum Foundation only had 3 million ETH left. Within 6 months, the\nneed to sell to financially survive decreased that to around 1 million\nETH.\n\nIn a way, the problems were related: the desire to minimize\nperceptions of centralization contributed to a smaller premine, and a\nsmaller premine was exhausted more quickly.\n\nThis is not the only way that things could have been done. Zcash has a different approach: a constant\n20% of the block reward goes to a set of recipients hard-coded in the\nprotocol, and the set of recipients gets re-negotiated every 4 years (so\nfar this has happened\nonce). This would have been much more sustainable, but it would have\nbeen much more heavily criticized as centralized (the Zcash community\nseems to be more openly okay with more technocratic leadership than the\nEthereum community).\n\nOne possible alternative path would be something similar to the \"DAO\nfrom day 1\" route popular among some defi projects today. Here is a\npossible strawman proposal:\n\n- We agree that for 2 years, a block reward of 2 ETH per block\ngoes into a dev fund.\n\n- Anyone who purchases ETH in the ether sale could specify a\nvote for their preferred distribution of the dev fund\n(eg. \"1 ETH per block to the Ethereum Foundation, 0.4 ETH to the\nConsensys research team, 0.2 ETH to Vlad Zamfir...\")\n\n- Recipients that got voted for get a share from the dev\nfund equal to the median of everyone's votes,\nscaled so that the total equals 2 ETH per block (median is to prevent\nself-dealing: if you vote for yourself you get nothing unless you get at\nleast half of other purchasers to mention you)\n\nThe sale could be run by a legal entity that promises to distribute\nthe bitcoin received during the sale along the same ratios as\nthe ETH dev fund (or burned, if we really wanted to make bitcoiners\nhappy). This probably would have led to the Ethereum Foundation getting\na lot of funding, non-EF groups also getting a lot of funding (leading\nto more ecosystem decentralization), all without breaking credible\nneutrality one single bit. The main downside is of course that coin voting really sucks,\nbut pragmatically we could have realized that 2014 was still an early\nand idealistic time and the most serious downsides of coin voting would\nonly start coming into play long after the sale ends.\n\nWould this have been a better idea and set a better precedent? Maybe!\nThough realistically even if the dev fund had been fully credibly\nneutral, the people who yell about Ethereum's premine today may well\nhave just started yelling twice as hard about the DAO fork instead.\nWhat can we learn from all\nthis?\n\nIn general, it sometimes feels to me like Ethereum's biggest\nchallenges come from balancing between two visions - a pure and simple\nblockchain that values safety and simplicity, and a highly performant\nand functional platform for building advanced applications.\nMany of the examples above are just aspects of this: do we have fewer\nfeatures and be more Bitcoin-like, or more features and be more\ndeveloper-friendly? Do we worry a lot about making development funding\ncredibly neutral and be more Bitcoin-like, or do we just worry first and\nforemost about making sure devs are rewarded enough to make Ethereum\ngreat?\n\nMy personal dream is to try to achieve both visions at the\nsame time - a base layer where the specification becomes\nsmaller each year than the year before it, and a powerful\ndeveloper-friendly advanced application ecosystem centered around\nlayer-2 protocols. That said, getting to such an ideal world takes a\nlong time, and a more explicit realization that it would take\ntime and we need to think about the roadmap step-by-step would have\nprobably helped us a lot.\n\nToday, there are a lot of things we cannot change, but there are many\nthings that we still can, and there is still a path solidly open to\nimproving both functionality and simplicity. Sometimes the path is a\nwinding one: we need to add some more complexity first to enable\nsharding, which in turn enables lots of layer-2 scalability on top. That\nsaid, reducing complexity is possible, and Ethereum's history has\nalready demonstrated this:\n\n- EIP-150 made\nthe call stack depth limit no longer relevant, reducing security worries\nfor contract developers.\n\n- EIP-161 made\nthe concept of an \"empty account\" as something separate from an account\nwhose fields are zero no longer exist.\n\n- EIP-3529\nremoved part of the refund mechanism and made gas tokens no longer\nviable.\n\nIdeas in the pipeline, like Verkle\ntrees, reduce complexity even further. But the question of how to\nbalance the two visions better in the future is one that we should start\nmore actively thinking about.",
    "contentLength": 18788,
    "summary": "Vitalik reflects on Ethereum's design choices, arguing simpler proof-of-stake and fewer EVM features could have freed up research for rollups.",
    "detailedSummary": {
      "theme": "Vitalik reflects on major design decisions and alternative paths in Ethereum's development, examining what could have been done differently and what lessons can be learned for future protocol development.",
      "summary": "Vitalik examines several critical decision points in Ethereum's history, questioning whether simpler approaches might have been better in some cases. He discusses the complexity of Gasper proof of stake versus simpler alternatives that could have been implemented years earlier, potentially freeing up research bandwidth for scaling solutions like rollups. He traces the evolution of sharding from complex cross-shard execution to the simplified data availability approach of danksharding, arguing this simplification was the right move. Vitalik also analyzes EVM design choices, including features that were considered but rejected (like POST and ALARM opcodes), decisions around the LOG opcode, and whether using existing VMs like WASM would have been viable. He explores alternative approaches to ETH distribution that could have been more credibly neutral while providing sustainable funding for development. Throughout, Vitalik identifies a central tension between two visions: Ethereum as a simple, safe blockchain versus a highly functional platform for advanced applications. His personal goal is achieving both through a minimal base layer with rich layer-2 functionality, though he acknowledges this requires careful long-term planning and sometimes adding complexity before reducing it.",
      "takeaways": [
        "Simpler proof of stake implemented earlier could have reduced environmental impact and freed research talent for scaling solutions, even if more sophisticated consensus would eventually be needed",
        "The progressive simplification of sharding from complex cross-shard execution to data availability sampling has been the right approach, making implementation actually feasible",
        "Most decisions to reject additional EVM features (POST, ALARM, byte array storage) proved wise due to complexity and safety concerns, though some alternatives like auto-logging ETH transfers might have been better",
        "Alternative funding models using credibly neutral mechanisms could have provided more sustainable development funding while avoiding centralization concerns",
        "Ethereum's core challenge involves balancing simplicity and safety with functionality and developer-friendliness, with the ideal being a minimal base layer enabling rich layer-2 applications"
      ],
      "controversial": [
        "The suggestion that Ethereum's premine and foundation funding model was not credibly neutral and over-rewarded early contributors while under-funding later development",
        "The proposal that the LOG opcode should potentially be abolished from the EVM in favor of simpler alternatives",
        "The implication that pursuing complex consensus research may have delayed important scaling innovations like rollups"
      ]
    }
  },
  {
    "id": "general-2022-03-14-trustedsetup",
    "title": "How do trusted setups work?",
    "date": "2022-03-14",
    "category": "math",
    "url": "https://vitalik.eth.limo/general/2022/03/14/trustedsetup.html",
    "path": "general/2022/03/14/trustedsetup.html",
    "content": "- \n\n- \n\n  \n    \n\n  \n  \n  Dark Mode Toggle\n  \n    \n\n  \n\n##  How do trusted setups work? \n\n 2022 Mar 14 \nSee all posts\n\n \n \n\n How do trusted setups work? \n\nNecessary background: elliptic curves and\nelliptic curve pairings. See also: Dankrad\nFeist's article on KZG polynomial commitments.\n\nSpecial thanks to Justin Drake, Dankrad Feist and Chih-Cheng\nLiang for feedback and review.\n\nMany cryptographic protocols, especially in the areas of data availability\nsampling and ZK-SNARKs\ndepend on trusted setups. A trusted setup ceremony is a\nprocedure that is done once to generate a piece of data that must then\nbe used every time some cryptographic protocol is run.\nGenerating this data requires some secret information; the \"trust\" comes\nfrom the fact that some person or some group of people has to generate\nthese secrets, use them to generate the data, and then publish the data\nand forget the secrets. But once the data is generated, and the secrets\nare forgotten, no further participation from the creators of the\nceremony is required.\n\nThere are many types of trusted setups. The earliest instance of a\ntrusted setup being used in a major protocol is the original\nZcash ceremony in 2016. This ceremony was very complex, and required\nmany rounds of communication, so it could only have six participants.\nEveryone using Zcash at that point was effectively trusting that at\nleast one of the six participants was honest. More modern protocols\nusually use the powers-of-tau setup, which has a 1-of-N trust model with \\(N\\) typically in the hundreds. That is to\nsay, hundreds of people participate in generating the data\ntogether, and only one of them needs to be honest and not publish their\nsecret for the final output to be secure. Well-executed setups like this\nare often considered \"close enough to trustless\" in\npractice.\n\nThis article will explain how the KZG setup works, why it works, and\nthe future of trusted setup protocols. Anyone proficient in code should\nalso feel free to follow along this code implementation: https://github.com/ethereum/research/blob/master/trusted_setup/trusted_setup.py.\nWhat does a\npowers-of-tau setup look like?\n\nA powers-of-tau setup is made up of two series of elliptic curve\npoints that look as follows:\n\n\\([G_1, G_1 * s, G_1 * s^2 ... G_1 *\ns^{n_1-1}]\\) \u00a0\n\n\\([G_2, G_2 * s, G_2 * s^2 ... G_2 *\ns^{n_2-1}]\\)\n\n\\(G_1\\) and \\(G_2\\) are the standardized generator points\nof the two elliptic curve groups; in BLS12-381, \\(G_1\\) points are (in compressed form) 48\nbytes long and \\(G_2\\) points are 96\nbytes long. \\(n_1\\) and \\(n_2\\) are the lengths of the \\(G_1\\) and \\(G_2\\) sides of the setup. Some protocols\nrequire \\(n_2 = 2\\), others require\n\\(n_1\\) and \\(n_2\\) to both be large, and some are in the\nmiddle (eg. Ethereum's data availability sampling in its current form\nrequires \\(n_1 = 4096\\) and \\(n_2 = 16\\)). \\(s\\) is the secret that is used to generate\nthe points, and needs to be forgotten.\n\nTo make a KZG commitment to a polynomial \\(P(x) = \\sum_i c_i x^i\\), we simply take a\nlinear combination \\(\\sum_i c_i S_i\\),\nwhere \\(S_i = G_1 * s^i\\) (the elliptic\ncurve points in the trusted setup). The \\(G_2\\) points in the setup are used to\nverify evaluations of polynomials that we make commitments to; I won't\ngo into verification here in more detail, though Dankrad\ndoes in his post.\nIntuitively,\nwhat value is the trusted setup providing?\n\nIt's worth understanding what is philosophically going on here, and\nwhy the trusted setup is providing value. A polynomial commitment is\ncommitting to a piece of size-\\(N\\)\ndata with a size \\(O(1)\\) object (a\nsingle elliptic curve point). We could do this with a plain\nPedersen commitment: just set the \\(S_i\\) values to be \\(N\\) random elliptic curve points that have\nno known relationship with each other, and commit to polynomials with\n\\(\\sum_i c_i S_i\\) as before. And in\nfact, this is exactly what IPA\nevaluation proofs do.\n\nHowever, any IPA-based proofs take \\(O(N)\\) time to verify, and there's an\nunavoidable reason why: a commitment to a polynomial \\(P(x)\\) using the base points \\([S_0, S_1 ... S_i ... S_{n-1}]\\) would\ncommit to a different polynomial if we use the base points \\([S_0, S_1 ... (S_i * 2) ... S_{n-1}]\\).\n\nA valid commitment to the polynomial \\(3x^3 + 8x^2 + 2x + 6\\) under one set of\nbase points is a valid commitment to \\(3x^3 +\n4x^2 + 2x + 6\\) under a different set of base points.\n\nIf we want to make an IPA-based proof for some statement\n(say, that this polynomial evaluated at \\(x =\n10\\) equals \\(3826\\)), the proof\nshould pass with the first set of base points and fail with the second.\nHence, whatever the proof verification procedure is cannot avoid somehow\ntaking into account each and every one of the \\(S_i\\) values, and so it unavoidably takes\n\\(O(N)\\) time.\n\nBut with a trusted setup, there is a hidden mathematical\nrelationship between the points. It's guaranteed that \\(S_{i+1} = s * S_i\\) with the same factor\n\\(s\\) between any two adjacent points.\nIf \\([S_0, S_1 ... S_i ... S_{n-1}]\\)\nis a valid setup, the \"edited setup\" \\([S_0,\nS_1 ... (S_i * 2) ... S_{n-1}]\\) cannot also be a valid\nsetup. Hence, we don't need \\(O(n)\\) computation; instead, we take\nadvantage of this mathematical relationship to verify anything we need\nto verify in constant time.\n\nHowever, the mathematical relationship has to remain secret: if \\(s\\) is known, then anyone could come up\nwith a commitment that stands for many different polynomials: if \\(C\\) commits to \\(P(x)\\), it also commits to \\(\\frac{P(x) * x}{s}\\), or \\(P(x) - x + s\\), or many other things. This\nwould completely break all applications of polynomial commitments.\nHence, while some secret \\(s\\)\nmust have existed at one point to make possible the mathematical link\nbetween the \\(S_i\\) values that enables\nefficient verification, the \\(s\\) must\nalso have been forgotten.\nHow do multi-participant\nsetups work?\n\nIt's easy to see how one participant can generate a setup: just pick\na random \\(s\\), and generate the\nelliptic curve points using that \\(s\\).\nBut a single-participant trusted setup is insecure: you have to trust\none specific person!\n\nThe solution to this is multi-participant trusted setups, where by\n\"multi\" we mean a lot of participants: over 100 is normal, and\nfor smaller setups it's possible to get over 1000. Here is how a\nmulti-participant powers-of-tau setup works.\n\nTake an existing setup (note that you don't know \\(s\\), you just know the points):\n\n\\([G_1, G_1 * s, G_1 * s^2 ... G_1 *\ns^{n_1-1}]\\) \u00a0\n\n\\([G_2, G_2 * s, G_2 * s^2 ... G_2 *\ns^{n_2-1}]\\)\n\nNow, choose your own random secret \\(t\\). Compute:\n\n\\([G_1, (G_1 * s) * t, (G_1 * s^2) * t^2\n... (G_1 * s^{n_1-1}) * t^{n_2-1}]\\) \u00a0\n\n\\([G_2, (G_2 * s) * t, (G_2 * s^2) * t^2\n... (G_2 * s^{n_2-1}) * t^{n_2-1}]\\)\n\nNotice that this is equivalent to:\n\n\\([G_1, G_1 * (st), G_1 * (st)^2 ... G_1 *\n(st)^{n_1-1}]\\) \u00a0\n\n\\([G_2, G_2 * (st), G_2 * (st)^2 ... G_2 *\n(st)^{n_2-1}]\\)\n\nThat is to say, you've created a valid setup with the secret \\(s * t\\)! You never give your \\(t\\) to the previous participants, and the\nprevious participants never give you their secrets that went into \\(s\\). And as long as any one of the\nparticipants is honest and does not reveal their part of the secret, the\ncombined secret does not get revealed. In particular, finite fields have\nthe property that if you know know \\(s\\) but not \\(t\\), and \\(t\\) is securely randomly generated, then\nyou know nothing about \\(s*t\\)!\n\n## Verifying the setup\n\nTo verify that each participant actually participated, each\nparticipant can provide a proof that consists of (i) the \\(G_1 * s\\) point that they received and (ii)\n\\(G_2 * t\\), where \\(t\\) is the secret that they introduce. The\nlist of these proofs can be used to verify that the final setup combines\ntogether all the secrets (as opposed to, say, the last participant just\nforgetting the previous values and outputting a setup with just their\nown secret, which they keep so they can cheat in any protocols that use\nthe setup).\n\n\\(s_1\\) is the first\nparticipant's secret, \\(s_2\\) is the\nsecond participant's secret, etc. The pairing check at each step proves\nthat the setup at each step actually came from a combination of the\nsetup at the previous step and a new secret known by the participant at\nthat step.\n\nEach participant should reveal their proof on some publicly\nverifiable medium (eg. personal website, transaction from their .eth\naddress, Twitter). Note that this mechanism does not prevent\nsomeone from claiming to have participated at some index where someone\nelse has (assuming that other person has revealed their proof), but it's\ngenerally considered that this does not matter: if someone is willing to\nlie about having participated, they would also be willing to lie about\nhaving deleted their secret. As long as at least one of the people who\npublicly claim to have participated is honest, the setup is secure.\n\nIn addition to the above check, we also want to verify that all the\npowers in the setup are correctly constructed (ie. they're powers of the\nsame secret). To do this, we could do a series of pairing\nchecks, verifying that \\(e(S_{i+1}, G_2) =\ne(S_i, T_1)\\) (where \\(T_1\\) is\nthe \\(G_2 * s\\) value in the setup) for\nevery \\(i\\). This verifies that the\nfactor between each \\(S_i\\) and \\(S_{i+1}\\) is the same as the factor between\n\\(T_1\\) and \\(G_2\\). We can then do the same on the \\(G_2\\) side.\n\nBut that's a lot of pairings and is expensive. Instead, we take a\nrandom linear combination \\(L_1 =\n\\sum_{i=0}^{n_1-2} r_iS_i\\), and the same linear combination\nshifted by one: \\(L_2 = \\sum_{i=0}^{n_1-2}\nr_iS_{i+1}\\). We use a single pairing check to verify that they\nmatch up: \\(e(L_2, G_2) = e(L_1,\nT_1)\\).\n\nWe can even combine the process for the \\(G_1\\) side and the \\(G_2\\) side together: in addition to\ncomputing \\(L_1\\) and \\(L_2\\) as above, we also compute \\(L_3 = \\sum_{i=0}^{n_2-2} q_iT_i\\) (\\(q_i\\) is another set of random\ncoefficients) and \\(L_4 = \\sum_{i=0}^{n_2-2}\nq_iT_{i+1}\\), and check \\(e(L_2, L_3) =\ne(L_1, L_4)\\).\n\n## Setups in Lagrange form\n\nIn many use cases, you don't want to work with polynomials in\ncoefficient form (eg. \\(P(x) = 3x^3 +\n8x^2 + 2x + 6\\)), you want to work with polynomials in\nevaluation form (eg. \\(P(x)\\)\nis the polynomial that evaluates to \\([19,\n146, 9, 187]\\) on the domain \\([1, 189,\n336, 148]\\) modulo 337). Evaluation form has many advantages (eg.\nyou can multiply and sometimes divide polynomials in \\(O(N)\\) time) and you can even use it to evaluate in\n\\(O(N)\\) time. In particular, data availability\nsampling expects the blobs to be in evaluation form.\n\nTo work with these cases, it's often convenient to convert the\ntrusted setup to evaluation form. This would allow you to take the\nevaluations (\\([19, 146, 9, 187]\\) in\nthe above example) and use them to compute the commitment directly.\n\nThis is done most easily with a Fast Fourier transform (FFT),\nbut passing the curve points as input instead of numbers. I'll avoid\nrepeating a full detailed explanation of FFTs here, but here\nis an implementation; it is actually not that difficult.\n\n## The future of trusted setups\n\nPowers-of-tau is not the only kind of trusted setup out there. Some\nother notable (actual or potential) trusted setups include:\n\n- The more complicated setups in older ZK-SNARK protocols (eg. see here), which are sometimes\nstill used (particularly Groth16)\nbecause verification is cheaper than PLONK.\n\n- Some cryptographic protocols (eg. DARK)\ndepend on hidden-order groups, groups where it is not\nknown what number an element can be multiplied by to get the zero\nelement. Fully trustless versions of this exist (see: class groups), but\nby far the most efficient version uses RSA groups (powers of \\(x\\) mod \\(n =\npq\\) where \\(p\\) and \\(q\\) are not known). Trusted setup\nceremonies for this with 1-of-n trust assumptions are possible, but are\nvery complicated to implement.\n\n- If/when indistinguishability\nobfuscation becomes viable, many protocols that depend on\nit will involve someone creating and publishing an obfuscated program\nthat does something with a hidden internal secret. This is a trusted\nsetup: the creator(s) would need to possess the secret to create the\nprogram, and would need to delete it afterwards.\n\nCryptography continues to be a rapidly evolving field, and how\nimportant trusted setups are could easily change. It's possible that\ntechniques for working with IPAs and Halo-style ideas will improve to\nthe point where KZG becomes outdated and unnecessary, or that quantum\ncomputers will make anything based on elliptic curves non-viable ten\nyears from now and we'll be stuck working with trusted-setup-free\nhash-based protocols. It's also possible that what we can do with KZG\nwill improve even faster, or that a new area of cryptography will emerge\nthat depends on a different kind of trusted setup.\n\nTo the extent that trusted setup ceremonies are necessary, it is\nimportant to remember that not all trusted setups are created\nequal. 176\nparticipants is better than 6, and 2000 would be even better. A\nceremony small enough that it can be run inside a browser or phone\napplication (eg. the ZKopru setup\nis web-based) could attract far more participants than one that\nrequires running a complicated software package. Every ceremony should\nideally have participants running multiple independently built software\nimplementations and running different operating systems and\nenvironments, to reduce common\nmode failure risks. Ceremonies that require only one round of\ninteraction per participant (like powers-of-tau) are far better than\nmulti-round ceremonies, both due to the ability to support far more\nparticipants and due to the greater ease of writing multiple\nimplementations. Ceremonies should ideally be universal (the\noutput of one ceremony being able to support a wide range of protocols).\nThese are all things that we can and should keep working on, to ensure\nthat trusted setups can be as secure and as trusted as possible.",
    "contentLength": 13960,
    "summary": "Trusted setups generate secret-based elliptic curve data for cryptographic protocols, using multi-participant ceremonies where only one honest participant is needed to ensure security.",
    "detailedSummary": {
      "theme": "Vitalik explains how trusted setups work in cryptographic protocols, focusing on the KZG powers-of-tau ceremony and its role in enabling efficient polynomial commitments for ZK-SNARKs and data availability sampling.",
      "summary": "Vitalik explains that trusted setups are cryptographic ceremonies where participants generate shared data using secret information that must be forgotten afterward, creating a foundation for protocols like ZK-SNARKs and data availability sampling. He focuses on the powers-of-tau setup, which creates two series of elliptic curve points using a secret value s, enabling efficient polynomial commitments through KZG schemes. The key insight is that while random elliptic curve points (like in IPA proofs) require O(N) verification time, the mathematical relationship in trusted setups enables constant-time verification - but only if the secret remains unknown, as knowledge of s would completely break the security. Vitalik details how multi-participant ceremonies work by having each participant multiply the existing setup by their own secret, creating a combined secret that remains secure as long as at least one participant is honest and deletes their contribution. He also covers verification methods, Lagrange form conversions, and future directions, emphasizing that well-executed ceremonies with hundreds of participants are considered 'close enough to trustless' in practice.",
      "takeaways": [
        "Trusted setups enable constant-time verification of polynomial commitments by creating mathematical relationships between elliptic curve points, unlike random point schemes that require O(N) verification time",
        "Multi-participant ceremonies achieve 1-of-N trust assumptions where only one honest participant needs to delete their secret for the entire setup to remain secure",
        "The secret value s must be permanently forgotten after setup generation, as knowledge of it would allow creating commitments that represent multiple different polynomials",
        "Powers-of-tau setups are universal and can support multiple protocols, making them more efficient than protocol-specific trusted setups",
        "Modern ceremonies with hundreds of participants are considered much more secure than early setups with only 6 participants, and future improvements should focus on maximizing participation and implementation diversity"
      ],
      "controversial": [
        "The claim that well-executed trusted setups with many participants are 'close enough to trustless' in practice may be debatable for those who prefer purely trustless systems",
        "The trade-off between efficiency gains from trusted setups versus the security risks of requiring trust assumptions could be contested by proponents of trustless alternatives like IPA proofs"
      ]
    }
  },
  {
    "id": "general-2022-02-28-complexity",
    "title": "Encapsulated vs systemic complexity in protocol design",
    "date": "2022-02-28",
    "category": "general",
    "url": "https://vitalik.eth.limo/general/2022/02/28/complexity.html",
    "path": "general/2022/02/28/complexity.html",
    "content": "- \n\n- \n\n  \n    \n\n  \n  \n  Dark Mode Toggle\n  \n    \n\n  \n\n##  Encapsulated vs systemic complexity in protocol design \n\n 2022 Feb 28 \nSee all posts\n\n \n \n\n Encapsulated vs systemic complexity in protocol design \n\nOne of the main goals\nof Ethereum protocol design is to minimize complexity: make the\nprotocol as simple as possible, while still making a blockchain that can\ndo what an effective blockchain\nneeds to do. The Ethereum protocol is far from perfect at this,\nespecially since much of it was designed in 2014-16 when we understood\nmuch less, but we nevertheless make an active effort to reduce\ncomplexity whenever possible.\n\nOne of the challenges of this goal, however, is that complexity is\ndifficult to define, and sometimes, you have to trade off between two\nchoices that introduce different kinds of complexity and have different\ncosts. How do we compare?\n\nOne powerful intellectual tool that allows for more nuanced thinking\nabout complexity is to draw a distinction between what we will call\nencapsulated complexity and systemic\ncomplexity.\n\nEncapsulated complexity occurs when there is a system with\nsub-systems that are internally complex, but that present a simple\n\"interface\" to the outside. Systemic complexity occurs when the\ndifferent parts of a system can't even be cleanly separated, and have\ncomplex interactions with each other.\n\nHere are a few examples.\nBLS signatures vs Schnorr\nsignatures\n\nBLS\nsignatures and Schnorr\nsignatures are two popular types of cryptographic signature schemes\nthat can be made with elliptic curves.\n\nBLS signatures appear mathematically very simple:\n\nSigning: \\(\\sigma = H(m) * k\\)\n\nVerifying: \\(e([1], \\sigma) \\stackrel{?}{=}\ne(H(m), K)\\)\n\n\\(H\\) is a hash function, \\(m\\) is the message, and \\(k\\) and \\(K\\) are the private and public keys. So\nfar, so simple. However, the true complexity is hidden inside the\ndefinition of the \\(e\\) function: elliptic curve\npairings, one of the most devilishly hard-to-understand pieces of\nmath in all of cryptography.\n\nNow, consider Schnorr signatures. Schnorr signatures rely only on\nbasic elliptic\ncurves. But the signing and verification logic is somewhat more\ncomplex:\n\nSo... which type of signature is \"simpler\"? It depends what you care\nabout! BLS signatures have a huge amount of technical complexity, but\nthe complexity is all buried within the definition of the \\(e\\) function. If you treat the \\(e\\) function as a black box, BLS signatures\nare actually really easy. Schnorr signatures, on the other hand, have\nless total complexity, but they have more pieces that could\ninteract with the outside world in tricky ways.\n\nFor example:\n\n- Doing a BLS multi-signature (a combined signature from two keys\n\\(k_1\\) and \\(k_2\\)) is easy: just take \\(\\sigma_1 + \\sigma_2\\). But a Schnorr\nmulti-signature requires two rounds of interaction, and there are tricky\nkey\ncancellation attacks that need to be dealt with.\n\n- Schnorr signatures require random number generation, BLS signatures\ndo not.\n\nElliptic curve pairings in general are a powerful \"complexity sponge\"\nin that they contain large amounts of encapsulated complexity, but\nenable solutions with much less systemic complexity. This is also true\nin the area of polynomial commitments: compare the simplicity\nof KZG commitments (which require pairings) to the much more\ncomplicated internal logic of inner\nproduct arguments (which do not).\nCryptography vs\ncryptoeconomics\n\nOne important design choice that appears in many blockchain designs\nis that of cryptography versus cryptoeconomics. Often (eg. in rollups) this comes in the\nform of a choice between validity proofs (aka.\nZK-SNARKs) and fraud proofs.\n\nZK-SNARKs are complex technology. While the basic ideas behind how\nthey work can be explained in a single post, actually implementing a\nZK-SNARK to verify some computation involves many times more complexity\nthan the computation itself (hence why ZK-SNARKs for the EVM are still under\ndevelopment while fraud proofs for the EVM are already in the\ntesting stage). Implementing a ZK-SNARK effectively involves circuit\ndesign with special-purpose optimization, working with unfamiliar\nprogramming languages, and many other challenges. Fraud proofs, on the\nother hand, are inherently simple: if someone makes a challenge, you\njust directly run the computation on-chain. For efficiency, a\nbinary-search scheme is sometimes added, but even that doesn't add too\nmuch complexity.\n\nBut while ZK-SNARKs are complex, their complexity is encapsulated\ncomplexity. The relatively light complexity of fraud proofs, on the\nother hand, is systemic. Here are some examples of systemic\ncomplexity that fraud proofs introduce:\n\n- They require careful incentive engineering to avoid the verifier's dilemma.\n\n- If done in-consensus, they require extra transaction types for the\nfraud proofs, along with reasoning about what happens if many actors\ncompete to submit a fraud proof at the same time.\n\n- They depend on a synchronous network.\n\n- They allow censorship attacks to be also used to commit theft.\n\n- Rollups based on fraud proofs require liquidity providers to support\ninstant withdrawals.\n\nFor these reasons, even from a complexity perspective purely\ncryptographic solutions based on ZK-SNARKs are likely to be long-run\nsafer: ZK-SNARKs have are more complicated parts that some\npeople have to think about, but they have fewer dangling caveats that\neveryone has to think about.\n\n## Miscellaneous examples\n\n- Proof of work (Nakamoto consensus) - low\nencapsulated complexity, as the mechanism is extremely simple and easy\nto understand, but higher systemic complexity (eg. selfish\nmining attacks).\n\n- Hash functions - high encapsulated complexity, but\nvery easy-to-understand properties so low systemic complexity.\n\n- Random shuffling algorithms - shuffling algorithms\ncan either be internally complicated (as in Whisk)\nbut lead to easy-to-understand guarantees of strong randomness, or\ninternally simpler but lead to randomness properties that are weaker and\nmore difficult to analyze (systemic complexity).\n\n- Miner extractable value (MEV)\n- a protocol that is powerful enough to support complex transactions can\nbe fairly simple internally, but those complex transactions can have\ncomplex systemic effects on the protocol's incentives by contributing to\nthe incentive to propose blocks in very irregular ways.\n\n- Verkle trees - Verkle trees do have some\nencapsulated complexity, in fact quite a bit more than plain Merkle hash\ntrees. Systemically, however, Verkle trees present the exact same\nrelatively clean-and-simple interface of a key-value map. The main\nsystemic complexity \"leak\" is the possibility of an attacker\nmanipulating the tree to make a particular value have a very long\nbranch; but this risk is the same for both Verkle trees and Merkle\ntrees.\n\n## How do we make the tradeoff?\n\nOften, the choice with less encapsulated complexity is also the\nchoice with less systemic complexity, and so there is one choice that is\nobviously simpler. But at other times, you have to make a hard choice\nbetween one type of complexity and the other. What should be clear at\nthis point is that complexity is less dangerous if it is\nencapsulated. The risks from complexity of a system are not a\nsimple function of how long the specification is; a small 10-line piece\nof the specification that interacts with every other piece adds more\ncomplexity than a 100-line function that is otherwise treated as a black\nbox.\n\nHowever, there are limits to this approach of preferring encapsulated\ncomplexity. Software bugs can occur in any piece of code, and as it gets\nbigger the probability of a bug approaches 1. Sometimes, when you need\nto interact with a sub-system in an unexpected and new way,\ncomplexity that was originally encapsulated can become\nsystemic.\n\nOne example of the latter is Ethereum's current two-level state tree,\nwhich features a tree of account objects, where each account object in\nturn has its own storage tree.\n\nThis tree structure is complex, but at the beginning the complexity\nseemed to be well-encapsulated: the rest of the protocol interacts with\nthe tree as a key/value store that you can read and write to, so we\ndon't have to worry about how the tree is structured.\n\nLater, however, the complexity turned out to have systemic effects:\nthe ability of accounts to have arbitrarily large storage trees meant\nthat there was no way to reliably expect a particular slice of the state\n(eg. \"all accounts starting with 0x1234\") to have a predictable size.\nThis makes it harder to split up the state into pieces, complicating the\ndesign of syncing protocols and attempts to distribute the storage process.\nWhy did encapsulated complexity become systemic? Because the\ninterface changed. The fix? The current proposal to\nmove to Verkle trees also includes a move to a well-balanced\nsingle-layer design for the tree,\n\nUltimately, which type of complexity to favor in any given situation\nis a question with no easy answers. The best that we can do is to have\nan attitude of moderately favoring encapsulated complexity, but not too\nmuch, and exercise our judgement in each specific case. Sometimes, a\nsacrifice of a little bit of systemic complexity to allow a great\nreduction of encapsulated complexity really is the best thing to do. And\nother times, you can even misjudge what is encapsulated and what isn't.\nEach situation is different.",
    "contentLength": 9380,
    "summary": "Ethereum protocol design should favor encapsulated complexity (internal complexity with simple interfaces) over systemic complexity.",
    "detailedSummary": {
      "theme": "Vitalik argues that protocol designers should distinguish between encapsulated complexity (complex internals with simple interfaces) and systemic complexity (complex interactions between system components) when making design tradeoffs.",
      "summary": "Vitalik introduces a framework for thinking about complexity in blockchain protocol design by distinguishing between encapsulated and systemic complexity. Encapsulated complexity involves systems with complex internals that present simple external interfaces (like BLS signatures or ZK-SNARKs), while systemic complexity arises when different parts of a system have intricate interactions that cannot be cleanly separated (like fraud proofs or MEV effects). Through examples ranging from cryptographic signature schemes to consensus mechanisms, Vitalik demonstrates how encapsulated complexity is generally preferable because it contains the complexity within specific components rather than spreading it throughout the system. However, he acknowledges that encapsulated complexity can sometimes become systemic when interfaces change or new interaction patterns emerge, as happened with Ethereum's two-level state tree structure. Vitalik concludes that while there's no universal rule, designers should generally favor encapsulated complexity while remaining vigilant about when seemingly contained complexity might leak into systemic effects.",
      "takeaways": [
        "Encapsulated complexity (complex internals, simple interfaces) is generally preferable to systemic complexity (complex interactions between components)",
        "ZK-SNARKs represent encapsulated complexity that may be superior to the systemic complexity of fraud proofs despite being more technically sophisticated",
        "Complexity that appears encapsulated can become systemic when interfaces change or new interaction patterns emerge",
        "The risks from system complexity are not simply proportional to specification length - small pieces that interact with everything are more dangerous than large isolated components",
        "Protocol designers should moderately favor encapsulated complexity while exercising judgment on a case-by-case basis"
      ],
      "controversial": [
        "The claim that ZK-SNARKs are 'likely to be long-run safer' than fraud proofs may be debatable given ZK-SNARKs' technical immaturity and implementation challenges",
        "The assertion that encapsulated complexity is generally preferable could be challenged by those who argue that transparent, distributed complexity is easier to audit and debug"
      ]
    }
  },
  {
    "id": "general-2022-01-26-soulbound",
    "title": "Soulbound",
    "date": "2022-01-26",
    "category": "applications",
    "url": "https://vitalik.eth.limo/general/2022/01/26/soulbound.html",
    "path": "general/2022/01/26/soulbound.html",
    "content": "- \n\n- \n\n  \n    \n\n  \n  \n  Dark Mode Toggle\n  \n    \n\n  \n\n##  Soulbound \n\n 2022 Jan 26 \nSee all posts\n\n \n \n\n Soulbound \n\nOne feature of World of Warcraft that is second nature to its\nplayers, but goes mostly undiscussed outside of gaming circles, is the\nconcept of soulbound items. A soulbound item, once picked up,\ncannot be transferred or sold to another player.\n\nMost very powerful items in the game are soulbound, and typically\nrequire completing a complicated quest or killing a very powerful\nmonster, usually with the help of anywhere from four to thirty nine\nother players. Hence, in order to get your character anywhere close to\nhaving the best weapons and armor, you have no choice but to participate\nin killing some of these extremely difficult monsters yourself.\n\nThe purpose of the mechanic is fairly clear: it keeps the game\nchallenging and interesting, by making sure that to get the best items\nyou have to actually go and do the hard thing and figure out how to kill\nthe dragon. You can't just go kill boars ten hours a day for a year, get\nthousands of gold, and buy the epic magic armor from other players who\nkilled the dragon for you.\n\nOf course, the system is very imperfect: you could just pay a team of\nprofessionals to kill the dragon with you and let you collect the loot,\nor even outright buy a character on a secondary market, and do this all\nwith out-of-game US dollars so you don't even have to kill boars. But\neven still, it makes for a much better game than every item always\nhaving a price.\nWhat if NFTs could be\nsoulbound?\n\nNFTs in their current form have many of the same properties as rare\nand epic items in a massively multiplayer online game. They have social\nsignaling value: people who have them can show them off, and there's\nmore and more tools precisely to help\nusers do that. Very recently, Twitter started rolling out an integration\nthat allows users to show off their NFTs on their picture profile.\n\nBut what exactly are these NFTs signaling? Certainly, one part of the\nanswer is some kind of skill in acquiring NFTs and knowing which NFTs to\nacquire. But because NFTs are tradeable items, another big part of the\nanswer inevitably becomes that NFTs are about signaling wealth.\n\nCryptoPunks are now regularly being sold for many millions of\ndollars, and they are not even the most\nexpensive NFTs out there. Image source here.\n\nIf someone shows you that they have an NFT that is obtainable by\ndoing X, you can't tell whether they did X themselves or whether they\njust paid someone else to do X. Some of the time this is not a problem:\nfor an NFT supporting a charity, someone buying it off the secondary\nmarket is sacrificing their own funds for the cause and they are helping\nthe charity by contributing to others' incentive to buy the NFT, and so\nthere is no reason to discriminate against them. And indeed, a lot of\ngood can come from charity NFTs alone. But what if we want to create\nNFTs that are not just about who has the most money, and that actually\ntry to signal something else?\n\nPerhaps the best example of a project trying to do this is POAP, the \"proof of attendance protocol\".\nPOAP is a standard by which projects can send NFTs that represent the\nidea that the recipient personally participated in some event.\n\nPart of my own POAP\ncollection, much of which came from the events that I attended over\nthe years.\n\nPOAP is an excellent example of an NFT that works better if it could\nbe soulbound. If someone is looking at your POAP, they are not\ninterested in whether or not you paid someone who attended some event.\nThey are interested in whether or not you personally attended\nthat event. Proposals to put certificates (eg. driver's licenses,\nuniversity degrees, proof of age) on-chain face a similar problem: they\nwould be much less valuable if someone who doesn't meet the condition\nthemselves could just go buy one from someone who does.\n\nWhile transferable NFTs have their place and can be really valuable\non their own for supporting artists and charities, there is also a large\nand underexplored design space of what non-transferable NFTs\ncould become.\nWhat if governance\nrights could be soulbound?\n\nThis is a topic I have written about ad nauseam (see [1] [2] [3] [4] [5]), but it continues to\nbe worth repeating: there are very bad things that can easily\nhappen to governance mechanisms if governance power is easily\ntransferable. This is true for two primary types of\nreasons:\n\n- If the goal is for governance power to be widely\ndistributed, then transferability is counterproductive as\nconcentrated interests are more likely to buy the governance rights up\nfrom everyone else.\n\n- If the goal is for governance power to go to the competent,\nthen transferability is counterproductive because nothing stops the\ngovernance rights from being bought up by the determined but\nincompetent.\n\nIf you take the proverb that \"those who most want to rule people are\nthose least suited to do it\" seriously, then you should be suspicious of\ntransferability, precisely because transferability makes governance\npower flow away from the meek who are most likely to provide valuable\ninput to governance and toward the power-hungry who are most likely to\ncause problems.\n\nSo what if we try to make governance rights non-transferable? What if\nwe try to make a CityDAO where more\nvoting power goes to the people who actually live in the city, or at\nleast is reliably democratic and avoids undue influence by whales\nhoarding a large number of citizen NFTs? What if DAO governance of\nblockchain protocols could somehow make governance power conditional on\nparticipation? Once again, a large and fruitful design space opens up\nthat today is difficult to access.\nImplementing\nnon-transferability in practice\n\nPOAP has made the technical decision to not block transferability of\nthe POAPs themselves. There are good reasons for this: users might have\na good reason to want to migrate all their assets from one wallet to\nanother (eg. for security), and the security of non-transferability\nimplemented \"naively\" is not very strong anyway because users could just\ncreate a wrapper account that holds the NFT and then sell the ownership\nof that.\n\nAnd indeed, there have been quite a few cases where POAPs have\nfrequently been bought and sold when an economic rationale was there to\ndo so. Adidas recently\nreleased a POAP for free to their fans that could give users\npriority access at a merchandise sale. What happened? Well, of course,\nmany of the POAPs were quickly transferred to the highest bidder.\n\nMore transfers than items. And not the only time.\n\nTo solve this problem, the POAP team is suggesting that developers\nwho care about non-transferability implement checks on their own: they\ncould check on-chain if the current owner is the same address as the\noriginal owner, and they could add more sophisticated checks over time\nif deemed necessary. This is, for now, a more future-proof approach.\n\nPerhaps the one NFT that is the most robustly non-transferable today\nis the proof-of-humanity\nattestation. Theoretically, anyone can create a proof-of-humanity\nprofile with a smart contract account that has transferable ownership,\nand then sell that account. But the proof-of-humanity protocol has a revocation\nfeature that allows the original owner to make a video asking for a\nprofile to be removed, and a Kleros\ncourt decides whether or not the video was from the same person as the\noriginal creator. Once the profile is successfully removed, they can\nre-apply to make a new profile. Hence, if you buy someone else's\nproof-of-humanity profile, your possession can be very quickly taken\naway from you, making transfers of ownership non-viable.\nProof-of-humanity profiles are de-facto soulbound, and infrastructure\nbuilt on top of them could allow for on-chain items in general to be\nsoulbound to particular humans.\n\nCan we limit transferability without going all the way and basing\neverything on proof of humanity? It becomes harder, but there are\nmedium-strength approaches that are probably good enough for some use\ncases. Making an NFT bound to an ENS name is one simple option, if we\nassume that users care enough about their ENS names that they are not\nwilling to transfer them. For now, what we're likely to see is a\nspectrum of approaches to limit transferability, with different projects\nchoosing different tradeoffs between security and convenience.\nNon-transferability and\nprivacy\n\nCryptographically strong privacy for transferable assets is fairly\neasy to understand: you take your coins, put them into tornado.cash or a similar platform, and\nwithdraw them into a fresh account. But how can we add privacy for\nsoulbound items where you cannot just move them into a fresh account or\neven a smart contract? If proof of humanity starts getting more\nadoption, privacy becomes even more important, as the alternative is all\nof our activity being mapped on-chain directly to a human face.\n\nFortunately, a few fairly simple technical options are possible:\n\n- Store the item at an address which is the hash of (i) an index, (ii)\nthe recipient address and (iii) a secret belonging to the recipient. You\ncould reveal your secret to an interface that would then scan for all\npossible items that belong to your, but no one without your secret could\nsee which items are yours.\n\n- Publish a hash of a bunch of items, and give each recipient their\nMerkle branch.\n\n- If a smart contract needs to check if you have an item of\nsome type, you can provide a ZK-SNARK.\n\nTransfers could be done on-chain; the simplest technique may just be\na transaction that calls a factory contract to make the old item invalid\nand the new item valid, using a ZK-SNARK to prove that the operation is\nvalid.\n\nPrivacy is an important part of making this kind of ecosystem work\nwell. In some cases, the underlying thing that the item is representing\nis already public, and so there is no point in trying to add privacy.\nBut in many other cases, users would not want to reveal everything that\nthey have. If, one day in the future, being vaccinated becomes a POAP,\none of the worst things we could do would be to create a system where\nthe POAP is automatically advertised for everyone to see and everyone\nhas no choice but to let their medical decision be influenced by what\nwould look cool in their particular social circle. Privacy being a core\npart of the design can avoid these bad outcomes and increase the chance\nthat we create something great.\n\n## From here to there\n\nA common criticism of the \"web3\" space as it exists today is how\nmoney-oriented everything is. People celebrate the ownership, and\noutright waste, of large\namounts of wealth, and this limits the appeal and the long-term\nsustainability of the culture that emerges around these items. There are\nof course important benefits that even financialized NFTs can\nprovide, such as funding artists and charities that would otherwise go\nunrecognized. However, there are limits to that approach, and a lot of\nunderexplored opportunity in trying to go beyond financialization.\nMaking more items in the crypto space \"soulbound\" can be one path toward\nan alternative, where NFTs can represent much more of who you are and\nnot just what you can afford.\n\nHowever, there are technical challenges to doing this, and an uneasy\n\"interface\" between the desire to limit or prevent transfers and a\nblockchain ecosystem where so far all of the standards are designed\naround maximum transferability. Attaching items to \"identity objects\"\nthat users are either unable (as with proof-of-humanity profiles) or\nunwilling (as with ENS names) to trade away seems like the most\npromising path, but challenges remain in making this easy-to-use,\nprivate and secure. We need more effort on thinking through and solving\nthese challenges. If we can, this opens a much wider door to blockchains\nbeing at the center of ecosystems that are collaborative and fun, and\nnot just about money.",
    "contentLength": 11925,
    "summary": "Vitalik proposes \"soulbound\" NFTs that can't be transferred between wallets to better prove personal achievements.",
    "detailedSummary": {
      "theme": "Vitalik proposes implementing 'soulbound' (non-transferable) NFTs to create blockchain-based systems that signal personal achievement and participation rather than just wealth.",
      "summary": "Vitalik draws an analogy from World of Warcraft's soulbound items - powerful equipment that cannot be traded and must be earned through personal effort - to propose a new category of non-transferable NFTs. He argues that current NFTs primarily signal wealth since they can be purchased, but soulbound NFTs could represent genuine personal achievements, attendance at events, or qualifications that cannot simply be bought. Vitalik identifies two key applications: attendance-based NFTs like POAP (Proof of Attendance Protocol) that verify personal participation in events, and governance tokens that prevent wealthy actors from buying up voting power and ensure democratic participation. He discusses technical implementation challenges, including how to maintain transferability for legitimate purposes like wallet migration while preventing economic trading, and emphasizes the importance of privacy to prevent unwanted social pressure or discrimination based on visible achievements.",
      "takeaways": [
        "Soulbound NFTs could shift blockchain culture away from wealth-signaling toward achievement and participation-based recognition",
        "Non-transferable governance tokens could prevent wealthy actors from concentrating voting power and undermining democratic decision-making",
        "POAP and similar attendance-based NFTs work better when they cannot be bought and sold, as they should verify personal participation",
        "Technical solutions like binding NFTs to ENS names or proof-of-humanity profiles can help implement non-transferability",
        "Privacy features are crucial for soulbound NFTs to prevent social pressure and protect personal information while maintaining their authenticity"
      ],
      "controversial": [
        "The proposal challenges the fundamental principle of maximum transferability that underlies most blockchain standards and cryptocurrency philosophy",
        "Privacy implementation for soulbound tokens could conflict with the transparency and auditability principles that many consider essential to blockchain technology"
      ]
    }
  },
  {
    "id": "general-2021-12-19-bullveto",
    "title": "The bulldozer vs vetocracy political axis",
    "date": "2021-12-19",
    "category": "society",
    "url": "https://vitalik.eth.limo/general/2021/12/19/bullveto.html",
    "path": "general/2021/12/19/bullveto.html",
    "content": "- \n\n- \n\n  \n    \n\n  \n  \n  Dark Mode Toggle\n  \n    \n\n  \n\n##  The bulldozer vs vetocracy political axis \n\n 2021 Dec 19 \nSee all posts\n\n \n \n\n The bulldozer vs vetocracy political axis \n\nTypically, attempts to collapse down political preferences into a few\ndimensions focus on two primary dimensions: \"authoritarian vs\nlibertarian\" and \"left vs right\". You've probably seen political\ncompasses like this:\n\nThere have been many variations on this, and even an entire subreddit\ndedicated to memes based on these charts. I even made a spin on the\nconcept myself, with this\n\"meta-political compass\" where at each point on the compass there is\na smaller compass depicting what the people at that point on the compass\nsee the axes of the compass as being.\n\nOf course, \"authoritarian vs libertarian\" and \"left vs right\" are\nboth incredibly un-nuanced gross oversimplifications. But us\npuny-brained human beings do not have the capacity to run anything close\nto accurate simulations of humanity inside our heads, and so sometimes\nincredibly un-nuanced gross oversimplifications are something we\nneed to understand the world. But what if there are other incredibly\nun-nuanced gross oversimplifications worth exploring?\nEnter the bulldozer vs\nvetocracy divide\n\nLet us consider a political axis defined by these two opposing\npoles:\n\n- Bulldozer: single actors can do important and\nmeaningful, but potentially risky and disruptive, things without asking\nfor permission\n\n- Vetocracy: doing anything potentially disruptive\nand controversial requires getting a sign-off from a large number of\ndifferent and diverse actors, any of whom could stop it\n\nNote that this is not the same as either authoritarian vs libertarian\nor left vs right. You can have vetocratic authoritarianism, the\nbulldozer left, or any other combination. Here are a few examples:\n\nThe key difference between authoritarian bulldozer and authoritarian\nvetocracy is this: is the government more likely to fail by doing\nbad things or by preventing good things from happening?\nSimilarly for libertarian bulldozer vs vetocracy: are private actors\nmore likely to fail by doing bad things, or by standing in the way of\nneeded good things?\n\nSometimes, I hear people complaining that eg. the United States (but\nother countries too) is falling behind because too many people use\nfreedom as an excuse to prevent needed reforms from happening. But is\nthe problem really freedom? Isn't, say, restrictive\nhousing policy preventing GDP from rising by 36% an example of the\nproblem precisely being people not having enough freedom to\nbuild structures on their own land? Shifting the argument over to saying\nthat there is too much vetocracy, on the other hand, makes the\nargument look much less confusing: individuals excessively blocking\ngovernments and governments excessively blocking individuals are not\nopposites, but rather two sides of the same coin.\n\nAnd indeed, recently there has been a bunch of political writing\npointing the finger straight at vetocracy as a source of many huge\nproblems:\n\n- https://astralcodexten.substack.com/p/ezra-klein-on-vetocracy\n\n- https://www.vox.com/2020/4/22/21228469/marc-andreessen-build-government-coronavirus\n\n- https://www.vox.com/2016/10/26/13352946/francis-fukuyama-ezra-klein\n\n- https://www.politico.com/news/magazine/2019/11/29/penn-station-robert-caro-073564\n\nAnd on the other side of the coin, people are often confused when\npoliticians who normally do not respect human rights suddenly appear\nvery pro-freedom in their love of Bitcoin. Are they libertarian, or are\nthey authoritarian? In this framework, the answer is simple: they're\nbulldozers, with all the benefits and risks that that side of the\nspectrum brings.\n\n## What is vetocracy good for?\n\nThough the change that cryptocurrency proponents seek to bring to the\nworld is often bulldozery, cryptocurrency governance internally is often\nquite vetocratic. Bitcoin governance famously makes it very difficult to\nmake changes, and some core \"constitutional norms\" (eg. the 21 million\ncoin limit) are considered so inviolate that many Bitcoin users consider\na chain that violates that rule to be by-definition not Bitcoin,\nregardless of how much support it has.\n\nEthereum protocol research is sometimes bulldozery in\noperation, but the Ethereum EIP process that governs the final\nstage of turning a research proposal into something that actually makes\nit into the blockchain includes a fair\nshare of vetocracy, though still less than Bitcoin. Governance over\nirregular state changes, hard forks that interfere with the\noperation of specific applications on-chain, is even more vetocratic:\nafter the DAO fork, not a single proposal to intentionally \"fix\" some\napplication by altering its code or moving its balance has been\nsuccessful.\n\nThe case for vetocracy in these contexts is clear: it gives people a\nfeeling of safety that the platform they build or invest on is not going\nto suddenly change the rules on them one day and destroy everything\nthey've put years of their time or money into. Cryptocurrency proponents\noften cite Citadel\ninterfering in Gamestop trading as an example of the opaque,\ncentralized (and bulldozery) manipulation that they are fighting\nagainst. Web2 developers often complain\nabout centralized platforms suddenly\nchanging their APIs in ways that destroy startups built around their\nplatforms. And, of course....\n\nVitalik Buterin, bulldozer victim\n\nOk fine, the story that WoW removing Siphon Life was the direct\ninspiration to Ethereum is exaggerated, but the infamous patch that\nruined my beloved warlock and my response to it were very real!\n\nAnd similarly, the case for vetocracy in politics is clear: it's a\nresponse to the often ruinous excesses of the bulldozers, both relatively minor\nand unthinkably\nsevere, of the\nearly 20th century.\n\n## So what's the synthesis?\n\nThe primary purpose of this point is to outline an axis, not to argue\nfor a particular position. And if the vetocracy vs bulldozer axis is\nanything like the libertarian vs authoritarian axis, it's inevitably\ngoing to have internal subtleties and contradictions: much like a free\nsociety will see people voluntarily joining internally autocratic\ncorporations (yes, even lots of people who are totally not economically\ndesperate make such choices), many movements will be vetocratic\ninternally but bulldozery in their relationship with the outside\nworld.\n\nBut here are a few possible things that one could believe about\nbulldozers and vetocracy:\n\n- The physical world has too much vetocracy, but the digital world has\ntoo many bulldozers, and there are no digital places that are truly\neffective refuges from the bulldozers (hence: why we need\nblockchains?)\n\n- Processes that create durable change need to be bulldozery toward\nthe status quo but protecting that change requires a vetocracy. There's\nsome optimal rate at which such processes should happen; too much and\nthere's chaos, not enough and there's stagnation.\n\n- A few key institutions should be protected by strong vetocracy, and\nthese institutions exist both to enable bulldozers needed to enact\npositive change and to give people things they can depend on\nthat are not going to be brought down by bulldozers.\n\n- In particular, blockchain base layers should be vetocratic, but\napplication-layer governance should leave more space for bulldozers\n\n- Better economic mechanisms (quadratic voting? Harberger\ntaxes?) can get us many of the benefits of both vetocracy and\nbulldozers without many of the costs.\n\nVetocracy vs bulldozer is a particularly useful axis to use when\nthinking about non-governmental forms of human organization,\nwhether for-profit companies, non-profit organizations, blockchains, or\nsomething else entirely. The relatively easier ability to exit from such\nsystems (compared to governments) confounds discussion of how\nlibertarian vs authoritarian they are, and so far blockchains and even\ncentralized tech platforms have not really found many ways to\ndifferentiate themselves on the left vs right axis (though I would love\nto see more attempts at left-leaning crypto projects!). The vetocracy vs\nbulldozer axis, on the other hand, continues to map to non-governmental\nstructures quite well - potentially making it very relevant in\ndiscussing these new kinds of non-governmental structures that are\nbecoming increasingly important.",
    "contentLength": 8347,
    "summary": "Vitalik proposes \"bulldozer vs vetocracy\" as a new political axis measuring whether single actors can act without permission vs requiring broad consensus.",
    "detailedSummary": {
      "theme": "Vitalik proposes a new political axis that contrasts 'bulldozer' systems (where single actors can make disruptive changes without permission) with 'vetocracy' systems (where multiple stakeholders can block potentially disruptive actions).",
      "summary": "Vitalik introduces the bulldozer vs vetocracy political axis as an alternative framework to traditional left-right and authoritarian-libertarian spectrums. Bulldozer systems allow single actors to make meaningful but potentially risky changes without seeking permission, while vetocracy systems require sign-off from multiple diverse stakeholders who can block disruptive actions. Vitalik argues this axis better explains certain political phenomena, such as why authoritarian politicians might support Bitcoin (they're bulldozers) or why the US struggles with reforms (excessive vetocracy from multiple blocking actors). He illustrates how cryptocurrency governance demonstrates both approaches: Bitcoin's extreme vetocracy protects core rules like the 21 million coin limit, while Ethereum balances both tendencies. Vitalik suggests this framework is particularly useful for analyzing non-governmental organizations like blockchains and tech platforms, where traditional political axes may not apply as clearly, and proposes several potential synthesis approaches that could capture benefits of both systems.",
      "takeaways": [
        "The bulldozer vs vetocracy axis cuts across traditional political spectrums and can coexist with both authoritarian/libertarian and left/right orientations",
        "Cryptocurrency governance exemplifies both approaches: Bitcoin uses vetocracy to protect core principles while the broader crypto movement is bulldozery in disrupting traditional finance",
        "Vetocracy provides safety and predictability by preventing sudden rule changes, while bulldozer approaches enable rapid innovation and necessary reforms",
        "The framework is especially valuable for analyzing non-governmental structures like blockchains, companies, and platforms where traditional political categories don't map well",
        "Optimal systems may require vetocracy for protecting foundational institutions while allowing bulldozer approaches for creating positive change and innovation"
      ],
      "controversial": [
        "The suggestion that the US and other countries suffer from 'too much vetocracy' could be seen as dismissive of democratic processes and stakeholder protections",
        "The characterization of authoritarian politicians supporting Bitcoin as simply 'bulldozers' may oversimplify complex political motivations and ignore human rights concerns"
      ]
    }
  },
  {
    "id": "general-2021-12-06-endgame",
    "title": "Endgame",
    "date": "2021-12-06",
    "category": "general",
    "url": "https://vitalik.eth.limo/general/2021/12/06/endgame.html",
    "path": "general/2021/12/06/endgame.html",
    "content": "- \n\n- \n\n  \n    \n\n  \n  \n  Dark Mode Toggle\n  \n    \n\n  \n\n##  Endgame \n\n 2021 Dec 06 \nSee all posts\n\n \n \n\n Endgame \n\nSpecial thanks to a whole bunch of people from Optimism and\nFlashbots for discussion and thought that went into this piece, and Karl\nFloersch, Phil Daian, Hasu and Alex Obadia for feedback and\nreview.\n\nConsider the average \"big block chain\" - very high block frequency,\nvery high block size, many thousands of transactions per second, but\nalso highly centralized: because the blocks are so big, only a few dozen\nor few hundred nodes can afford to run a fully participating node that\ncan create blocks or verify the existing chain. What would it take to\nmake such a chain acceptably trustless and censorship resistant, at\nleast by my\nstandards?\n\nHere is a plausible roadmap:\n\n- Add a second tier of staking, with low resource\nrequirements, to do distributed block validation. The\ntransactions in a block are split into 100 buckets, with a Merkle or Verkle tree state root after\neach bucket. Each second-tier staker gets randomly assigned to one of\nthe buckets. A block is only accepted when at least 2/3 of the\nvalidators assigned to each bucket sign off on it.\n\n- Introduce either fraud proofs or ZK-SNARKs to let users\ndirectly (and cheaply) check block validity. ZK-SNARKs can\ncryptographically prove block validity directly; fraud proofs are a\nsimpler scheme where if a block has an invalid bucket, anyone can\nbroadcast a fraud proof of just that bucket. This provides another layer\nof security on top of the randomly-assigned validators.\n\n- Introduce data\navailability sampling to let users check block availability.\nBy using DAS checks, light clients can verify that a block was published\nby only downloading a few randomly selected pieces.\n\n- Add secondary transaction channels to prevent\ncensorship. One way to do this is to allow secondary stakers to\nsubmit lists of transactions which the next\nmain block must include.\n\nWhat do we get after all of this is done? We get a chain\nwhere block production is still centralized, but block\nvalidation is trustless and highly decentralized, and\nspecialized anti-censorship magic prevents the block producers from\ncensoring. It's somewhat aesthetically ugly, but it does\nprovide the basic guarantees that we are looking for: even if every\nsingle one of the primary stakers (the block producers) is intent on\nattacking or censoring, the worst that they could do is all go offline\nentirely, at which point the chain stops accepting transactions until\nthe community pools their resources and sets up one\nprimary-staker node that is honest.\nNow,\nconsider one possible long-term future for rollups...\n\nImagine that one particular rollup - whether Arbitrum, Optimism,\nZksync, StarkNet or something completely new - does a really good job of\nengineering their node implementation, to the point where it really can\ndo 10,000 transactions per second if given powerful enough hardware. The\ntechniques for doing this are in-principle well-known, and\nimplementations were made by Dan\nLarimer and others many years ago: split up execution into one CPU\nthread running the unparallelizable but cheap business logic and a huge\nnumber of other threads running the expensive but highly parallelizable\ncryptography. Imagine also that Ethereum implements sharding with data\navailability sampling, and has the space to store that rollup's\non-chain data between its 64 shards. As a result, everyone migrates to\nthis rollup. What would that world look like?\n\nOnce again, we get a world where, block production\nis centralized, block validation is trustless and highly\ndecentralized, and censorship is still prevented. Rollup block\nproducers have to process a huge number of transactions, and so it is a\ndifficult market to enter, but they have no way to push invalid blocks\nthrough. Block availability is secured by the underlying chain, and\nblock validity is guaranteed by the rollup logic: if it's a ZK rollup,\nit's ensured by SNARKs, and an optimistic rollup is secure as long as\nthere is one honest actor somewhere running a fraud prover node (they\ncan be subsidized with Gitcoin\ngrants). Furthermore, because users always have the option of\nsubmitting transactions through the on-chain secondary inclusion\nchannel, rollup sequencers also cannot effectively censor.\nNow,\nconsider the other possible long-term future of rollups...\n\nNo single rollup succeeds at holding anywhere close to the majority\nof Ethereum activity. Instead, they all top out at a few hundred\ntransactions per second. We get a multi-rollup future for Ethereum - the\nCosmos multi\u2013chain vision,\nbut on top of a base layer providing data availability and shared\nsecurity. Users frequently rely on cross-rollup\nbridging to jump between different rollups without paying the high\nfees on the main chain. What would that world look like?\n\nIt seems like we could have it all: decentralized validation, robust\ncensorship resistance, and even distributed block production,\nbecause the rollups are all invididually small and so easy to start\nproducing blocks in. But the decentralization of block production may\nnot last, because of the possibility of cross-domain MEV. There are a\nnumber of benefits to being able to construct the next block on many\ndomains at the same time: you can create blocks that use arbitrage\nopportunities that rely on making transactions in two rollups, or one\nrollup and the main chain, or even more complex combinations.\n\nA cross-domain MEV opportunity discovered by Western Gate\n\nHence, in a multi-domain world, there are strong pressures toward the\nsame people controlling block production on all domains. It may not\nhappen, but there's a good chance that it will, and we have to be\nprepared for that possibility. What can we do about it? So far, the best\nthat we know how to do is to use two techniques in combination:\n\n- Rollups implement some mechanism for auctioning off block production\nat each slot, or the Ethereum base layer implements proposer/builder\nseparation (PBS) (or both). This ensures that at least any\ncentralization tendencies in block production don't lead to a completely\nelite-captured and concentrated staking pool market dominating block\nvalidation.\n\n- Rollups implement censorship-resistant bypass\nchannels, and the Ethereum base layer implements PBS\nanti-censorship techniques. This ensures that if the winners of the\npotentially highly centralized \"pure\" block production market try to\ncensor transactions, there are ways to bypass the censorship.\n\nSo what's the result? Block production is\ncentralized, block validation is trustless and highly\ndecentralized, and censorship is still prevented.\n\nThree paths toward the same destination.\n\n## So what does this mean?\n\nWhile there are many paths toward building a scalable and secure\nlong-term blockchain ecosystem, it's looking like they are all building\ntoward very similar futures. There's a high chance that block production\nwill end up centralized: either the network effects within rollups or\nthe network effects of cross-domain MEV push us in that direction in\ntheir own different ways. But what we can do is use\nprotocol-level techniques such as committee validation, data\navailability sampling and bypass channels to \"regulate\" this market,\nensuring that the winners cannot abuse their power.\n\nWhat does this mean for block producers? Block\nproduction is likely to become a specialized market, and the domain\nexpertise is likely to carry over across different domains. 90% of what\nmakes a good Optimism block producer also makes a good Arbitrum block\nproducer, and a good Polygon block producer, and even a good Ethereum\nbase layer block producer. If there are many domains, cross-domain\narbitrage may also become an important source of revenue.\n\nWhat does this mean for Ethereum? First of all,\nEthereum is very well-positioned to adjust to this future world, despite\nthe inherent uncertainty. The profound benefit of the Ethereum rollup-centric\nroadmap is that it means that Ethereum is open to all of the\nfutures, and does not have to commit to an opinion about which one will\nnecessarily win. Will users very strongly want to be on a single rollup?\nEthereum, following its existing course, can be the base layer of that,\nautomatically providing the anti-fraud and anti-censorship \"armor\" that\nhigh-capacity domains need to be secure. Is making a high-capacity\ndomain too technically complicated, or do users just have a great need\nfor variety? Ethereum can be the base layer of that too - and a very\ngood one, as the common root of trust makes it far easier to move assets\nbetween rollups safely and cheaply.\n\nBut also, Ethereum researchers should think hard about what levels of\ndecentralization in block production are actually achievable. It may not\nbe worth it to add complicated plumbing to make highly decentralized\nblock production easy if cross-domain MEV (or even cross-shard MEV from\none rollup taking up multiple shards) make it unsustainable\nregardless.\n\nWhat does this mean for big block chains? There is a\npath for them to turn into something trustless and censorship resistant,\nand we'll soon find out if their core developers and communities\nactually value censorship resistance and decentralization enough for\nthem to do it!\n\nIt will likely take years for all of this to play out. Sharding and\ndata availability sampling are complex technologies to implement. It\nwill take years of refinement and audits for people to be fully\ncomfortable storing their assets in a ZK-rollup running a full EVM. And\ncross-domain MEV research too is still in its infancy. But it does look\nincreasingly clear how a realistic but bright future for scalable\nblockchains is likely to emerge.",
    "contentLength": 9706,
    "summary": "Ethereum researcher argues that despite multiple scaling paths (big blocks, single rollup dominance, multi-rollup), all lead to centralized block production with decentralized validation.",
    "detailedSummary": {
      "theme": "Vitalik explores how different blockchain scaling approaches - big block chains, single dominant rollups, and multi-rollup ecosystems - all converge toward the same architectural pattern of centralized block production with decentralized validation and censorship resistance mechanisms.",
      "summary": "Vitalik presents three distinct paths for blockchain scaling and demonstrates how they all lead to a similar endgame: centralized block production paired with trustless, decentralized validation and anti-censorship protections. He first examines how big block chains could achieve trustlessness through committee validation, fraud proofs/ZK-SNARKs, data availability sampling, and secondary transaction channels. Next, he explores scenarios where either one rollup dominates due to superior engineering, or multiple rollups coexist but face centralization pressures from cross-domain MEV opportunities. In both rollup scenarios, the same pattern emerges - specialized block producers dominate due to technical complexity and economic incentives, but protocol-level safeguards ensure they cannot abuse their power. Vitalik argues that Ethereum's rollup-centric roadmap positions it well for this future regardless of which specific path emerges, as it can serve as the foundational layer providing the necessary 'armor' of anti-fraud and anti-censorship protections that make centralized block production acceptable.",
      "takeaways": [
        "All major blockchain scaling approaches converge toward centralized block production with decentralized validation and censorship resistance",
        "Technical complexity and cross-domain MEV create strong economic pressures toward block production centralization",
        "Protocol-level safeguards like committee validation, fraud proofs, ZK-SNARKs, and bypass channels can 'regulate' centralized block producers",
        "Ethereum's rollup-centric approach makes it well-positioned for multiple possible futures without committing to a specific scaling path",
        "Block production will likely become a specialized, cross-domain market where expertise transfers between different blockchain environments"
      ],
      "controversial": [
        "The acceptance that block production centralization may be inevitable and potentially acceptable if properly regulated",
        "The suggestion that big block chains could become trustless and censorship resistant if their communities actually value these properties enough to implement the necessary changes"
      ]
    }
  },
  {
    "id": "general-2021-11-16-retro1",
    "title": "Review of Optimism retro funding round 1",
    "date": "2021-11-16",
    "category": "general",
    "url": "https://vitalik.eth.limo/general/2021/11/16/retro1.html",
    "path": "general/2021/11/16/retro1.html",
    "content": "- \n\n- \n\n  \n    \n\n  \n  \n  Dark Mode Toggle\n  \n    \n\n  \n\n##  Review of Optimism retro funding round 1 \n\n 2021 Nov 16 \nSee all posts\n\n \n \n\n Review of Optimism retro funding round 1 \n\nSpecial thanks to Karl Floersch and Haonan Li for feedback and\nreview, and Jinglan Wang for discussion.\n\nLast month, Optimism ran their\nfirst round of retroactive\npublic goods funding, allocating a total of $1 million to 58\nprojects to reward the good work that these projects have already done\nfor the Optimism and Ethereum ecosystems. In addition to being the first\nmajor retroactive general-purpose public goods funding experiment, it's\nalso the first experiment in a new kind of governance through\nbadge holders - not a very small decision-making board\nand also not a fully public vote, but instead a quadratic vote among a\nmedium-sized group of 22 participants.\n\nThe entire process was highly transparent from start to finish:\n\n- The rules that the badge holders were supposed to follow were\nenshrined in the\nbadge holder instructions\n\n- You can see the projects that were nominated in\nthis spreadsheet\n\n- All discussion between the badge holders happened in publicly\nviewable forums. In addition to Twitter conversation (eg. Jeff\nColeman's thread and also others),\nall of the explicit structured discussion channels were publicly\nviewable: the #retroactive-public-goods channel on the Optimism discord, and a published\nZoom call\n\n- The full results, and the individual badge holder votes that went\ninto the results, can be viewed in\nthis spreadsheet\n\nAnd finally, here are the results in an easy-to-read chart form:\n\nMuch like the Gitcoin\nquadratic funding rounds and the MolochDAO grants, this is yet\nanother instance of the Ethereum ecosystem establishing itself as a key\nplayer in the innovative public goods funding mechanism design space.\nBut what can we learn from this experiment?\n\n## Analyzing the results\n\nFirst, let us see if there are any interesting takeaways that can be\nseen by looking at the results. But what do we compare the results to?\nThe most natural point of comparison is the other major public goods\nfunding experiment that we've had so far: the Gitcoin quadratic funding\nrounds (in this case, round 11).\n\nGitcoin round 11 (tech only)\n\nOptimism retro round 1\n\nProbably the most obvious property of the Optimism retro results that\ncan be seen without any comparisons is the category of the winners:\nevery major Optimism retro winner was a technology\nproject. There was nothing in the badge holder instructions\nthat specified this; non-tech projects (say, the translations at ethereum.cn) were absolutely eligible.\nAnd yet, due to some combination of choice of badge holders and\nsubconscious biases, the round seems to have been understood as being\ntech-oriented. Hence, I restricted the Gitcoin results in the table\nabove to technology (\"DApp Tech\" + \"Infra Tech\") to focus on the\nremaining differences.\n\nSome other key remaining differences are:\n\n- The retro round was low variance: the top-receiving\nproject only got three times more (in fact, exactly three times\nmore) than the 25th, whereas in the Gitcoin chart combining the two\ncategories the gap was over 5x, and if you look at DApp Tech or Infra\nTech separately the gap is over 15x! I personally blame this on\nGitcoin using standard quadratic funding (\\(reward \\approx (\\sum_i \\sqrt x_i) ^2\\)) and\nthe retro round using \\(\\sum_i \\sqrt\nx_i\\) without the square; perhaps the next retro round should\njust add the square.\n\n- The retro round winners are more well-known\nprojects: this is actually an intended consequence:\nthe retro round focused on rewarding projects for value already\nprovided, whereas the Gitcoin round was open-ended and many\ncontributions were to promising new projects in expectation of future\nvalue.\n\n- The retro round focused more on infrastructure, the Gitcoin\nround more on more user-facing projects: this is of course a\ngeneralization, as there are plenty of infrastructure projects in the\nGitcoin list, but in general applications that are directly\nuser-facing are much more prominent there. A particularly interesting\nconsequence (or cause?) of this is that the Gitcoin round more on\nprojects appealing to sub-communities (eg. gamers), whereas the retro\nround focused more on globally-valuable projects - or, less charitably,\nprojects appealing to the one particular sub-community that is Ethereum\ndevelopers.\n\nIt is my own (admittedly highly subjective) opinion that the\nretro round winner selection is somewhat higher quality. This\nis independent of the above three differences; it's more a general\nimpression that the specific projects that were chosen as top recipients\non the right were very high quality projects, to a greater extent than\ntop recipients on the left.\n\nOf course, this could have two causes: (i) a smaller but more skilled\nnumber of badge holders (\"technocrats\") can make better decisions than\n\"the crowd\", and (ii) it's easier to judge quality retroactively than\nahead of time. And this gets us an interesting question: what if\na simple way to summarize much of the above findings is that technocrats\nare smarter but the crowd is more diverse?\nCould we\nmake badge holders and their outputs more diverse?\n\nTo better understand the problem, let us zoom in on the one specific\nexample that I already mentioned above: ethereum.cn. This is an excellent Chinese\nEthereum community project (though not the only one! See also EthPlanet), which has been\nproviding a lot of resources in Chinese for people to learn about\nEthereum, including translations of many highly technical articles\nwritten by Ethereum community members and about Ethereum originally in\nEnglish.\n\nEthereum.cn webpage. Plenty of high quality technical material -\nthough they have still not yet gotten the memo that they were supposed to\nrename \"eth2\" to \"consensus layer\". Minus ten retroactive reward\npoints for them.\n\nWhat knowledge does a badge holder need to be able to effectively\ndetermine whether ethereum.cn is an awesome project, a well-meaning but\nmediocre project that few Chinese people actually visit, or a scam?\nLikely the following:\n\n- Ability to speak and understand Chinese\n\n- Being plugged into the Chinese community and understanding the\nsocial dynamics of that specific project\n\n- Enough understanding of both the tech and of the frame of\nmind of non-technical readers to judge the site's usefulness for\nthem\n\nOut of the current, heavily US-focused, badge holders, the number\nthat satisfy these requirements is basically zero. Even the two\nChinese-speaking badge holders are US-based and not close to the Chinese\nEthereum community.\n\nSo, what happens if we expand the badge holder set? We could add five\nbadge holders from the Chinese Ethereum community, five from India, five\nfrom Latin America, five from Africa, and five from Antarctica to\nrepresent the penguins. At the same time we could also diversify among\nareas of expertise: some technical experts, some community leaders, some\npeople plugged into the Ethereum gaming world. Hopefully, we can get\nenough coverage that for each project that's valuable to Ethereum, we\nwould have at least 1-5 badge holders who understands enough about that\nproject to be able to intelligently vote on it. But then we see the\nproblem: there would only be 1-5 badge holders able to\nintelligently vote on it.\n\nThere are a few families of solutions that I see:\n\n- Tweak the quadratic voting design. In theory,\nquadratic voting has the unique property that there's very little\nincentive to vote on projects that you do not understand. Any vote you\nmake takes away credits that you could use to vote on projects you\nunderstand better. However, the current quadratic voting design has a\nflaw here: not voting on a project isn't truly a neutral non-vote, it's\na vote for the project getting nothing. I don't yet have great ideas for\nhow to do this. A key question is: if zero becomes a truly neutral vote,\nthen how much money does a project get if nobody makes any\nvotes on it? However, this is worth looking into more.\n\n- Split up the voting into categories or\nsub-committees. Badge holders would first vote to sort projects\ninto buckets, and the badge holders within each bucket (\"zero knowledge\nproofs\", \"games\", \"India\"...) would then make the decisions from there.\nThis could also be done in a more \"liquid\" way through delegation - a\nbadge holder could select some other badge holder to decide their vote\non some project, and they would automatically copy their vote.\n\n- Everyone still votes on everything, but facilitate more\ndiscussion. The badge holders that do have the needed\ndomain expertise to assess a given project (or a single aspect of some\ngiven project) come up with their opinions and write a document or\nspreadsheet entry to express their reasoning. Other badge holders use\nthis information to help make up their minds.\n\nOnce the number of decisions to be made gets even higher, we could\neven consider ideas like in-protocol random sortition\n(eg. see this\nidea to incorporate sortition into quadratic voting) to reduce the\nnumber of decisions that each participant needs to make. Quadratic\nsortition has the particularly nice benefit that it naturally leads to\nlarge decisions being made by the entire group and small decisions being\nmade by smaller groups.\n\n## The means-testing debate\n\nIn the post-round retrospective discussion among the badgeholders,\none of the key questions that was brought up is: when choosing which\nprojects to fund, should badge holders take into account whether\nthat project is still in dire need of funding, and\nde-prioritize projects that are already well-funded through some other\nmeans? That is to say, should retroactive rewards be means-tested?\n\nIn a \"regular\" grants-funding round, the rationale for answering\n\"yes\" is clear: increasing a project's funding from $0 to $100k has a\nmuch bigger impact on its ability to do its job than increasing a\nproject's funding from $10m to $10.1m. But Optimism retro funding round\n1 is not a regular grants-funding round. In retro funding, the\nobjective is not to give people money in expectation of future\nwork that money could help them do. Rather, the objective is to reward\npeople for work already done, to change the incentives for anyone\nworking on projects in the future. With this in mind,\nto what extent should retroactive project funding depend on how\nmuch a given project actually needs the funds?\n\n## The case for means testing\n\nSuppose that you are a 20-year-old developer, and you are deciding\nwhether to join some well-funded defi project with a fancy token, or to\nwork on some cool open-source fully public good work that will benefit\neveryone. If you join the well-funded defi project, you will get a $100k\nsalary, and your financial situation will be guaranteed to be very\nsecure. If you work on public-good projects on your own, you will have\nno income. You have some savings and you could make some money with side\ngigs, but it will be difficult, and you're not sure if the sacrifice is\nworth it.\n\nNow, consider two worlds, World A and World\nB. First, the similarities:\n\n- There are ten people exactly like you out there that could get retro\nrewards, and five of you will. Hence, there's a 50% chance\nyou'll get a retro reward.\n\n- Theres's a 30% chance that your work will propel you to\nmoderate fame and you'll be hired by some company with even\nbetter terms than the original defi project (or even start your\nown).\n\nNow, the differences:\n\n- World A (means testing): retro rewards are\nconcentrated among the actors that do not find success some\nother way\n\n- World B (no means testing): retro rewards are given\nout independently of whether or not the project finds success in some\nother way\n\nLet's look at your chances in each world.\n\nEvent\nProbability (World A)\nProbability (World B)\n\nIndependent success and retroactive reward\n0%\n15%\n\nIndependent success only\n30%\n15%\n\nRetroactive reward only\n50%\n35%\n\nNothing\n20%\n35%\n\nFrom your point of view as a non-risk-neutral human being, the 15%\nchance of getting success twice in world B matters much less than the\nfact that in world B your chances of being left completely in the cold\nwith nothing are nearly double.\n\nHence, if we want to encourage people in this hypothetical 20 year\nold's position to actually contribute, concentrating retro rewards among\nprojects who did not already get rewarded some other way seems\nprudent.\nThe case against means\ntesting\n\nSuppose that you are someone who contributes a small amount to many\nprojects, or an investor seed-funding public good projects in\nanticipation of retroactive rewards. In this case, the share that you\nwould get from any single retroactive reward is small. Would you rather\nhave a 10% chance of getting $10,100, or a 10% chance of getting $10,000\nand a 10% chance of getting $100? It really doesn't matter.\n\nFurthermore, your chance of getting rewarded via retroactive funding\nmay well be quite disjoint from your chance of getting rewarded some\nother way. There are countless stories on the internet of people putting\na big part of their lives into a project when that project was\nnon-profit and open-source, seeing that project go for-profit and become\nsuccessful, and getting absolutely nothing out of it for themselves. In\nall of these cases, it doesn't really matter whether or not retroactive\nrewards care on whether or not projects are needy. In fact, it would\nprobably be better for them to just focus on judging\nquality.\n\nMeans testing has downsides of its own. It would require badge\nholders to expend effort to determine to what extent a project is\nwell-funded outside the retroactive reward system. It could lead to\nprojects expending effort to hide their wealth and appear\nscrappy to increase their chance of getting more rewards. Subjective\nevaluations of neediness of recipients could turn into politicized\nevaluations of moral worthiness of recipients that introduce more\ncontroversy into the mechanism. In the extreme, an elaborate tax return\nsystem might be required to properly enforce fairness.\n\n## What do I think?\n\nIn general, it seems like doing a little bit of prioritizing projects\nthat have not discovered business models has advantages, but we should\nnot do too much of that. Projects should be judged by their effect on\nthe world first and foremost.\n\n## Nominations\n\nIn this round, anyone could nominate projects by submitting them in a\nGoogle form, and there were only 106 projects nominated. What about the\nnext round, now that people know for sure that they stand a chance at\ngetting thousands of dollars in payout? What about the round a year in\nthe hypothetical future, when fees from millions of daily transactions\nare paying into the retro funding rounds, and individual projects are\ngetting more money than the entire round is today?\n\nSome kind of multi-level structure for nominations seems inevitable.\nThere's probably no need to enshrine it directly into the voting rules.\nInstead, we can look at this as one particular way of changing the\nstructure of the discussion: nomination rules filter out the nominations\nthat badge holders need to look at, and anything the badge holders do\nnot look at will get zero votes by default (unless a badge holder\nreally cares to bypass the rules because they have their own\nreasons to believe that some project is valuable).\n\nSome possible ideas:\n\n- Badge holder pre-approval: for a proposal to become\nvisible, it must be approved by N badge holders (eg. N=3?). Any N badge\nholders could pre-approve any project; this is an anti-spam speed bump,\nnot a gate-keeping sub-committee.\n\n- Require proposers to provide more information about\ntheir proposal, justifying it and reducing the work badge holders need\nto do to go through it. Badge holders would also appoint a separate\ncommittee and entrust it with sorting through these proposals and\nforwarding the ones that follow the rules and pass a basic smell test of\nnot being spam\n\n- Proposals have to specify a category (eg. \"zero\nknowledge proofs\", \"games\", \"India\"), and badge holders who had declared\nthemselves experts in that category would review those proposals and\nforward them to a vote only if they chose the right category and pass a\nbasic smell test.\n\n- Proposing requires a deposit of 0.02 ETH. If your\nproposal gets 0 votes (alternatively: if your proposal is explicitly\ndeemed to be \"spam\"), your deposit is lost.\n\n- Proposing requires a proof-of-humanity ID, with a\nmaximum of 3 proposals per human. If your proposals get 0 votes\n(alternatively: if any of your proposals is explicitly deemed to be\n\"spam\"), you can no longer submit proposals for a year (or you have to\nprovide a deposit).\n\n## Conflict of interest rules\n\nThe first part of the post-round retrospective discussion was taken\nup by discussion of conflict of interest rules. The badge\nholder instructions include the following lovely clause:\n\n- \u00a0 No self-dealing or conflicts of interest \n\nRetroDAO governance participants should refrain from voting on sending\nfunds to organizations where any portion of those funds is expected to\nflow to them, their other projects, or anyone they have a close personal\nor economic relationship with.\n\nAs far as I can tell, this was honored. Badge holders did not try to\nself-deal, as they were (as far as I can tell) good people, and they\nknew their reputations were on the line. But there were also some\nsubjective edge cases:\n\n- Wording issues causing confusion. Some badge\nholders wondered about the word \"other\": could badge holders direct\nfunds to their own primary projects? Additionally, the \"sending\nfunds to organizations where...\" language does not strictly prohibit\ndirect transfers to self. These were arguably simple mistakes\nin writing this clause; the word \"other\" should just be removed and\n\"organizations\" replaced with \"addresses\".\n\n- What if a badge holder is part of a nonprofit that\nitself gives out grants to other projects? Could the badge holder vote\nfor that nonprofit? The badge holder would not benefit, as the\nfunds would 100% pass-through to others, but they could benefit\nindirectly.\n\n- What level of connection counts as close\nconnection? Ethereum is a tight-knit community and the people\nqualified to judge the best projects are often at least to some degree\nfriends with the team or personally involved in those projects precisely\nbecause they respect those projects. When do those connections step over\nthe line?\n\nI don't think there are perfect answers to this; rather, the line\nwill inevitably be gray and can only be discussed and refined over time.\nThe main mechanism-design tweaks that can mitigate it are (i) increasing\nthe number of badge holders, diluting the portion of them that can be\ninsiders in any single project, (ii) reducing the rewards going to\nprojects that only a few badge holders support (my suggestion\nabove to set the reward to \\((\\sum_i \\sqrt\nx_i)^2\\) instead of \\(\\sum_i \\sqrt\nx_i\\) would help here too), and (iii) making sure it's possible\nfor badge holders to counteract clear abuses if they do show up.\nShould badge holder\nvotes be secret ballot?\n\nIn this round, badge holder votes were completely transparent; anyone\ncan see how each badge holder votes. But transparent voting has a\nhuge downside: it's vulnerable to bribery, including informal bribery of\nthe kind that even good people easily succumb to. Badge holders could\nend up supporting projects in part with the subconscious motivation of\nwinning favor with them. Even more realistically, badge holders may be\nunwilling to make negative votes even when they are justified,\nbecause a public negative vote could easily rupture a relationship.\n\nSecret ballots are the natural alternative. Secret ballots are used\nwidely in democratic elections where any citizen (or sometimes resident)\ncan vote, precisely to prevent vote buying and more coercive forms of\ninfluencing how people vote. However, in typical elections,\nvotes within executive and legislative bodies are typically\npublic. The usual reasons for this have to do with theories of\ndemocratic accountability: voters need to know how their representatives\nvote so that they can choose their representatives and know that they\nare not completely lying about their stated values. But there's also a\ndark side to accountability: elected officials making public votes are\naccountable to anyone who is trying to bribe them.\n\nSecret ballots within government bodies do have precedent:\n\n- The Israeli Knesset uses\nsecret votes to elect the president and a few other officials\n\n- The Italian parliament has used\nsecret votes in a variety of contexts. In the 19th century, it was\nconsidered an important way to protect parliament votes from\ninterference by a monarchy.\n\n- Discussions in US parliaments were less transparent before 1970, and\nsome researchers\nargue that the switch\nto more transparency led to more corruption.\n\n- Voting in juries is often secret. Sometimes, even the\nidentities of jurors are secret.\n\nIn general, the conclusion seems to be that secret votes in\ngovernment bodies have complicated consequences; it's not clear that\nthey should be used everywhere, but it's also not clear that\ntransparency is an absolute good either.\n\nIn the context of Optimism retro funding specifically, the main\nspecific argument I heard against secret voting is that it would make it\nharder for badge holders to rally and vote against other badge holders\nmaking votes that are clearly very wrong or even malicious. Today, if a\nfew rogue badge holders start supporting a project that has not provided\nvalue and is clearly a cash grab for those badge holders, the other\nbadge holders can see this and make negative votes to counteract this\nattack. With secret ballots, it's not clear how this could be done.\n\nI personally would favor the second round of Optimism retro funding\nusing completely secret votes (except perhaps open to a few researchers\nunder conditions of non-disclosure) so we can tell what the material\ndifferences are in the outcome. Given the current small and tight-knit\nset of badge holders, dealing with rogue badge hodlers is likely not a\nprimary concern, but in the future it will be; hence, coming up with a\nsecret ballot design that allows counter-voting or some alternative\nstrategy is an important research problem.\nOther ideas for\nstructuring discussion\n\nThe level of participation among badge holders was very uneven. Some\n(particularly Jeff Coleman and Matt Garnett) put a lot of effort into\ntheir participation, publicly expressing their detailed reasoning in\nTwitter threads and helping to set up calls for more detailed\ndiscussion. Others participated in the discussion on Discord and still\nothers just voted and did little else.\n\nThere was a choice made (ok fine, I was the one who suggested it)\nthat the #retroactive-public-goods\nchannel should be readable by all (it's in the Optimism discord), but to prevent spam\nonly badge holders should be able to speak. This reduced many people's\nability to participate, especially ironically enough my own (I am not a\nbadge holder, and my self-imposed Twitter quarantine, which only allows\nme to tweet links to my own long-form content, prevented me from\nengaging on Twitter).\n\nThese two factors together meant that there was not that much\ndiscussion taking place; certainly less than I had been hoping for. What\nare some ways to encourage more discussion?\n\nSome ideas:\n\n- Badge holders could vote in advisors, who cannot\nvote but can speak in the #retroactive-public-goods channel and other\nbadge-holder-only meetings.\n\n- Badge holders could be required to explain their\ndecisions, eg. writing a post or a paragraph for each project\nthey made votes on.\n\n- Consider compensating badge holders, either through\nan explicit fixed fee or through a norm that badge holders themselves\nwho made exceptional contributions to discussion are eligible for\nrewards in future rounds.\n\n- Add more discussion formats. If the number of badge\nholders increases and there are subgroups with different specialties,\nthere could be more chat rooms and each of them could invite outsiders.\nAnother option is to create a dedicated subreddit.\n\nIt's probably a good idea to start experimenting with more ideas like\nthis.\n\n## Conclusions\n\nGenerally, I think Round 1 of Optimism retro funding has been a\nsuccess. Many interesting and valuable projects were funded, there was\nquite a bit of discussion, and all of this despite it only being the\nfirst round.\n\nThere are a number of ideas that could be introduced or experimented\nwith in subsequent rounds:\n\n- Increase the number and diversity of badge holders,\nwhile making sure that there is some solution to the problem\nthat only a few badge holders will be experts in any individual\nproject's domain.\n\n- Add some kind of two-layer nomination structure, to\nlower the decision-making burden that the entire badge holder set is\nexposed to\n\n- Use secret ballots\n\n- Add more discussion channels, and more ways for\nnon-badge-holders to participate. This could involve reforming\nhow existing channels work, or it could involve adding new channels, or\neven specialized channels for specific categories of projects.\n\n- Change the reward formula to increase variance,\nfrom the current \\(\\sum_i \\sqrt x_i\\)\nto the standard quadratic funding formula of \\((\\sum_i \\sqrt x_i) ^2\\).\n\nIn the long term, if we want retro funding to be a sustainable\ninstitution, there is also the question of how new badge holders\nare to be chosen (and, in cases of malfeasance, how\nbadge holders could be removed). Currently, the selection is\ncentralized. In the future, we need some alternative. One possible idea\nfor round 2 is to simply allow existing badge holders to vote in a few\nnew badge holders. In the longer term, to prevent it from being an\ninsular bureaucracy, perhaps one badge holder each round could be chosen\nby something with more open participation, like a proof-of-humanity\nvote?\n\nIn any case, retroactive public goods funding is still an exciting\nand new experiment in institutional innovation in multiple ways. It's an\nexperiment in non-coin-driven decentralized governance, and it's an\nexperiment in making things happen through retroactive, rather than\nproactive, incentives. To make the experiment fully work, a lot more\ninnovation will need to happen both in the mechanism itself and in the\necosystem that needs to form around it. When will we see the first retro\nfunding-focused angel investor? Whatever ends up happening, I'm looking\nforward to seeing how this experiment evolves in the rounds to come.",
    "contentLength": 26541,
    "summary": "Optimism's first $1M retroactive funding round using 22 badge holders favored well-known tech projects with low variance in payouts.",
    "detailedSummary": {
      "theme": "Vitalik's comprehensive analysis of Optimism's first retroactive public goods funding round, examining its successes, limitations, and recommendations for improvement.",
      "summary": "Vitalik analyzes Optimism's groundbreaking $1 million retroactive public goods funding experiment, which distributed funds to 58 projects through quadratic voting by 22 badge holders. He compares the results to Gitcoin's funding rounds, noting that Optimism's approach produced lower variance outcomes, focused more on established infrastructure projects, and may have achieved higher quality selections overall. Vitalik identifies several key challenges including the lack of diversity among badge holders (which led to tech-focused outcomes and inability to properly evaluate projects like ethereum.cn), the debate over means-testing (whether to prioritize funding needy vs. high-quality projects), and structural issues around nominations, conflict of interest rules, and voting transparency. He proposes various solutions including expanding and diversifying the badge holder set, implementing multi-layer nomination structures, experimenting with secret ballots, and changing the reward formula to increase variance in outcomes.",
      "takeaways": [
        "Retroactive funding can produce higher quality project selections than predictive funding, but current badge holder composition lacks geographic and expertise diversity",
        "The tension between means-testing (prioritizing needy projects) versus merit-based funding (rewarding quality regardless of need) requires careful balance",
        "Transparent voting enables accountability but creates vulnerability to bribery and social pressure, suggesting secret ballots should be tested",
        "Multi-layer nomination structures will become necessary as funding rounds scale up and attract more applicants",
        "The mechanism needs systematic reforms in badge holder selection, discussion formats, and reward formulas to maintain effectiveness as it grows"
      ],
      "controversial": [
        "Vitalik's subjective assessment that the retro round produced 'somewhat higher quality' winner selection compared to Gitcoin's more democratic approach",
        "The suggestion that 'technocrats are smarter but the crowd is more diverse' when comparing funding mechanisms",
        "The recommendation to use secret ballots, which could reduce accountability and make it harder to counter malicious voting"
      ]
    }
  },
  {
    "id": "general-2021-11-05-halo",
    "title": "Halo and more: exploring incremental verification and SNARKs without pairings",
    "date": "2021-11-05",
    "category": "math",
    "url": "https://vitalik.eth.limo/general/2021/11/05/halo.html",
    "path": "general/2021/11/05/halo.html",
    "content": "- \n\n- \n\n  \n    \n\n  \n  \n  Dark Mode Toggle\n  \n    \n\n  \n\n##  Halo and more: exploring incremental verification and SNARKs without pairings \n\n 2021 Nov 05 \nSee all posts\n\n \n \n\n Halo and more: exploring incremental verification and SNARKs without pairings \n\nSpecial thanks to Justin Drake and Sean Bowe for wonderfully\npedantic and thoughtful feedback and review, and to Pratyush Mishra for\ndiscussion that contributed to the original IPA exposition.\n\nReaders who have been following the ZK-SNARK space closely should by now\nbe familiar with the high level of how ZK-SNARKs work. ZK-SNARKs are\nbased on checking equations where the elements going into the equations\nare mathematical abstractions like polynomials (or in rank-1 constraint systems\nmatrices and vectors) that can hold a lot of data. There are three major\nfamilies of cryptographic technologies that allow us to represent these\nabstractions succinctly: Merkle trees (for FRI), regular elliptic\ncurves (for inner\nproduct arguments (IPAs)), and elliptic curves with pairings and\ntrusted setups (for KZG\ncommitments). These three technologies lead to the three types of\nproofs: FRI leads to STARKs, KZG commitments lead to \"regular\" SNARKs,\nand IPA-based schemes lead to bulletproofs. These three technologies\nhave very distinct tradeoffs:\n\nTechnology\nCryptographic assumptions\nProof size\nVerification time\n\nFRI\nHashes only (quantum safe!)\nLarge (10-200 kB)\nMedium (poly-logarithmic)\n\nInner product arguments (IPAs)\nBasic elliptic curves\nMedium (1-3 kB)\nVery high (linear)\n\nKZG commitments\nElliptic curves + pairings + trusted setup\nShort (~500 bytes)\nLow (constant)\n\nSo far, the first and the third have seen the most attention. The\nreason for this has to do with that pesky right column in the second row\nof the table: elliptic curve-based inner product arguments have linear\nverification time. What this means that even though the size of\na proof is small, the amount of time needed to verify the proof always\ntakes longer than just running the computation yourself. This makes IPAs\nnon-viable for scalability-related ZK-SNARK use cases: there's no point\nin using an IPA-based argument to prove the validity of an Ethereum\nblock, because verifying the proof will take longer than just checking\nthe block yourself. KZG and FRI-based proofs, on the other hand, really\nare much faster to verify than doing the computation yourself, so one of\nthose two seems like the obvious choice.\n\nMore recently, however, there has been a slew of research\ninto techniques for merging multiple IPA proofs into\none. Much of the initial work on this was done as part of\ndesigning the Halo\nprotocol which is going\ninto Zcash. These merging techniques are cheap, and a merged proof\ntakes no longer to verify than a single one of the proofs that it's\nmerging. This opens a way forward for IPAs to be useful: instead of\nverifying a size-\\(n\\) computation with\na proof that takes still takes \\(O(n)\\)\ntime to verify, break that computation up into smaller size-\\(k\\) steps, make \\(\\frac{n}{k}\\) proofs for each step, and\nmerge them together so the verifier's work goes down to a little more\nthan \\(O(k)\\). These techniques also\nallow us to do incremental verification: if new things keep\nbeing introduced that need to be proven, you can just keep taking the\nexisting proof, mixing it in with a proof of the new statement, and\ngetting a proof of the new combined statement out. This is really useful\nfor verifying the integrity of, say, an entire blockchain.\n\nSo how do these techniques work, and what can they do? That's exactly\nwhat this post is about.\nBackground: how\ndo inner product arguments work?\n\nInner product arguments are a proof scheme that can work over many\nmathematical structures, but usually we focus on IPAs over elliptic curve\npoints. IPAs can be made over simple elliptic curves, theoretically\neven Bitcoin and Ethereum's secp256k1 (though some\nspecial properties are preferred to make FFTs more efficient); no need\nfor insanely complicated pairing schemes that despite having written an\nexplainer article\nand an implementation\nI can still barely understand myself.\n\nWe'll start off with the commitment scheme, typically called\nPedersen vector commitments. To be able to commit to\ndegree \\(< n\\) polynomials, we first\npublicly choose a set of base points, \\(G_0\n... G_{n-1}\\). These points can be generated through a\npseudo-random procedure that can be re-executed by anyone (eg. the x\ncoordinate of \\(G_i\\) can be \\(hash(i, j)\\) for the lowest integer \\(j \\ge 0\\) that produces a valid point);\nthis is not a trusted setup as it does not rely on any specific\nparty to introduce secret information.\n\nTo commit to a polynomial \\(P(x) = \\sum_i\nc_i x^i\\), the prover computes \\(com(P)\n= \\sum_i c_i G_i\\). For example, \\(com(x^2 + 4)\\) would equal \\(G_2 + 4 * G_0\\) (remember, the \\(+\\) and \\(*\\) here are elliptic\ncurve addition and multiplication). Cryptographers will also often\nadd an extra \\(r \\cdot H\\) hiding\nparameter for privacy, but for simplicity of exposition we'll ignore\nprivacy for now; in general, it's not that hard to add privacy into all\nof these schemes.\n\nThough it's not really mathematically accurate to think of\nelliptic curve points as being like real numbers that have sizes, area\nis nevertheless a good intuition for thinking about linear combinations\nof elliptic curve points like we use in these commitments. The blue area\nhere is the value of the Pedersen commitment \\(C = \\sum_i c_i G_i\\) to the polynomial\n\\(P = \\sum_i c_i x^i\\).\n\nNow, let's get into how the proof works. Our final goal will\nbe a polynomial evaluation proof: given some \\(z\\), we want to make a proof that \\(P(z) = a\\), where this proof can be\nverified by anyone who has the commitment \\(C\n= com(P)\\). But first, we'll focus on a simpler task:\nproving that \\(C\\) is a valid\ncommitment to any polynomial at all - that is, proving\nthat \\(C\\) was constructed by taking a\nlinear combination \\(\\sum_i c_i G_i\\)\nof the points \\(\\{G_0 ... G_{n-1}\\}\\),\nwithout anything else mixed in.\n\nOf course, technically any point is some multiple of \\(G_0\\) and so it's theoretically a valid\ncommitment of something, but what we care about is proving that the\nprover knows some \\(\\{c_0 ...\nc_{n-1}\\}\\) such that \\(\\sum_i c_i G_i\n= C\\). A commitment \\(C\\) cannot\ncommit to multiple distinct polynomials that the prover knows\nabout, because if it could, that would imply that elliptic curves\nare broken.\n\nThe prover could, of course, just provide \\(\\{c_0 ... c_{n-1}\\}\\) directly and let the\nverifier check the commitment. But this takes too much space. So\ninstead, we try to reduce the problem to a smaller problem of half the\nsize. The prover provides two points, \\(L\\) and \\(R\\), representing the yellow and green\nareas in this diagram:\n\nYou may be able to see where this is going: if you add \\(C + L + R\\) together (remember: \\(C\\) was the original commitment, so the\nblue area), the new combined point can be expressed as a sum of\nfour squares instead of eight. And so now, the prover could\nfinish by providing only four sums, the widths of each of the new\nsquares. Repeat this protocol two more times, and we're down to a single\nfull square, which the prover can prove by sending a single value\nrepresenting its width.\n\nBut there's a problem: if \\(C\\) is\nincorrect in some way (eg. the prover added some extra point \\(H\\) into it), then the prover could just\nsubtract \\(H\\) from \\(L\\) or \\(R\\) to compensate for it. We plug this hole\nby randomly scaling our points after the prover provides \\(L\\) and \\(R\\):\n\nChoose a random factor \\(\\alpha\\)\n(typically, we set \\(\\alpha\\) to be the\nhash of all data added to the proof so far, including the \\(L\\) and \\(R\\), to ensure the verifier can also\ncompute \\(\\alpha\\)). Every even \\(G_i\\) point gets scaled by \\(\\alpha\\), every odd \\(G_i\\) point gets scaled down by\nthe same factor. Every odd coefficient gets scaled up by \\(\\alpha\\) (notice the flip), and every even\ncoefficient gets scaled down by \\(\\alpha\\). Now, notice that:\n\n- The yellow area (\\(L\\)) gets\nmultiplied by \\(\\alpha^2\\) (because\nevery yellow square is scaled up by \\(\\alpha\\) on both dimensions)\n\n- The green area (\\(R\\)) gets divided\nby \\(\\alpha^2\\) (because every green\nsquare is scaled down by \\(\\alpha\\) on\nboth dimensions)\n\n- The blue area (\\(C\\)) remains\nunchanged (because its width is scaled up but its height is scaled\ndown)\n\nHence, we can generate our new half-size instance of the problem with\nsome simple transformations:\n\n- \\(G'_{i} = \\alpha G_{2i} +\n\\frac{G_{2i+1}}{\\alpha}\\)\n\n- \\(c'_{i} = \\frac{c_{2i}}{\\alpha} +\n\\alpha c_{2i+1}\\)\n\n- \\(C' = C + \\alpha^2 L +\n\\frac{R}{\\alpha^2}\\)\n\n(Note: in some implementations you instead do \\(G'_i = \\alpha G_{2i} + G_{2i+1}\\) and\n\\(c'_i = c_{2i} + \\alpha c_{2i+1}\\)\nwithout dividing the odd points by \\(\\alpha\\). This makes the equation \\(C' = \\alpha C + \\alpha^2 L + R\\), which\nis less symmetric, but ensures that the function to compute any \\(G'\\) in any round of the protocol\nbecomes a polynomial without any division. Yet another\nalternative is to do \\(G'_i = \\alpha\nG_{2i} + G_{2i+1}\\) and \\(c'_i =\nc_{2i} + \\frac{c_{2i+1}}{\\alpha}\\), which avoids any \\(\\alpha^2\\) terms.)\n\nAnd then we repeat the process until we get down to one point:\n\nFinally, we have a size-1 problem: prove that the final modified\n\\(C^*\\) (in this diagram it's \\(C'''\\) because we had to do\nthree iterations, but it's \\(log(n)\\)\niterations generally) equals the final modified \\(G^*_0\\) and \\(c^*_0\\). Here, the prover just provides\n\\(c^*_0\\) in the clear, and the\nverifier checks \\(c^*_0 G^*_0 = C^*\\).\nComputing \\(c^*_0\\) required being able\nto compute a linear combination of \\(\\{c_0 ...\nc_{n-1}\\}\\) that was not known ahead of time, so providing it and\nverifying it convinces the verifier that the prover actually does know\nall the coefficients that go into the commitment. This concludes the\nproof.\n\n## Recapping:\n\n- The statement we are proving is that \\(C\\) is a commitment to some\npolynomial \\(P(x) = \\sum_i c_i x^i\\)\ncommitted to using the agreed-upon base points \\(\\{G_0 ... G_{n-1}\\}\\)\n\n- The proof consists of \\(log(n)\\)\npairs of \\((L, R)\\) values,\nrepresenting the yellow and green areas at each step. The prover also\nprovides the final \\(c^*_0\\)\n\n- The verifier walks through the proof, generating the \\(\\alpha\\) value at each step using the same\nalgorithm as the prover and computing the new \\(C'\\) and \\(G'_i\\) values (the verifier doesn't\nknow the \\(c_i\\) values so they can't\ncompute any \\(c'_i\\) values)\n\n- At the end, they check whether or not \\(c^*_0 G^*_0 = C^*\\)\n\nOn the whole, the proof contains \\(2 *\nlog(n)\\) elliptic curve points and one number (for pedants: one\nfield element). Verifying the proof takes logarithmic time in\nevery step except one: computing the new \\(G'_i\\) values. This step is,\nunfortunately, linear.\n\nSee also: Dankrad\nFeist's more detailed explanation of inner product\narguments.\nExtension to polynomial\nevaluations\n\nWe can extend to polynomial evaluations with a simple clever trick.\nSuppose we are trying to prove \\(P(z) =\na\\). The prover and the verifier can extend the base points \\(G_0 ... G_{n-1}\\) by attaching powers of\n\\(z\\) to them: the new base points\nbecome \\((G_0, 1), (G_1, z) ... (G_{n-1},\nz^{n-1})\\). These pairs can be treated as mathematical objects\n(for pedants: group elements) much like elliptic curve points\nthemselves; to add them you do so element-by-element: \\((A, x) + (B, y) =\\) \\((A + B,\\ x + y)\\), using elliptic curve\naddition for the points and regular field addition for the numbers.\n\nWe can make a Pedersen commitment using this extended base!\n\nNow, here's a puzzle. Suppose \\(P(x) =\n\\sum_i c_i x^i\\), where \\(P(z) =\na\\), would have a commitment \\(C =\n\\sum_i c_i G_i\\) if we were to use the regular elliptic curve\npoints we used before as a base. If we use the pairs \\((G_i, z^i)\\) as a base instead, the\ncommitment would be \\((C, y)\\) for some\n\\(y\\). What must be the value of \\(y\\)?\n\nThe answer is: it must be equal to \\(a\\)! This is easy to see: the commitment is\n\\((C, y) = \\sum_i c_i (G_i, z^i)\\),\nwhich we can decompose as \\((\\sum_i c_i G_i,\\\n\\sum_i c_i z^i)\\). The former is equal to \\(C\\), and the latter is just the evaluation\n\\(P(z)\\)!\n\nHence, if \\(C\\) is a \"regular\"\ncommitment to \\(P\\) using \\(\\{G_0 ... G_{n-1}\\}\\) as a base, then to\nprove that \\(P(z) = a\\) we need only\nuse the same protocol above, but proving that \\((C, a)\\) is a valid commitment using \\((G_0, 1), (G_1, z) ... (G_{n-1}, z^{n-1})\\)\nas a base!\n\nNote that in practice, this is usually done slightly differently as\nan optimization: instead of attaching the numbers to the points and\nexplicitly dealing with structures of the form \\((G_i, z^i)\\), we add another randomly\nchosen base point \\(H\\) and express it\nas \\(G_i + z^i H\\). This saves\nspace.\n\nSee here\nfor an example implementation of this whole protocol.\nSo, how do we combine these\nproofs?\n\nSuppose that you are given two polynomial evaluation proofs, with\ndifferent polynomials and different evaluation points, and want to make\na proof that they are both correct. You have:\n\n- Proof \\(\\Pi_1\\) proving that \\(P_1(z_1) = y_1\\), where \\(P_1\\) is represented by \\(com(P_1) = C_1\\)\n\n- Proof \\(\\Pi_2\\) proving that \\(P_2(z_2) = y_2\\), where \\(P_2\\) is represented by \\(com(P_2) = C_2\\)\n\nVerifying each proof takes linear time. We want to make a proof that\nproves that both proofs are correct. This will still take linear time,\nbut the verifier will only have to make one round of linear\ntime verification instead of two.\n\nWe start off with an observation. The only linear-time step in\nperforming the verification of the proofs is computing the \\(G'_i\\) values. This is \\(O(n)\\) work because you have to combine\n\\(\\frac{n}{2}\\) pairs of \\(G_i\\) values into \\(G'_i\\) values, then \\(\\frac{n}{4}\\) pairs of \\(G'_i\\) values into \\(G''_i\\) values, and so on, for a\ntotal of \\(n\\) combinations of pairs.\nBut if you look at the algorithm carefully, you will notice that we\ndon't actually need any of the intermediate \\(G'_i\\) values; we only need the final\n\\(G^*_0\\). This \\(G^*_0\\) is a linear combination of the\ninitial \\(G_i\\) values. What are the\ncoefficients to that linear combination? It turns out that the \\(G_i\\) coefficient is the \\(X^i\\) term of this polynomial:\n\n\\[(X + \\alpha_1) * (X^2 + \\alpha_2)\\ *\\\n...\\ *\\ (X^{\\frac{n}{2}} + \\alpha_{log(n)}) \\]\n\nThis is using the \\(C' = \\alpha C +\n\\alpha^2 L + R\\) version we mentioned above. The ability to\ndirectly compute \\(G^*_0\\) as a linear\ncombination already cuts down our work to \\(O(\\frac{n}{log(n)})\\) due to fast\nlinear combination algorithms, but we can go further.\n\nThe above polynomial has degree \\(n -\n1\\), with \\(n\\) nonzero\ncoefficients. But its un-expanded form has size \\(log(n)\\), and so you can evaluate\nthe polynomial at any point in \\(O(log(n))\\) time. Additionally, you might\nnotice that \\(G^*_0\\) is a commitment\nto this polynomial, so we can directly prove evaluations! So\nhere is what we do:\n\n- The prover computes the above polynomial for each proof; we'll call\nthese polynomials \\(K_1\\) with \\(com(K_1) = D_1\\) and \\(K_2\\) with \\(com(K_2) = D_2\\). In a \"normal\"\nverification, the verifier would be computing \\(D_1\\) and \\(D_2\\) themselves as these are just the\n\\(G^*_0\\) values for their respective\nproofs. Here, the prover provides \\(D_1\\) and \\(D_2\\) and the rest of the work is proving\nthat they're correct.\n\n- To prove the correctness of \\(D_1\\)\nand \\(D_2\\) we'll prove that they're\ncorrect at a random point. We choose a random point \\(t\\), and evaluate both \\(e_1 = K_1(t)\\) and \\(e_2 = K_2(t)\\)\n\n- The prover generates a random linear combination \\(L(x) = K_1(x) + rK_2(x)\\) (and the verifier\ncan generate \\(com(L) = D_1 + rD_2\\)).\nThe prover now just needs to make a single proof that \\(L(t) = e_1 + re_2\\).\n\nThe verifier still needs to do a bunch of extra steps, but all of\nthose steps take either \\(O(1)\\) or\n\\(O(log(n))\\) work: evaluate \\(e_1 = K_1(t)\\) and \\(e_2 = K_2(t)\\), calculate the \\(\\alpha_i\\) coefficients of both \\(K_i\\) polynomials in the first place, do\nthe elliptic curve addition \\(com(L) = D_1 +\nrD_2\\). But this all takes vastly less than linear time, so all\nin all we still benefit: the verifier only needs to do the linear-time\nstep of computing a \\(G^*_0\\) point\nthemselves once.\n\nThis technique can easily be generalized to merge \\(m > 2\\) signatures.\nFrom merging\nIPAs to merging IPA-based SNARKs: Halo\n\nNow, we get into the core mechanic of the Halo protocol being\nintegrated in Zcash, which uses this proof combining technique to create\na recursive proof system. The setup is simple: suppose you have a chain,\nwhere each block has an associated IPA-based SNARK (see here for how generic SNARKs\nfrom polynomial commitments work) proving its correctness. You want to\ncreate a new block, building on top of the previous tip of the chain.\nThe new block should have its own IPA-based SNARK proving the\ncorrectness of the block. In fact, this proof should cover both the\ncorrectness of the new block and the correctness of the\nprevious block's proof of the correctness of the entire chain before\nit.\n\nIPA-based proofs by themselves cannot do this, because a proof of a\nstatement takes longer to verify than checking the statement itself, so\na proof of a proof will take even longer to verify than both proofs\nseparately. But proof merging can do it!\n\nEssentially, we use the usual \"recursive SNARK\" technique to verify\nthe proofs, except the \"proof of a proof\" part is only proving the\nlogarithmic part of the work. We add an extra chain of aggregate proofs,\nusing a trick similar to the proof merging scheme above, to handle the\nlinear part of the work. To verify the whole chain, the verifier need\nonly verify one linear-time proof at the very tip of the chain.\n\nThe precise details are somewhat different from the exact\nproof-combining trick in the previous section for efficiency reasons.\nInstead of using the proof-combining trick to combine multiple proofs,\nwe use it on a single proof, just to re-randomize the point\nthat the polynomial committed to by \\(G^*_0\\) needs to be evaluated at. We then\nuse the same newly chosen evaluation point to evaluate the\npolynomials in the proof of the block's correctness, which allows us to\nprove the polynomial evaluations together in a single IPA.\n\nExpressed in math:\n\n- Let \\(P(z) = a\\) be the previous\nstatement that needs to be proven\n\n- The prover generates \\(G^*_0\\)\n\n- The prover proves the correctness of the new block plus the\nlogarithmic work in the previous statements by generating a PLONK\nproof: \\(Q_L * A + Q_R * B + Q_O * C + Q_M\n* A * B + Q_C = Z * H\\)\n\n- The prover chooses a random point \\(t\\), and proves the evaluation of a linear\ncombination of \\(\\{G^*_0,\\ Q_L,\\ A,\\ Q_R,\\ B,\\\nQ_O,\\ C,\\ Q_M,\\ Q_C,\\ Z,\\ H\\}\\) at \\(t\\). We can then check the above equation,\nreplacing each polynomial with its now-verified evaluation at \\(t\\), to verify the PLONK proof.\n\nIncremental\nverification, more generally\n\nThe size of each \"step\" does not need to be a full block\nverification; it could be something as small as a single step of a\nvirtual machine. The smaller the steps the better: it ensures that the\nlinear work that the verifier ultimately has to do at the end is less.\nThe only lower bound is that each step has to be big enough to contain a\nSNARK verifying the \\(log(n)\\) portion\nof the work of a step.\n\nBut regardless of the fine details, this mechanism allows us to make\nsuccinct and easy-to-verify SNARKs, including easy support for recursive\nproofs that allow you to extend proofs in real time as the computation\nextends and even have different provers to do different parts of the\nproving work, all without pairings or a trusted setup! The main downside\nis some extra technical complexity, compared with a \"simple\"\npolynomial-based proof using eg. KZG-based commitments.\n\nTechnology\nCryptographic assumptions\nProof size\nVerification time\n\nFRI\nHashes only (quantum safe!)\nLarge (10-200 kB)\nMedium (poly-logarithmic)\n\nInner product arguments (IPAs)\nBasic elliptic curves\nMedium (1-3 kB)\nVery high (linear)\n\nKZG commitments\nElliptic curves + pairings + trusted setup\nShort (~500 bytes)\nLow (constant)\n\nIPA + Halo-style aggregation\nBasic elliptic curves\nMedium (1-3 kB)\nMedium (constant but higher than KZG)\n\nNot just polynomials!\nMerging R1CS proofs\n\nA common alternative to building SNARKs out of polynomial games is\nbuilding SNARKs out of matrix-vector multiplication games. Polynomials\nand vectors+matrices are both natural bases for SNARK protocols because\nthey are mathematical abstractions that can store and compute over large\namounts of data at the same time, and that admit commitment schemes that\nallow verifiers to check equations quickly.\n\nIn R1CS (see a more detailed description here), an instance of the game\nconsists of three matrices \\(A\\), \\(B\\), \\(C\\), and a solution is a vector \\(Z\\) such that \\((A \\cdot Z) \\circ (B \\cdot Z) = C \\cdot Z\\)\n(the problem is often in practice restricted further by requiring the\nprover to make part of \\(Z\\) public and\nrequiring the last entry of \\(Z\\) to be\n1).\n\nAn R1CS instance with a single constraint (so \\(A\\), \\(B\\)\nand \\(C\\) have width 1), with a\nsatisfying \\(Z\\) vector, though notice\nthat here the \\(Z\\) appears on the left\nand has 1 in the top position instead of the bottom.\n\nJust like with polynomial-based SNARKs, this R1CS game can be turned\ninto a proof scheme by creating commitments to \\(A\\), \\(B\\)\nand \\(C\\), requiring the prover to\nprovide a commitment to (the private portion of) \\(Z\\), and using fancy proving tricks to\nprove the equation \\((A \\cdot Z) \\circ (B\n\\cdot Z) = C \\cdot Z\\), where \\(\\circ\\) is item-by-item multiplication,\nwithout fully revealing any of these objects. And just like with IPAs,\nthis R1CS game has a proof merging scheme!\n\nIoanna Tzialla et al describe such a scheme in a recent paper (see page\n8-9 for their description). They first modify the game by introducing an\nexpanded equation:\n\n\\[ (A \\cdot Z) \\circ (B \\cdot Z) - u * (C\n\\cdot Z) = E\\]\n\nFor a \"base\" instance, \\(u = 1\\) and\n\\(E = 0\\), so we get back the original\nR1CS equation. The extra slack variables are added to make aggregation\npossible; aggregated instances will have other values of \\(u\\) and \\(E\\). Now, suppose that you have two\nsolutions to the same instance, though with different \\(u\\) and \\(E\\) variables:\n\n\\[(A \\cdot Z_1) \\circ (B \\cdot Z_1) - u_1\n* (C \\cdot Z_1) = E_1\\]\n\n\\[(A \\cdot Z_2) \\circ (B \\cdot Z_2) - u_2\n* (C \\cdot Z_2) = E_2\\]\n\nThe trick involves taking a random linear combination \\(Z_3 = Z_1 + r Z_2\\), and making the\nequation work with this new value. First, let's evaluate the left\nside:\n\n\\[ (A \\cdot (Z_1 + rZ_2)) \\circ (B \\cdot\n(Z_1 + rZ_2)) - (u_1 + ru_2)*(C \\cdot (Z_1 + rZ_2)) \\]\n\nThis expands into the following (grouping the \\(1\\), \\(r\\)\nand \\(r^2\\) terms together):\n\n\\[(A \\cdot Z_1) \\circ (B \\cdot Z_1) - u_1\n* (C \\cdot Z_1)\\]\n\n\\[r((A \\cdot Z_1) \\circ (B \\cdot Z_2) + (A\n\\cdot Z_2) \\circ (B \\cdot Z_1) - u_1 * (C \\cdot Z_2) - u_2 * (C \\cdot\nZ_1))\\]\n\n\\[r^2((A \\cdot Z_2) \\circ (B \\cdot Z_2) -\nu_2 * (C \\cdot Z_2))\\]\n\nThe first term is just \\(E_1\\); the\nthird term is \\(r^2 * E_2\\). The middle\nterm is very similar to the cross-term (the yellow + green areas) near\nthe very start of this post. The prover simply provides the middle term\n(without the \\(r\\) factor), and just\nlike in the IPA proof, the randomization forces the prover to be\nhonest.\n\nHence, it's possible to make merging schemes for R1CS-based protocols\ntoo. Interestingly enough, we don't even technically need to have a\n\"succinct\" protocol for proving the \\[ (A\n\\cdot Z) \\circ (B \\cdot Z) = u * (C \\cdot Z) + E\\] relation at\nthe end; instead, the prover could just prove by opening all the\ncommitments directly! This would still be \"succinct\" because the\nverifier would only need to verify one proof that actually represents an\narbitrarily large number of statements. However, in practice having a\nsuccinct protocol for this last step is better because it keeps the\nproofs smaller, and Tzialla et al's paper\nprovides such a protocol too (see page 10).\n\n## Recap\n\n- We don't know of a way to make a commitment to a size-\\(n\\) polynomial where evaluations of the\npolynomial can be verified in \\(<\nO(n)\\) time directly. The best that we can do is make a \\(log(n)\\) sized proof, where all of the work\nto verify it is logarithmic except for one final \\(O(n)\\)-time piece.\n\n- But what we can do is merge multiple proofs together. Given\n\\(m\\) proofs of evaluations of\nsize-\\(n\\) polynomials, you can make a\nproof that covers all of these evaluations, that takes\nlogarithmic work plus a single size-\\(n\\) polynomial proof to verify.\n\n- With some clever trickery, separating out the logarithmic parts from\nthe linear parts of proof verification, we can leverage this to make\nrecursive SNARKs.\n\n- These recursive SNARKs are actually more efficient than doing\nrecursive SNARKs \"directly\"! In fact, even in contexts where direct\nrecursive SNARKs are possible (eg. proofs with KZG commitments),\nHalo-style techniques are typically used instead because they are more\nefficient.\n\n- It's not just about polynomials; other games used in SNARKs like\nR1CS can also be aggregated in similar clever ways.\n\n- No pairings or trusted setups required!\n\nThe march toward faster and more efficient and safer ZK-SNARKs just\nkeeps going...",
    "contentLength": 25318,
    "summary": "Halo protocol enables efficient incremental verification by merging multiple inner product argument proofs to reduce linear verification time.",
    "detailedSummary": {
      "theme": "Vitalik explores how inner product arguments (IPAs) and Halo-style proof aggregation techniques can create efficient recursive SNARKs without requiring pairings or trusted setups, making them viable alternatives to traditional SNARK constructions.",
      "summary": "Vitalik examines three main approaches to ZK-SNARKs: FRI-based STARKs, KZG commitment-based SNARKs, and inner product argument (IPA)-based proofs like bulletproofs. While IPAs historically suffered from linear verification time that made them impractical for scalability, Vitalik explains how recent innovations like the Halo protocol solve this through proof merging techniques. The post provides detailed technical explanations of how IPAs work using Pedersen vector commitments and elliptic curve operations, demonstrating how multiple proofs can be combined to reduce verification overhead from multiple linear operations to a single linear step plus logarithmic work. Vitalik shows how these techniques enable incremental verification and recursive SNARKs that can prove the validity of entire blockchains efficiently, extending beyond polynomial-based systems to also cover R1CS (Rank-1 Constraint System) proofs, all without requiring the complex pairing schemes or trusted setups needed by KZG-based approaches.",
      "takeaways": [
        "Inner product arguments (IPAs) can now achieve practical efficiency through proof merging techniques that combine multiple proofs into one, reducing verification overhead significantly",
        "The Halo protocol enables recursive SNARKs using only basic elliptic curves, avoiding the need for complex pairings or trusted setups while supporting incremental verification",
        "IPA-based proofs work by using Pedersen vector commitments and a clever halving protocol that reduces polynomial evaluation proofs to logarithmic size with one linear verification step",
        "Proof aggregation techniques extend beyond polynomial-based systems to R1CS (matrix-vector) based SNARKs, showing the broad applicability of these merging approaches",
        "Halo-style aggregation techniques are often more efficient than direct recursive SNARKs even in contexts where alternatives like KZG commitments are available"
      ],
      "controversial": [
        "The claim that Halo-style techniques are 'typically used instead' of direct recursive SNARKs even when KZG commitments are available may not reflect all practical implementations and use cases",
        "The assertion that proof merging makes IPAs viable for scalability applications may be debated given that they still require at least one linear verification step per aggregate proof"
      ]
    }
  },
  {
    "id": "general-2021-10-31-cities",
    "title": "Crypto Cities",
    "date": "2021-10-31",
    "category": "applications",
    "url": "https://vitalik.eth.limo/general/2021/10/31/cities.html",
    "path": "general/2021/10/31/cities.html",
    "content": "- \n\n- \n\n  \n    \n\n  \n  \n  Dark Mode Toggle\n  \n    \n\n  \n\n##  Crypto Cities \n\n 2021 Oct 31 \nSee all posts\n\n \n \n\n Crypto Cities \n\nSpecial thanks to Mr Silly and Tina Zhen for early feedback on\nthe post, and to a big long list of people for discussion of the\nideas.\n\nOne interesting trend of the last year has been the growth of\ninterest in local government, and in the idea of local governments that\nhave wider variance and do more experimentation. Over the past year,\nMiami mayor Francis Suarez has pursued a Twitter-heavy\ntech-startup-like strategy of attracting interest in the city,\nfrequently engaging\nwith the mainstream tech\nindustry and crypto\ncommunity on Twitter. Wyoming now has a DAO-friendly\nlegal structure, Colorado is experimenting\nwith quadratic voting, and we're seeing more and more experiments\nmaking more pedestrian-friendly\nstreet environments for the offline world. We're even seeing\nprojects with varying degrees of radicalness - Cul de sac, Telosa, CityDAO, Nkwashi, Prospera and many more - trying to\ncreate entire neighborhoods and cities from scratch.\n\nAnother interesting trend of the last year has been the rapid\nmainstreaming of crypto ideas such as coins, non-fungible tokens and\ndecentralized autonomous organizations (DAOs). So what would happen if\nwe combine the two trends together? Does it make sense to have a city\nwith a coin, an NFT, a DAO, some record-keeping on-chain for\nanti-corruption, or even all four? As it turns out, there are already\npeople trying to do just that:\n\n- CityCoins.co, a project\nthat sets up coins intended to become local media of exchange, where a\nportion of the issuance of the coin goes to the city government. MiamiCoin already exists,\nand \"San Francisco Coin\" appears to be coming soon.\n\n- Other experiments with coin issuance (eg. see this\nproject in Seoul)\n\n- Experiments with NFTs, often as a way of funding\nlocal artists. Busan\nis hosting a government-backed conference exploring what they could do\nwith NFTs.\n\n- Reno mayor Hillary Schieve's expansive\nvision for blockchainifying the\ncity, including NFT\nsales to support local art, a RenoDAO with RenoCoins issued to local\nresidents that could get revenue from the government renting out\nproperties, blockchain-secured lotteries, blockchain voting and\nmore.\n\n- Much more ambitious projects creating crypto-oriented cities\nfrom scratch: see CityDAO, which describes itself as,\nwell, \"building a city on the Ethereum blockchain\" - DAOified governance\nand all.\n\nBut are these projects, in their current form, good ideas? Are there\nany changes that could make them into better ideas? Let us find\nout...\nWhy should we care about\ncities?\n\nMany national governments around the world are showing themselves to\nbe inefficient and slow-moving in response to long-running problems and\nrapid changes in people's underlying needs. In short, many national\ngovernments are missing live\nplayers. Even worse, many of the outside-the-box political ideas\nthat are being considered or implemented for national\ngovernance today are honestly quite terrifying. Do you want the\nUSA to be taken over\nby a clone of WW2-era Portuguese dictator Antonio Salazar, or\nperhaps an \"American\nCaesar\", to beat down the evil scourge of American leftism? For\nevery idea that can be reasonably described as freedom-expanding or\ndemocratic, there are ten that are just different forms of centralized\ncontrol and walls and universal surveillance.\n\nNow consider local governments. Cities and states, as we've\nseen from the examples at the start of this post, are at least in theory\ncapable of genuine dynamism. There are large and very real\ndifferences of culture between cities, so it's easier to find a single\ncity where there is public interest in adopting any particular radical\nidea than it is to convince an entire country to accept it. There are\nvery real challenges and opportunities in local public goods, urban\nplanning, transportation and many other sectors in the governance of\ncities that could be addressed. Cities have tightly cohesive internal\neconomies where things like widespread cryptocurrency adoption could\nrealistically independently happen. Furthermore, it's less likely that\nexperiments within cities will lead to terrible outcomes both because\ncities are regulated by higher-level governments and because cities have\nan easier escape valve: people who are unhappy with what's going on can\nmore easily exit.\n\nSo all in all, it seems like the local level of government is a very\nundervalued one. And given that criticism\nof existing smart\ncity initiatives often heavily focuses on concerns around\ncentralized governance, lack of transparency and data\nprivacy, blockchain and cryptographic technologies seem like a\npromising key ingredient for a more open and participatory way\nforward.\nWhat are city projects up to\ntoday?\n\nQuite a lot actually! Each of these experiments is still small scale\nand largely still trying to find its way around, but they are all at\nleast seeds that could turn into interesting things. Many of the most\nadvanced projects are in the United States, but there is interest across\nthe world; over in Korea the government of Busan is running\nan NFT conference. Here are a few examples of what is being done\ntoday.\nBlockchain experiments in\nReno\n\nReno, Nevada mayor Hillary Schieve\nis a blockchain fan, focusing primarily on the Tezos ecosystem, and she\nhas recently been exploring blockchain-related ideas (see her podcast here) in the\ngovernance of her city:\n\n- Selling NFTs to fund local art, starting with an\nNFT of the\n\"Space Whale\" sculpture in the middle of the city\n\n- Creating a Reno DAO, governed by Reno coins that\nReno residents would be eligible to receive via an airdrop. The Reno DAO\ncould start to get sources of revenue; one proposed idea was the city\nrenting out properties that it owns and the revenue going into a\nDAO\n\n- Using blockchains to secure all kinds of processes:\nblockchain-secured random number generators for casinos,\nblockchain-secured voting, etc.\n\nReno space whale. Source\nhere.\n\n## CityCoins.co\n\nCityCoins.co is a project built on Stacks, a blockchain run by an\nunusual \"proof of transfer\" (for some reason abbreviated PoX and not\nPoT) block\nproduction algorithm that is built around the Bitcoin blockchain and\necosystem. 70% of the coin's supply is generated by an ongoing sale\nmechanism: anyone with STX (the Stacks native token) can send their STX\nto the city coin contract to generate city coins; the STX revenues are\ndistributed to existing city coin holders who stake their\ncoins. The remaining 30% is made available to the city government.\n\nCityCoins has made the interesting decision of trying to make\nan economic model that does not depend on any government\nsupport. The local government does not need to be involved in\ncreating a CityCoins.co coin; a community group can launch a coin by\nthemselves. An FAQ-provided answer to\n\"What can I do with CityCoins?\" includes examples like \"CityCoins\ncommunities will create apps that use tokens for rewards\" and \"local\nbusinesses can provide discounts or benefits to people who ... stack their\nCityCoins\". In practice, however, the MiamiCoin community is not going\nat it alone; the Miami government has already de-facto\npublicly endorsed it.\n\nMiamiCoin\nhackathon winner: a site that allows coworking spaces to give\npreferential offers to MiamiCoin holders.\n\n## CityDAO\n\nCityDAO is the most radical of the experiments: Unlike Miami and\nReno, which are existing cities with existing infrastructure to be\nupgraded and people to be convinced, CityDAO a DAO with legal status\nunder the Wyoming DAO\nlaw (see their docs here)\ntrying to create entirely new cities from scratch.\n\nSo far, the project is still in its early stages. The team is\ncurrently finalizing a\npurchase of their first plot of land in a far-off\ncorner of Wyoming. The plan is to start with this plot of land, and\nthen add other plots of land in the future, to build cities, governed by\na DAO and making heavy use of radical\neconomic ideas like Harberger taxes to allocate the land, make\ncollective decisions and manage resources. Their DAO is one of the\nprogressive few that is avoiding coin voting\ngovernance; instead, the governance is a voting scheme based on\n\"citizen\" NFTs, and ideas have been floated to further limit votes to\none-per-person by using proof-of-humanity\nverification. The NFTs are currently being sold to crowdfund the\nproject; you can buy them on OpenSea.\n\nWhat do I think cities\ncould be up to?\n\nObviously there are a lot of things that cities could do in\nprinciple. They could add more bike lanes, they could use CO2\nmeters and far-UVC\nlight to more effectively reduce COVID spread without\ninconveniencing people, and they could even fund life extension\nresearch. But my primary specialty is blockchains and this post is about\nblockchains, so... let's focus on blockchains.\n\nI would argue that there are two distinct categories of blockchain\nideas that make sense:\n\n- Using blockchains to create more trusted, transparent and\nverifiable versions of existing processes.\n\n- Using blockchains to implement new and experimental forms of\nownership for land and other scarce assets, as well as\nnew and experimental forms of democratic\ngovernance.\n\nThere's a natural fit between blockchains and both of these\ncategories. Anything happening on a blockchain is very easy to publicly\nverify, with lots of ready-made freely available tools to help people do\nthat. Any application built on a blockchain can immediately plug in to\nand interface with other applications in the entire global blockchain\necosystem. Blockchain-based systems are efficient in a way that paper is\nnot, and publicly verifiable in a way that centralized computing systems\nare not - a necessary combination if you want to, say, make a new form\nof voting that allows citizens to give high-volume real-time feedback on\nhundreds or thousands of different issues.\n\nSo let's get into the specifics.\nWhat\nare some existing processes that blockchains could make more trusted and\ntransparent?\n\nOne simple idea that plenty of people, including government officials\naround the world, have brought up to me on many occasions is the idea of\ngovernments creating a whitelisted internal-use-only stablecoin for\ntracking internal government payments. Every tax payment from an\nindividual or organization could be tied to a publicly visible on-chain\nrecord minting that number of coins (if we want individual tax payment\nquantities to be private, there are zero-knowledge ways to make\nonly the total public but still convince everyone that it was computed\ncorrectly). Transfers between departments could be done \"in the clear\",\nand the coins would be redeemed only by individual contractors or\nemployees claiming their payments and salaries.\n\nThis system could easily be extended. For example, procurement\nprocesses for choosing which bidder wins a government contract could\nlargely be done on-chain.\n\nMany more processes could be made more trustworthy with\nblockchains:\n\n- Fair random number generators (eg. for lotteries) -\nVDFs, such as\nthe one Ethereum is expected to include, could serve as a fair random\nnumber generator that could be used to make government-run lotteries\nmore trustworthy. Fair randomness could also be used for many other use\ncases, such as sortition as a form\nof government.\n\n- Certificates, for example cryptographic proofs that\nsome particular individual is a resident of the city, could be done\non-chain for added verifiability and security (eg. if such certificates\nare issued on-chain, it would become obvious if a large number of false\ncertificates are issued). This can be used by all kinds of\nlocal-government-issued certificates.\n\n- Asset registries, for land and other assets, as\nwell as more complicated forms of property ownership such as development\nrights. Due to the need for courts to be able to make assignments in\nexceptional situations, these registries will likely never be fully\ndecentralized bearer instruments in the same way that cryptocurrencies\nare, but putting records on-chain can still make it easier to see what\nhappened in what order in a dispute.\n\nEventually, even voting could be done on-chain.\nHere, many complexities and\ndragons loom and it's really important to be careful; a\nsophisticated solution combining blockchains, zero knowledge proofs and\nother cryptography is needed to achieve all the desired privacy and\nsecurity properties. However, if humanity is ever going to move to\nelectronic voting at all, local government seems like the perfect place\nto start.\nWhat\nare some radical economic and governance experiments that could be\ninteresting?\n\nBut in addition to these kinds of blockchain overlays onto things\nthat governments already do, we can also look at blockchains as\nan opportunity for governments to make completely new and\nradical experiments in economics and governance. These are not\nnecessarily final ideas on what I think should be done; they are more\ninitial explorations and suggestions for possible directions. Once an\nexperiment starts, real-world feedback is often by far the most useful\nvariable to determine how the experiment should be adjusted in the\nfuture.\nExperiment\n#1: a more comprehensive vision of city tokens\n\nCityCoins.co is one vision for how city tokens could work. But it is\nfar from the only vision. Indeed, the CityCoins.so approach has\nsignificant risks, particularly in how economic model is heavily tilted\ntoward early adopters. 70% of the STX revenue from minting new coins is\ngiven to existing stakers of the city coin. More coins will\nbe issued in the next five years than in the fifty years that\nfollow. It's a good deal for the government in 2021, but what about\n2051? Once a government endorses a particular city coin, it becomes\ndifficult for it to change directions in the future. Hence, it's\nimportant for city governments to think carefully about these issues,\nand choose a path that makes sense for the long term.\n\nHere is a different possible sketch of a narrative of how\ncity tokens might work. It's far from the only\npossible alternative to the CityCoins.co vision; see Steve Waldman's excellent article\narguing for a city-localized medium of exchange for yet another possible\ndirection. In any case, city tokens are a wide design space, and there\nare many different options worth considering. Anyway, here goes...\n\nThe concept of home ownership in its current form is a notable\ndouble-edged sword, and the specific ways in which it's actively\nencouraged and legally structured is considered by many to be one\nof the biggest economic policy mistakes that we are making today.\nThere is an inevitable political tension between a home as a\nplace to live and a home as an investment asset, and the\npressure to satisfy communities who care about the latter often ends up\nseverely harming the affordability of the former. A resident in a city\neither owns a home, making them massively over-exposed to land prices\nand introducing perverse incentives to fight against construction of new\nhomes, or they rent a home, making them negatively exposed to\nthe real estate market and thus putting them economically at odds with\nthe goal of making a city a nice place to live.\n\nBut even despite all of these problems, many still find home\nownership to be not just a good personal choice, but something worthy of\nactively subsidizing or socially encouraging. One big reason is that it\nnudges people to save money and build up their net worth. Another big\nreason is that despite its flaws, it creates economic alignment between\nresidents and the communities they live in. But what if we could\ngive people a way to save and create that economic alignment without the\nflaws? What if we could create a divisible and fungible city\ntoken, that residents could hold as many units of as they can afford or\nfeel comfortable with, and whose value goes up as the city prospers?\n\nFirst, let's start with some possible objectives. Not all are\nnecessary; a token that accomplishes only three of the five is already a\nbig step forward. But we'll try to hit as many of them as possible:\n\n- Get sustainable sources of revenue for the\ngovernment. The city token economic model should avoid\nredirecting existing tax revenue; instead, it should find\nnew sources of revenue.\n\n- Create economic alignment between residents and the\ncity. This means first of all that the coin itself should\nclearly become more valuable as the city becomes more attractive. But it\nalso means that the economics should actively encourage\nresidents to hold the coin more than faraway hedge funds.\n\n- Promote saving and wealth-building. Home ownership\ndoes this: as home owners make mortgage payments, they build up their\nnet worth by default. City tokens could do this too, making it\nattractive to accumulate coins over time, and even gamifying the\nexperience.\n\n- Encourage more pro-social activity, such as\npositive actions that help the city and more sustainable use of\nresources.\n\n- Be egalitarian. Don't unduly favor wealthy people\nover poor people (as badly designed economic mechanisms often do\naccidentally). A token's divisibility, avoiding a sharp binary divide\nbetween haves and have-nots, does a lot already, but we can go further,\neg. by allocating a large portion of new issuance to residents as a\nUBI.\n\nOne pattern that seems to easily meet the first three objectives is\nproviding benefits to holders: if you hold at least X coins (where X can\ngo up over time), you get some set of services for free. MiamiCoin is\ntrying to encourage businesses to do this, but we could go further and\nmake government services work this way too. One simple example\nwould be making existing public parking spaces only available for free\nto those who hold at least some number of coins in a locked-up form.\nThis would serve a few goals at the same time:\n\n- Create an incentive to hold the coin, sustaining\nits value.\n\n- Create an incentive specifically for residents to\nhold the coin, as opposed to otherwise-unaligned faraway\ninvestors. Furthermore, the incentive's usefulness is capped per-person,\nso it encourages widely distributed holdings.\n\n- Creates economic alignment (city becomes more\nattractive -> more people want to park -> coins have more value).\nUnlike home ownership, this creates alignment with an entire\ntown, and not merely a very specific location in a\ntown.\n\n- Encourage sustainable use of resources: it would\nreduce usage of parking spots (though people without coins who really\nneed them could still pay), supporting many local governments' desires\nto open up more space on the roads to be more pedestrian-friendly.\nAlternatively, restaurants could also be allowed to lock up coins\nthrough the same mechanism and claim parking spaces to use for outdoor\nseating.\n\nBut to avoid perverse incentives, it's extremely important to avoid\noverly depending on one specific idea and instead to have a diverse\narray of possible revenue sources. One excellent gold mine of\nplaces to give city tokens value, and at the same time experiment with\nnovel governance ideas, is zoning. If you hold at least Y\ncoins, then you can quadratically vote on the fee that nearby landowners\nhave to pay to bypass zoning restrictions. This hybrid market + direct\ndemocracy based approach would be much more efficient than current\noverly cumbersome permitting processes, and the fee itself would be\nanother source of government revenue. More generally, any of the ideas\nin the next section could be combined with city tokens to give city\ntoken holders more places to use them.\nExperiment\n#2: more radical and participatory forms of governance\n\nThis is where Radical Markets\nideas such as Harberger taxes, quadratic voting and quadratic\nfunding come in. I already brought up some of these ideas in the\nsection above, but you don't have to have a dedicated city token to do\nthem. Some limited government use of quadratic voting and funding has\nalready happened: see the Colorado\nDemocratic party and the Taiwanese\npresidential hackathon, as well as not-yet-government-backed\nexperiments like Gitcoin's Boulder\nDowntown Stimulus. But we could do more!\n\nOne obvious place where these ideas can have long-term value is\ngiving developers incentives to improve the aesthetics of\nbuildings that they are building (see here,\nhere,\nhere\nand here\nfor some recent examples of professional blabbers debating the\naesthetics of modern architecture). Harberger taxes and other mechanisms\ncould be used to radically reform zoning rules, and\nblockchains could be used to administer such mechanisms in a more\ntrustworthy and efficient way. Another idea that is more viable in the\nshort term is subsidizing local businesses, similar to\nthe Downtown Stimulus but on a larger and more permanent scale.\nBusinesses produce various kinds of positive externalities in their\nlocal communities all the time, and those externalities could be more\neffectively rewarded. Local news could be quadratically\nfunded, revitalizing a long-struggling industry. Pricing for\nadvertisements could be set based on real-time votes of\nhow much people enjoy looking at each particular ad, encouraging more\noriginality and creativity.\n\nMore democratic feedback (and possibly even retroactive\ndemocratic feedback!) could plausibly create better incentives in all of\nthese areas. And 21st-century digital democracy through\nreal-time online quadratic voting and funding could plausibly do a much\nbetter job than 20th-century democracy, which seems in practice to have\nbeen largely characterized by rigid building codes and obstruction at\nplanning and permitting hearings. And of course, if you're\ngoing to use blockchains to secure voting, starting off by doing it with\nfancy new kinds of votes seems far more safe and politically feasible\nthan re-fitting existing voting systems.\n\nMandatory solarpunk picture intended to evoke a positive image of\nwhat might happen to our cities if real-time quadratic votes could set\nsubsidies and prices for everything.\n\n## Conclusions\n\nThere are a lot of worthwhile ideas for cities to experiment with\nthat could be attempted by existing cities or by new cities. New cities\nof course have the advantage of not having existing residents with\nexisting expectations of how things should be done; but the concept of\ncreating a new city itself is, in modern times, relatively untested.\nPerhaps the multi-billion-dollar capital pools in the hands of people\nand projects enthusiastic to try new things could get us over the hump.\nBut even then, existing cities will likely continue to be the place\nwhere most people live for the foreseeable future, and existing cities\ncan use these ideas too.\n\nBlockchains can be very useful in both the more incremental\nand more radical ideas that were proposed here, even despite the\ninherently \"trusted\" nature of a city government. Running any\nnew or existing mechanism on-chain gives the public an easy ability to\nverify that everything is following the rules. Public chains are better:\nthe benefits from existing infrastructure for users to independently\nverify what is going on far outweigh the losses from transaction fees,\nwhich are expected to quickly decrease very soon from rollups and sharding. If strong privacy\nis required, blockchains can be combined zero knowledge cryptography\nto give privacy and security at the same time.\n\nThe main trap that governments should avoid is too quickly\nsacrificing optionality. An existing city could fall\ninto this trap by launching a bad city token instead of taking things\nmore slowly and launching a good one. A new city could fall\ninto this trap by selling off too much land, sacrificing the entire\nupside to a small group of early adopters. Starting with self-contained\nexperiments, and taking things slowly on moves that are truly\nirreversible, is ideal. But at the same time, it's also important to\nseize the opportunity in the first place. There's a lot that can and\nshould be improved with cities, and a lot of opportunities; despite the\nchallenges, crypto cities broadly are an idea whose time has come.",
    "contentLength": 24069,
    "summary": "Miami, Reno and CityDAO are experimenting with blockchain integration like city coins, NFT funding and DAO governance for cities.",
    "detailedSummary": {
      "theme": "Vitalik explores how blockchain technology can be integrated into city governance through both incremental improvements to existing processes and radical new experiments in economics and democratic participation.",
      "summary": "Vitalik argues that local governments represent an undervalued level of governance that could benefit significantly from blockchain integration, as cities are more capable of genuine dynamism and experimentation than national governments. He examines existing crypto city projects like CityCoins.co, Reno's blockchain experiments, and CityDAO, while identifying two main categories where blockchains could help cities: creating more trusted and transparent versions of existing processes, and implementing experimental forms of ownership and democratic governance. Vitalik proposes improvements to current city token models, suggesting they should create economic alignment between residents and their cities while avoiding the problems of traditional home ownership. He also advocates for more radical governance experiments using concepts like quadratic voting, Harberger taxes, and quadratic funding to create more participatory democracy and better incentive structures for urban development.",
      "takeaways": [
        "Local governments are more capable of implementing radical experiments than national governments due to greater cultural cohesion, easier exit options, and oversight from higher-level authorities",
        "Blockchain technology can improve city governance in two ways: making existing processes more transparent and verifiable, and enabling new experimental forms of ownership and democratic participation",
        "Current city token projects like CityCoins.co have significant flaws, particularly economic models that heavily favor early adopters over long-term sustainability",
        "City tokens could create better economic alignment than home ownership by being divisible, fungible, and tied to overall city prosperity rather than specific locations",
        "Radical governance mechanisms like quadratic voting and Harberger taxes could be implemented more safely at the city level, potentially improving everything from zoning decisions to public art funding"
      ],
      "controversial": [
        "The criticism of home ownership as 'one of the biggest economic policy mistakes' challenges a fundamental aspect of the American Dream and middle-class wealth building",
        "Proposals for using blockchain voting systems, even at local levels, touch on highly sensitive issues around election security and democratic legitimacy",
        "The suggestion that cities should experiment with Harberger taxes on land could be seen as undermining traditional property rights"
      ]
    }
  },
  {
    "id": "general-2021-09-26-limits",
    "title": "On Nathan Schneider on the limits of cryptoeconomics",
    "date": "2021-09-26",
    "category": "society",
    "url": "https://vitalik.eth.limo/general/2021/09/26/limits.html",
    "path": "general/2021/09/26/limits.html",
    "content": "- \n\n- \n\n  \n    \n\n  \n  \n  Dark Mode Toggle\n  \n    \n\n  \n\n##  On Nathan Schneider on the limits of cryptoeconomics \n\n 2021 Sep 26 \nSee all posts\n\n \n \n\n On Nathan Schneider on the limits of cryptoeconomics \n\nNathan Schneider has recently released an\narticle describing his perspectives on cryptoeconomics, and\nparticularly on the limits of cryptoeconomic approaches to governance\nand what cryptoeconomics could be augmented with to improve its\nusefulness. This is, of course, a topic that is dear to me ([1] [2] [3] [4] [5]), so it is heartening to\nsee someone else take the blockchain space seriously as an intellectual\ntradition and engage with the issues from a different and unique\nperspective.\n\nThe main question that Nathan's piece is trying to explore is simple.\nThere is a large body of intellectual work that criticizes a bubble of\nconcepts that they refer to as \"economization\", \"neoliberalism\" and\nsimilar terms, arguing that they corrode democratic political values and\nleave many people's needs unmet as a result. The world of cryptocurrency\nis very economic (lots of tokens flying around everywhere, with lots of\nfunctions being assigned to those tokens), very neo (the space is 12\nyears old!) and very liberal (freedom and voluntary participation are\ncore to the whole thing). Do these critiques also apply to blockchain\nsystems? If so, what conclusions should we draw, and how could\nblockchain systems be designed to account for these critiques? Nathan's\nanswer: more hybrid approaches combining ideas from both economics and\npolitics. But what will it actually take to achieve that, and will it\ngive the results that we want? My answer: yes, but there's a lot of\nsubtleties involved.\nWhat\nare the critiques of neoliberalism and economic logic?\n\nNear the beginning of Nathan's piece, he describes the critiques of\noveruse of economic logic briefly. That said, he does not go much\nfurther into the underlying critiques himself, preferring to point to\nother sources that have already covered the issue in depth:\n\nThe economics in cryptoeconomics raises a particular set of\nanxieties. Critics have long warned against the expansion of economic\nlogics, crowding out space for vigorous politics in public life. From\nthe Zapatista insurgents of southern Mexico (Hayden, 2002) to political\ntheorists like William Davies (2014) and Wendy Brown (2015), the\n\"neoliberal\" aspiration for economics to guide all aspects of society\nrepresents a threat to democratic governance and human personhood\nitself. Here is Brown:\n\nNeoliberalism transmogrifies every human domain and endeavor, along\nwith humans themselves, according to a specific image of the economic.\nAll conduct is economic conduct; all spheres of existence are framed and\nmeasured by economic terms and metrics, even when those spheres are not\ndirectly monetized. In neoliberal reason and in domains governed by it,\nwe are only and everywhere homo oeconomicus (p.\u00a010)\n\nFor Brown and other critics of neoliberalism, the ascent of the\neconomic means the decline of the political\u2014the space for collective\ndeterminations of the common good and the means of getting there.\n\nAt this point, it's worth pointing out that the \"neoliberalism\" being\ncriticized here is not the same as the \"neoliberalism\" that is\ncheerfully promoted by the lovely folks at The Neoliberal\nProject; the thing being critiqued here is a kind of \"enough\ntwo-party trade can solve everything\" mentality, whereas The Neoliberal\nProject favors a mix of markets and democracy. But what is the thrust of\nthe critique that Nathan is pointing to? What's the problem with\neveryone acting much more like homo oeconomicus? For this, we can take a\ndetour and peek into the source, Wendy Brown's Undoing\nthe Demos, itself. The book helpfully provides a list of the\ntop \"four deleterious effects\" (the below are reformatted and abridged\nbut direct quotes):\n\n- Intensified inequality, in which the very top\nstrata acquires and retains ever more wealth, the very bottom is\nliterally turned out on the streets or into the growing urban and\nsub-urban slums of the world, while the middle strata works more hours\nfor less pay, fewer benefits, less security...\n\n- Crass or unethical commercialization of things and\nactivities considered inappropriate for marketization. The claim is that\nmarketization contributes to human exploitation or degradation, [...]\nlimits or stratifies access to what ought to be broadly accessible and\nshared, [...] or because it enables something intrinsically horrific or\nseverely denigrating to the planet.\n\n- Ever-growing intimacy of corporate and finance capital with\nthe state, and corporate domination of political decisions and\neconomic policy\n\n- Economic havoc wreaked on the economy by the ascendance and\nliberty of finance capital, especially the destabilizing\neffects of the inherent bubbles and other dramatic fluctuations of\nfinancial markets.\n\nThe bulk of Nathan's article follows along with analyses of how these\nissues affect DAOs and governance mechanisms within the crypto space\nspecifically. Nathan focuses on three key problems:\n\n- Plutocracy: \"Those with more tokens than others\nhold more [I would add, disproportionately more]\ndecision-making power than others...\"\n\n- Limited exposure to diverse motivations:\n\"Cryptoeconomics sees only a certain slice of the people involved.\nConcepts such as self- sacrifice, duty, and honor are bedrock features\nof most political and business organizations, but difficult to simulate\nor approximate with cryptoeconomic incentive design\"\n\n- Positive and negative externalities: \"Environmental\ncosts are classic externalities\u2014invisible to the feedback loops that the\nsystem understands and that communicate to its users as incentives ... the\nchallenge of funding\"public goods\" is another example of an externality\n- and one that threatens the sustainability of crypteconomic\nsystems\"\n\nThe natural questions that arise for me are (i) to what extent do I\nagree with this critique at all and how it fits in with my own thinking,\nand (ii) how does this affect blockchains, and what do blockchain\nprotocols need to actually do to avoid these traps?\nWhat do\nI think of the critiques of neoliberalism generally?\n\nI disagree with some, agree with others. I have always been\nsuspicious of criticism of \"crass and unethical commercialization\",\nbecause it frequently feels like the author is attempting to launder\ntheir own feelings of disgust and aesthetic preferences into grand\nethical and political ideologies - a sin common among all such\nideologies, often the right (random\nexample here) even more than the left. Back in the days when I had\nmuch less money and would sometimes walk a full hour to the airport to\navoid a taxi fare, I remember thinking that I would love to get\ncompensated for donating blood or using my body for clinical trials. And\nso to me, the idea that such transactions are inhuman exploitation has\nnever been appealing.\n\nBut at the same time, I am far from a Walter\nBlock-style defender of all locally-voluntary two-party\ncommerce. I've written up my own viewpoints expressing similar concerns\nto parts of Wendy Brown's list in various articles:\n\n- Multiple pieces decrying the evils of vote\nbuying, or even financialized\ngovernance generally\n\n- The importance of\npublic goods\nfunding.\n\n- Failure modes in financial markets due to subtle issues like capital\nefficiency.\n\nSo where does my own opposition to mixing finance and\ngovernance come from? This is a complicated topic, and my conclusions\nare in large part a result of my own failure after years of attempts to\nfind a financialized governance mechanism that is economically\nstable. So here goes...\nFinance is the\nabsence of collusion prevention\n\nOut of the standard assumptions in what gets pejoratively called\n\"spherical cow economics\", people normally tend to focus on the\nunrealistic nature of perfect information and perfect\nrationality. But the unrealistic assumption that is hidden in the\nlist that strikes me as even more misleading is individual\nchoice: the idea that each agent is separately making their own\ndecisions, no agent has a positive or negative stake in another agent's\noutcomes, and there are no \"side games\"; the only thing that sees each\nagent's decisions is the black box that we call \"the mechanism\".\n\nThis assumption is often used to bootstrap complex contraptions such\nas the\nVCG mechanism, whose theoretical optimality is based on beautiful\narguments that because the price each player pays only depends on\nother players' bids, each player has no incentive to make a bid\nthat does not reflect their true value in order to manipulate the price.\nA beautiful argument in theory, but it breaks down completely once you\nintroduce the possibility that even two of the players are either allies\nor adversaries outside the mechanism.\n\nEconomics, and economics-inspired philosophy, is great at describing\nthe complexities that arise when the number of players \"playing the\ngame\" increases from one to two (see the tale of Crusoe and Friday in\nMurray Rothbard's The Ethics of\nLiberty for one example). But what this philosophical tradition\ncompletely misses is that going up to three players adds an\neven further layer of complexity. In an interaction between two people,\nthe two can ignore each other, fight or trade. In an interaction between\nthree people, there exists a new strategy: any two of the three can\ncommunicate and band together to gang up on the third. Three is the\nsmallest denominator where it's possible to talk about a 51%+ attack\nthat has someone outside the clique to be a victim.\n\nWhen there's only two people, more coordination can only be\ngood. But once there's three people, the wrong kind of coordination can be harmful, and\ntechniques to prevent harmful coordination (including\ndecentralization itself) can become very valuable. And it's this\nmanagement of coordination that is the essence of\n\"politics\".\n\nGoing from two people to three introduces the\npossibility of harms from unbalanced coordination: it's not just \"the\nindividual versus the group\", it's \"the individual versus the group\nversus the world\". \n\nNow, we can understand try to use this framework to understand the\npitfalls of \"finance\". Finance can be viewed as a set of\npatterns that naturally emerge in many kinds of systems that do not\nattempt to prevent collusion. Any system which claims\nto be non-finance, but does not actually make an effort to prevent\ncollusion, will eventually acquire the characteristics of finance, if\nnot something worse. To see why this is the case, compare two point\nsystems we are all familiar with: money, and Twitter likes. Both kinds\nof points are valuable for extrinsic reasons, both have inevitably\nbecome status symbols, and both are number games where people spend a\nlot of time optimizing to try to get a higher score. And yet, they\nbehave very differently. So what's the fundamental difference between\nthe two?\n\nThe answer is simple: it's the lack of an efficient market to enable\nagreements like \"I like your tweet if you like mine\", or \"I like your\ntweet if you pay me in some other currency\". If such a market existed\nand was easy to use, Twitter would collapse completely (something like\nhyperinflation would happen, with the likely outcome that everyone would\nrun automated bots that like every tweet to claim rewards), and even the\nlikes-for-money markets that exist illicitly today are a big\nproblem for Twitter. With money, however, \"I send X to you if you send Y\nto me\" is not an attack vector, it's just a boring old currency\nexchange transaction. A Twitter clone that does not prevent\nlike-for-like markets would \"hyperinflate\" into everyone liking\neverything, and if that Twitter clone tried to stop the hyperinflation\nby limiting the number of likes each user can make, the likes would\nbehave like a currency, and the end result would behave the same as if\nTwitter just added a tipping feature.\n\nSo what's the problem with finance? Well, if finance is optimized and\nstructured collusion, then we can look for places where finance causes\nproblems by using our existing economic tools to understand which\nmechanisms break if you introduce collusion! Unfortunately, governance\nby voting is a central example of this category; I've covered why in the\n\"moving\nbeyond coin voting governance\" post and many other occasions. Even worse,\ncooperative game theory suggests that there might be no\npossible way to make a fully collusion-resistant governance\nmechanism.\n\nAnd so we get the fundamental conundrum: the cypherpunk spirit is\nfundamentally about making maximally immutable systems that work with as\nlittle information as possible about who is participating (\"on the\ninternet, nobody knows you're a dog\"), but making new forms of\ngovernance requires the system to have richer information about\nits participants and ability to dynamically respond to attacks in order\nto remain stable in the face of actors with unforeseen incentives.\nFailure to do this means that everything looks like finance, which\nmeans, well.... perennial over-representation of concentrated interests,\nand all the problems that come as a result.\n\nOn the internet, nobody knows if you're 0.0244 of a dog (image\nsource). But what does this mean for governance?\n\nThe\ncentral role of collusion in understanding the difference between Kleros\nand regular courts\n\nNow, let us get back to Nathan's article. The distinction between\nfinancial and non-financial mechanisms is key in the article. Let us\nstart off with a description of the Kleros court:\n\nThe jurors stood to earn rewards by correctly choosing the answer\nthat they expected other jurors to independently select. This process\nimplements the \"Schelling point\" concept in game theory (Aouidef et al.,\n2021; Dylag & Smith, 2021). Such a jury does not deliberate, does\nnot seek a common good together; its members unite through\nself-interest. Before coming to the jury, the factual basis of the case\nwas supposed to come not from official organs or respected news\norganizations but from anonymous users similarly disciplined by\nreward-seeking. The prediction market itself was premised on the\nsupposition that people make better forecasts when they stand to gain or\nlose the equivalent of money in the process. The politics of the\npresidential election in question, here, had been thoroughly transmuted\ninto a cluster of economies.\n\nThe implicit critique is clear: the Kleros court is ultimately\nmotivated to make decisions not on the basis of their \"true\" correctness\nor incorrectness, but rather on the basis of their financial interests.\nIf Kleros is deciding whether Biden or Trump won the 2020 election, and\none Kleros juror really likes Trump, precommits to voting in his favor,\nand bribes other jurors to vote the same way, other jurors are likely to\nfall in line because of Kleros's conformity incentives: jurors are\nrewarded if their vote agrees with the majority vote, and penalized\notherwise. The theoretical answer to this is the right to exit: if the\nmajority of Kleros jurors vote to proclaim that Trump won the election,\na minority can spin off a fork of Kleros where Biden is considered to\nhave won, and their fork may well get a higher market price than the\noriginal. Sometimes, this\nactually works! But, as Nathan points out, it is not always so\nsimple:\n\nBut exit may not be as easy as it appears, whether it be from a\nsocial-media network or a protocol. The persistent dominance of\nearly-to-market blockchains like Bitcoin and Ethereum suggests that\ncryptoeconomics similarly favors incumbency.\n\nBut alongside the implicit critique is an implicit promise: that\nregular courts are somehow able to rise above self-interest and\n\"seek a common good together\" and thereby avoid some of these failure\nmodes. What is it that financialized Kleros courts lack, but\nnon-financialized regular courts retain, that makes them more robust?\nOne possible answer is that courts lack Kleros's explicit conformity\nincentive. But if you just take Kleros as-is, remove the conformity\nincentive (say, there's a reward for voting that does not depend on how\nyou vote), and do nothing else, you risk creating even more problems.\nKleros judges could get lazy, but more importantly if there's no\nincentive at all to choose how you vote, even the tiniest bribe could\naffect a judge's decision.\n\nSo now we get to the real answer: the key difference between\nfinancialized Kleros courts and non-financialized regular courts is that\nfinancialized Kleros courts are, well... financialized. They make\nno effort to explicitly prevent collusion. Non-financialized courts, on\nthe other hand, do prevent collusion in two key ways:\n\n- Bribing a judge to vote in a particular way is explicitly\nillegal\n\n- The judge position itself is non-fungible. It gets awarded to\nspecific carefully-selected individuals, and they cannot simply go and\nsell or reallocate their entire judging rights and salary to someone\nelse.\n\nThe only reason why political and legal systems work is that a lot of\nhard thinking and work has gone on behind the scenes to insulate the\ndecision-makers from extrinsic incentives, and punish them\nexplicitly if they are discovered to be accepting incentives from the\noutside. The lack of extrinsic motivation allows the intrinsic\nmotivation to shine through. Furthermore, the lack of transferability\nallows governance power to be given to specific actors whose intrinsic\nmotivations we trust, avoiding governance power always flowing to \"the\nhighest bidder\". But in the case of Kleros, the lack of hostile\nextrinsic motivation cannot be guaranteed, and transferability is\nunavoidable, and so overpoweringly strong in-mechanism extrinsic\nmotivation (the conformity incentive) was the best solution they could\nfind to deal with the problem.\n\nAnd of course, the \"final backstop\" that Kleros relies on, the right\nof users to fork away, itself depends on social coordination to take\nplace - a messy and difficult institution, often derided by\ncryptoeconomic purists as \"proof of social media\", that works precisely\nbecause public discussion has lots of informal collusion detection and\nprevention all over the place.\nCollusion in\nunderstanding DAO governance issues\n\nBut what happens when there is no single right answer that they can\nexpect voters to converge on? This is where we move away from\nadjudication and toward governance (yes, I know that\nadjudication has unavoidably grey edge cases too. Governance just has\nthem much more often). Nathan writes:\n\nGovernance by economics is nothing new. Joint-stock companies\nconventionally operate on plutocratic governance\u2014more shares equals more\nvotes. This arrangement is economically efficient for aligning\nshareholder interests (Davidson and Potts, this issue), even while it\nmay sideline such externalities as fair wages and environmental\nimpacts...\n\nIn my opinion, this actually concedes too much! Governance by\neconomics is not \"efficient\" once you drop the spherical-cow assumption\nof no collusion, because it is inherently vulnerable to 51% of the\nstakeholders colluding to liquidate the company and split its resources\namong themselves. The only reason why this does not happen much more\noften \"in real life\" is because of many decades of shareholder\nregulation that have been explicitly built up to ban the most common\ntypes of abuses. This regulation is, of course, non-\"economic\" (or, in\nmy lingo, it makes corporate governance less financialized),\nbecause it's an explicit attempt to prevent collusion.\n\nNotably, Nathan's favored solutions do not try to regulate\ncoin voting. Instead, they try to limit the harms of its weaknesses by\ncombining it with additional mechanisms:\n\nRather than relying on direct token voting, as other protocols have\ndone, The Graph uses a board-like mediating layer, the Graph Council, on\nwhich the protocol's major stakeholder groups have representatives. In\nthis case, the proposal had the potential to favor one group of\nstakeholders over others, and passing a decision through the Council\nrequires multiple stakeholder groups to agree. At the same time, the\nSnapshot vote put pressure on the Council to implement the will of\ntoken-holders.\n\nIn the case of 1Hive, the anti-financialization protections are\ndescribed as being purely cultural:\n\nAccording to a slogan that appears repeatedly in 1Hive discussions,\n\"Come for the honey, stay for the bees.\" That is, although economics\nfigures prominently as one first encounters and explores 1Hive,\nparticipants understand the community's primary value as interpersonal,\nsocial, and non-economic.\n\nI am personally skeptical of the latter approach: it can work well in\nlow-economic-value communities that are fun oriented, but if such an\napproach is attempted in a more serious system with widely open\nparticipation and enough at stake to invite determined attack, it will\nnot survive for long. As I wrote above, \"any system which\nclaims to be non-finance, but does not actually make an effort\nto prevent collusion, will eventually acquire the characteristics of\nfinance\".\n\n[Edit/correction 2021.09.27: it has been brought to my\nattention that in addition to culture, financialization is limited\nby (i) conviction voting, and (ii) juries enforcing a covenant. I'm\nskeptical of conviction voting in the long run; many DAOs use it today,\nbut in the long term it can be defeated\nby wrapper tokens. The covenant, on the other hand, is interesting.\nMy fault for not checking in more detail.]\n\nThe money is called honey. But is calling money\nhoney enough to make it work differently than money? If not, how much\nmore do you have to do?\n\nThe solution in TheGraph is very much an instance of collusion\nprevention: the participants\nhave been hand-picked to come from diverse constituencies and to be\ntrusted and upstanding people who are unlikely to sell their voting\nrights. Hence, I am bullish on that approach if it successfully avoids\ncentralization.\nSo how can we\nsolve these problems more generally?\n\nNathan's post argues:\n\nA napkin sketch of classical, never-quite-achieved liberal democracy\n(Brown, 2015) would depict a market (governed through economic\nincentives) enclosed in politics (governed through deliberation on the\ncommon good). Economics has its place, but the system is not economics\nall the way down; the rules that guide the market, and that enable it in\nthe first place, are decided democratically, on the basis of citizens'\ncivil rights rather than their economic power. By designing democracy\ninto the base-layer of the system, it is possible to overcome the kinds\nof limitations that cryptoeconomics is vulnerable to, such as by\ncounteracting plutocracy with mass participation and making visible the\nexternalities that markets might otherwise fail to see.\n\nThere is one key difference between blockchain political theory and\ntraditional nation-state political theory - and one where, in the long\nrun, nation states may well have to learn from blockchains. Nation-state\npolitical theory talks about \"markets embedded in democracy\" as though\ndemocracy is an encompassing base layer that encompasses all of society.\nIn reality, this is not true: there are multiple countries, and every\ncountry at least to some degree permits trade with outside countries\nwhose behavior they cannot regulate. Individuals and companies have\nchoices about which countries they live in and do business in. Hence,\nmarkets are not just embedded in democracy, they also surround it, and\nthe real world is a complicated interplay between the two.\n\nBlockchain systems, instead of trying to fight this\ninterconnectedness, embrace it. A blockchain system has no ability to\nregular \"the market\" in the sense of people's general ability to freely\nmake transactions. But what it can do is regulate and structure\n(or even create) specific markets, setting up patterns of specific\nbehaviors whose incentives are ultimately set and guided by institutions\nthat have anti-collusion guardrails built in, and can resist pressure\nfrom economic actors. And indeed, this is the direction Nathan ends up\ngoing in as well. He talks positively about the design of Civil as an\nexample of precisely this spirit:\n\nThe aborted Ethereum-based project Civil sought to leverage\ncryptoeconomics to protect journalism against censorship and degraded\nprofessional standards (Schneider, 2020). Part of the system was the\nCivil Council, a board of prominent journalists who served as a kind of\nsupreme court for adjudicating the practices of the network's newsrooms.\nToken holders could earn rewards by successfully challenging a\nnewsroom's practices; the success or failure of a challenge ultimately\ndepended on the judgment of the Civil Council, designed to be free of\neconomic incentives clouding its deliberations. In this way, a\ncryptoeconomic enforcement market served a non-economic social mission.\nThis kind of design could enable cryptoeconomic networks to serve\npurposes not reducible to economic feedback loops.\n\nThis is fundamentally very similar to an idea that I proposed in\n2018: prediction\nmarkets to scale up content moderation. Instead of doing content\nmoderation by running a low-quality AI algorithm on all content, with\nlots of false positives, there could be an open mini prediction market\non each post, and if the volume got high enough a high-quality committee\ncould step in an adjudicate, and the prediction market participants\nwould be penalized or rewarded based on whether or not they had\ncorrectly predicted the outcome. In the mean time, posts with prediction\nmarket scores predicting that the post would be removed would not be\nshown to users who did not explicitly opt-in to participate in the\nprediction game. There is precedent for this kind of open but\naccountable moderation: Slashdot meta\nmoderation is arguably a limited version of it. This more\nfinancialized version of meta-moderation through prediction markets\ncould produce superior outcomes because the incentives invite highly\ncompetent and professional participants to take part.\n\nNathan then expands:\n\nI have argued that pairing cryptoeconomics with political systems can\nhelp overcome the limitations that bedevil cryptoeconomic governance\nalone. Introducing purpose-centric mechanisms and temporal modulation\ncan compensate for the blind-spots of token economies. But I am not\narguing against cryptoeconomics altogether. Nor am I arguing that these\nsorts of politics must occur in every app and protocol. Liberal\ndemocratic theory permits diverse forms of association and business\nwithin a democratic structure, and similarly politics may be necessary\nonly at key leverage points in an ecosystem to overcome the limitations\nof cryptoeconomics alone.\n\nThis seems broadly correct. Financialization, as Nathan points out in\nhis conclusion, has benefits in that it attracts a large amount of\nmotivation and energy into building and participating in systems that\nwould not otherwise exist. Furthermore, preventing\nfinancialization is very difficult and high cost, and works best when\ndone sparingly, where it is needed most. However, it is also true that\nfinancialized systems are much more stable if their incentives are\nanchored around a system that is ultimately non-financial.\n\nPrediction markets avoid the plutocracy issues inherent in coin\nvoting because they introduce individual accountability: users\nwho acted in favor of what ultimately turns out to be a bad decision\nsuffer more than users who acted against it. However, a prediction\nmarket requires some statistic that it is measuring, and measurement\noracles cannot be made secure through cryptoeconomics alone: at the very\nleast, community forking as a backstop against attacks is required. And\nif we want to avoid the messiness of frequent forks, some other explicit\nnon-financialized mechanism at the center is a valuable alternative.\n\n## Conclusions\n\nIn his conclusion, Nathan writes:\n\nBut the autonomy of cryptoeconomic systems from external regulation\ncould make them even more vulnerable to runaway feedback loops, in which\nnarrow incentives overpower the common good. The designers of these\nsystems have shown an admirable capacity to devise cryptoeconomic\nmechanisms of many kinds. But for cryptoeconomics to achieve the\ninstitutional scope its advocates hope for, it needs to make space for\nless-economic forms of governance.\n\nIf cryptoeconomics needs a political layer, and is no longer\nself-sufficient, what good is cryptoeconomics? One answer might be that\ncryptoeconomics can be the basis for securing more democratic and\nvalues-centered governance, where incentives can reduce reliance on\nmilitary or police power. Through mature designs that integrate with\nless-economic purposes, cryptoeconomics might transcend its initial\nlimitations. Politics needs cryptoeconomics, too ... by integrating\ncryptoeconomics with democracy, both legacies seem poised to\nbenefit.\n\nI broadly agree with both conclusions. The language of collusion\nprevention can be helpful for understanding why cryptoeconomic\npurism so severely constricts the design space. \"Finance\" is a category\nof patterns that emerge when systems do not attempt to prevent\ncollusion. When a system does not prevent collusion, it cannot treat\ndifferent individuals differently, or even different numbers of\nindividuals differently: whenever a \"position\" to exert influence\nexists, the owner of that position can just sell it to the highest\nbidder.\n\nGavels on Amazon. A world where these were NFTs that actually\ncame with associated judging power may well be a fun one, but I would\ncertainly not want to be a defendant!\n\nThe language of defense-focused\ndesign, on the other hand, is an underrated way to think about where\nsome of the advantages of blockchain-based designs can be.\nNation state systems often deal with threats with one of two totalizing\nmentalities: closed borders vs conquer the world. A\nclosed borders approach attempts to make hard distinctions between an\n\"inside\" that the system can regulate and an \"outside\" that the\nsystem cannot, severely restricting flow between the inside and the\noutside. Conquer-the-world approaches attempt to extraterritorialize a\nnation state's preferences, seeking a state of affairs where there is no\nplace in the entire world where some undesired activity can happen.\nBlockchains are structurally unable to take either approach, and so they\nmust seek alternatives.\n\nFortunately, blockchains do have one very powerful tool in their\ngrasp that makes security under such porous conditions actually\nfeasible: cryptography. Cryptography allows everyone to verify\nthat some governance procedure was executed exactly according to the\nrules. It leaves a verifiable evidence trail of all actions, though zero\nknowledge proofs allow mechanism designers freedom in picking and\nchoosing exactly what evidence is visible and what evidence is not.\nCryptography can even prevent\ncollusion! Blockchains allow applications to live on a substrate\nthat their governacne does not control, which allows them to effectively\nimplement techniques such as, for example, ensure that every change to\nthe rules only takes effect with a 60 day delay. Finally, freedom to\nfork is much more practical, and forking is much lower in economic and\nhuman cost, than most centralized systems.\n\nBlockchain-based contraptions have a lot to offer the world that\nother kinds of systems do not. On the other hand, Nathan is completely\ncorrect to emphasize that blockchainized should not be equated\nwith financialized. There is plenty of room for\nblockchain-based systems that do not look like money, and indeed we need\nmore of them.",
    "contentLength": 31594,
    "summary": "Nathan Schneider critiques cryptoeconomics for enabling plutocracy and ignoring non-economic motivations, advocating hybrid economic-political approaches.",
    "detailedSummary": {
      "theme": "Vitalik responds to Nathan Schneider's critique of cryptoeconomics by arguing that the core problem is collusion, not economics itself, and that blockchain systems need hybrid approaches combining economic incentives with anti-collusion mechanisms rather than purely political solutions.",
      "summary": "Vitalik engages with Nathan Schneider's article about the limitations of cryptoeconomics and the need for more democratic governance in blockchain systems. While acknowledging some validity in critiques of pure economic approaches, Vitalik reframes the central issue as one of collusion prevention rather than economics versus politics. He argues that 'finance' emerges naturally in any system that fails to prevent collusion, using examples like the difference between money and Twitter likes to illustrate how transferability and lack of collusion resistance create financial-like behaviors. Vitalik contends that traditional courts work better than systems like Kleros not because they avoid economics entirely, but because they actively prevent collusion through legal prohibitions on bribery and non-transferable positions for judges. He ultimately agrees with Schneider that hybrid approaches are needed, but emphasizes that successful solutions require explicit anti-collusion mechanisms rather than relying purely on political or cultural safeguards.",
      "takeaways": [
        "The fundamental problem with pure cryptoeconomic systems is not economics itself, but the lack of collusion prevention mechanisms",
        "Any system that claims to be non-financial but doesn't actively prevent collusion will eventually behave like a financial system",
        "Traditional governance systems work because they explicitly prevent collusion through legal restrictions and non-transferable positions",
        "Blockchain systems need hybrid approaches that combine economic incentives with anti-collusion safeguards anchored in non-financial mechanisms",
        "Cryptography and blockchain infrastructure offer unique tools for creating verifiable, fork-friendly systems that can implement effective governance without totalizing approaches"
      ],
      "controversial": [
        "Vitalik's dismissal of cultural approaches to preventing financialization as insufficient for high-stakes systems may be seen as overly pessimistic about community-driven governance",
        "His critique of 'crass commercialization' concerns as potentially being disguised aesthetic preferences rather than legitimate ethical issues could be contentious among critics of market expansion"
      ]
    }
  },
  {
    "id": "general-2021-08-22-prices",
    "title": "Alternatives to selling at below-market-clearing prices for achieving fairness (or community sentiment, or fun)",
    "date": "2021-08-22",
    "category": "general",
    "url": "https://vitalik.eth.limo/general/2021/08/22/prices.html",
    "path": "general/2021/08/22/prices.html",
    "content": "- \n\n- \n\n  \n    \n\n  \n  \n  Dark Mode Toggle\n  \n    \n\n  \n\n##  Alternatives to selling at below-market-clearing prices for achieving fairness (or community sentiment, or fun) \n\n 2021 Aug 22 \nSee all posts\n\n \n \n\n Alternatives to selling at below-market-clearing prices for achieving fairness (or community sentiment, or fun) \n\nWhen a seller wants to sell a fixed supply of an item that is in high\n(or uncertain and possibly high) demand, one choice that they often make\nis to set a price significantly lower than what \"the market will bear\".\nThe result is that the item quickly sells out, with the lucky buyers\nbeing those who attempted to buy first. This has happened in a number of\nsituations within the Ethereum ecosystem, notably NFT\nsales and token sales / ICOs. But this\nphenomenon is much older than that; concerts and restaurants frequently\nmake similar choices, keeping prices cheap and leading to seats quickly\nselling out or buyers waiting in long lines.\n\nEconomists have for a long time asked the question: why do sellers do\nthis? Basic economic theory suggests that it's best if sellers\nsell at the market-clearing price - that is, the price at which\nthe amount that buyers are willing to buy exactly equals the amount the\nseller has to sell. If the seller doesn't know what the\nmarket-clearing price is, the seller should sell through an\nauction, and let the market determine the price. Selling below\nmarket-clearing price not only sacrifices revenue for the seller; it\nalso can harm the buyers: the item may sell out so quickly that\nmany buyers have no opportunity to get it at all, no matter how much\nthey want it and are willing to pay to get it. Sometimes, the\ncompetitions created by these non-price-based allocation mechanisms even\ncreate negative externalities that harm third parties - an effect that,\nas we will see, is particularly severe in the Ethereum ecosystem.\n\nBut nevertheless, the fact that below-market-clearing pricing\nis so prevalent suggests that there must be some convincing reasons why\nsellers do it. And indeed, as the research into this topic over\nthe last few decades has shown, there often are. And so it's worth\nasking the question: are there ways of achieving the same goals with\nmore fairness, less inefficiency and less harm?\nSelling\nat below market-clearing prices has large inefficiencies and negative\nexternalities\n\nIf a seller sells an item at market price, or through an auction,\nsomeone who really really wants that item has a simple path to\ngetting it: they can pay the high price or if it's an auction they can\nbid a high amount. If a seller sells the item at below market price,\nthen demand exceeds supply, and so some people will get the item and\nothers won't. But the mechanism deciding who will get the item is\ndecidedly not random, and it's often not well-correlated with how much\nparticipants want the item. Sometimes, it involves being faster at\nclicking buttons than everyone else. At other times, it involves waking\nup at 2 AM in your timezone (but 11 PM or even 2 PM in someone else's).\nAnd at still other times, it just turns into an \"auction by other\nmeans\", one which is more chaotic, less efficient and laden with far\nmore negative externalties.\n\nWithin the Ethereum ecosystem, there are many clear examples of this.\nFirst, we can look at the ICO\ncraze of 2017. In 2017, there were a large number of projects\nlaunching initial coin offerings (ICOs), and a typical model was the\ncapped sale: the project would set the price of the\ntoken and a hard maximum for how many tokens they are willing to sell,\nand at some point in time the sale would start automatically. Once the\nnumber of tokens hit the cap, the sale ends.\n\nWhat's the result? In practice, these sales would often end in as\nlittle as 30 seconds. As soon as (or rather, just before) the sale\nstarts, everyone would start sending transactions in to try to get in,\noffering higher and higher fees to encourage miners to include their\ntransaction first. An auction by another name - except with revenues\ngoing to the miners instead of the token seller, and the extremely\nharmful negative externality of pricing out every other application\non-chain while the sale is going on.\n\nThe most expensive transaction in the BAT sale set a fee of\n580,000 gwei, paying a fee of $6,600 to get included in the\nsale.\n\nMany ICOs after that tried various strategies to avoid these gas\nprice auctions; one ICO notably had a smart contract that checked the\ntransaction's gasprice and rejected it if it exceeded 50 gwei. But that\nof course, did not solve the problem. Buyers wishing to cheat the system\nsent many transactions, hoping that at least one would get in.\nOnce again, an auction by another name, and this time clogging up the\nchain even more.\n\nIn more recent times, ICOs have become less popular, but NFTs and NFT\nsales are now very popular. Unfortunately, the NFT space failed to learn\nthe lessons from 2017; they make fixed-quantity fixed-supply sales just\nlike the ICOs did (eg. see the mint function on lines\n97-108 of this\ncontract here). What's the result?\n\nAnd this isn't even the biggest one; some NFT sales have\ncreated gas price spikes as high as 2000 gwei.\n\nOnce again, sky-high gas prices from users fighting each other by\nsending higher and higher transaction fees to get in first. An auction\nby another name, pricing out every other application on-chain for 15\nminutes, just as before.\nSo why do\nsellers sometimes sell below market price?\n\nSelling at below market price is hardly a new phenomenon, both within\nthe blockchain space and outside, and over the decades there have been many articles\nand papers and podcasts\nwriting (and sometimes\nbitterly complaining)\nabout the unwillingness to use auctions or set prices to market-clearing\nlevels.\n\nMany of the arguments are very similar between the examples in the\nblockchain space (NFTs and ICOs) and outside the blockchain space\n(popular restaurants and concerts). A particular concern is fairness and\nthe desire to not lock poorer people out and not lose fans or create\ntension as a result of being perceived as greedy. Kahneman, Knetsch and\nThaler's 1986 paper\nis a good exposition of how perceptions of fairness and greed can\ninfluence these decisions. In my own recollection of the 2017 ICO\nseason, the desire to avoid perceptions of greed was similarly a\ndecisive factor in discouraging the use of auction-like mechanisms (I am\nmostly going off memory here and do not have many sources, though I\ndid find a link\nto a no-longer-available parody video making some kind of comparison\nbetween the auction-based Gnosis ICO and the National Socialist German\nWorkers' Party).\n\nIn addition to fairness issues, there are also the perennial\narguments that products selling out and having long lines creates a\nperception of popularity and prestige, which makes the product seem even\nmore attractive to others further down the line. Sure, in a rational\nactor model, high prices should have the same effect as long lines, but\nin reality long lines are much more visible than high prices are. This\nis just as true for ICOs and NFTs as it is for restaurants. In addition\nto these strategies generating more marketing value, some people\nactually find participating in or watching the game of grabbing up a\nlimited set of opportunities first before everyone else takes them all\nto be quite fun.\n\nBut there are also some factors specific to the blockchain space. One\nargument for selling ICO tokens at below-market-clearing prices (and one\nthat was decisive in convincing the OmiseGo team to adopt their capped\nsale strategy) has to do with community dynamics of token issuance. The\nmost basic rule of community sentiment management is simple: you want\nprices to go up, not down. If community members are \"in the green\", they\nare happy. But if the price goes lower than what it was when the\ncommunity members bought, leaving them at a net loss, they become\nunhappy and start calling you a scammer, and possibly creating a social\nmedia cascade leading to everyone else calling you a scammer.\n\nThe only way to avoid this effect is to set a sale price low enough\nthat the post-launch market price will almost certainly be higher. But,\nhow do you actually do this without creating a rush-for-the-gates\ndynamic that leads to an auction by other means?\nSome more interesting\nsolutions\n\nThe year is 2021. We have a blockchain. The blockchain contains not\njust a powerful decentralized finance ecosystem, but also a rapidly\ngrowing suite of all kinds of non-financial tools. The blockchain\nalso presents us with a unique opportunity to reset social\nnorms. Uber legitimized surge pricing where decades of economists\nyelling about \"efficiency\" failed; surely, blockchains can also be an\nopportunity to legitimize new uses of mechanism design. And surely,\ninstead of fiddling around with a coarse-grained one-dimensional\nstrategy space of selling at market price versus below market price\n(with perhaps a second dimension for auction versus fixed-price sale),\nwe could use our more advanced tools to create an approach that more\ndirectly solves the problems, with fewer side effects?\n\nFirst, let us list the goals. We'll try to cover the cases of (i)\nICOs, (ii) NFTs and (iii) conference tickets (really a type of NFT) at\nthe same time; most of the desired properties are shared between the\nthree cases.\n\n- Fairness: don't completely lock low-income people\nout of participating, give them at least some chance to get in. For\ntoken sales, there's the not\nquite identical but related goal of avoiding high initial wealth\nconcentration and having a larger and more diverse initial token holder\ncommunity.\n\n- Don't create races: avoid creating situations where\nlots of people are rushing to take the same action and only the first\nfew get in (this is the type of situation that leads to the horrible\nauctions-by-another-name that we saw above).\n\n- Don't require fine-grained knowledge of market\nconditions: the mechanism should work even if the seller has\nabsolutely no idea how much demand there is.\n\n- Fun: the process of participating in the sale\nshould ideally be interesting and have game-like qualities, but without\nbeing frustrating.\n\n- Give buyers positive expected returns: in the case\nof a token (or, for that matter, an NFT), buyers should be more likely\nto see the item go up in price than go down. This necessarily implies\nselling to buyers at below the market price.\n\nWe can start by looking at (1). Looking at it from the point of view\nof Ethereum, there is a pretty clear solution. Instead of creating race\nconditions, just use an explicitly designed tool for the job: proof of personhood\nprotocols! Here's one quick proposed mechanism:\n\nMechanism 1 Each participant (verified by\nproof-of-personhood) can buy up to X units at price\nP, and if they want to buy more they can buy in an\nauction.\n\nIt seems like it satisfies a lot of the goals already: the per-person\naspect provides fairness, if the auction price turns out higher than\nP buyers can get positive expected returns for the portion\nsold through the per-person mechanism, and the auction part does not\nrequire the seller to understand the level of demand. Does it avoid\ncreating races? If the number of participants buying through the\nper-person pool is not that high, it seems like it does. But what if so\nmany people show up that the per-person pool is not big enough to\nprovide an allocation for all of them?\n\nHere's an idea: make the per-person allocation amount itself\ndynamic.\n\nMechanism 2 Each participant (verified by\nproof-of-personhood) can make a deposit into a smart contract to declare\ninterest for up to X tokens. At the end, each buyer is\ngiven an allocation of min(X, N / number_of_buyers) tokens,\nwhere N is the total amount sold through the per-person\npool (some other amount can also be sold by auction). The portion of the\nbuyer's deposit going above the amount needed to buy their allocation is\nrefunded to them.\n\nNow, there's no race condition regardless of the number of buyers\ngoing through the per-person pool. No matter how high the demand,\nthere's no way in which it's more beneficial to participate earlier\nrather than later.\n\nHere's yet another idea, if you like your game mechanics to be more\nclever and use fancy quadratic formulas.\n\nMechanism 3 Each participant (verified by\nproof-of-personhood) can buy \\(X\\)\nunits at a price \\(P * X^2\\), up to a\nmaximum of \\(C\\) tokens per buyer.\n\\(C\\) starts at some low number, and\nthen increases over time until enough units are sold.\n\nThis mechanism has the particularly interesting property that if\nyou're making a governance token (please don't do that; this\nis purely harm-reduction advice), the quantity allocated to each buyer\nis theoretically optimal, though of course post-sale transfers will\ndegrade this optimality over time. Mechanisms 2 and 3 seem like they\nboth satisfy all of the above goals, at least to some extent. They're\nnot necessarily perfect and ideal, but they do make good\nstarting points.\n\nThere is one remaining issue. For fixed and limited-supply NFTs, you\nmight get the problem that the equilibrium purchased quantity per\nparticipant is fractional (in mechanism 2, perhaps\nnumber_of_buyers > N, and in mechanism 3, perhaps\nsetting \\(C = 1\\) already leads to\nenough demand to over-subscribe the sale). In this case, you can sell\nfractional items by offering lottery tickets: if there are\nN items to be sold, then if you subscribe you have a chance\nof N / number_of_buyers that you will actually get the\nitem, and otherwise you get a refund. For a conference, groups that want\nto go together could be allowed to bundle their lottery tickets to\nguarantee either all-win or all-lose. Ability to get the item for\ncertain can be sold at auction.\n\nA fun mildly-grey-hat tactic for conference tickets is to\ndisguise the pool being sold at market rate as the bottom tier of\n\"sponsorships\". You may end up with a bunch of people's faces on the\nsponsor board, but... maybe that's fine? After all, EthCC had John Lilic's\nface on their sponsor board!\n\nIn all of these cases, the core of the solution is simple: if you\nwant to be reliably fair to people, then your mechanism should\nhave some input that explicitly measures people. Proof of personhood\nprotocols do this (and if desired can be combined with zero knowledge proofs to\nensure privacy). Ergo, we should take the efficiency benefits of market\nand auction-based pricing, and the egalitarian benefits of proof of\npersonhood mechanics, and combine them together.\nAnswers to possible\nquestions\n\nQ: Wouldn't lots of people who don't even care about\nyour project buy the item through the egalitarian scheme and immediately\nresell it?\n\nA: Initially, probably not. In practice, such\nmeta-games take time to show up. But if/when they do, one possible\nmitigation is to make them untradeable for some period of time. This\nactually works because proof-of-personhood identities are untradeable:\nyou can always use your face to claim that your previous account got\nhacked and the identity corresponding to you, including everything in\nit, should be moved to a new account.\n\nQ: What if I want to make my item accessible not\njust to people in general, but to a particular community?\n\nA: Instead of proof of personhood, use proof of participation tokens connected to\nevents in that community. An additional alternative, also serving both\negalitarian and gamification value, is to lock some items inside\nsolutions to some publicly-published puzzles.\n\nQ: How do we know people will accept this? People\nhave been resistant to weird new mechanisms in the past.\n\nA: It's very difficult to get people to accept a new\nmechanism that they find weird by having economists write screeds about\nhow they \"should\" accept it for the sake of \"efficiency\" (or even\n\"equity\"). However, rapid changes in context do an excellent job of\nresetting people's set expectations. So if there's any good time at all\nto try this, the blockchain space is that time. You could also wait for\nthe \"metaverse\",\nbut it's quite possible that the best version of the metaverse will run on Ethereum anyway, so you\nmight as well just start now.",
    "contentLength": 16155,
    "summary": "The blog post proposes better alternatives to below-market pricing (like NFT/ICO sales) which create gas fee wars and inefficiencies.",
    "detailedSummary": {
      "theme": "Vitalik proposes using proof-of-personhood protocols and dynamic allocation mechanisms as alternatives to below-market pricing for NFT/ICO sales to achieve fairness without creating harmful gas price auctions.",
      "summary": "Vitalik analyzes why sellers in crypto (ICOs, NFTs) and traditional markets often price items below market-clearing rates, leading to inefficient \"auctions by other means\" that create negative externalities like extreme gas price spikes on Ethereum. He identifies key motivations including fairness concerns, avoiding perceptions of greed, creating prestige through scarcity, and ensuring token prices rise post-launch to maintain positive community sentiment. Vitalik proposes three mechanism designs that combine proof-of-personhood verification with market-based pricing: fixed per-person allocations with auction overflow, dynamic per-person allocations that adjust based on participation, and quadratic pricing schemes. These solutions aim to achieve fairness, avoid rush conditions, work without precise demand knowledge, maintain fun game-like qualities, and give buyers positive expected returns while leveraging blockchain technology's unique capabilities to reset social norms around pricing mechanisms.",
      "takeaways": [
        "Below-market pricing in crypto sales creates harmful \"auctions by other means\" with extreme gas fees that price out other blockchain applications",
        "Sellers use below-market pricing for fairness, avoiding greed perceptions, creating prestige through scarcity, and ensuring post-launch price increases",
        "Proof-of-personhood protocols can enable fair allocation mechanisms that don't create race conditions or require precise demand forecasting",
        "Dynamic allocation systems can adjust per-person limits based on total participation to eliminate rush dynamics",
        "The blockchain space presents a unique opportunity to legitimize new mechanism designs and reset social norms around pricing"
      ],
      "controversial": [
        "The assumption that proof-of-personhood protocols are reliable and won't be gamed or create privacy concerns",
        "The suggestion that making items temporarily untradeable is acceptable to prevent speculation",
        "The claim that blockchain context makes people more accepting of novel mechanisms compared to traditional economic theory advocacy"
      ]
    }
  },
  {
    "id": "general-2021-08-16-voting3",
    "title": "Moving beyond coin voting governance",
    "date": "2021-08-16",
    "category": "governance",
    "url": "https://vitalik.eth.limo/general/2021/08/16/voting3.html",
    "path": "general/2021/08/16/voting3.html",
    "content": "- \n\n- \n\n  \n    \n\n  \n  \n  Dark Mode Toggle\n  \n    \n\n  \n\n##  Moving beyond coin voting governance \n\n 2021 Aug 16 \nSee all posts\n\n \n \n\n Moving beyond coin voting governance \n\nSpecial thanks to Karl Floersch, Dan Robinson and Tina Zhen for\nfeedback and review. See also Notes on Blockchain\nGovernance, Governance, Part 2:\nPlutocracy Is Still Bad, On Collusion and Coordination, Good and\nBad for earlier thinking on similar topics.\n\nOne of the important trends in the blockchain space over the past\nyear is the transition from focusing on decentralized finance\n(DeFi) to also thinking about decentralized governance\n(DeGov). While the 2020 is often\nwidely, and with much justification, hailed\nas a\nyear of DeFi, over the year since then the growing complexity and\ncapability of DeFi projects that make up this trend has led to growing\ninterest in decentralized governance to handle that complexity. There\nare examples inside of Ethereum: YFI, Compound,\nSynthetix,\nUNI, Gitcoin\nand others have all launched, or even started with, some kind\nof DAO. But it's also true outside of Ethereum, with arguments over infrastructure\nfunding\nproposals\nin Bitcoin Cash, infrastructure\nfunding votes in Zcash, and much more.\n\nThe rising popularity of formalized decentralized governance of some\nform is undeniable, and there are important reasons why people are\ninterested in it. But it is also important to keep in mind the risks of\nsuch schemes, as the recent hostile\ntakeover of Steem and subsequent mass exodus to Hive makes clear. I\nwould further argue that these trends are unavoidable.\nDecentralized governance in some contexts is both necessary and\ndangerous, for reasons that I will get into in this post. How\ncan we get the benefits of DeGov while minimizing the risks? I will\nargue for one key part of the answer: we need to move beyond\ncoin voting as it exists in its present form.\n\n## DeGov is necessary\n\nEver since the Declaration of\nIndependence of Cyberspace in 1996, there has been a key unresolved\ncontradiction in what can be called cypherpunk ideology. On the one\nhand, cypherpunk values are all about using cryptography to minimize\ncoercion, and maximize the efficiency and reach of the main non-coercive\ncoordination mechanism available at the time: private property and\nmarkets. On the other hand, the economic logic of private property and\nmarkets is optimized for activities that can\nbe \"decomposed\" into repeated one-to-one interactions, and the\ninfosphere, where art, documentation, science and code are produced and\nconsumed through irreducibly one-to-many interactions, is the exact\nopposite of that.\n\nThere are two key problems inherent to such an environment that need\nto be solved:\n\n- Funding public goods: how do projects that are\nvaluable to a wide and unselective group of people in the community, but\nwhich often do not have a business model (eg. layer-1 and layer-2\nprotocol research, client development, documentation...), get funded?\n\n- Protocol maintenance and upgrades: how are upgrades\nto the protocol, and regular maintenance and adjustment operations on\nparts of the protocol that are not long-term stable (eg. lists of safe\nassets, price oracle sources, multi-party computation keyholders),\nagreed upon?\n\nEarly blockchain projects largely ignored both of these challenges,\npretending that the only public good that mattered was network security,\nwhich could be achieved with a single algorithm set in stone forever and\npaid for with fixed proof of work rewards. This state of affairs in\nfunding was possible at first because of extreme Bitcoin price rises\nfrom 2010-13, then the one-time ICO boom from 2014-17, and again from\nthe simultaneous second crypto bubble of 2014-17, all of which made the\necosystem wealthy enough to temporarily paper over the large market\ninefficiencies. Long-term governance of public resources was similarly\nignored: Bitcoin took the path of extreme minimization, focusing on\nproviding a fixed-supply currency and ensuring support for layer-2\npayment systems like Lightning and nothing else, Ethereum continued\ndeveloping mostly harmoniously (with one major\nexception) because of the strong legitimacy of its\npre-existing roadmap (basically: \"proof of stake and sharding\"), and\nsophisticated application-layer projects that required anything more did\nnot yet exist.\n\nBut now, increasingly, that luck is running out, and challenges of\ncoordinating protocol maintenance and upgrades and funding\ndocumentation, research and development while avoiding the risks of\ncentralization are at the forefront.\nThe need for DeGov\nfor funding public goods\n\nIt is worth stepping back and seeing the absurdity of the present\nsituation. Daily mining issuance rewards from Ethereum are about 13500\nETH, or about $40m, per\nday. Transaction\nfees are similarly high; the non-EIP-1559-burned\nportion continues to be around 1,500 ETH (~$4.5m) per day. So there\nare many billions of dollars per year going to fund network security.\nNow, what is the budget of the Ethereum Foundation? About $30-60 million\nper year. There are non-EF actors (eg. Consensys) contributing to\ndevelopment, but they are not much larger. The situation in Bitcoin is\nsimilar, with perhaps even less funding going into non-security public\ngoods.\n\nHere is the situation in a chart:\n\nWithin the Ethereum ecosystem, one can make a case that this\ndisparity does not matter too much; tens of millions of dollars per year\nis \"enough\" to do the needed R&D and adding more funds does not\nnecessarily improve things, and so the risks to the platform's credible neutrality\nfrom instituting in-protocol developer funding exceed the benefits. But\nin many smaller ecosystems, both ecosystems within Ethereum and entirely\nseparate blockchains like BCH and Zcash, the same debate is brewing, and\nat those smaller scales the imbalance makes a big difference.\n\nEnter DAOs. A project that launches as a \"pure\" DAO from day 1 can\nachieve a combination of two properties that were previously impossible\nto combine: (i) sufficiency of developer funding, and (ii) credible\nneutrality of funding (the much-coveted \"fair launch\"). Instead of\ndeveloper funding coming from a hardcoded list of receiving addresses,\nthe decisions can be made by the DAO itself.\n\nOf course, it's difficult to make a launch perfectly fair, and\nunfairness from information asymmetry can often be worse than unfairness\nfrom explicit premines (was Bitcoin really a fair launch\nconsidering how few people had a chance to even hear about it by the\ntime 1/4 of the supply had already been handed out by the end of 2010?).\nBut even still, in-protocol compensation for non-security public goods\nfrom day one seems like a potentially significant step forward toward\ngetting sufficient and more credibly neutral developer funding.\nThe\nneed for DeGov for protocol maintenance and upgrades\n\nIn addition to public goods funding, the other equally important\nproblem requiring governance is protocol maintenance and upgrades. While\nI advocate trying to minimize all non-automated parameter adjustment\n(see the \"limited governance\"\nsection below) and I am a fan of RAI's\n\"un-governance\" strategy, there are times where governance is\nunavoidable. Price oracle inputs must come from somewhere, and\noccasionally that somewhere needs to change. Until a protocol \"ossifies\"\ninto its final form, improvements have to be coordinated somehow.\nSometimes, a protocol's community might think that they are\nready to ossify, but then the world throws a curveball that requires a\ncomplete and controversial restructuring. What happens if the US dollar\ncollapses, and RAI has to scramble to create and maintain their own\ndecentralized CPI index for their stablecoin to remain stable and\nrelevant? Here too, DeGov is necessary, and so avoiding it outright is\nnot a viable solution.\n\nOne important distinction is whether or not off-chain\ngovernance is possible. I have for a long time been a fan of off-chain governance\nwherever possible. And indeed, for base-layer blockchains, off-chain\ngovernance absolutely is possible. But for\napplication-layer projects, and especially defi projects, we run into\nthe problem that application-layer smart contract systems often\ndirectly control external assets, and that control cannot be\nforked away. If Tezos's on-chain governance gets captured by an\nattacker, the community can hard-fork away without any losses beyond\n(admittedly high) coordination costs. If MakerDAO's on-chain governance\ngets captured by an attacker, the community can absolutely spin up a new\nMakerDAO, but they will lose all the ETH and other assets that are stuck\nin the existing MakerDAO CDPs. Hence, while off-chain governance\nis a good solution for base layers and some application-layer\nprojects, many application-layer projects, particularly DeFi, will\ninevitably require formalized on-chain governance of some\nform.\n\n## DeGov is dangerous\n\nHowever, all current instantiations of decentralized governance come\nwith great risks. To followers of my writing, this discussion will not\nbe new; the risks are much the same as those that I talked about here, here and here. There are two\nprimary types of issues with coin voting that I worry about: (i)\ninequalities and incentive misalignments even in the absence of\nattackers, and (ii) outright attacks through various forms of (often\nobfuscated) vote buying. To the former, there have already been\nmany proposed mitigations (eg. delegation), and there will be more. But\nthe latter is a much more dangerous elephant in the room to which I see\nno solution within the current coin voting paradigm.\nProblems\nwith coin voting even in the absence of attackers\n\nThe problems with coin voting even without explicit attackers are\nincreasingly well-understood (eg. see this\nrecent piece by DappRadar and Monday Capital), and mostly fall into\na few buckets:\n\n- Small groups of wealthy participants (\"whales\") are better\nat successfully executing decisions than large groups of\nsmall-holders. This is because of the tragedy of the commons\namong small-holders: each small-holder has only an insignificant\ninfluence on the outcome, and so they have little incentive to not be\nlazy and actually vote. Even if there are rewards for voting, there is\nlittle incentive to research and think carefully about what they are\nvoting for.\n\n- Coin voting governance empowers coin holders and coin holder\ninterests at the expense of other parts of the community:\nprotocol communities are made up of diverse constituencies that have\nmany different values, visions and goals. Coin voting, however, only\ngives power to one constituency (coin holders, and especially wealthy\nones), and leads to over-valuing the goal of making the coin price go up\neven if that involves harmful rent extraction.\n\n- Conflict of interest issues: giving voting power to\none constituency (coin holders), and especially over-empowering wealthy\nactors in that constituency, risks over-exposure to the\nconflicts-of-interest within that particular elite (eg. investment funds\nor holders that also hold tokens of other DeFi platforms that\ninteract with the platform in question)\n\nThere is one major type of strategy being attempted for solving the\nfirst problem (and therefore also mitigating the third problem): delegation.\nSmallholders don't have to personally judge each decision; instead, they\ncan delegate to community members that they trust. This is an honorable\nand worthy experiment; we shall see how well delegation can mitigate the\nproblem.\n\nMy voting delegation page in the Gitcoin DAO\n\nThe problem of coin holder centrism, on the other hand, is\nsignificantly more challenging: coin holder centrism is inherently baked\ninto a system where coin holder votes are the only input. The\nmis-perception that coin holder centrism is an intended goal, and not a\nbug, is already causing confusion and harm; one (broadly excellent) article\ndiscussing blockchain public goods complains:\n\nCan crypto protocols be considered public goods if ownership is\nconcentrated in the hands of a few whales? Colloquially, these market\nprimitives are sometimes described as \"public infrastructure,\" but if\nblockchains serve a \"public\" today, it is primarily one of decentralized\nfinance. Fundamentally, these tokenholders share only one common object\nof concern: price.\n\nThe complaint is false; blockchains serve a public much richer and\nbroader than DeFi token holders. But our coin-voting-driven governance\nsystems are completely failing to capture that, and it seems difficult\nto make a governance system that captures that richness without a more\nfundamental change to the paradigm.\nCoin\nvoting's deep fundamental vulnerability to attackers: vote buying\n\nThe problems get much worse once determined attackers trying to\nsubvert the system enter the picture. The fundamental vulnerability of\ncoin voting is simple to understand. A token in a protocol with\ncoin voting is a bundle of two rights that are combined into a single\nasset: (i) some kind of economic interest in the protocol's revenue and\n(ii) the right to participate in governance. This combination is\ndeliberate: the goal is to align power and responsibility. But in fact,\nthese two rights are very easy to unbundle from each other.\nImagine a simple wrapper contract that has these rules: if you deposit 1\nXYZ into the contract, you get back 1 WXYZ. That WXYZ can be converted\nback into an XYZ at any time, plus in addition it accrues dividends.\nWhere do the dividends come from? Well, while the XYZ coins are inside\nthe wrapper contract, it's the wrapper contract that has the ability to\nuse them however it wants in governance (making proposals, voting on\nproposals, etc). The wrapper contract simply auctions off this right\nevery day, and distributes the profits among the original\ndepositors.\n\nAs an XYZ holder, is it in your interest to deposit your coins into\nthe contract? If you are a very large holder, it might not be; you like\nthe dividends, but you are scared of what a misaligned actor might do\nwith the governance power you are selling them. But if you are a smaller\nholder, then it very much is. If the governance power auctioned by the\nwrapper contract gets bought up by an attacker, you personally only\nsuffer a small fraction of the cost of the bad governance decisions that\nyour token is contributing to, but you personally gain the full benefit\nof the dividend from the governance rights auction. This situation is a\nclassic tragedy of the commons.\n\nSuppose that an attacker makes a decision that corrupts the DAO to\nthe attacker's benefit. The harm per participant from the decision\nsucceeding is \\(D\\), and the chance\nthat a single vote tilts the outcome is \\(p\\). Suppose an attacker makes a bribe of\n\\(B\\). The game chart looks like\nthis:\n\nDecision\nBenefit to you\nBenefit to others\n\nAccept attacker's bribe\n\\(B - D * p\\)\n\\(-999 * D * p\\)\n\nReject bribe, vote your conscience\n\\(0\\)\n\\(0\\)\n\nIf \\(B > D * p\\), you are\ninclined to accept the bribe, but as long as \\(B < 1000 * D * p\\), accepting the bribe\nis collectively harmful. So if \\(p\n< 1\\) (usually, \\(p\\) is far\nbelow \\(1\\)), there is an opportunity\nfor an attacker to bribe users to adopt a net-negative decision,\ncompensating each user far less than the harm they suffer.\n\nOne natural critique of voter bribing fears is: are voters\nreally going to be so immoral as to accept such obvious bribes?\nThe average DAO token holder is an enthusiast, and it would be hard for\nthem to feel good about so selfishly and blatantly selling out the\nproject. But what this misses is that there are much more obfuscated\nways to separate out profit sharing rights from governance rights, that\ndon't require anything remotely as explicit as a wrapper contract.\n\nThe simplest example is borrowing from a defi lending platform (eg.\nCompound). Someone who already\nholds ETH can lock up their ETH in a CDP (\"collateralized debt\nposition\") in one of these platforms, and once they do that the CDP\ncontract allows them to borrow an amount of XYZ up to eg. half the value\nof the ETH that they put in. They can then do whatever they want with\nthis XYZ. To recover their ETH, they would eventually need to pay back\nthe XYZ that they borrowed, plus interest.\n\nNote that throughout this process, the borrower has no financial\nexposure to XYZ. That is, if they use their XYZ to vote for a\ngovernance decision that destroys the value of XYZ, they do not lose a\npenny as a result. The XYZ they are holding is XYZ that they have to\neventually pay back into the CDP regardless, so they do not care if its\nvalue goes up or down. And so we have achieved unbundling: the\nborrower has governance power without economic interest, and the lender\nhas economic interest without governance power.\n\nThere are also centralized mechanisms for separating profit sharing\nrights from governance rights. Most notably, when users deposit their\ncoins on a (centralized) exchange, the exchange holds full custody of\nthose coins, and the exchange has the ability to use those coins to\nvote. This is not mere theory; there is evidence of exchanges using\ntheir users' coins in several DPoS systems. The most notable recent\nexample is the attempted\nhostile takeover of Steem, where exchanges used their customers'\ncoins to vote for some proposals that helped to cement a takeover of the\nSteem network that the bulk of the community strongly opposed. The\nsituation was only resolved through an outright mass exodus, where a\nlarge portion of the community moved to a different chain called Hive.\n\nSome DAO protocols are using timelock techniques to limit these\nattacks, requiring users to lock their coins and make them immovable for\nsome period of time in order to vote. These techniques can limit\nbuy-then-vote-then-sell attacks in the short term, but ultimately timelock\nmechanisms can be bypassed by users holding and voting with their\ncoins through a contract that issues a wrapped version of the token (or,\nmore trivially, a centralized exchange). As far as security\nmechanisms go, timelocks are more like a paywall on a newspaper website\nthan they are like a lock and key.\n\nAt present, many blockchains and DAOs with coin voting have so far\nmanaged to avoid these attacks in their most severe forms. There are\noccasional signs of attempted bribes:\n\nBut despite all of these important issues, there have been much fewer\nexamples of outright voter bribing, including obfuscated forms such as\nusing financial markets, that simple economic reasoning would suggest.\nThe natural question to ask is: why haven't more outright attacks\nhappened yet?\n\nMy answer is that the \"why not yet\" relies on three contingent\nfactors that are true today, but are likely to get less true over\ntime:\n\n- Community spirit from having a tightly-knit\ncommunity, where everyone feels a sense of camaraderie in a common tribe\nand mission..\n\n- High wealth concentration and coordination of token\nholders; large holders have higher ability to affect the\noutcome and have investments in long-term relationships with each other\n(both the \"old boys clubs\" of VCs, but also many other equally powerful\nbut lower-profile groups of wealthy token holders), and this makes them\nmuch more difficult to bribe.\n\n- Immature financial markets in governance tokens:\nready-made tools for making wrapper tokens exist\nin proof-of-concept forms but are not widely used, bribing contracts\nexist but\nare similarly immature, and liquidity in lending markets is low.\n\nWhen a small coordinated group of users holds over 50% of the coins,\nand both they and the rest are invested in a tightly-knit\ncommunity, and there are few tokens being lent out at reasonable rates,\nall of the above bribing attacks may perhaps remain theoretical. But\nover time, (1) and (3) will inevitably become less true no matter what\nwe do, and (2) must become less true if we want DAOs to become\nmore fair. When those changes happen, will DAOs remain safe? And if coin\nvoting cannot be sustainably resistant against attacks, then what\ncan?\nSolution 1: limited\ngovernance\n\nOne possible mitigation to the above issues, and one that is to\nvarying extents being tried already, is to put limits on what\ncoin-driven governance can do. There are a few ways to do this:\n\n- Use on-chain governance only for applications, not base\nlayers: Ethereum does this already, as the protocol itself is\ngoverned through off-chain governance, while DAOs and other apps on top\nof this are sometimes (but not always) governed through on-chain\ngovernance\n\n- Limit governance to fixed parameter choices:\nUniswap does this, as it only allows governance to affect (i) token\ndistribution and (ii) a 0.05% fee in the Uniswap exchange. Another great\nexample is RAI's\n\"un-governance\" roadmap, where governance has control over fewer and\nfewer features over time.\n\n- Add time delays: a governance decision made at time\nT only takes effect at eg. T + 90 days. This allows users and\napplications that consider the decision unacceptable to move to another\napplication (possibly a fork). Compound has a time delay\nmechanism in its governance, but in principle the delay can (and\neventually should) be much longer.\n\n- Be more fork-friendly: make it easier for users to\nquickly coordinate on and execute a fork. This makes the payoff of\ncapturing governance smaller.\n\nThe Uniswap case is particularly interesting: it's an intended\nbehavior that the on-chain governance funds teams, which may develop\nfuture versions of the Uniswap protocol, but it's up to users to\nopt-in to upgrading to those versions. This is a hybrid of on-chain\nand off-chain governance that leaves only a limited role for the\non-chain side.\n\nBut limited governance is not an acceptable solution by itself; those\nareas where governance is needed the most (eg. funds distribution for\npublic goods) are themselves among the most vulnerable to attack. Public\ngoods funding is so vulnerable to attack because there is a very direct\nway for an attacker to profit from bad decisions: they can try to push\nthrough a bad decision that sends funds to themselves. Hence, we also\nneed techniques to improve governance itself...\nSolution 2:\nnon-coin-driven governance\n\nA second approach is to use forms of governance that are not\ncoin-voting-driven. But if coins do not determine what weight an account\nhas in governance, what does? There are two natural alternatives:\n\n- Proof of personhood systems: systems that verify\nthat accounts correspond to unique individual humans, so that governance\ncan assign one vote per human. See here for a review of some\ntechniques being developed, and ProofOfHumanity and BrightID and Idenanetwork for three\nattempts to implement this.\n\n- Proof of participation: systems that attest to the\nfact that some account corresponds to a person that has participated in\nsome event, passed some educational training, or performed some useful\nwork in the ecosystem. See POAP for one\nattempt to implement thus.\n\nThere are also hybrid possibilities: one example is quadratic voting, which\nmakes the power of a single voter proportional to the square\nroot of the economic resources that they commit to a decision.\nPreventing people from gaming the system by splitting their resource\nacross many identities requires proof of personhood, and the\nstill-existent financial component allows participants to credibly\nsignal how strongly they care about an issue, as well as how strongly\nthey care about the ecosystem. Gitcoin quadratic funding is a form of\nquadratic voting, and quadratic voting DAOs are being\nbuilt.\n\nProof of participation is less well-understood. The key challenge is\nthat determining what counts as how much participation itself requires a\nquite robust governance structure. It's possible that the easiest\nsolution involves bootstrapping the system with a hand-picked choice of\n10-100 early contributors, and then decentralizing over time as the\nselected participants of round N determine participation criteria for\nround N+1. The possibility of a fork helps provide a path to recovery\nfrom, and an incentive against, governance going off the rails.\n\nProof of personhood and proof of participation both require some form\nof anti-collusion (see article explaining the issue\nhere and MACI\ndocumentation here) to ensure that the non-money resource being used\nto measure voting power remains non-financial, and does not itself end\nup inside of smart contracts that sell the governance power to the\nhighest bidder.\n\n## Solution 3: skin in the game\n\nThe third approach is to break the tragedy of the commons, by\nchanging the rules of the vote itself. Coin voting fails because\nwhile voters are collectively accountable for their decisions\n(if everyone votes for a terrible decision, everyone's coins drop to\nzero), each voter is not individually accountable (if a\nterrible decision happens, those who supported it suffer no more than\nthose who opposed it). Can we make a voting system that changes this\ndynamic, and makes voters individually, and not just collectively,\nresponsible for their decisions?\n\nFork-friendliness is arguably a skin-in-the-game strategy, if forks\nare done in the way that Hive forked from Steem. In the case that a\nruinous governance decision succeeds and can no longer be opposed inside\nthe protocol, users can take it upon themselves to make a fork.\nFurthermore, in that fork, the coins that voted for the bad decision can\nbe destroyed.\n\nThis sounds harsh, and perhaps it even feels like a violation of an\nimplicit norm that the \"immutability of the ledger\" should remain\nsacrosanct when forking a coin. But the idea seems much more reasonable\nwhen seen from a different perspective. We keep the idea of a strong\nfirewall where individual coin balances are expected to be inviolate,\nbut only apply that protection to coins that do not participate in\ngovernance. If you participate in governance, even indirectly by\nputting your coins into a wrapper mechanism, then you may be held liable\nfor the costs of your actions.\n\nThis creates individual responsibility: if an attack happens,\nand your coins vote for the attack, then your coins are\ndestroyed. If your coins do not vote for the attack, your coins\nare safe. The responsibility propagates upward: if you put your coins\ninto a wrapper contract and the wrapper contract votes for an attack,\nthe wrapper contract's balance is wiped and so you lose your coins. If\nan attacker borrows XYZ from a defi lending platform, when the platform\nforks anyone who lent XYZ loses out (note that this makes lending the\ngovernance token in general very risky; this is an intended\nconsequence).\nSkin-in-the-game in\nday-to-day voting\n\nBut the above only works for guarding against decisions that are\ntruly extreme. What about smaller-scale heists, which unfairly favor\nattackers manipulating the economics of the governance but not severely\nenough to be ruinous? And what about, in the absence of any attackers at\nall, simple laziness, and the fact that coin-voting governance has no\nselection pressure in favor of higher-quality opinions?\n\nThe most popular solution to these kinds of issues is futarchy,\nintroduced by Robin Hanson in the early 2000s. Votes become bets: to\nvote in favor of a proposal, you make a bet that the proposal will lead\nto a good outcome, and to vote against the proposal, you make a bet that\nthe proposal will lead to a poor outcome. Futarchy introduces individual\nresponsibility for obvious reasons: if you make good bets, you get more\ncoins, and if you make bad bets you lose your coins.\n\n\"Pure\" futarchy has proven difficult to introduce, because in\npractice objective functions are very difficult to define (it's not just\ncoin price that people want!), but various hybrid forms of futarchy may\nwell work. Examples of hybrid futarchy include:\n\n- Votes as buy orders: see ethresear.ch\npost. Voting in favor of a proposal requires making an enforceable\nbuy order to buy additional tokens at a price somewhat lower than the\ntoken's current price. This ensures that if a terrible decision\nsucceeds, those who support it may be forced to buy their opponents out,\nbut it also ensures that in more \"normal\" decisions coin holders have\nmore slack to decide according to non-price criteria if they so\nwish.\n\n- Retroactive public goods funding: see post\nwith the Optimism team. Public goods are funded by some voting\nmechanism retroactively, after they have already achieved a\nresult. Users can buy project tokens to fund their project\nwhile signaling confidence in it; buyers of project tokens get a share\nof the reward if that project is deemed to have achieved a desired\ngoal.\n\n- Escalation games: see Augur and Kleros.\nValue-alignment on lower-level decisions is incentivized by the\npossibility to appeal to a higher-effort but higher-accuracy\nhigher-level process; voters whose votes agree with the ultimate\ndecision are rewarded.\n\nIn the latter two cases, hybrid futarchy depends on some form of\nnon-futarchy governance to measure against the objective function or\nserve as a dispute layer of last resort. However, this non-futarchy\ngovernance has several advantages that it does not if used directly: (i)\nit activates later, so it has access to more information, (ii) it is\nused less frequently, so it can expend less effort, and (iii) each use\nof it has greater consequences, so it's more acceptable to just rely on\nforking to align incentives for this final layer.\n\n## Hybrid solutions\n\nThere are also solutions that combine elements of the above\ntechniques. Some possible examples:\n\n- Time delays plus elected-specialist governance:\nthis is one possible solution to the ancient conundrum of how to make an\ncrypto-collateralized stablecoin whose locked funds can exceed the value\nof the profit-taking token without risking governance capture. The\nstable coin uses a price oracle constructed from the median of values\nsubmitted by N (eg. N = 13) elected providers. Coin voting chooses the\nproviders, but it can only cycle out one provider each week. If users\nnotice that coin voting is bringing in untrustworthy price providers,\nthey have N/2 weeks before the stablecoin breaks to switch to a\ndifferent one.\n\n- Futarchy + anti-collusion = reputation: Users vote\nwith \"reputation\", a token that cannot be transferred. Users gain more\nreputation if their decisions lead to desired results, and lose\nreputation if their decisions lead to undesired results. See here\nfor an article advocating for a reputation-based scheme.\n\n- Loosely-coupled (advisory) coin votes: a coin vote\ndoes not directly implement a proposed change, instead it simply exists\nto make its outcome public, to build legitimacy for off-chain\ngovernance to implement that change. This can provide the benefits of\ncoin votes, with fewer risks, as the legitimacy of a coin vote drops off\nautomatically if evidence emerges that the coin vote was bribed or\notherwise manipulated.\n\nBut these are all only a few possible examples. There is much more\nthat can be done in researching and developing non-coin-driven\ngovernance algorithms. The most important thing that can be done\ntoday is moving away from the idea that coin voting is the only\nlegitimate form of governance decentralization. Coin voting is\nattractive because it feels credibly neutral: anyone can go and\nget some units of the governance token on Uniswap. In practice, however,\ncoin voting may well only appear secure today precisely because\nof the imperfections in its neutrality (namely, large portions\nof the supply staying in the hands of a tightly-coordinated clique of\ninsiders).\n\nWe should stay very wary of the idea that current forms of coin\nvoting are \"safe defaults\". There is still much that remains to be seen\nabout how they function under conditions of more economic stress and\nmature ecosystems and financial markets, and the time is now to start\nsimultaneously experimenting with alternatives.",
    "contentLength": 31802,
    "summary": "Vitalik argues that cryptocurrency governance must move beyond coin voting, which enables wealthy \"whales\" and vote-buying attacks to dominate decisions.",
    "detailedSummary": {
      "theme": "Vitalik argues that while decentralized governance is necessary for blockchain ecosystems, current coin voting mechanisms are fundamentally flawed and dangerous, requiring new governance models that move beyond token-weighted voting.",
      "summary": "Vitalik identifies decentralized governance (DeGov) as both necessary and dangerous for blockchain ecosystems. He argues it's necessary for two key reasons: funding public goods (like protocol research and development) and managing protocol maintenance and upgrades. Current systems show massive imbalances, with billions going to network security while only tens of millions fund development. However, Vitalik warns that coin voting governance is deeply vulnerable to attack through vote buying, wealth concentration among whales, and the ability to separate economic interests from governance rights through financial instruments like lending protocols and exchanges. He demonstrates how attackers can profitably bribe voters through various mechanisms, including simple wrapper contracts that auction off voting rights while distributing economic benefits to token holders. Vitalik proposes three main solution categories: limited governance (restricting what governance can control), non-coin-driven governance (using proof of personhood, proof of participation, or quadratic voting), and skin-in-the-game mechanisms (making voters individually accountable for their decisions through systems like futarchy). He emphasizes that the current apparent safety of coin voting may only exist due to tight community coordination and immature financial markets, conditions that will inevitably change over time.",
      "takeaways": [
        "Decentralized governance is essential for funding public goods and protocol maintenance, but current coin voting systems are fundamentally vulnerable to attack",
        "Vote buying attacks can occur through sophisticated financial mechanisms like lending protocols and wrapper contracts, not just direct bribes",
        "The apparent current safety of many DAOs relies on temporary factors: tight community bonds, wealth concentration, and immature financial markets",
        "Three main solution approaches exist: limiting governance scope, using non-coin-driven voting mechanisms, and implementing skin-in-the-game accountability",
        "Moving beyond coin voting is urgent because the blockchain space needs governance systems that can withstand mature, adversarial financial environments"
      ],
      "controversial": [
        "The suggestion that coins used for governance voting could be destroyed in forks as punishment for supporting bad decisions",
        "The argument that current coin voting safety relies on wealth concentration and insider coordination, which contradicts decentralization goals",
        "The claim that proof of personhood systems could be a viable alternative to coin voting despite their current technical limitations"
      ]
    }
  },
  {
    "id": "general-2021-07-29-gini",
    "title": "Against overuse of the Gini coefficient",
    "date": "2021-07-29",
    "category": "general",
    "url": "https://vitalik.eth.limo/general/2021/07/29/gini.html",
    "path": "general/2021/07/29/gini.html",
    "content": "- \n\n- \n\n  \n    \n\n  \n  \n  Dark Mode Toggle\n  \n    \n\n  \n\n##  Against overuse of the Gini coefficient \n\n 2021 Jul 29 \nSee all posts\n\n \n \n\n Against overuse of the Gini coefficient \n\nSpecial thanks to Barnabe Monnot and Tina Zhen for feedback and\nreview\n\nThe Gini\ncoefficient (also called the Gini index) is by far the most popular\nand widely known measure of inequality, typically used to measure\ninequality of income or wealth in some country, territory or other\ncommunity. It's popular because it's easy to understand, with a\nmathematical definition that can easily be visualized on a graph.\n\nHowever, as one might expect from any scheme that tried to reduce\ninequality to a single number, the Gini coefficient also has its limits.\nThis is true even in its original context of measuring income and wealth\ninequality in countries, but it becomes even more true when the Gini\ncoefficient is transplanted into other contexts (particularly:\ncryptocurrency). In this post I will talk about some of the limits of\nthe Gini coefficient, and propose some alternatives.\n\n## What is the Gini coefficient?\n\nThe Gini\ncoefficient is a measure of inequality introduced by Corrado Gini in\n1912. It is typically used to measure inequality of income and wealth of\ncountries, though it is also increasingly being used in other\ncontexts.\n\nThere are two equivalent definitions of the Gini coefficient:\n\nArea-above-curve definition: draw the graph of a\nfunction, where \\(f(p)\\) equals the\nshare of total income earned by the lowest-earning portion of the\npopulation (eg. \\(f(0.1)\\) is the share\nof total income earned by the lowest-earning 10%). The Gini coefficient\nis the area between that curve and the \\(y=x\\) line, as a portion of the whole\ntriangle:\n\n \n\nAverage-difference definition: the Gini coefficient\nis half the average difference of incomes between each all possible\npairs of individuals, divided by the mean income.\n\nFor example, in the above example chart, the four incomes are\n[1, 2, 4, 8], so the 16 possible differences are\n[0, 1, 3, 7, 1, 0, 2, 6, 3, 2, 0, 4, 7, 6, 4, 0]. Hence the\naverage difference is 2.875 and the mean income is 3.75, so Gini = \\(\\frac{2.875}{2 * 3.75} \\approx\n0.3833\\).\n\nIt turns out that the two are mathematically equivalent (proving this\nis an exercise to the reader)!\nWhat's wrong with the\nGini coefficient?\n\nThe Gini coefficient is attractive because it's a reasonably simple\nand easy-to-understand statistic. It might not look simple, but\ntrust me, pretty much everything in statistics that deals with\npopulations of arbitrary size is that bad, and often much worse. Here,\nstare at the formula of something as basic as the standard\ndeviation:\n\n\\(\\sigma = \\frac{\\sum_{i=1}^n x_i^2}{n} -\n(\\frac{\\sum_{i=1}^n x_i}{n})^2\\)\n\nAnd here's the Gini:\n\n\\(G = \\frac{2 * \\sum_{i=1}^n i*x_i}{n *\n\\sum_{i=1}^n x_i} - \\frac{n+1}{n}\\)\n\nIt's actually quite tame, I promise!\n\nSo, what's wrong with it? Well, there are lots of things wrong with\nit, and people have written\nlots of articles\nabout various\nproblems with the Gini coefficient. In this article, I will focus on\none specific problem that I think is under-discussed about the Gini as a\nwhole, but that has particular relevance to analyzing inequality in\ninternet communities such as blockchains. The Gini coefficient\ncombines together into a single inequality index two problems that\nactually look quite different: suffering due to lack of resources and\nconcentration of power.\n\nTo understand the difference between the two problems more clearly,\nlet's look at two dystopias:\n\n- Dystopia A: half the population equally shares all\nthe resources, everyone else has none\n\n- Dystopia B: one person has half of all the\nresources, everyone else equally shares the remaining half\n\nHere are the Lorenz curves (fancy charts like we saw above) for both\ndystopias:\n\n \n\nClearly, neither of those two dystopias are good places to live.\nBut they are not-very-nice places to live in very different\nways. Dystopia A gives each resident a coin flip between\nunthinkably horrific mass starvation if they end up on the left half on\nthe distribution and egalitarian harmony if they end up on the right\nhalf. If you're Thanos, you might\nactually like it! If you're not, it's worth avoiding with the strongest\nforce. Dystopia B, on the other hand, is Brave New World-like: everyone\nhas decently good lives (at least at the time when that snapshot of\neveryone's resources is taken), but at the high cost of an extremely\nundemocratic power structure where you'd better hope you have a good\noverlord. If you're Curtis Yarvin,\nyou might actually like it! If you're not, it's very much worth avoiding\ntoo.\n\nThese two problems are different enough that they're worth analyzing\nand measuring separately. And this difference is not just theoretical.\nHere is a chart showing share of total income earned by the bottom 20%\n(a decent proxy for avoiding dystopia A) versus share of total income\nearned by the top 1% (a decent proxy for being near dystopia B):\n\nSources: https://data.worldbank.org/indicator/SI.DST.FRST.20\n(merging 2015 and 2016 data) and http://hdr.undp.org/en/indicators/186106.\n\nThe two are clearly correlated (coefficient -0.62), but very far from\nperfectly correlated (the high priests of statistics apparently\nconsider 0.7 to be the lower threshold for being \"highly\ncorrelated\", and we're even under that). There's an interesting second\ndimension to the chart that can be analyzed - what's the difference\nbetween a country where the top 1% earn 20% of the total income and the\nbottom 20% earn 3% and a country where the top 1% earn 20% and the\nbottom 20% earn 7%? Alas, such an exploration is best left to other\nenterprising data and culture explorers with more experience than\nmyself.\nWhy\nGini is very problematic in non-geographic communities (eg.\ninternet/crypto communities)\n\nWealth concentration within the blockchain space in particular is an\nimportant problem, and it's a problem worth measuring and understanding.\nIt's important for the blockchain space as a whole, as many people (and\nUS senate hearings) are trying to figure out to what extent crypto\nis truly anti-elitist and to what extent it's just replacing old elites\nwith new ones. It's also important when comparing different\ncryptocurrencies with each other.\n\nShare of coins explicitly allocated to specific insiders in a\ncryptocurrency's initial supply is one type of inequality. Note that the\nEthereum data is slightly wrong: the insider and foundation shares\nshould be 12.3% and 4.2%, not 15% and 5%. \n\nGiven the level of concern about these issues, it should be not at\nall surprising that many people have tried computing Gini indices of\ncryptocurrencies:\n\n- The\nobserved Gini index for staked EOS tokens (2018)\n\n- Gini\ncoefficients of cryptocurrencies (2018)\n\n- Measuring\ndecentralization in Bitcoin and Ethereum using Multiple Metrics and\nGranularities (2021, includes Gini and 2 other metrics)\n\n- Nouriel\nRoubini comparing Bitcoin's Gini to North Korea (2018)\n\n- On-chain\nInsights in the Cryptocurrency Markets (2021, uses Gini to measure\nconcentration)\n\nAnd even earlier than that, we had to deal with this\nsensationalist article from 2014:\n\nIn addition to common plain methodological mistakes (often either\nmixing up income vs wealth inequality, mixing up users vs accounts, or\nboth) that such analyses make quite frequently, there is a deep and\nsubtle problem with using the Gini coefficient to make these kinds of\ncomparisons. The problem lies in key distinction between typical\ngeographic communities (eg. cities, countries) and typical internet\ncommunities (eg. blockchains):\n\nA typical resident of a geographic community spends most of their\ntime and resources in that community, and so measured inequality in a\ngeographic community reflects inequality in total resources available to\npeople. But in an internet community, measured inequality can\ncome from two sources: (i) inequality in total resources available to\ndifferent participants, and (ii) inequality in level of\ninterest in participating in the community.\n\nThe average person with $15 in fiat currency is poor and is missing\nout on the ability to have a good life. The average person with $15 in\ncryptocurrency is a dabbler who opened up a wallet once for fun.\nInequality in level of interest is a healthy thing; every community has\nits dabblers and its full-time hardcore fans with no life. So if a\ncryptocurrency has a very high Gini coefficient, but it turns out that\nmuch of this inequality comes from inequality in level of interest, then\nthe number points to a much less scary reality than the headlines\nimply.\n\nCryptocurrencies, even those that turn out to be highly plutocratic,\nwill not turn any part of the world into anything close to dystopia A.\nBut badly-distributed cryptocurrencies may well look like dystopia B, a\nproblem compounded if coin\nvoting governance is\nused to make protocol decisions. Hence, to detect the problems that\ncryptocurrency communities worry about most, we want a metric that\ncaptures proximity to dystopia B more specifically.\nAn\nalternative: measuring dystopia A problems and dystopia B problems\nseparately\n\nAn alternative approach to measuring inequality involves directly\nestimating suffering from resources being unequally distributed (that\nis, \"dystopia A\" problems). First, start with some utility function\nrepresenting the value of having a certain amount of money. \\(log(x)\\) is popular, because it captures\nthe intuitively appealing approximation that doubling one's income is\nabout as useful at any level: going from $10,000 to $20,000 adds the\nsame utility as going from $5,000 to $10,000 or from $40,000 to\n$80,000). The score is then a matter of measuring how much utility is\nlost compared to if everyone just got the average income:\n\n\\(log(\\frac{\\sum_{i=1}^n x_i}{n}) -\n\\frac{\\sum_{i=1}^n log(x_i)}{n}\\)\n\nThe first term (log-of-average) is the utility that everyone would\nhave if money were perfectly redistributed, so everyone earned the\naverage income. The second term (average-of-log) is the average utility\nin that economy today. The difference represents lost utility from\ninequality, if you look narrowly at resources as something used for\npersonal consumption. There are other ways to define this formula, but\nthey end up being close to equivalent (eg. the 1969\npaper by Anthony Atkinson suggested an \"equally distributed\nequivalent level of income\" metric which, in the \\(U(x) = log(x)\\) case, is just a monotonic\nfunction of the above, and the Theil L index is\nperfectly mathematically equivalent to the above formula).\n\nTo measure concentration (or \"dystopia B\" problems), the Herfindahl-Hirschman\nindex is an excellent place to start, and is already used to measure\neconomic concentration in industries:\n\n\\(\\frac{\\sum_{i=1}^n x_i^2}{(\\sum_{i=1}^n\nx_i)^2}\\)\n\nOr for you visual learners out there:\n\n Herfindahl-Hirschman index: green area divided by total area.\n\nThere are other alternatives to this; the Theil T index has\nsome similar properties though also some differences. A\nsimpler-and-dumber alternative is the Nakamoto coefficient: the minimum\nnumber of participants needed to add up to more than 50% of the total.\nNote that all three of these concentration indices focus heavily on what\nhappens near the top (and deliberately so): a large number of dabblers\nwith a small quantity of resources contributes little or nothing to the\nindex, while the act of two top participants merging can make a very big\nchange to the index.\n\nFor cryptocurrency communities, where concentration of resources is\none of the biggest risks to the system but where someone only having\n0.00013 coins is not any kind of evidence that they're actually\nstarving, adopting indices like this is the obvious approach. But even\nfor countries, it's probably worth talking about, and measuring,\nconcentration of power and suffering from lack of resources more\nseparately.\n\nThat said, at some point we have to move beyond even these\nindices. The harms from concentration are not just a function\nof the size of the actors; they are also heavily dependent on the\nrelationships between the actors and their ability to collude with\neach other. Similarly, resource allocation is network-dependent: lack of\nformal resources may not be that harmful if the person lacking resources\nhas an informal network to tap into. But dealing with these issues is a\nmuch harder challenge, and so we do also need the simpler tools while we\nstill have less data to work with.",
    "contentLength": 12483,
    "summary": "The Gini coefficient wrongly combines suffering-from-lack-of-resources with power-concentration into one measure, making it problematic for crypto.",
    "detailedSummary": {
      "theme": "Vitalik argues that the Gini coefficient is inadequate for measuring inequality, especially in cryptocurrency communities, and proposes alternative metrics that separate resource scarcity from power concentration.",
      "summary": "Vitalik critiques the widespread use of the Gini coefficient as a measure of inequality, arguing that it conflates two distinct problems: suffering from lack of resources ('Dystopia A') and concentration of power ('Dystopia B'). He illustrates this with examples showing how different distributions can have similar Gini coefficients while representing fundamentally different societal problems. Vitalik emphasizes that this limitation becomes particularly problematic when analyzing cryptocurrency communities, where inequality may reflect differences in participation interest rather than actual resource deprivation. Unlike geographic communities where residents depend primarily on local resources, internet communities include many casual participants or 'dabblers' who hold small amounts not due to poverty but due to limited engagement. For cryptocurrency analysis, Vitalik proposes using separate metrics: utility-based measures (like log-based formulas) to assess resource distribution problems, and concentration indices (like the Herfindahl-Hirschman index or Nakamoto coefficient) to evaluate power concentration risks. He argues these targeted approaches better capture the specific concerns relevant to blockchain governance and plutocracy risks.",
      "takeaways": [
        "The Gini coefficient inappropriately combines two distinct problems: resource scarcity and power concentration, which require different analytical approaches",
        "In cryptocurrency communities, high inequality often reflects varying levels of participation interest rather than actual resource deprivation, making traditional inequality metrics misleading",
        "Geographic communities and internet communities have fundamentally different inequality dynamics that require different measurement approaches",
        "Concentration indices like the Herfindahl-Hirschman index or Nakamoto coefficient are better suited for measuring power concentration risks in decentralized systems",
        "Utility-based measures using logarithmic functions can better capture actual welfare impacts of resource distribution than the Gini coefficient"
      ],
      "controversial": [
        "Vitalik's dismissal of resource inequality concerns in crypto communities could be seen as minimizing legitimate wealth concentration issues",
        "The argument that crypto inequality primarily reflects 'dabbling' rather than systematic exclusion may overlook barriers to meaningful participation"
      ]
    }
  },
  {
    "id": "general-2021-06-18-verkle",
    "title": "Verkle trees",
    "date": "2021-06-18",
    "category": "protocol",
    "url": "https://vitalik.eth.limo/general/2021/06/18/verkle.html",
    "path": "general/2021/06/18/verkle.html",
    "content": "- \n\n- \n\n  \n    \n\n  \n  \n  Dark Mode Toggle\n  \n    \n\n  \n\n##  Verkle trees \n\n 2021 Jun 18 \nSee all posts\n\n \n \n\n Verkle trees \n\nSpecial thanks to Dankrad Feist and Justin Drake for feedback and\nreview.\n\nVerkle trees are shaping up to be an important part of Ethereum's\nupcoming scaling upgrades. They serve the same function as Merkle trees:\nyou can put a large amount of data into a Verkle tree, and make a short\nproof (\"witness\") of any single piece, or set of pieces, of that data\nthat can be verified by someone who only has the root of the tree. The\nkey property that Verkle trees provide, however, is that they are\nmuch more efficient in proof size. If a tree contains a billion\npieces of data, making a proof in a traditional binary Merkle tree would\nrequire about 1 kilobyte, but in a Verkle tree the proof would be\nless than 150 bytes - a reduction sufficient to make stateless\nclients finally viable in practice.\n\nVerkle trees are still a new idea; they were first introduced by John\nKuszmaul in this\npaper from 2018, and they are still not as widely known as many\nother important new cryptographic constructions. This post will explain\nwhat Verkle trees are and how the cryptographic magic behind them works.\nThe price of their short proof size is a higher level of dependence on\nmore complicated cryptography. That said, the cryptography still much\nsimpler, in my opinion, than the advanced cryptography found in modern ZK SNARK schemes. In this post I'll do\nthe best job that I can at explaining it.\nMerkle Patricia\nvs Verkle Tree node structure\n\nIn terms of the structure of the tree (how the nodes in the\ntree are arranged and what they contain), a Verkle tree is very similar\nto the Merkle\nPatricia tree currently used in Ethereum. Every node is either (i)\nempty, (ii) a leaf node containing a key and value, or (iii) an\nintermediate node that has some fixed number of children (the \"width\" of\nthe tree). The value of an intermediate node is computed as a hash of\nthe values of its children.\n\nThe location of a value in the tree is based on its key: in the\ndiagram below, to get to the node with key 4cc, you start\nat the root, then go down to the child at position 4, then\ngo down to the child at position c (remember:\nc = 12 in hexadecimal), and then go down again to the child\nat position c. To get to the node with key\nbaaa, you go to the position-b child of the\nroot, and then the position-a child of that node.\nThe node at path (b,a) directly contains the node with key\nbaaa, because there are no other keys in the tree starting\nwith ba.\n\nThe structure of nodes in a hexary (16 children per parent)\nVerkle tree, here filled with six (key, value)\npairs.\n\nThe only real difference in the structure of Verkle trees\nand Merkle Patricia trees is that Verkle trees are wider in practice.\nMuch wider. Patricia trees are at their most efficient when\nwidth = 2 (so Ethereum's hexary Patricia tree is actually\nquite suboptimal). Verkle trees, on the other hand, get shorter and\nshorter proofs the higher the width; the only limit is that if width\ngets too high, proofs start to take too long to create. The Verkle tree\nproposed for Ethereum has a width of 256, and some even favor\nraising it to 1024 (!!).\n\n## Commitments and proofs\n\nIn a Merkle tree (including Merkle Patricia trees), the proof of a\nvalue consists of the entire set of sister nodes: the proof\nmust contain all nodes in the tree that share a parent with any\nof the nodes in the path going down to the node you are trying to prove.\nThat may be a little complicated to understand, so here's a picture of a\nproof for the value in the 4ce position. Sister nodes that\nmust be included in the proof are highlighted in red.\n\nThat's a lot of nodes! You need to provide the sister nodes at each\nlevel, because you need the entire set of children of a node to compute\nthe value of that node, and you need to keep doing this until you get to\nthe root. You might think that this is not that bad because most of the\nnodes are zeroes, but that's only because this tree has very few nodes.\nIf this tree had 256 randomly-allocated nodes, the top layer would\nalmost certainly have all 16 nodes full, and the second layer would on\naverage be ~63.3% full.\n\nIn a Verkle tree, on the other hand, you do not need to\nprovide sister nodes; instead, you just provide the path, with a little\nbit extra as a proof. This is why Verkle trees benefit from\ngreater width and Merkle Patricia trees do not: a tree with greater\nwidth leads to shorter paths in both cases, but in a Merkle Patricia\ntree this effect is overwhelmed by the higher cost of needing to provide\nall the width - 1 sister nodes per level in a proof. In a\nVerkle tree, that cost does not exist.\n\nSo what is this little extra that we need as a proof? To understand\nthat, we first need to circle back to one key detail: the hash function\nused to compute an inner node from its children is not a regular hash.\nInstead, it's a vector commitment.\n\nA vector commitment scheme is a special type of hash function,\nhashing a list \\(h(z_1, z_2 ... z_n)\n\\rightarrow C\\). But vector commitments have the special property\nthat for a commitment \\(C\\) and a value\n\\(z_i\\), it's possible to make a short\nproof that \\(C\\) is the commitment to\nsome list where the value at the i'th position is \\(z_i\\). In a Verkle proof, this short proof\nreplaces the function of the sister nodes in a Merkle Patricia proof,\ngiving the verifier confidence that a child node really is the child at\nthe given position of its parent node.\n\nNo sister nodes required in a proof of a value in the tree;\njust the path itself plus a few short proofs to link each commitment in\nthe path to the next.\n\nIn practice, we use a primitive even more powerful than a vector\ncommitment, called a polynomial commitment. Polynomial\ncommitments let you hash a polynomial, and make a proof for the\nevaluation of the hashed polynomial at any point. You can use\npolynomial commitments as vector commitments: if we agree on a set of\nstandardized coordinates \\((c_1, c_2 ...\nc_n)\\), given a list \\((y_1, y_2 ...\ny_n)\\) you can commit to the polynomial \\(P\\) where \\(P(c_i) = y_i\\) for all \\(i \\in [1..n]\\) (you can find this\npolynomial with Lagrange\ninterpolation). I talk about polynomial commitments at length in my\narticle on ZK-SNARKs. The two polynomial commitment schemes that are\nthe easiest to use are KZG\ncommitments and bulletproof-style\ncommitments (in both cases, a commitment is a single 32-48 byte\nelliptic curve point). Polynomial commitments give us more flexibility\nthat lets us improve efficiency, and it just so happens that the\nsimplest and most efficient vector commitments available are\nthe polynomial commitments.\n\nThis scheme is already very powerful as it is: if you use a\nKZG commitment and proof, the proof size is 96 bytes per intermediate\nnode, nearly 3x more space-efficient than a simple Merkle proof\nif we set width = 256. However, it turns out that we can increase\nspace-efficiency even further.\n\n## Merging the proofs\n\nInstead of requiring one proof for each commitment along the path,\nby using the extra properties of polynomial commitments we can\nmake a single fixed-size proof that proves all parent-child\nlinks between commitments along the paths for an unlimited number of\nkeys. We do this using a scheme\nthat implements multiproofs through random evaluation.\n\nBut to use this scheme, we first need to convert the problem into a\nmore structured one. We have a proof of one or more values in a Verkle\ntree. The main part of this proof consists of the intermediary nodes\nalong the path to each node. For each node that we provide, we also have\nto prove that it actually is the child of the node above it (and in the\ncorrect position). In our single-value-proof example above, we needed\nproofs to prove:\n\n- That the key: 4ce node actually is the\nposition-e child of the prefix: 4c\nintermediate node.\n\n- That the prefix: 4c intermediate node actually is the\nposition-c child of the prefix: 4 intermediate\nnode.\n\n- That the prefix: 4 intermediate node actually is the\nposition-4 child of the root\n\nIf we had a proof proving multiple values (eg. both 4ce\nand 420), we would have even more nodes and even more\nlinkages. But in any case, what we are proving is a sequence of\nstatements of the form \"node A actually is the position-i child of node\nB\". If we are using polynomial commitments, this turns into\nequations: \\(A(x_i) = y\\), where \\(y\\) is the hash of the commitment to \\(B\\).\n\nThe details of this proof are technical and better explained\nby Dankrad Feist than myself. By far the bulkiest and time-consuming\nstep in the proof generation involves computing a polynomial \\(g\\) of the form:\n\n\\(g(X) = r^0\\frac{A_0(X) - y_0}{X - x_0} +\nr^1\\frac{A_1(X) - y_1}{X - x_1} + ... + r^n\\frac{A_n(X) - y_n}{X -\nx_n}\\)\n\nIt is only possible to compute each term \\(r^i\\frac{A_i(X) - y_i}{X - x_i}\\) if that\nexpression is a polynomial (and not a fraction). And that requires \\(A_i(X)\\) to equal \\(y_i\\) at the point \\(x_i\\).\n\nWe can see this with an example. Suppose:\n\n- \\(A_i(X) = X^2 + X + 3\\)\n\n- We are proving for \\((x_i = 2, y_i =\n9)\\). \\(A_i(2)\\) does equal\n\\(9\\) so this will work.\n\n\\(A_i(X) - 9 = X^2 + X - 6\\), and\n\\(\\frac{X^2 + X - 6}{X - 2}\\) gives a\nclean \\(X - 3\\). But if we tried to fit\nin \\((x_i = 2, y_i = 10)\\), this would\nnot work; \\(X^2 + X - 7\\)\ncannot be cleanly divided by \\(X -\n2\\) without a fractional remainder.\n\nThe rest of the proof involves providing a polynomial commitment to\n\\(g(X)\\) and then proving that the\ncommitment is actually correct. Once again, see Dankrad's\nmore technical description for the rest of the proof.\n\nOne single proof proves an unlimited number of parent-child\nrelationships.\n\nAnd there we have it, that's what a maximally efficient Verkle proof\nlooks like.\nKey properties\nof proof sizes using this scheme\n\n- Dankrad's multi-random-evaluation proof allows the prover to\nprove an arbitrary number of evaluations \\(A_i(x_i) = y_i\\), given\ncommitments to each \\(A_i\\) and the\nvalues that are being proven. This proof is constant\nsize (one polynomial commitment, one number, and two proofs;\n128-1000 bytes depending on what scheme is being used).\n\n- The \\(y_i\\) values do not\nneed to be provided explicitly, as they can be directly\ncomputed from the other values in the Verkle proof: each \\(y_i\\) is itself the hash of the next value\nin the path (either a commitment or a leaf).\n\n- The \\(x_i\\) values also do\nnot need to be provided explicitly, since the paths (and hence\nthe \\(x_i\\) values) can be computed\nfrom the keys and the coordinates derived from the paths.\n\n- Hence, all we need is the leaves (keys and values) that we\nare proving, as well as the commitments along the path from each leaf to\nthe root.\n\n- Assuming a width-256 tree, and \\(2^{32}\\) nodes, a proof would require the\nkeys and values that are being proven, plus (on average) three\ncommitments for each value along the path from that value to\nthe root.\n\n- If we are proving many values, there are further\nsavings: no matter how many values you are proving, you will\nnot need to provide more than the 256 values at the top level.\n\nProof\nsizes (bytes). Rows: tree size, cols: key/value pairs proven\n\n1\n10\n100\n1,000\n10,000\n\n256\n176\n176\n176\n176\n176\n\n65,536\n224\n608\n4,112\n12,176\n12,464\n\n16,777,216\n272\n1,040\n8,864\n59,792\n457,616\n\n4,294,967,296\n320\n1,472\n13,616\n107,744\n937,472\n\nAssuming width 256, and 48-byte KZG commitments/proofs. Note also\nthat this assumes a maximally even tree; for a realistic randomized\ntree, add a depth of ~0.6 (so ~30 bytes per element). If\nbulletproof-style commitments are used instead of KZG, it's safe to go\ndown to 32 bytes, so these sizes can be reduced by 1/3.\nProver and verifier\ncomputation load\n\nThe bulk of the cost of generating a proof is\ncomputing each \\(r^i\\frac{A_i(X) - y_i}{X -\nx_i}\\) expression. This requires roughly four field operations\n(ie. 256 bit modular arithmetic operations) times the width of the tree.\nThis is the main constraint limiting Verkle tree widths. Fortunately,\nfour field operations is a small cost: a single elliptic curve\nmultiplication typically takes hundreds of field operations. Hence,\nVerkle tree widths can go quite high; width 256-1024 seems like an\noptimal range.\n\nTo edit the tree, we need to \"walk up the\ntree\" from the leaf to the root, changing the intermediate commitment at\neach step to reflect the change that happened lower down. Fortunately,\nwe don't have to re-compute each commitment from scratch. Instead, we\ntake advantage of the homomorphic property: given a polynomial\ncommitment \\(C = com(F)\\), we can\ncompute \\(C' = com(F + G)\\) by\ntaking \\(C' = C + com(G)\\). In our\ncase, \\(G = L_i * (v_{new} -\nv_{old})\\), where \\(L_i\\) is a\npre-computed commitment for the polynomial that equals 1 at the position\nwe're trying to change and 0 everywhere else.\n\nHence, a single edit requires ~4 elliptic curve multiplications (one\nper commitment between the leaf and the root, this time including the\nroot), though these can be sped up considerably by pre-computing and\nstoring many multiples of each \\(L_i\\).\n\nProof verification is quite efficient. For a proof\nof N values, the verifier needs to do the following steps, all of which\ncan be done within a hundred milliseconds for even thousands of\nvalues:\n\n- One size-\\(N\\) elliptic\ncurve fast linear combination\n\n- About \\(4N\\) field operations (ie.\n256 bit modular arithmetic operations)\n\n- A small constant amount of work that does not depend on the size of\nthe proof\n\nNote also that, like Merkle Patricia proofs, a Verkle proof gives the\nverifier enough information to modify the values in the tree\nthat are being proven and compute the new root hash after the changes\nare applied. This is critical for verifying that eg. state changes in a\nblock were processed correctly.\n\n## Conclusions\n\nVerkle trees are a powerful upgrade to Merkle proofs that allow for\nmuch smaller proof sizes. Instead of needing to provide all \"sister\nnodes\" at each level, the prover need only provide a single proof that\nproves all parent-child relationships between all commitments\nalong the paths from each leaf node to the root. This allows proof sizes\nto decrease by a factor of ~6-8 compared to ideal Merkle trees, and by a\nfactor of over 20-30 compared to the hexary Patricia trees that Ethereum\nuses today (!!).\n\nThey do require more complex cryptography to implement, but they\npresent the opportunity for large gains to scalability. In the medium\nterm, SNARKs can improve things further: we can either SNARK the\nalready-efficient Verkle proof verifier to reduce witness size to\nnear-zero, or switch back to SNARKed Merkle proofs if/when SNARKs get\nmuch better (eg. through\nGKR, or very-SNARK-friendly hash functions, or ASICs). Further down\nthe line, the rise of quantum computing will force a change to STARKed\nMerkle proofs with hashes as it makes the linear homomorphisms that\nVerkle trees depend on insecure. But for now, they give us the same\nscaling gains that we would get with such more advanced technologies,\nand we already have all the tools that we need to implement them\nefficiently.",
    "contentLength": 15106,
    "summary": "Verkle trees reduce blockchain proof sizes from ~1KB to <150 bytes using polynomial commitments instead of traditional Merkle tree sister nodes.",
    "detailedSummary": {
      "theme": "Verkle trees represent a cryptographic advancement that dramatically reduces proof sizes compared to traditional Merkle trees, making stateless Ethereum clients practically viable through shorter witnesses.",
      "summary": "Vitalik explains how Verkle trees serve the same function as Merkle trees - allowing efficient proofs of data within large datasets - but with dramatically smaller proof sizes. While a traditional Merkle tree proof for data in a billion-item tree requires about 1 kilobyte, a Verkle tree proof needs less than 150 bytes, achieved through vector commitments and polynomial commitments rather than providing all 'sister nodes' at each level. Vitalik details how Verkle trees benefit from much wider structures (256-1024 children per node versus 16 in current Ethereum trees) because they don't suffer the same proof size penalties as traditional Merkle trees. The technical implementation relies on polynomial commitments that can prove parent-child relationships with a single fixed-size proof covering unlimited relationships, using schemes that merge multiple proofs through random evaluation. While requiring more complex cryptography than simple hash functions, Vitalik argues this is still much simpler than ZK-SNARK schemes and provides immediate scaling benefits for Ethereum's transition to stateless clients.",
      "takeaways": [
        "Verkle trees reduce proof sizes by 6-8x compared to ideal Merkle trees and 20-30x compared to Ethereum's current hexary Patricia trees",
        "The key innovation is using vector/polynomial commitments instead of providing all sister nodes, enabling much wider tree structures (256-1024 children per node)",
        "A single proof can verify unlimited parent-child relationships through polynomial commitment schemes and random evaluation techniques",
        "Verkle trees make stateless Ethereum clients practically viable by reducing witness sizes from ~1KB to ~150 bytes for large datasets",
        "While more cryptographically complex than simple Merkle trees, they provide immediate scaling benefits and are much simpler than ZK-SNARK alternatives"
      ],
      "controversial": [
        "The increased dependence on more complex cryptography introduces new security assumptions and potential attack vectors compared to simple hash-based Merkle trees",
        "The quantum computing vulnerability of linear homomorphisms that Verkle trees depend on may require eventual migration back to different proof systems"
      ]
    }
  },
  {
    "id": "general-2021-05-25-voting2",
    "title": "Blockchain voting is overrated among uninformed people but underrated among informed people",
    "date": "2021-05-25",
    "category": "governance",
    "url": "https://vitalik.eth.limo/general/2021/05/25/voting2.html",
    "path": "general/2021/05/25/voting2.html",
    "content": "- \n\n- \n\n  \n    \n\n  \n  \n  Dark Mode Toggle\n  \n    \n\n  \n\n##  Blockchain voting is overrated among uninformed people but underrated among informed people \n\n 2021 May 25 \nSee all posts\n\n \n \n\n Blockchain voting is overrated among uninformed people but underrated among informed people \n\nSpecial thanks to Karl Floersch, Albert Ni, Mr Silly and others\nfor feedback and discussion\n\nVoting is a procedure that has a very important need for process\nintegrity. The result of the vote must be correct, and this must be\nguaranteed by a transparent process so that everyone can be convinced\nthat the result is correct. It should not be possible to successfully interfere with\nanyone's attempt to vote or prevent their vote from being\ncounted.\n\nBlockchains are a technology which is all about providing guarantees\nabout process integrity. If a process is run on a blockchain, the\nprocess is guaranteed to run according to some pre-agreed code and\nprovide the correct output. No one can prevent the execution, no one can\ntamper with the execution, and no one can censor and block any users'\ninputs from being processed.\n\nSo at first glance, it seems that blockchains provide exactly what\nvoting needs. And I'm far from the only person to have had that thought;\nplenty\nof major prospective\nusers are\ninterested. But as it turns out, some people have a very different\nopinion....\n\nDespite the seeming perfect match between the needs of voting and the\ntechnological benefits that blockchains provide, we regularly see scary\narticles arguing\nagainst the combination of the two. And it's not just a single\narticle: here's\nan anti-blockchain-voting piece from Scientific American, here's another\nfrom CNet, and here's another\nfrom ArsTechnica. And it's not just random tech journalists: Bruce\nSchneier is against blockchain voting, and researchers at MIT wrote a whole\npaper arguing that it's a bad idea. So what's going on?\n\n## Outline\n\nThere are two key lines of criticism that are most\ncommonly levied by critics of blockchain voting protocols:\n\n- Blockchains are the wrong software tool to run an\nelection. The trust properties they provide are not a good match for the\nproperties that voting needs, and other kinds of software tools with\ndifferent information flow and trust properties would work better.\n\n- Software in general cannot be trusted to run\nelections, no matter what software it is. The risk of undetectable\nsoftware and hardware bugs is too high, no matter how the platform is\norganized.\n\nThis article will discuss both of these claims in turn\n(\"refute\" is too strong a word, but I definitely disagree more than I\nagree with both claims). First, I will discuss the security\nissues with existing attempts to use blockchains for voting, and how\nthe correct solution is not to abandon blockchains, but to\ncombine them with other cryptographic technologies. Second, I\nwill address the concern about whether or not software (and hardware)\ncan be trusted. The answer: computer security is actually\ngetting quite a bit better, and we can work hard to continue\nthat trend.\n\nOver the long term, insisting on paper permanently would be a\nhuge handicap to our ability to make voting better.\nOne vote per N years is a 250-year-old form of democracy, and we can\nhave much better democracy if voting were much more convenient and\nsimpler, so that we could do it much more often.\n\nNeedless to say, this entire post is predicated on good\nblockchain scaling technology (eg. sharding) being\navailable. Of course, if blockchains cannot scale, none of this\ncan happen. But so far, development of this technology is proceeding\nquickly, and there's no reason to believe that it can't happen.\nBad blockchain voting\nprotocols\n\nBlockchain voting protocols get hacked all the time. Two years ago, a\nblockchain voting tech company called Voatz was all the rage, and many\npeople were very excited about it. But last year, some MIT researchers\ndiscovered a string\nof critical security vulnerabilities in their platform. Meanwhile,\nin Moscow, a blockchain voting system that was going to be used for an\nupcoming election was\nhacked, fortunately a month before the election took place.\n\nThe hacks were pretty serious. Here is a table of the attack\ncapabilities that researchers\nanalyzing Voatz managed to uncover:\n\nThis by itself is not an argument against ever using\nblockchain voting. But it is an argument that blockchain voting software\nshould be designed more carefully, and scaled up slowly and\nincrementally over time.\nPrivacy and coercion\nresistance\n\nBut even the blockchain voting protocols that are not technically\nbroken often suck. To understand why, we need to delve deeper into\nwhat specific security properties blockchains provide, and what\nspecific security properties voting needs - when we do, we'll see that\nthere is a mismatch.\n\nBlockchains provide two key properties: correct\nexecution and censorship resistance. Correct\nexecution just means that the blockchain accepts inputs (\"transactions\")\nfrom users, correctly processes them according to some pre-defined\nrules, and returns the correct output (or adjusts the blockchain's\n\"state\" in the correct way). Censorship resistance is also simple to\nunderstand: any user that wants to send a transaction, and is\nwilling to pay a high enough fee, can send the transaction and\nexpect to see it quickly included on-chain.\n\nBoth of these properties are very important for voting: you want the\noutput of the vote to actually be the result of counting up the number\nof votes for each candidate and selecting the candidate with the most\nvotes, and you definitely want anyone who is eligible to vote to be able\nto vote, even if some powerful actor is trying to block them.\nBut voting also requires some crucial properties that\nblockchains do not provide:\n\n- Privacy: you should not be able to tell which\ncandidate someone specific voted for, or even if they voted at all\n\n- Coercion resistance: you should not be able to\nprove to someone else how you voted, even if you want\nto\n\nThe need for the first requirement is obvious: you want people to\nvote based on their personal feelings, and not how people around them or\ntheir employer or the police or random thugs on the street will feel\nabout their choice. The second requirement is needed to prevent vote\nselling: if you can prove how you voted, selling your vote becomes very\neasy. Provability of votes would also enable forms of coercion where the\ncoercer demands to see some kind of proof of voting for their preferred\ncandidate. Most people, even those aware of the first requirement, do\nnot think about the second requirement. But the second requirement is\nalso necessary, and it's quite technically nontrivial to provide it.\nNeedless to say, the average \"blockchain voting system\" that you\nsee in the wild does not even try to provide the second property, and\nusually fails at providing the first.\nSecure electronic\nvoting without blockchains\n\nThe concept of cryptographically secured execution of social\nmechanisms was not invented by blockchain geeks, and indeed existed far\nbefore us. Outside the blockchain space, there is a 20-year-old\ntradition of cryptographers working on the secure electronic voting\nproblem, and the good news is that there have been solutions.\nAn important paper that is cited by much of the literature of the last\ntwo decades is Juels, Catalano and Jakobsson's 2002 paper titled \"Coercion-Resistant\nElectronic Elections\":\n\nSince then, there have been many iterations on the concept; Civitas\nis one prominent example, though there are also\nmany\nothers.\nThese protocols all use a similar set of core techniques. There is an\nagreed-upon set of \"talliers\" and there is a trust assumption that the\nmajority of the talliers is honest. The talliers each have \"shares\" of a\nprivate key secret-shared among themselves, and the corresponding public\nkey is published. Voters publish votes encrypted to the talliers' public\nkey, and talliers use a secure\nmulti-party computation (MPC) protocol to decrypt and verify the\nvotes and compute the tally. The tallying computation is done \"inside\nthe MPC\": the talliers never learn their private key, and they compute\nthe final result without learning anything about any individual vote\nbeyond what can be learned from looking at the final result itself.\n\nEncrypting votes provides privacy, and some additional infrastructure\nsuch as mix-nets\nis added on top to make the privacy stronger. To provide coercion\nresistance, one of two techniques is used. One option is that during the\nregistration phase (the phase in which the talliers learn each\nregistered voter's public key), the voter generates or receives a secret\nkey. The corresponding public key is secret shared among the talliers,\nand the talliers' MPC only counts a vote if it is signed with the secret\nkey. A voter has no way to prove to a third party what their secret key\nis, so if they are bribed or coerced they can simply show and cast a\nvote signed with the wrong secret key. Alternatively, a voter could have\nthe ability to send a message to change their secret key. A\nvoter has no way of proving to a third party that they did not\nsend such a message, leading to the same result.\n\nThe second option is a technique where voters can make multiple votes\nwhere the second overrides the first. If a voter is bribed or coerced,\nthey can make a vote for the briber/coercer's preferred candidate, but\nlater send another vote to override the first.\n\n Giving voters the ability to make a later vote that\ncan override an earlier vote is the key coercion-resistance mechanism of\nthis\nprotocol from 2015.\n\nNow, we get to a key important nuance in all of these protocols. They\nall rely on an outside primitive to complete their security guarantees:\nthe bulletin board (this is the \"BB\" in the figure\nabove). The bulletin board is a place where any voter can send a\nmessage, with a guarantee that (i) anyone can read the bulletin board,\nand (ii) anyone can send a message to the bulletin board that gets\naccepted. Most of the coercion-resistant voting papers that you can find\nwill casually reference the existence of a bulletin board (eg.\n\"as is common for electronic voting schemes, we assume a publicly\naccessible append-only bulletin board\"), but far fewer papers talk about\nhow this bulletin board can actually be implemented. And here,\nyou can hopefully see where I am going with this: the most\nsecure way to implement a bulletin board is to just use an\nexisting blockchain!\nSecure electronic\nvoting with blockchains\n\nOf course, there have been plenty of pre-blockchain attempts at\nmaking a bulletin board. This paper\nfrom 2008 is such an attempt; its trust model is a standard\nrequirement that \"k of n servers must be\nhonest\" (k = n/2 is common). This literature review from\n2021 covers some pre-blockchain attempts at bulletin boards as well\nas exploring the use of blockchains for the job; the pre-blockchain\nsolutions reviewed similarly rely on a k-of-n trust model.\n\nA blockchain is also a k-of-n trust model; it requires at least half\nof miners or proof of stake validators to be following the protocol, and\nif that assumption fails that often results in a \"51% attack\". So why is\na blockchain better than a special purpose bulletin board? The answer\nis: setting up a k-of-n system that's actually trusted is hard, and\nblockchains are the only system that has already solved it, and at\nscale. Suppose that some government announced that it was making a\nvoting system, and provided a list of 15 local organizations and\nuniversities that would be running a special-purpose bulletin board. How\nwould you, as an outside observer, know that the government didn't just\nchoose those 15 organizations from a list of 1000 based on their\nwillingness to secretly collude with an intelligence agency?\n\nPublic blockchains, on the other hand, have permissionless\neconomic consensus mechanisms (proof of work or proof of stake) that\nanyone can participate in, and they have an existing diverse and highly\nincentivized infrastructure of block explorers, exchanges and other\nwatching nodes to constantly verify in real time that nothing bad is\ngoing on.\n\nThese more sophisticated voting systems are not just using\nblockchains; they rely on cryptography such as zero knowledge proofs to\nguarantee correctness, and on multi-party computation to guarantee\ncoercion resistance. Hence, they avoid the weaknesses of more naive\nsystems that simply just \"put votes directly on the blockchain\" and\nignore the resulting privacy and coercion resistance issues. However,\nthe blockchain bulletin board is nevertheless a key part of the security\nmodel of the whole design: if the committee is broken but the blockchain\nis not, coercion resistance is lost but all the other guarantees around\nthe voting process still remain.\nMACI:\ncoercion-resistant blockchain voting in Ethereum\n\nThe Ethereum ecosystem is currently experimenting with a system called MACI that\ncombines together a blockchain, ZK-SNARKs and a single central actor\nthat guarantees coercion resistance (but has no power to compromise any\nproperties other than coercion resistance). MACI is not very technically\ndifficult. Users participate by signing a message with their private\nkey, encrypting the signed message to a public key published by a\ncentral server, and publishing the encrypted signed message to the\nblockchain. The server downloads the messages from the blockchain,\ndecrypts them, processes them, and outputs the result along with a\nZK-SNARK to ensure that they did the computation correctly.\n\nUsers cannot prove how they participated, because they have the\nability to send a \"key change\" message to trick anyone trying to audit\nthem: they can first send a key change message to change their key from\nA to B, and then send a \"fake message\" signed with A. The server would\nreject the message, but no one else would have any way of knowing that\nthe key change message had ever been sent. There is a trust requirement\non the server, though only for privacy and coercion resistance; the\nserver cannot publish an incorrect result either by computing\nincorrectly or by censoring messages. In the long term, multi-party\ncomputation can be used to decentralize the server somewhat,\nstrengthening the privacy and coercion resistance guarantees.\n\nThere is a working demo of this scheme at clr.fund being used for quadratic funding.\nThe use of the Ethereum blockchain to ensure censorship resistance of\nvotes ensures a much higher degree of censorship resistance than would\nbe possible if a committee was relied on for this instead.\n\n## Recap\n\n- The voting process has four important security requirements that\nmust be met for a vote to be secure: correctness,\ncensorship resistance, privacy and\ncoercion resistance.\n\n- Blockchains are good at the first two. They are bad at the last\ntwo.\n\n- Encryption of votes put on a blockchain can add\nprivacy. Zero knowledge proofs can bring back\ncorrectness despite observers being unable to add up votes directly\nbecause they are encrypted.\n\n- Multi-party computation decrypting and checking\nvotes can provide coercion resistance, if combined with a mechanic where\nusers can interact with the system multiple times; either the first\ninteraction invalidates the second, or vice versa\n\n- Using a blockchain ensures that you have very high-security\ncensorship resistance, and you keep this censorship resistance even\nif the committee colludes and breaks coercion resistance.\nIntroducing a blockchain can significantly increase the level of\nsecurity of the system.\n\nBut can technology be\ntrusted?\n\nBut now we get back to the second, deeper, critique of electronic\nvoting of any kind, blockchain or not: that technology itself is too\ninsecure to be trusted.\n\nThe recent MIT\npaper criticizing blockchain voting includes this helpful table,\ndepicting any form of paperless voting as being fundamentally\ntoo difficult to secure:\n\nThe key property that the authors focus on is\nsoftware-independence, which they define as \"the\nproperty that an undetected change or error in a system's software\ncannot cause an undetectable change in the election outcome\". Basically,\na bug in the code should not be able to accidentally make Prezzy\nMcPresidentface the new president of the country (or, more\nrealistically, a deliberately inserted bug should not be able to\nincrease some candidate's share from 42% to 52%).\n\nBut there are other ways to deal with bugs. For example, any\nblockchain-based voting system that uses publicly verifiable\nzero-knowledge proofs can be independently verified. Someone can write\ntheir own implementation of the proof verifier and verify the Zk-SNARK\nthemselves. They could even write their own software to vote. Of course,\nthe technical complexity of actually doing this is beyond 99.99% of any\nrealistic voter base, but if thousands of independent experts have the\nability to do this and verify that it works, that is more than good\nenough in practice.\n\nTo the MIT authors, however, that is not enough:\n\nThus, any system that is electronic only, even if end-to-end\nverifiable, seems unsuitable for political elections in the foreseeable\nfuture. The U.S. Vote Foundation has noted the promise of E2E-V methods\nfor improving online voting security, but has issued a detailed report\nrecommending avoiding their use for online voting unless and until the\ntechnology is far more mature and fully tested in pollsite voting\n[38].\n\nOthers have proposed extensions of these ideas. For example, the\nproposal of Juels et al.\u00a0[55] emphasizes the use of cryptography to\nprovide a number of forms of \"coercion resistance.\" The Civitas proposal\nof Clarkson et al.\u00a0[24] implements additional mechanisms for coercion\nresistance, which Iovino et al.\u00a0[53] further incorporate and elaborate\ninto their Selene system. From our perspective, these proposals are\ninnovative but unrealistic: they are quite complex, and most seriously,\ntheir security relies upon voters' devices being uncompromised and\nfunctioning as intended, an unrealistic assumption.\n\nThe problem that the authors focus on is not the voting system's\nhardware being secure; risks on that side actually can be mitigated\nwith zero knowledge proofs. Rather, the authors focus on a different\nsecurity problem: can users' devices even in principle be made\nsecure?\n\nGiven the long history of all kinds of exploits and hacks of consumer\ndevices, one would be very justified in thinking the answer is \"no\".\nQuoting my own article\non Bitcoin wallet security from 2013:\n\nLast night around 9PM PDT, I clicked a link to go to\nCoinChat[.]freetzi[.]com \u2013 and I was prompted to run java. I did\n(thinking this was a legitimate chatoom), and nothing happened. I closed\nthe window and thought nothing of it. I opened my bitcoin-qt wallet\napprox 14 minutes later, and saw a transaction that I did NOT approve go\nto wallet 1Es3QVvKN1qA2p6me7jLCVMZpQXVXWPNTC for almost my entire\nwallet...\n\nAnd:\n\nIn June 2011, the Bitcointalk member \"allinvain\" lost 25,000 BTC\n(worth $500,000 at the time) after an unknown intruder somehow gained\ndirect access to his computer. The attacker was able to access\nallinvain's wallet.dat file, and quickly empty out the wallet \u2013 either\nby sending a transaction from allinvain's computer itself, or by simply\nuploading the wallet.dat file and emptying it on his own machine.\n\nBut these disasters obscure a greater truth: over the\npast twenty years, computer security has actually been slowly\nand steadily improving. Attacks are much harder to\nfind, often requiring the attacker to find bugs in multiple sub-systems\ninstead of finding a single hole in a large complex piece of code.\nHigh-profile incidents are larger than ever, but this is not a sign that\nanything is getting less secure; rather, it's simply a sign that we are\nbecoming much more dependent on the internet.\n\nTrusted\nhardware is a very important recent source of improvements.\nSome of the new \"blockchain phones\" (eg. this one\nfrom HTC) go quite far with this technology and put a minimalistic\nsecurity-focused operating system on the trusted hardware chip, allowing\nhigh-security-demanding applications (eg. cryptocurrency wallets) to\nstay separate from the other applications. Samsung has started making\nphones using similar\ntechnology.\nAnd even devices that are never advertised as \"blockchain devices\" (eg.\niPhones) frequently have trusted hardware of some kind. Cryptocurrency\nhardware wallets are effectively the same thing, except the trusted\nhardware module is physically located outside the computer instead of\ninside it. Trusted hardware (deservedly!) often gets a bad rap in\nsecurity circles and especially the blockchain community, because it\njust keeps\ngetting broken\nagain and again.\nAnd indeed, you definitely don't want to use it to replace your\nsecurity protection. But as an augmentation, it's a huge\nimprovement.\n\nFinally, single applications, like cryptocurrency wallets and voting\nsystems, are much simpler and have less room for error than an entire\nconsumer operating system - even if you have to incorporate support for\nquadratic voting, sortition, quadratic\nsortition and whatever horrors the next generation's Glen Weyl\ninvents in 2040. The benefit of tools like trusted hardware is their\nability to isolate the simple thing from the complex and\npossibly broken thing, and these tools are having some success.\nSo\nthe risks might decrease over time. But what are the benefits?\n\nThese improvements in security technology point to a future where\nconsumer hardware might be more trusted in the future than it is today.\nInvestments made in this area in the last few years are likely to keep\npaying off over the next decade, and we could expect further significant\nimprovements. But what are the benefits of making voting electronic\n(blockchain based or otherwise) that justify exploring this whole\nspace?\n\nMy answer is simple: voting would become much more efficient,\nallowing us to do it much more often. Currently, formal\ndemocratic input into organizations (governmental or corporate)\ntends to be limited to a single vote once every 1-6 years. This\neffectively means that each voter is only putting less than one bit of\ninput into the system each year. Perhaps in large part as a result of\nthis, decentralized decision-making in our society is heavily bifurcated\ninto two extremes: pure democracy and pure markets. Democracy is either\nvery inefficient (corporate and government votes) or very insecure\n(social media likes/retweets). Markets are far more technologically\nefficient and are much more secure than social media, but their\nfundamental economic logic makes them a poor fit for many kinds of\ndecision problems, particularly having to do with public goods.\n\nYes, I know it's yet another triangle, and I really really\napologize for having to use it. But please bear with me just this once....\n(ok fine, I'm sure I'll make even more triangles in the future; just\nsuck it up and deal with it)\n\nThere is a lot that we could do if we could build more systems that\nare somewhere in between democracy and markets, benefiting from the\negalitarianism of the former, the technical efficiency of the latter and\neconomic properties all along the spectrum in between the two extremes.\nQuadratic funding is an\nexcellent example of this. Liquid democracy is another excellent\nexample. Even if we don't introduce fancy new delegation mechanisms or\nquadratic math, there's a lot that we could do by doing voting much\nmore and at smaller scales more adapted to the information\navailable to each individual voter. But the challenge with all of these\nideas is that in order to have a scheme that durably maintains\nany level of democraticness at all, you need some form of sybil\nresistance and vote-buying mitigation: exactly the problems that these\nfancy ZK-SNARK + MPC + blockchain voting schemes are trying to\nsolve.\n\n## The crypto space can help\n\nOne of the underrated benefits of the crypto space is that it's an\nexcellent \"virtual special economic zone\" for testing out economic and\ncryptographic ideas in a highly adversarial environment. Whatever you\nbuild and release, once the economic power that it controls gets above a\ncertain size, a whole host of diverse, sometimes altruistic, sometimes\nprofit-motivated, and sometimes malicious actors, many of whom are\ncompletely anonymous, will descend upon the system and try to twist that\neconomic power toward their own various objectives.\n\nThe incentives for attackers are high: if an attacker steals $100\nfrom your cryptoeconomic gadget, they can often get the full $100 in\nreward, and they can often get away with it. But the incentives for\ndefenders are also high: if you develop a tool that helps users\nnot lose their funds, you could (at least sometimes) turn that\ninto a tool and earn millions. Crypto is the ultimate training zone: if\nyou can build something that can survive in this environment at scale,\nit can probably also survive in the bigger world as well.\n\nThis applies to quadratic\nfunding, it applies to multisig and social recovery wallets,\nand it can apply to voting systems too. The blockchain space has already\nhelped to motivate the rise of important security technologies:\n\n- Hardware wallets\n\n- Efficient general-purpose zero knowledge proofs\n\n- Formal verification tools\n\n- \"Blockchain phones\" with trusted hardware chips\n\n- Anti-sybil schemes like Proof of Humanity\n\nIn all of these cases, some version of the technology existed before\nblockchains came onto the scene. But it's hard to deny that blockchains\nhave had a significant impact in pushing these efforts forward, and the\nlarge role of incentives inherent to the space plays a key role in\nraising the stakes enough for the development of the tech to actually\nhappen.\n\n## Conclusion\n\nIn the short term, any form of blockchain voting should\ncertainly remain confined to small experiments, whether in\nsmall trials for more mainstream applications or for the blockchain\nspace itself. Security is at present definitely not good enough to rely\non computers for everything. But it's improving, and if I am wrong and\nsecurity fails to improve then not only blockchain voting, but\nalso cryptocurrency as a whole, will have a hard time being successful.\nHence, there is a large incentive for the technology to continue to\nimprove.\n\nWe should all continue watching the technology and the efforts being\nmade everywhere to try and increase security, and slowly become more\ncomfortable using technology in very important social processes.\nTechnology is already key in our financial markets, and a\ncrypto-ization of a large part of the economy (or even just replacing\ngold) will put an even greater portion of the economy into the hands of\nour cryptographic algorithms and the hardware that is running them. We\nshould watch and support this process carefully, and over time take\nadvantage of its benefits to bring our governance technologies into the\n21st century.",
    "contentLength": 26914,
    "summary": "Blockchain voting needs privacy/coercion-resistance which blockchains lack, but combining blockchains with cryptographic tech can solve this.",
    "detailedSummary": {
      "theme": "Blockchain voting faces legitimate criticism from uninformed skeptics but offers underappreciated potential when combined with advanced cryptography, despite current security limitations.",
      "summary": "Vitalik argues that blockchain voting occupies a misunderstood middle ground - casual observers overestimate its current capabilities while experts underestimate its long-term potential. He acknowledges that naive implementations putting votes directly on blockchains are fundamentally flawed due to privacy and coercion resistance issues, citing multiple security breaches and academic criticism. However, Vitalik contends that sophisticated systems combining blockchains with zero-knowledge proofs, multi-party computation, and encryption can address these shortcomings while maintaining the key blockchain benefits of correct execution and censorship resistance. The blockchain serves as a crucial 'bulletin board' component that existing cryptographic voting literature assumes but rarely implements securely. Vitalik addresses the deeper criticism that all electronic voting is inherently untrustworthy by arguing that computer security is steadily improving through trusted hardware and other advances. He envisions a future where secure, frequent electronic voting enables more sophisticated democratic mechanisms like quadratic funding and liquid democracy, moving beyond the current limitation of 'less than one bit of input per voter per year' in traditional elections.",
      "takeaways": [
        "Current blockchain voting implementations are severely flawed, lacking essential privacy and coercion resistance properties needed for secure elections",
        "Sophisticated voting systems can combine blockchains with zero-knowledge proofs and multi-party computation to achieve all four critical security requirements: correctness, censorship resistance, privacy, and coercion resistance",
        "Blockchains excel as 'bulletin boards' for voting systems, providing better censorship resistance than traditional committee-based approaches through permissionless consensus mechanisms",
        "Computer security is gradually improving through trusted hardware and application isolation, making electronic voting potentially more viable in the future",
        "The ultimate goal is enabling frequent, secure electronic voting to support advanced democratic mechanisms like quadratic funding and liquid democracy, moving beyond traditional vote-every-few-years democracy"
      ],
      "controversial": [
        "Vitalik's optimism about improving computer security contradicts widespread expert skepticism about trusting consumer devices for critical democratic processes",
        "The argument that blockchain voting could be superior to paper-based systems challenges the strong academic consensus favoring software-independent voting methods",
        "The vision of frequent electronic voting enabling 'better democracy' may be seen as techno-solutionism that overlooks fundamental political and social aspects of democratic participation"
      ]
    }
  },
  {
    "id": "general-2021-05-23-scaling",
    "title": "The Limits to Blockchain Scalability",
    "date": "2021-05-23",
    "category": "general",
    "url": "https://vitalik.eth.limo/general/2021/05/23/scaling.html",
    "path": "general/2021/05/23/scaling.html",
    "content": "- \n\n- \n\n  \n    \n\n  \n  \n  Dark Mode Toggle\n  \n    \n\n  \n\n##  The Limits to Blockchain Scalability \n\n 2021 May 23 \nSee all posts\n\n \n \n\n The Limits to Blockchain Scalability \n\nSpecial thanks to Felix Lange, Martin Swende, Marius van der\nWijden and Mark Tyneway for feedback and review.\n\nJust how far can you push the scalability of a blockchain? Can you\nreally, as Elon Musk\nwishes, \"speed up block time 10X, increase block size 10X & drop\nfee 100X\" without leading to extreme centralization and compromising the\nfundamental properties that make a blockchain what it is? If not, how\nfar can you go? What if you change the consensus algorithm? Even more\nimportantly, what if you change the technology to introduce features\nsuch as ZK-SNARKs or sharding? A sharded blockchain can theoretically\njust keep adding more shards; is there such a thing as adding too\nmany?\n\nAs it turns out, there are important and quite subtle technical\nfactors that limit blockchain scaling, both with sharding and without.\nIn many cases there are solutions, but even with the solutions there are\nlimits. This post will go through what many of these issues are.\n\nJust increase the parameters, and all problems are solved. But\nat what cost?\n\nIt's\ncrucial for blockchain decentralization for regular users to be able to\nrun a node\n\nAt 2:35 AM, you receive an emergency call from your partner on the\nopposite side of the world who helps run your mining pool (or it could\nbe a staking pool). Since about 14 minutes ago, your partner tells you,\nyour pool and a few others split off from the chain which still carries\n79% of the network. According to your node, the majority chain's blocks\nare invalid. There's a balance error: the key block appeared to\nerroneously assign 4.5 million extra coins to an unknown address.\n\nAn hour later, you're in a telegram chat with the other two small\npools who were caught blindsided just as you were, as well as some block\nexplorers and exchanges. You finally see someone paste a link to a\ntweet, containing a published message. \"Announcing new on-chain\nsustainable protocol development fund\", the tweet begins.\n\nBy the morning, arguments on Twitter, and on the one community forum\nthat was not censoring the discussion, discussions are everywhere. But\nby then a significant part of the 4.5 million coins had been converted\non-chain to other assets, and billions of dollars of defi transactions\nhad taken place. 79% of the consensus nodes, and all the major block\nexplorers and endpoints for light wallets, were following this new\nchain. Perhaps the new dev fund will fund some development, or perhaps\nit will just all be embezzled by the leading pools and exchanges and\ntheir cronies. But regardless of how it turns out, the fund is for all\nintents and purposes a fait accompli, and regular users have no way to\nfight back.\n\nMovie coming soon. Maybe it can be funded by MolochDAO or\nsomething.\n\nCan this happen on your blockchain? The elites of your blockchain\ncommunity, including pools, block explorers and hosted nodes, are\nprobably quite well-coordinated; quite likely they're all in the same\ntelegram channels and wechat groups. If they really want to\norganize a sudden change to the protocol rules to further their own\ninterests, then they probably can. The Ethereum blockchain has fully\nresolved consensus\nfailures in ten hours; if your blockchain has only one client\nimplementation, and you only need to deploy a code change to a few dozen\nnodes, coordinating a change to client code can be done much faster. The\nonly reliable way to make this kind of coordinated social attack\nnot effective is through passive defense from the one\nconstituency that actually is decentralized: the users.\n\nImagine how the story would have played out if the users were running\nnodes that verify the chain (whether directly or through more\nadvanced indirect techniques), and automatically reject blocks that\nbreak the protocol rules even if over 90% of the miners or\nstakers support those blocks. If every user ran a verifying\nnode, then the attack would have quickly failed: a few mining pools and\nexchanges would have forked off and looked quite foolish in the process.\nBut even if some users ran verifying nodes, the attack would\nnot have led to a clean victory for the attacker; rather, it\nwould have led to chaos, with different users seeing different\nviews of the chain. At the very least, the ensuing market panic and\nlikely persistent chain split would greatly reduce the attackers'\nprofits. The thought of navigating such a protracted conflict would\nitself deter most attacks.\n\nListen to Hasu on this one.\n\nIf you have a community of 37 node runners and 80000 passive\nlisteners that check signatures and block headers, the attacker wins. If\nyou have a community where everyone runs a node, the attacker loses. We\ndon't know what the exact threshold is at which herd immunity against\ncoordinated attacks kicks in, but there is one thing that's absolutely\nclear: more nodes good, fewer nodes bad, and we definitely need more\nthan a few dozen or few hundred.\nSo,\nwhat are the limits to how much work we can require full nodes to\ndo?\n\nTo maximize the number of users who can run a node, we'll focus on\nregular consumer hardware. There are some increases to capacity that can\nbe achieved by demanding some specialized hardware purchases that are\neasy to obtain (eg. from Amazon), but they actually don't increase\nscalability by that much.\n\nThere are three key limitations to a full node's ability to process a\nlarge number of transactions:\n\n- Computing power: what % of the CPU can we safely\ndemand to run a node?\n\n- Bandwidth: given the realities of current internet\nconnections, how many bytes can a block contain?\n\n- Storage: how many gigabytes on disk can we require\nusers to store? Also, how quickly must it be readable? (ie. is HDD okay\nor do we need SSD)\n\nMany erroneous takes on how far a blockchain can scale using\n\"simple\" techniques stem from overly optimistic estimates for each of\nthese numbers. We can go through these three factors one by\none:\n\n## Computing power\n\n- Bad answer: 100% of CPU power can be spent on block\nverification\n\n- Correct answer: ~5-10% of CPU power can be spent on\nblock verification\n\nThere are four key reasons why the limit is so low:\n\n- We need a safety margin to cover the possibility of DoS attacks\n(transactions crafted by an attacker to take advantage of weaknesses in\ncode to take longer to process than regular transactions)\n\n- Nodes need to be able to sync the chain after being offline. If I\ndrop off the network for a minute, I should be able to catch up in a few\nseconds\n\n- Running a node should not drain your battery very quickly and make\nall your other apps very slow\n\n- There are other non-block-production tasks that nodes need to do as\nwell, mostly around verifying and responding to incoming transactions\nand requests on the p2p network\n\nNote that up until recently, most explanations for \"why only 5-10%?\"\nfocused on a different problem: that because PoW blocks come at random\ntimes, it taking a long time to verify blocks increases the risk that\nmultiple blocks get created at the same time. There are many fixes to\nthis problem (eg. Bitcoin\nNG, or just using proof of stake). But these fixes do NOT solve the\nother four problems, and so they don't enable large gains in scalability\nas many had initially thought.\n\nParallelism is also not a magic bullet. Often, even clients of\nseemingly single-threaded blockchains are parallelized already:\nsignatures can be verified by one thread while execution is done by\nother threads, and there's a separate thread that's handling transaction\npool logic in the background. And the closer you get to 100% usage\nacross all threads, the more energy-draining running a node\nbecomes and the lower your safety margin against DoS.\n\n## Bandwidth\n\n- Bad answer: if we have 10 MB blocks every 2-3\nseconds, then most users have a >10 MB/sec network, so of course they\ncan handle it\n\n- Correct answer: maybe we can handle 1-5 MB\nblocks every 12 seconds. It's hard though.\n\nNowadays we frequently hear very high advertised statistics for how\nmuch bandwidth internet connections can offer: numbers of 100 Mbps and\neven 1 Gbps are common to hear. However, there is a large difference\nbetween advertised bandwidth and the expected actual bandwidth of a\nconnection for several reasons:\n\n- \"Mbps\" refers to \"millions of bits per second\"; a bit is\n1/8 of a byte, so you need to divide advertised bit numbers by 8 to get\nthe advertised byte numbers.\n\n- Internet providers, just like all companies, often lie.\n\n- There's always multiple applications using the same internet\nconnection, so a node can't hog the entire bandwidth.\n\n- p2p networks inevitably introduce their own overhead: nodes often\nend up downloading and re-uploading the same block multiple times (not\nto mention transactions being broadcasted through the mempool\nbefore being included in a block).\n\nWhen Starkware did an experiment in 2019 where they published 500 kB\nblocks after the transaction data gas cost\ndecrease made that possible for the first time, a few nodes were\nactually unable to handle blocks of that size. Ability to handle large\nblocks has since been improved and will continue to be improved. But no\nmatter what we do, we'll still be very far from being able to\nnaively take the average bandwidth in MB/sec, convince ourselves that\nwe're okay with 1s latency, and be able to have blocks that are that\nsize.\n\n## Storage\n\n- Bad answer: 10 terabytes\n\n- Correct answer: 512 gigabytes\n\nThe main argument here is, as you might guess, the same as elsewhere:\nthe difference between theory and practice. In theory, there\nare 8\nTB solid state drives that you can buy on Amazon (you do\nneed SSDs or NVME; HDDs are too slow for storing the blockchain state).\nIn practice, the laptop that was used to write this blog post\nhas 512 GB, and if you make people go buy their own hardware, many of\nthem will just get lazy (or they can't afford $800 for an 8 TB SSD) and\nuse a centralized provider. And even if you can fit a blockchain onto\nsome storage, a high level of activity can easily quickly burn through\nthe disk and force you to keep getting a new one.\n\nA poll in a group of blockchain protocol researchers of how\nmuch disk space everyone has. Small sample size, I know, but\nstill...\n\nAdditionally, storage size determines the time needed for a new node\nto be able to come online and start participating in the network. Any\ndata that existing nodes have to store is data that a new node has to\ndownload. This initial sync time (and bandwidth) is also a major barrier\nto users being able to run nodes. While writing this blog post, syncing\na new geth node took me ~15 hours. If Ethereum had 10x more usage,\nsyncing a new geth node would take at least a week, and it would be much\nmore likely to just lead to your internet connection getting throttled.\nThis is all even more important during an attack, when a successful\nresponse to the attack will likely involve many users spinning up new\nnodes when they were not running nodes before.\n\n## Interaction effects\n\nAdditionally, there are interaction effects between these three types\nof costs. Because databases use tree structures internally to store and\nretrieve data, the cost of fetching data from a database increases with\nthe logarithm of the size of the database. In fact, because the top\nlevel (or top few levels) can be cached in RAM, the disk access cost is\nproportional to the size of the database as a multiple of the size\nof the data cached in RAM.\n\nDon't take this diagram too literally; different databases\nwork in different ways, and often the part in memory is just a single\n(but big) layer (see\nLSM\ntrees as used in leveldb). But the basic principles are the\nsame.\n\nFor example, if the cache is 4 GB, and we assume that each layer of\nthe database is 4x bigger than the previous, then Ethereum's current ~64\nGB state would require ~2 accesses. But if the state size increases by\n4x to ~256 GB, then this would increase to ~3 accesses (so 1.5x more\naccesses per read). Hence, a 4x increase in the gas limit, which would\nincrease both the state size and the number of reads, could actually\ntranslate into a ~6x increase in block verification time. The effect may\nbe even stronger: hard disks often take longer to read and write when\nthey are full than when they are near-empty.\nSo what does this mean for\nEthereum?\n\nToday in the Ethereum blockchain, running a node already is\nchallenging for many users, though it is still at least\npossible on regular hardware (I just synced a node on my laptop\nwhile writing this post!). Hence, we are close to hitting bottlenecks.\nThe issue that core developers are most concerned with is\nstorage size. Thus, at present, valiant efforts at solving\nbottlenecks in computation and data, and even changes to the consensus\nalgorithm, are unlikely to lead to large gas limit increases being\naccepted. Even solving\nEthereum's largest outstanding DoS vulnerability only led to a gas\nlimit increase of 20%.\n\nThe only solution to storage size problems is statelessness and\nstate expiry. Statelessness allows for a class of\nnodes that verify the chain without maintaining permanent\nstorage. State expiry pushes out state that has not\nbeen recently accessed, forcing users to manually provide proofs to\nrenew it. Both of these paths have been worked at for a long time, and\nproof-of-concept implementation on statelessness has already started.\nThese two improvements combined can greatly alleviate these concerns and\nopen up room for a significant gas limit increase. But even\nafter statelessness and state expiry are implemented, gas limits may\nonly increase safely by perhaps ~3x until the other limitations start to\ndominate.\n\nAnother possible medium-term solution is using ZK-SNARKs to verify\ntransactions. ZK-SNARKs would ensure that regular users do not have to\npersonally store the state or verify blocks, though they still\nwould need to download all the data in blocks to protect against data\nunavailability attacks. Additionally, even if attackers cannot force\ninvalid blocks through, if capacity is increased to the point\nwhere running a consensus node is too difficult, there is still\nthe risk of coordinated censorship attacks. Hence, ZK-SNARKs cannot\nincrease capacity infinitely, but they still can increase capacity by a\nsignificant margin (perhaps 1-2 orders of magnitude). Some chains are\nexploring this approach at layer 1; Ethereum is getting the benefits of\nthis approach through layer-2 protocols (called ZK rollups) such as zksync, Loopring and Starknet.\n\n## What happens after sharding?\n\nSharding fundamentally gets around the above limitations,\nbecause it decouples the data contained on a blockchain from the data\nthat a single node needs to process and store. Instead of nodes\nverifying blocks by personally downloading and executing them, they use\nadvanced mathematical and\ncryptographic techniques to verify blocks indirectly.\n\nAs a result, sharded blockchains can safely have very high levels of\ntransaction throughput that non-sharded blockchains cannot. This does\nrequire a lot of cryptographic cleverness in creating efficient\nsubstitutes for naive full validation that successfully reject invalid\nblocks, but it can be done: the theory is well-established and\nproof-of-concepts based on draft\nspecifications are already being worked on.\n\nEthereum is planning to use quadratic sharding,\nwhere total scalability is limited by the fact that a node has to be\nable to process both a single shard and the beacon chain which has to\nperform some fixed amount of management work for each shard. If shards\nare too big, nodes can no longer process individual shards, and if there\nare too many shards, nodes can no longer process the beacon chain. The\nproduct of these two constraints forms the upper bound.\n\nConceivably, one could go further by doing cubic sharding, or even\nexponential sharding. Data availability sampling would certainly become\nmuch more complex in such a design, but it can be done. But Ethereum is\nnot going further than quadratic. The reason is that the extra\nscalability gains that you get by going from shards-of-transactions to\nshards-of-shards-of-transactions actually cannot be realized without\nother risks becoming unacceptably high.\n\nSo what are these risks?\n\n## Minimum user count\n\nA non-sharded blockchain can conceivably run as long as there is even\none user that cares to participate in it. Sharded blockchains are not\nlike this: no single node can process the whole chain, and so you need\nenough nodes so that they can at least process the chain\ntogether. If each node can process 50 TPS, and the chain can\nprocess 10000 TPS, then the chain needs at least 200 nodes to survive.\nIf the chain at any point gets to less than 200 nodes, then either nodes\nstop being able to keep up with the chain, or nodes stop being able to\ndetect invalid blocks, or a number of other bad things may happen,\ndepending on how the node software is set up.\n\nIn practice, the safe minimum count is several times higher than the\nnaive \"chain TPS divided by node TPS\" heuristic due to the need for\nredundancy (including for data\navailability sampling); for our above example, let's call it 1000\nnodes.\n\nIf a sharded blockchain's capacity increases by 10x, the minimum user\ncount also increases by 10x. Now, you might ask: why don't we start with\na little bit of capacity, and increase it only when we see lots of users\nso we actually need it, and decrease it if the user count goes back\ndown?\n\nThere are a few problems with this:\n\n- A blockchain itself cannot reliably detect how many unique users are\non it, and so this would require some kind of governance to detect and\nset the shard count. Governance over capacity limits can\neasily become a locus of division and conflict.\n\n- What if many users suddenly and unexpectedly drop out at the same\ntime?\n\n- Increasing the minimum number of users needed for a fork to start\nmakes it harder to defend against hostile takeovers.\n\nA minimum user count of under 1,000 is almost certainly fine. A\nminimum user count of 1 million, on the other hand, is certainly not.\nEven a minimum user count of 10,000 is arguably starting to get risky.\nHence, it seems difficult to justify a sharded blockchain having\nmore than a few hundred shards.\n\n## History retrievability\n\nAn important property of a blockchain that users really value is\npermanence. A digital asset stored on a server will\nstop existing in 10 years when the company goes bankrupt or loses\ninterest in maintaining that ecosystem. An NFT on Ethereum, on the other\nhand, is forever.\n\nYes, people will still be downloading and examining your\ncryptokitties in the year 2371. Deal with it.\n\nBut once a blockchain's capacity gets too high, it becomes harder to\nstore all that data, until at some point there's a large risk that some\npart of history will just end up being stored by... nobody.\n\nQuantifying this risk is easy. Take the blockchain's data capacity in\nMB/sec, and multiply by ~30 to get the amount of data stored in\nterabytes per year. The current sharding plan has a data capacity of\n~1.3 MB/sec, so about 40 TB/year. If that is increased by 10x, this\nbecomes 400 TB/year. If we want the data to be not just accessible, but\naccessible conveniently, we would also need metadata (eg.\ndecompressing rollup transactions), so make that 4 petabytes per year,\nor 40 petabytes after a decade. The Internet Archive uses 50 petabytes. So that's a\nreasonable upper bound for how large a sharded blockchain can safely\nget.\n\nHence, it looks like on both of these dimensions, the\nEthereum sharding design is actually already roughly targeted fairly\nclose to reasonable maximum safe values. The constants can be\nincreased a little bit, but not too much.\n\n## Summary\n\nThere are two ways to try to scale a blockchain: fundamental\ntechnical improvements, and simply increasing the\nparameters. Increasing the parameters sounds very attractive at\nfirst: if you do the math on a napkin, it is easy to convince yourself\nthat a consumer laptop can process thousands of transactions per second,\nno ZK-SNARKs or rollups or sharding required. Unfortunately, there are\nmany subtle reasons why this approach is fundamentally flawed.\n\nComputers running blockchain nodes cannot spend 100% of CPU power\nvalidating the chain; they need a large safety margin to resist\nunexpected DoS attacks, they need spare capacity for tasks like\nprocessing transactions in the mempool, and you don't want running a\nnode on a computer to make that computer unusable for any other\napplications at the same time. Bandwidth similarly has overhead: a 10\nMB/s connection does NOT mean you can have a 10 megabyte block every\nsecond! A 1-5 megabyte block every 12 seconds, maybe. And it is\nthe same with storage. Increasing hardware requirements for running a\nnode and limiting node-running to specialized actors is not a\nsolution. For a blockchain to be decentralized, it's crucially\nimportant for regular users to be able to run a node, and to have a\nculture where running nodes is a common activity.\n\nFundamental technical improvements, on the other hand, can\nwork. Currently, the main bottleneck in Ethereum is storage\nsize, and statelessness and state expiry can fix this and allow\nan increase of perhaps up to ~3x - but not more, as we want running a\nnode to become easier than it is today. Sharded\nblockchains can scale much further, because no single node in a\nsharded blockchain needs to process every transaction. But even\nthere, there are limits to capacity: as capacity goes up, the minimum\nsafe user count goes up, and the cost of archiving the chain (and the\nrisk that data is lost if no one bothers to archive the chain) goes up.\nBut we don't have to worry too much: those limits are high enough that\nwe can probably process over a million transactions per second with the\nfull security of a blockchain. But it's going to take work to do this\nwithout sacrificing the decentralization that makes blockchains so\nvaluable.",
    "contentLength": 22054,
    "summary": "Blockchain scalability is limited by consumer hardware constraints: only 5-10% CPU usage, 1-5MB blocks, and 512GB storage due to DoS risks and user access needs.",
    "detailedSummary": {
      "theme": "Blockchain scalability faces fundamental technical limits that cannot be solved simply by increasing parameters, requiring sophisticated solutions like sharding while maintaining decentralization.",
      "summary": "Vitalik argues that naive approaches to blockchain scalability\u2014such as simply increasing block sizes, reducing block times, or demanding more computational resources\u2014are fundamentally flawed due to hardware and network realities. He explains that nodes can only safely use 5-10% of CPU power for verification, bandwidth limitations are much more restrictive than advertised speeds suggest, and storage requirements must remain accessible to regular users to maintain decentralization. Vitalik emphasizes that allowing only specialized actors to run nodes would compromise blockchain's core value proposition of decentralization, as regular users running nodes provide crucial defense against coordinated attacks by mining pools, exchanges, and other elites. While fundamental technical improvements like statelessness, state expiry, ZK-SNARKs, and sharding can provide significant scalability gains, even these advanced solutions have limits\u2014sharding requires minimum user counts that scale with capacity, and higher throughput creates data permanence challenges that could risk losing historical blockchain data.",
      "takeaways": [
        "Simply increasing blockchain parameters (block size, frequency) fails due to real-world hardware and network constraints that are much more restrictive than theoretical calculations suggest",
        "Decentralization requires regular users to run nodes as defense against coordinated attacks by mining pools, exchanges, and other blockchain elites",
        "Current Ethereum is near its scaling limits with traditional approaches, with storage size being the primary bottleneck constraining further parameter increases",
        "Sharding can enable massive scalability improvements but introduces new constraints including minimum user count requirements and data archival challenges",
        "Even with advanced techniques like sharding and ZK-SNARKs, blockchains can theoretically process over a million transactions per second while maintaining decentralization, but this requires careful technical implementation"
      ],
      "controversial": [
        "The claim that blockchain elites (pools, exchanges, hosted nodes) could successfully coordinate sudden protocol changes against user interests may be seen as overly pessimistic about governance mechanisms",
        "The argument that specialized hardware requirements inherently lead to dangerous centralization could be disputed by those who believe economic incentives adequately secure networks regardless of node operator types"
      ]
    }
  },
  {
    "id": "general-2021-04-07-sharding",
    "title": "Why sharding is great: demystifying the technical properties",
    "date": "2021-04-07",
    "category": "protocol",
    "url": "https://vitalik.eth.limo/general/2021/04/07/sharding.html",
    "path": "general/2021/04/07/sharding.html",
    "content": "- \n\n- \n\n  \n    \n\n  \n  \n  Dark Mode Toggle\n  \n    \n\n  \n\n##  Why sharding is great: demystifying the technical properties \n\n 2021 Apr 07 \nSee all posts\n\n \n \n\n Why sharding is great: demystifying the technical properties \n\nSpecial thanks to Dankrad Feist and Aditya Asgaonkar for\nreview\n\nSharding is the future of Ethereum scalability, and it will be key to\nhelping the ecosystem support many thousands of transactions per second\nand allowing large portions of the world to regularly use the platform\nat an affordable cost. However, it is also one of the more misunderstood\nconcepts in the Ethereum ecosystem and in blockchain ecosystems more\nbroadly. It refers to a very specific set of ideas with very specific\nproperties, but it often gets conflated with techniques that have very\ndifferent and often much weaker security properties. The purpose of this\npost will be to explain exactly what specific properties sharding\nprovides, how it differs from other technologies that are not\nsharding, and what sacrifices a sharded system has to make to achieve\nthese properties.\n\nOne of the many depictions of a sharded version of Ethereum.\nOriginal diagram by Hsiao-wei Wang, design by\nQuantstamp.\n\n## The Scalability Trilemma\n\nThe best way to describe sharding starts from the problem statement\nthat shaped and inspired the solution: the Scalability\nTrilemma.\n\nThe scalability trilemma says that there are three properties that a\nblockchain try to have, and that, if you stick to \"simple\"\ntechniques, you can only get two of those three. The three\nproperties are:\n\n- Scalability: the chain can process more\ntransactions than a single regular node (think: a consumer laptop) can\nverify.\n\n- Decentralization: the chain can run without any\ntrust dependencies on a small group of large centralized actors. This is\ntypically interpreted to mean that there should not be any trust (or\neven honest-majority assumption) of a set of nodes that you cannot join\nwith just a consumer laptop.\n\n- Security: the chain can resist a large percentage\nof participating nodes trying to attack it (ideally 50%; anything above\n25% is fine, 5% is definitely not fine).\n\nNow we can look at the three classes of \"easy solutions\" that only\nget two of the three:\n\n- Traditional blockchains - including Bitcoin,\npre-PoS/sharding Ethereum, Litecoin, and other similar chains. These\nrely on every participant running a full node that verifies every\ntransaction, and so they have decentralization and security, but not\nscalability.\n\n- High-TPS chains - including the DPoS family but\nalso many others. These rely on a small number of nodes (often 10-100)\nmaintaining consensus among themselves, with users having to trust a\nmajority of these nodes. This is scalable and secure (using the\ndefinitions above), but it is not decentralized.\n\n- Multi-chain ecosystems - this refers to the general\nconcept of \"scaling out\" by having different applications live on\ndifferent chains and using cross-chain-communication protocols to talk\nbetween them. This is decentralized and scalable, but it is not secure,\nbecause an attacker need only get a consensus node majority in one of\nthe many chains (so often <1% of the whole ecosystem) to break that\nchain and possibly cause ripple effects that cause great damage to\napplications in other chains.\n\nSharding is a technique that gets you all three. A\nsharded blockchain is:\n\n- Scalable: it can process far more transactions than\na single node\n\n- Decentralized: it can survive entirely on consumer\nlaptops, with no dependency on \"supernodes\" whatsoever\n\n- Secure: an attacker can't target a small part of\nthe system with a small amount of resources; they can only try to\ndominate and attack the whole thing\n\nThe rest of the post will be describing how sharded blockchains\nmanage to do this.\nSharding through Random\nSampling\n\nThe easiest version of sharding to understand is sharding through\nrandom sampling. Sharding through random sampling has weaker trust\nproperties than the forms of sharding that we are building towards in\nthe Ethereum ecosystem, but it uses simpler technology.\n\nThe core idea is as follows. Suppose that you have a proof of stake\nchain with a large number (eg. 10000) validators, and you have a large\nnumber (eg. 100) blocks that need to be verified. No single computer is\npowerful enough to validate all of these blocks before the next\nset of blocks comes in.\n\nHence, what we do is we randomly split up the work of doing\nthe verification. We randomly shuffle the validator list, and\nwe assign the first 100 validators in the shuffled list to verify the\nfirst block, the second 100 validators in the shuffled list to verify\nthe second block, etc. A randomly selected group of validators that gets\nassigned to verify a block (or perform some other task) is called a\ncommittee.\n\nWhen a validator verifies a block, they publish a signature attesting\nto the fact that they did so. Everyone else, instead of verifying 100\nentire blocks, now only verifies 10000 signatures - a much smaller\namount of work, especially with BLS\nsignature aggregation. Instead of every block being broadcasted\nthrough the same P2P network, each block is broadcasted on a different\nsub-network, and nodes need only join the subnets corresponding to the\nblocks that they are responsible for (or are interested in for other\nreasons).\n\nConsider what happens if each node's computing power increases by 2x.\nBecause each node can now safely validate 2x more signatures, you could\ncut the minimum staking deposit size to support 2x more validators, and\nso hence you can make 200 committees instead of 100. Hence, you can\nverify 200 blocks per slot instead of 100. Furthermore, each\nindividual block could be 2x bigger. Hence, you have 2x more blocks\nof 2x the size, or 4x more chain capacity altogether.\n\nWe can introduce some math lingo to talk about what's going on. Using\nBig O\nnotation, we use \"O(C)\" to refer to the\ncomputational capacity of a single node. A traditional blockchain can\nprocess blocks of size O(C). A sharded chain as\ndescribed above can process O(C) blocks in parallel\n(remember, the cost to each node to verify each block indirectly is\nO(1) because each node only needs to verify a fixed\nnumber of signatures), and each block has O(C)\ncapacity, and so the sharded chain's total capacity is\nO(C2). This is why we call this type of\nsharding quadratic sharding, and this effect is a key\nreason why we think that in the long run, sharding is the best way to\nscale a blockchain.\nFrequently\nasked question: how is splitting into 100 committees different from\nsplitting into 100 separate chains?\n\nThere are two key differences:\n\n- The random sampling prevents the attacker from concentrating\ntheir power on one shard. In a 100-chain multichain ecosystem,\nthe attacker only needs ~0.5% of the total stake to wreak havoc: they\ncan focus on 51% attacking a single chain. In a sharded blockchain, the\nattacker must have close to ~30-40% of the entire stake to do\nthe same (in other words, the chain has shared\nsecurity). Certainly, they can wait until they get lucky and\nget 51% in a single shard by random chance despite having less than 50%\nof the total stake, but this gets exponentially harder for attackers\nthat have much less than 51%. If an attacker has less than ~30%, it's\nvirtually impossible.\n\n- Tight coupling: if even one shard gets a bad block, the\nentire chain reorgs to avoid it. There is a social contract\n(and in later sections of this document we describe some ways to enforce\nthis technologically) that a chain with even one bad block in even one\nshard is not acceptable and should get thrown out as soon as it is\ndiscovered. This ensures that from the point of view of an application\nwithin the chain, there is perfect security: contract A can\nrely on contract B, because if contract B misbehaves due to an attack on\nthe chain, that entire history reverts, including the transactions in\ncontract A that misbehaved as a result of the malfunction in contract\nB.\n\nBoth of these differences ensure that sharding creates an environment\nfor applications that preserves the key safety properties of a\nsingle-chain environment, in a way that multichain ecosystems\nfundamentally do not.\nImproving\nsharding with better security models\n\nOne common refrain in Bitcoin circles, and one that I completely\nagree with, is that blockchains like Bitcoin (or Ethereum) do\nNOT completely rely on an honest majority assumption. If there\nis a 51% attack on such a blockchain, then the attacker can do\nsome nasty things, like reverting or censoring transactions,\nbut they cannot insert invalid transactions. And even if they do revert\nor censor transactions, users running regular nodes could easily detect\nthat behavior, so if the community wishes to coordinate to resolve the\nattack with a fork that takes away the attacker's power they could do so\nquickly.\n\nThe lack of this extra security is a key weakness of the more\ncentralized high-TPS chains. Such chains do not, and cannot,\nhave a culture of regular users running nodes, and so the major nodes\nand ecosystem players can much more easily get together and impose a\nprotocol change that the community heavily dislikes. Even worse, the\nusers' nodes would by default accept it. After some time, users would\nnotice, but by then the forced protocol change would be a fait accompli:\nthe coordination burden\nwould be on users to reject the change, and they would have to make\nthe painful decision to revert a day's worth or more of activity that\neveryone had thought was already finalized.\n\nIdeally, we want to have a form of sharding that avoids 51%\ntrust assumptions for validity, and preserves the powerful bulwark of\nsecurity that traditional blockchains get from full verification. And\nthis is exactly what much of our research over the last few years has\nbeen about.\nScalable verification of\ncomputation\n\nWe can break up the 51%-attack-proof scalable validation problem into\ntwo cases:\n\n- Validating computation: checking that some\ncomputation was done correctly, assuming you have possession of all the\ninputs to the computation\n\n- Validating data availability: checking that the\ninputs to the computation themselves are stored in some form where you\ncan download them if you really need to; this checking should be\nperformed without actually downloading the entire inputs\nthemselves (because the data could be too large to download for every\nblock)\n\nValidating a block in a blockchain involves both computation and data\navailability checking: you need to be convinced that the transactions in\nthe block are valid and that the new state root hash claimed in the\nblock is the correct result of executing those transactions, but you\nalso need to be convinced that enough data from the block was actually\npublished so that users who download that data can compute the state and\ncontinue processing the blockchain. This second part is a very subtle\nbut important concept called the data\navailability problem; more on this later.\n\nScalably validating computation is relatively easy; there are two\nfamilies of techniques: fraud proofs and\nZK-SNARKs.\n\nFraud proofs are one way to verify computation\nscalably.\n\nThe two technologies can be described simply as follows:\n\n- Fraud proofs are a system where to accept the\nresult of a computation, you require someone with a staked\ndeposit to sign a message of the form \"I certify that\nif you make computation C with input X, you\nget output Y\". You trust these messages by default, but you\nleave open the opportunity for someone else with a staked deposit to\nmake a challenge (a signed message saying \"I disagree,\nthe output is Z\"). Only when there is a challenge, all nodes run the\ncomputation. Whichever of the two parties was wrong loses their deposit,\nand all computations that depend on the result of that computation are\nrecomputed.\n\n- ZK-SNARKs are a form of cryptographic\nproof that directly proves the claim \"performing computation\nC on input X gives output Y\". The\nproof is cryptographically \"sound\": if C(x) does\nnot equal Y, it's computationally infeasible to\nmake a valid proof. The proof is also quick to verify, even if running\nC itself takes a huge amount of time. See this post for more\nmathematical details on ZK-SNARKs.\n\nComputation based on fraud proofs is scalable because \"in the normal\ncase\" you replace running a complex computation with verifying a single\nsignature. There is the exceptional case, where you do have to verify\nthe computation on-chain because there is a challenge, but the\nexceptional case is very rare because triggering it is very expensive\n(either the original claimer or the challenger loses a large deposit).\nZK-SNARKs are conceptually simpler - they just replace a computation\nwith a much cheaper proof verification - but the math behind how they\nwork is considerably more complex.\n\nThere is a class of semi-scalable system which only scalably\nverifies computation, while still requiring every node to verify all the\ndata. This can be made quite effective by using a set of compression\ntricks to replace most data with computation. This is the realm of rollups.\nScalable\nverification of data availability is harder\n\nA fraud proof cannot be used to verify availability of data. Fraud\nproofs for computation rely on the fact that the inputs to the\ncomputation are published on-chain the moment the original claim is\nsubmitted, and so if someone challenges, the challenge execution is\nhappening in the exact same \"environment\" that the original execution\nwas happening. In the case of checking data availability, you cannot do\nthis, because the problem is precisely the fact that there is too much\ndata to check to publish it on chain. Hence, a fraud proof scheme for\ndata availability runs into a key problem: someone could claim \"data X\nis available\" without publishing it, wait to get challenged, and only\nthen publish data X and make the challenger appear to the rest\nof the network to be incorrect.\n\nThis is expanded on in the\nfisherman's dilemma:\n\nThe core idea is that the two \"worlds\", one where V1 is an evil\npublisher and V2 is an honest challenger and the other where V1 is an\nhonest publisher and V2 is an evil challenger, are indistinguishable to\nanyone who was not trying to download that particular piece of data at\nthe time. And of course, in a scalable decentralized blockchain, each\nindividual node can only hope to download a small portion of the data,\nso only a small portion of nodes would see anything about what went on\nexcept for the mere fact that there was a disagreement.\n\nThe fact that it is impossible to distinguish who was right and who\nwas wrong makes it impossible to have a working fraud proof scheme for\ndata availability.\nFrequently\nasked question: so what if some data is unavailable? With a ZK-SNARK you\ncan be sure everything is valid, and isn't that enough?\n\nUnfortunately, mere validity is not sufficient to ensure a correctly\nrunning blockchain. This is because if the blockchain is valid\nbut all the data is not available, then users have no way of\nupdating the data that they need to generate proofs that any\nfuture block is valid. An attacker that generates a\nvalid-but-unavailable block but then disappears can effectively stall\nthe chain. Someone could also withhold a specific user's account data\nuntil the user pays a ransom, so the problem is not purely a liveness\nissue.\n\nThere are some strong information-theoretic arguments that this\nproblem is fundamental, and there is no clever trick (eg. involving cryptographic\naccumulators) that can get around it. See this paper for\ndetails.\nSo,\nhow do you check that 1 MB of data is available without actually trying\nto download it? That sounds impossible!\n\nThe key is a technology called data\navailability sampling. Data availability sampling works as\nfollows:\n\n- Use a tool called erasure coding to expand a piece\nof data with N chunks into a piece of data with 2N chunks such that\nany N of those chunks can recover the entire data.\n\n- To check for availability, instead of trying to download the\nentire data, users simply randomly select a constant\nnumber of positions in the block (eg. 30 positions), and accept\nthe block only when they have successfully found the chunks in the block\nat all of their selected positions.\n\nErasure codes transform a \"check for 100% availability\" (every single\npiece of data is available) problem into a \"check for 50% availability\"\n(at least half of the pieces are available) problem. Random sampling\nsolves the 50% availability problem. If less than 50% of the data is\navailable, then at least one of the checks will almost certainly fail,\nand if at least 50% of the data is available then, while some nodes may\nfail to recognize a block as available, it takes only one honest node to\nrun the erasure code reconstruction procedure to bring back the\nremaining 50% of the block. And so, instead of needing to download 1 MB\nto check the availability of a 1 MB block, you need only download a few\nkilobytes. This makes it feasible to run data availability checking on\nevery block. See this\npost for how this checking can be efficiently implemented with\npeer-to-peer subnets.\n\nA ZK-SNARK can be used to verify that the erasure coding on a piece\nof data was done correctly, and then Merkle branches can be\nused to verify individual chunks. Alternatively, you can use\npolynomial commitments (eg. Kate\n(aka KZG) commitments), which essentially do erasure coding\nand proving individual elements and correctness\nverification all in one simple component - and that's what Ethereum\nsharding is using.\nRecap:\nhow are we ensuring everything is correct again?\n\nSuppose that you have 100 blocks and you want to efficiently verify\ncorrectness for all of them without relying on committees. We need to do\nthe following:\n\n- Each client performs data availability sampling on\neach block, verifying that the data in each block is available, while\ndownloading only a few kilobytes per block even if the block as a whole\nis a megabyte or larger in size. A client only accepts a block when all\ndata of their availability challenges have been correctly responded\nto.\n\n- Now that we have verified data availability, it becomes easier to\nverify correctness. There are two techniques:\n\n- We can use fraud proofs: a few participants with\nstaked deposits can sign off on each block's correctness. Other nodes,\ncalled challengers (or fishermen)\nrandomly check and attempt to fully process blocks. Because we already\nchecked data availability, it will always be possible to download the\ndata and fully process any particular block. If they find an invalid\nblock, they post a challenge that everyone verifies. If\nthe block turns out to be bad, then that block and all future blocks\nthat depend on that need to be re-computed.\n\n- We can use ZK-SNARKs. Each block would come with a\nZK-SNARK proving correctness.\n\n- In either of the above cases, each client only needs to do a small\namount of verification work per block, no matter how big the block is.\nIn the case of fraud proofs, occasionally blocks will need to be fully\nverified on-chain, but this should be extremely rare because triggering\neven one challenge is very expensive.\n\nAnd that's all there is to it! In the case of Ethereum sharding, the\nnear-term plan is to make sharded blocks data-only;\nthat is, the shards are purely a \"data availability engine\",\nand it's the job of layer-2 rollups to\nuse that secure data space, plus either fraud proofs or ZK-SNARKs, to\nimplement high-throughput secure transaction processing capabilities.\nHowever, it's completely possible to create such a built-in system to\nadd \"native\" high-throughput execution.\nWhat\nare the key properties of sharded systems and what are the\ntradeoffs?\n\nThe key goal of sharding is to come as close as possible to\nreplicating the most important security properties of traditional\n(non-sharded) blockchains but without the need for each node to\npersonally verify each transaction.\n\nSharding comes quite close. In a traditional blockchain:\n\n- Invalid blocks cannot get through because\nvalidating nodes notice that they are invalid and ignore them.\n\n- Unavailable blocks cannot get through because\nvalidating nodes fail to download them and ignore them.\n\nIn a sharded blockchain with advanced security features:\n\n- Invalid blocks cannot get through because either:\n\n- A fraud proof quickly catches them and informs the entire network of\nthe block's incorrectness, and heavily penalizes the creator, or\n\n- A ZK-SNARK proves correctness, and you cannot make a valid ZK-SNARK\nfor an invalid block.\n\n- Unavailable blocks cannot get through because:\n\n- If less than 50% of a block's data is available, at least one data\navailability sample check will almost certainly fail for each client,\ncausing the client to reject the block,\n\n- If at least 50% of a block's data is available, then actually the\nentire block is available, because it takes only a single honest node to\nreconstruct the rest of the block.\n\nTraditional high-TPS chains without sharding do not have a way of\nproviding these guarantees. Multichain ecosystems do not have a way of\navoiding the problem of an attacker selecting one chain for attack and\neasily taking it over (the chains could share security, but if\nthis was done poorly it would turn into a de-facto traditional high-TPS\nchain with all its disadvantages, and if it was done well, it would just\nbe a more complicated implementation of the above sharding\ntechniques).\n\nSidechains are highly implementation-dependent, but\nthey are typically vulnerable to either the weaknesses of traditional\nhigh-TPS chains (this is if they share miners/validators), or the\nweaknesses of multichain ecosystems (this is if they do not share\nminers/validators). Sharded chains avoid these issues.\n\nHowever, there are some chinks in the sharded system's\narmor. Notably:\n\n- Sharded chains that rely only on committees are\nvulnerable to adaptive adversaries, and have weaker\naccountability. That is, if the adversary has the ability to\nhack into (or just shut down) any set of nodes of their choosing in real\ntime, then they only need to attack a small number of nodes to break a\nsingle committee. Furthermore, if an adversary (whether an adaptive\nadversary or just an attacker with 50% of the total stake) does break a\nsingle committee, only a few of their nodes (the ones in that committee)\ncan be publicly confirmed to be participating in that attack, and so\nonly a small amount of stake can be penalized. This is another key\nreason why data availability sampling together with either fraud proofs\nor ZK-SNARKs are an important complement to random sampling\ntechniques.\n\n- Data availability sampling is only secure if there is a\nsufficient number of online clients that they collectively make\nenough data availability sampling requests that the responses almost\nalways overlap to comprise at least 50% of the block. In practice, this\nmeans that there must be a few hundred clients online\n(and this number increases the higher the ratio of the capacity of the\nsystem to the capacity of a single node). This is a few-of-N trust model -\ngenerally quite trustworthy, but certainly not as robust as the 0-of-N\ntrust that nodes in non-sharded chains have for availability.\n\n- If the sharded chain relies on fraud proofs, then it relies\non timing assumptions; if the network is too slow, nodes could\naccept a block as finalized before the fraud proof comes in showing that\nit is wrong. Fortunately, if you follow a strict rule of reverting all\ninvalid blocks once the invalidity is discovered, this threshold is a\nuser-set parameter: each individual user chooses how long they wait\nuntil finality and if they didn't want long enough then suffer, but more\ncareful users are safe. Even still, this is a weakening of the user\nexperience. Using ZK-SNARKs to verify validity solves\nthis.\n\n- There is a much larger amount of raw data that needs to be\npassed around, increasing the risk of failures under extreme\nnetworking conditions. Small amounts of data are easier to send (and\neasier to safely\nhide, if a powerful government attempts to censor the chain) than\nlarger amounts of data. Block explorers need to store more data if they\nwant to hold the entire chain.\n\n- Sharded blockchains depend on sharded peer-to-peer networks, and\neach individual p2p \"subnet\" is easier to attack because it has\nfewer nodes. The subnet\nmodel used for data availability sampling mitigates this because\nthere is some redundancy between subnets, but even still there is a\nrisk.\n\nThese are valid concerns, though in our view they are far outweighed\nby the reduction in user-level centralization enabled by\nallowing more applications to run on-chain instead of through\ncentralized layer-2 services. That said, these concerns, especially the\nlast two, are in practice the real constraint on increasing a sharded\nchain's throughput beyond a certain point. There is a limit to the\nquadraticness of quadratic sharding.\n\nIncidentally, the growing safety risks of sharded blockchains if\ntheir throughput becomes too high are also the key reason why the effort\nto extend to super-quadratic sharding has been largely\nabandoned; it looks like keeping quadratic sharding just\nquadratic really is the happy medium.\nWhy not\ncentralized production and sharded verification?\n\nOne alternative to sharding that gets often proposed is to have a\nchain that is structured like a centralized high-TPS chain, except it\nuses data availability sampling and sharding on top to allow\nverification of validity and availability.\n\nThis improves on centralized high-TPS chains as they exist today, but\nit's still considerably weaker than a sharded system. This is for a few\nreasons:\n\n- It's much harder to detect censorship by block producers in\na high-TPS chain. Censorship detection requires either (i)\nbeing able to see every transaction and verify that there are\nno transactions that clearly deserve to get in that inexplicably fail to\nget in, or (ii) having a 1-of-N trust model in block producers\nand verifying that no blocks fail to get in. In a centralized high-TPS\nchain, (i) is impossible, and (ii) is harder because the small node\ncount makes even a 1-of-N trust model more likely to break, and if the\nchain has a block time that is too fast for DAS (as most centralized\nhigh-TPS chains do), it's very hard to prove that a node's blocks are\nnot being rejected simply because they are all being published too\nslowly.\n\n- If a majority of block producers and ecosystem members tries to\nforce through an unpopular protocol change, users' clients will\ncertainly detect it, but it's much harder for the\ncommunity to rebel and fork away because they would need to\nspin up a new set of very expensive high-throughput nodes to maintain a\nchain that keeps the old rules.\n\n- Centralized infrastructure is more vulnerable to censorship\nimposed by external actors. The high throughput of the block\nproducing nodes makes them very detectable and easier to shut down. It's\nalso politically and logistically easier to censor dedicated\nhigh-performance computation than it is to go after individual users'\nlaptops.\n\n- There's a stronger pressure for high-performance computation\nto move to centralized cloud services, increasing the risk that\nthe entire chain will be run within 1-3 companies' cloud services, and\nhence risk of the chain going down because of many block producers\nfailing simultaneously. A sharded chain with a culture of running\nvalidators on one's own hardware is again much less vulnerable to\nthis.\n\nProperly sharded systems are better as a base layer. Given a sharded\nbase layer, you can always create a centralized-production system (eg.\nbecause you want a high-throughput domain with synchronous\ncomposability for defi) layered on top by building it as a rollup.\nBut if you have a base layer with a dependency on centralized block\nproduction, you cannot build a more-decentralized layer 2 on top.",
    "contentLength": 27913,
    "summary": "Sharding enables blockchains to achieve scalability, decentralization, and security simultaneously through random validator sampling.",
    "detailedSummary": {
      "theme": "Vitalik explains how blockchain sharding solves the scalability trilemma by achieving scalability, decentralization, and security simultaneously through advanced cryptographic techniques.",
      "summary": "Vitalik introduces the scalability trilemma, which states that traditional blockchain systems can only achieve two of three properties: scalability, decentralization, and security. He explains how sharding breaks this limitation by using random sampling to distribute validation work across committees while maintaining shared security through tight coupling. The post details two critical technical challenges: scalable verification of computation (solved through fraud proofs or ZK-SNARKs) and scalable verification of data availability (solved through erasure coding and data availability sampling). Vitalik emphasizes that sharding differs fundamentally from multichain ecosystems because it prevents attackers from concentrating on weak points and ensures that any invalid block in any shard causes the entire chain to revert. He concludes by comparing sharded systems to alternatives like centralized high-TPS chains, arguing that proper sharding provides better censorship resistance, decentralization, and serves as a superior base layer for building additional scaling solutions on top.",
      "takeaways": [
        "Sharding solves the scalability trilemma by achieving quadratic scaling - O(C\u00b2) total capacity where C is individual node capacity",
        "Random sampling prevents attackers from concentrating power on individual shards, requiring ~30-40% of total stake rather than 0.5% for multichain attacks",
        "Data availability sampling using erasure coding allows verification of large blocks by downloading only small random samples",
        "Tight coupling ensures that invalid blocks in any shard cause the entire chain to revert, maintaining application security",
        "Sharded systems have tradeoffs including vulnerability to adaptive adversaries, timing assumptions for fraud proofs, and increased network complexity"
      ],
      "controversial": [
        "The claim that sharding is superior to all alternative scaling approaches, particularly dismissing multichain ecosystems as fundamentally insecure",
        "The assertion that few hundred online clients are sufficient for data availability sampling security, which some might consider a significant trust assumption"
      ]
    }
  },
  {
    "id": "general-2021-04-02-round9",
    "title": "Gitcoin Grants Round 9: The Next Phase of Growth",
    "date": "2021-04-02",
    "category": "governance",
    "url": "https://vitalik.eth.limo/general/2021/04/02/round9.html",
    "path": "general/2021/04/02/round9.html",
    "content": "- \n\n- \n\n  \n    \n\n  \n  \n  Dark Mode Toggle\n  \n    \n\n  \n\n##  Gitcoin Grants Round 9: The Next Phase of Growth \n\n 2021 Apr 02 \nSee all posts\n\n \n \n\n Gitcoin Grants Round 9: The Next Phase of Growth \n\nSpecial thanks to the Gitcoin team for feedback and\ndiagrams.\n\nSpecial note: Any criticism in these review posts of actions\ntaken by people or organizations, especially using terms like\n\"collusion\", \"bribe\" and \"cabal\", is only in the spirit of analysis and\nmechanism design, and should not be taken as (especially moral)\ncriticism of the people and organizations themselves. You're all\nwell-intentioned and wonderful people and I love you.\n\nGitcoin Grants Round 9 has just finished, and as usual the round has\nbeen a success. Along with 500,000 in matching funds, $1.38 million was\ndonated by over 12,000 contributors to 812 different projects, making\nthis the largest round so far. Not only old projects, but also new ones,\nreceived a large amount of funding, proving the mechanism's ability to\navoid entrenchment and adapt to changing circumstances. The new East\nAsia-specific category in the latest two rounds has also been a success,\nhelping to catapult multiple East Asian Ethereum projects to the\nforefront.\n\nHowever, with growing scale, round 9 has also brought out unique and\nunprecedented challenges. The most important among them is collusion and\nfraud: in round 9, over 15% of contributions were detected as being\nprobably fraudulent. This was, of course, inevitable and expected from\nthe start; I have actually been surprised at how long it has taken for\npeople to start to make serious attempts to exploit the mechanism. The\nGitcoin team has responded in force, and has published\na blog post detailing their strategies for detecting and responding\nto adversarial behavior along with a general\ngovernance overview. However, it is my opinion that to\nsuccessfully limit adversarial behavior in the long run more serious\nreforms, with serious sacrifices, are going to be required.\n\n## Many new, and bigger, funders\n\nGitcoin continues to be successful in attracting many matching\nfunders this round. BadgerDAO, a project that\ndescribes itself as a \"DAO dedicated to building products and\ninfrastructure to bring Bitcoin to DeFi\", has donated $300,000 to the\nmatching pool - the largest single donation so far.\n\nOther new funders include Uniswap, Stakefish, Maskbook, FireEyes, Polygon, SushiSwap and TheGraph. As Gitcoin Grants continues\nto establish itself as a successful home for Ethereum public goods\nfunding, it is also continuing to attract legitimacy as a focal point\nfor donations from projects wishing to support the ecosystem. This is a\nsign of success, and hopefully it will continue and grow further. The\nnext goal should be to get not just one-time contributions to the\nmatching pool, but long-term commitments to repeated contributions (or\neven newly launched tokens donating a percentage of their holdings to\nthe matching pool)!\nChurn continues to be\nhealthy\n\nOne long-time concern with Gitcoin Grants is the balance between\nstability and entrenchment: if each project's match award changes too\nmuch from round to round, then it's hard for teams to rely on Gitcoin\nGrants for funding, and if the match awards change too little, it's hard\nfor new projects to get included.\n\nWe can measure this! To start off, let's compare the top-10 projects\nin this round to the top-10 projects in the previous round.\n\nIn all cases, about half of the top-10 carries over from the previous\nround and about half is new (the flipside, of course is that half the\ntop-10 drops out). The charts are a slight understatement: the Gitcoin\nGrants dev fund and POAP appear to have dropped out but actually merely\nchanged categories, so something like 40% churn may be a more accurate\nnumber.\n\nIf you check the results from round 8 against round 7, you also get about\n50% churn, and comparing round 7 to round 6 gives similar values.\nHence, it is looking like the degree of churn is stable. To me, it seems\nlike roughly 40-50% churn is a healthy level, balancing long-time\nprojects' need for stability with the need to avoid new projects getting\nlocked out, but this is of course only my subjective judgement.\n\n## Adversarial behavior\n\nThe challenging new phenomenon this round was the sheer scale of the\nadversarial behavior that was attempted. In this round, there were two\nmajor issues. First, there were large clusters of contributors\ndiscovered that were probably a few individual or small closely\ncoordinated groups with many accounts trying to cheat the mechanism.\nThis was discovered by proprietary analysis algorithms used by the\nGitcoin team.\n\nFor this round, the Gitcoin team, in consultation with the community,\ndecided to eat the cost of the fraud. Each project received the maximum\nof the match award it would receive if fraudulent transactions were\naccepted and the match award it would receive if they were not; the\ndifference, about $33,000 in total, was paid out of Gitcoin's treasury.\nFor future rounds, however, the team aims to be significantly stricter\nabout security.\n\n A diagram from\nthe\nGitcoin team's post describin their process for finding and dealing\nwith adversarial behavior. \n\nIn the short term, simply ignoring fraud and accepting its costs has\nso far worked okay. In the long term, however, fraud must be dealt with,\nand this raises a challenging political concern. The algorithms that the\nGitcoin team used to detect the adversarial behavior are proprietary and\nclosed-source, and they have to be closed-source because\notherwise the attackers could adapt and get around them. Hence, the\noutput of the quadratic funding round is not just decided by a clear\nmathematical formula of the inputs. Rather, if fraudulent transactions\nwere to be removed, it would also be fudged by what risks becoming a\nclosed group twiddling with the outputs according to their arbitrary\nsubjective judgements.\n\nIt is worth stressing that this is not Gitcoin's fault.\nRather, what is happening is that Gitcoin has gotten big enough that it\nhas finally bumped into the exact same problem that every social media\nsite, no matter how well-meaning its team, has been bumping into for the\npast twenty years. Reddit, despite its well-meaning and\nopen-source-oriented team, employs many\nsecretive tricks\nto detect and clamp down on vote manipulation, as does every other\nsocial media site.\n\nThis is because making algorithms that prevent undesired\nmanipulation, but continue to do so despite the attackers themselves\nknowing what these algorithms are, is really hard. In fact,\nthe entire science of mechanism\ndesign is a half-century-long effort to try to solve this\nproblem. Sometimes, there are successes. But often, they keep\nrunning into the same challenge: collusion. It turns out\nthat it's not that hard to make mechanisms that give the outcomes you\nwant if all of the participants are acting independently, but once you\nadmit the possibility of one individual controlling many accounts, the\nproblem quickly becomes much harder (or even intractable).\n\nBut the fact that we can't achieve perfection doesn't mean that we\ncan't try to come closer, and benefit from coming closer. Good\nmechanisms and opaque centralized intervention are substitutes: the\nbetter the mechanism, the closer to a good result the mechanism gets all\nby itself, and the more the secretive moderation cabal can go on\nvacation (an outcome that the actually-quite-friendly-and-cuddly and\ndecentralization-loving Gitcoin moderation cabal very much wants!). In\nthe short term, the Gitcoin team is also proactively taking a third\napproach: making fraud detection and response accountable by inviting\nthird-party analysis and community oversight.\n\n Picture courtesy of the Gitcoin team's excellent blog\npost.\n\nInviting community oversight is an excellent step in preserving the\nmechanism's legitimacy, and in paving\nthe way for an eventual decentralization of the Gitcoin grants\ninstitution. However, it's not a 100% solution: as we've seen with\ntechnocratic organizations inside national governments, it's actually\nquite easy for them to retain a large amount of power despite formal\ndemocratic oversight and control. The long-term solution is\nshoring up Gitcoin's passive security, so that active security\nof this type becomes less necessary.\n\nOne important form of passive security is making some form of\nunique-human verification no longer optional, but instead mandatory.\nGitcoin already adds the option to use phone number verification,\nBrightID and several other techniques to \"improve an account's trust\nscore\" and get greater matching. But what Gitcoin will likely be forced\nto do is make it so that some verification is required to get any\nmatching at all. This will be a reduction in convenience, but the\neffects can be mitigated by the Gitcoin team's work on enabling more\ndiverse and decentralized verification options, and the long-term\nbenefit in enabling security without heavy reliance on centralized\nmoderation, and hence getting longer-lasting legitimacy, is very much\nworth it.\n\n## Retroactive airdrops\n\nA second major issue this round had to do with Maskbook. In February,\nMaskbook announced\na token and the token distribution included a retroactive airdrop to\nanyone who had donated to Maskbook in previous rounds.\n\nThe table from Maskbook's announcement post showing who is\neligible for the airdrops.\n\nThe controversy was that Maskbook was continuing to maintain a\nGitcoin grant this round, despite now being wealthy and having set a\nprecedent that donors to their grant might be rewarded in the future.\nThe latter issue was particularly problematic as it could be\nconstrued as a form of obfuscated vote buying. Fortunately, the\nsituation was defused quickly; it turned out that the Maskbook team had\nsimply forgotten to consider shutting down the grant after they released\ntheir token, and they agreed to shut it down. They are now even part of\nthe funders' league, helping to provide matching funds for future\nrounds!\n\nAnother project attempted what some construed as a \"wink wink nudge\nnudge\" strategy of obfuscated vote buying: they hinted in chat rooms\nthat they have a Gitcoin grant and they are going to have a token. No\nexplicit promise to reward contributors was made, but there's a case\nthat the people reading those messages could have interpreted it as\nsuch.\n\nIn both cases, what we are seeing is that collusion is a\nspectrum, not a binary. In fact, there's a pretty wide part of the\nspectrum that even completely well-meaning and legitimate projects and\ntheir contributors could easily engage in.\n\nNote that this is a somewhat unusual \"moral hierarchy\". Normally, the\nmore acceptable motivations would be the altruistic ones, and the less\nacceptable motivations would be the selfish ones. Here, though, the\nmotivations closest to the left and the right are selfish; the\naltruistic motivation is close to the left, but it's not the\nonly motivation close to the left. The key differentiator is\nsomething more subtle: are you contributing because you like the\nconsequences of the project getting funded (inside-the-mechanism), or\nare you contributing because you like some (outside-the-mechanism)\nconsequences of you personally funding the\nproject?\n\nThe latter motivation is problematic because it subverts the workings\nof quadratic funding. Quadratic funding is all about assuming that\npeople contribute because they like the consequences of the project\ngetting funded, recognizing that the amounts that people contribute will\nbe much less than they ideally \"should be\" due to the tragedy of the\ncommons, and mathematically compensating for that. But if there are\nlarge side-incentives for people to contribute, and these\nside-incentives are attached to that person specifically and so they are\nnot reduced by the tragedy of the commons at all, then the quadratic\nmatching magnifies those incentives into a very large\ndistortion.\n\nIn both cases (Maskbook, and the other project), we saw something in\nthe middle. The case of the other project is clear: there was an\naccusation that they made hints at the possibility of formal\ncompensation, though it was not explicitly promised. In the case of\nMaskbook, it seems as though Maskbook did nothing wrong: the airdrop was\nretroactive, and so none of the contributions to Maskbook were \"tainted\"\nwith impute motives. But the problem is more long-term and subtle:\nif there's a long-term pattern of projects making\nretroactive airdrops to Gitcoin contributors, then users will feel a\npressure to contribute primarily not to projects that they think are\npublic goods, but rather to projects that they think are likely to later\nhave tokens. This subverts the dream of using Gitcoin quadratic\nfunding to provide alternatives to token issuance as a monetization\nstrategy.\nThe\nsolution: making bribes (and retroactive airdrops) cryptographically\nimpossible\n\nThe simplest approach would be to delist projects whose behavior\ncomes too close to collusion from Gitcoin. In this case, though, this\nsolution cannot work: the problem is not projects doing airdrops\nwhile soliciting contributions, the problem is projects doing\nairdrops after soliciting contributions. While such a project\nis still soliciting contributions and hence vulnerable to being\ndelisted, there is no indication that they are planning to do an\nairdrop. More generally, we can see from the examples above that\npolicing motivations is a tough challenge with many gray areas, and is\ngenerally not a good fit for the spirit of mechanism design. But if\ndelisting and policing motivations is not the solution, then what\nis?\n\nThe solution comes in the form of a technology called MACI.\n\nMACI is a toolkit that allows you to run collusion-resistant\napplications, which simultaneously guarantee several key\nproperties:\n\n- Correctness: invalid messages do not get processed,\nand the result that the mechanism outputs actually is the result of\nprocessing all valid messages and correctly computing the result.\n\n- Censorship resistance: if someone participates, the\nmechanism cannot cheat and pretend they did not participate by\nselectively ignoring their messages.\n\n- Privacy: no one else can see how each individual\nparticipated.\n\n- Collusion resistance: a participant cannot prove to\nothers how they participated, even if they wanted to prove\nthis.\n\nCollusion resistance is the key property: it makes bribes (or\nretroactive airdrops) impossible, because users would have no way to\nprove that they actually contributed to someone's grant or voted for\nsomeone or performed whatever other action. This is a realization of the\nsecret ballot\nconcept which makes vote buying impractical today, but with\ncryptography.\n\nThe technical description of how this works is not that difficult.\nUsers participate by signing a message with their private key,\nencrypting the signed message to a public key published by a\ncentral server, and publishing the encrypted signed message to the\nblockchain. The server downloads the messages from the blockchain,\ndecrypts them, processes them, and outputs the result along with a ZK-SNARK to ensure that they\ndid the computation correctly.\n\nUsers cannot prove how they participated, because they have the\nability to send a \"key change\" message to trick anyone trying to audit\nthem: they can first send a key change message to change their key from\nA to B, and then send a \"fake message\" signed with A. The server would\nreject the message, but no one else would have any way of knowing that\nthe key change message had ever been sent. There is a trust requirement\non the server, though only for privacy and coercion resistance; the\nserver cannot publish an incorrect result either by computing\nincorrectly or by censoring messages. In the long term, multi-party\ncomputation can be used to decentralize the server somewhat,\nstrengthening the privacy and coercion resistance guarantees.\n\nThere is already a quadratic funding system using MACI: clr.fund. It works, though at the moment\nproof generation is still quite expensive; ongoing work on the project\nwill hopefully decrease these costs soon.\n\n## Practical concerns\n\nNote that adopting MACI does come with necessary\nsacrifices. In particular, there would no longer be the ability\nto see who contributed to what, weakening Gitcoin's \"social\" aspects.\nHowever, the social aspects could be redesigned and changed by taking\ninsights from elections: elections, despite their secret ballot,\nfrequently give out \"I voted\" stickers. They are not \"secure\" (in that a\nnon-voter can easily get one), but they still serve the social function.\nOne could go further while still preserving the secret ballot property:\none could make a quadratic funding setup where MACI outputs the\nvalue of how much each participant contributed, but not who\nthey contributed to. This would make it impossible for specific\nprojects to pay people to contribute to them, but would still leave lots\nof space for users to express their pride in contributing. Projects\ncould airdrop to all Gitcoin contributors without\ndiscriminating by project, and announce that they're doing this together\nwith a link to their Gitcoin profile. However, users would still be able\nto contribute to someone else and collect the airdrop; hence, this would\narguably be within bounds of fair play.\n\nHowever, this is still a longer-term concern; MACI is likely not\nready to be integrated for round 10. For the next few rounds, focusing\non stepping up unique-human verification is still the best priority.\nSome ongoing reliance on centralized moderation will be required, though\nhopefully this can be simultaneously reduced and made more accountable\nto the community. The Gitcoin team has already been taking excellent\nsteps in this direction. And if the Gitcoin team does successfully play\ntheir role as pioneers in being the first to brave and overcome these\nchallenges, then we will end up with a secure and scalable quadratic\nfunding system that is ready for much broader mainstream\napplications!",
    "contentLength": 18013,
    "summary": "Gitcoin Grants Round 9 raised $1.38M but faced unprecedented fraud (15% of contributions) requiring closed-source detection algorithms.",
    "detailedSummary": {
      "theme": "Vitalik analyzes the successes and growing challenges of Gitcoin Grants Round 9, particularly addressing fraud, collusion, and the need for better security mechanisms in quadratic funding systems.",
      "summary": "Vitalik examines Gitcoin Grants Round 9, which achieved record success with $1.38 million donated by over 12,000 contributors to 812 projects, supported by $500,000 in matching funds from major funders like BadgerDAO, Uniswap, and others. However, this growth brought unprecedented challenges, with over 15% of contributions detected as fraudulent, forcing the Gitcoin team to rely on proprietary, closed-source algorithms for fraud detection. Vitalik argues this creates a concerning centralization problem similar to what social media platforms face when combating manipulation. He also discusses issues with retroactive airdrops, particularly the Maskbook case, where projects rewarded past contributors with tokens, potentially creating perverse incentives that subvert quadratic funding's intended mechanics. Vitalik proposes MACI (Minimum Anti-Collusion Infrastructure) as a long-term cryptographic solution that would make bribes and vote buying impossible through secret ballot mechanisms, though he acknowledges this would require sacrificing some social features and isn't ready for immediate implementation.",
      "takeaways": [
        "Gitcoin Grants Round 9 was the largest yet but faced significant fraud challenges with over 15% of contributions being fraudulent",
        "The reliance on proprietary fraud detection algorithms creates centralization concerns that mirror problems faced by social media platforms",
        "Retroactive airdrops by funded projects can create perverse incentives that undermine quadratic funding's effectiveness",
        "MACI technology offers a cryptographic solution to prevent collusion and vote buying through secret ballot mechanisms",
        "Short-term solutions should focus on mandatory unique-human verification while working toward longer-term cryptographic safeguards"
      ],
      "controversial": [
        "Vitalik's assertion that some level of centralized moderation and secretive algorithms may be necessary and acceptable in the short term",
        "The proposal that MACI should eliminate transparency about who contributes to what projects, potentially reducing community engagement and social aspects"
      ]
    }
  },
  {
    "id": "general-2021-03-23-legitimacy",
    "title": "The Most Important Scarce Resource is Legitimacy",
    "date": "2021-03-23",
    "category": "governance",
    "url": "https://vitalik.eth.limo/general/2021/03/23/legitimacy.html",
    "path": "general/2021/03/23/legitimacy.html",
    "content": "- \n\n- \n\n  \n    \n\n  \n  \n  Dark Mode Toggle\n  \n    \n\n  \n\n##  The Most Important Scarce Resource is Legitimacy \n\n 2021 Mar 23 \nSee all posts\n\n \n \n\n The Most Important Scarce Resource is Legitimacy \n\nSpecial thanks to Karl Floersch, Aya Miyaguchi and Mr Silly for\nideas, feedback and review.\n\nThe Bitcoin and Ethereum blockchain ecosystems both spend far more on\nnetwork security - the goal of proof of work mining - than they do on\neverything else combined. The Bitcoin blockchain has paid an average of\nabout $38 million per day in block rewards to miners since the\nstart of the year, plus about\n$5m/day in transaction fees. The Ethereum blockchain comes in\nsecond, at $19.5m/day in block rewards plus $18m/day in tx\nfees. Meanwhile, the Ethereum Foundation's annual budget, paying for\nresearch, protocol development, grants and all sorts of other expenses,\nis a mere $30 million per year. Non-EF-sourced funding exists\ntoo, but it is at most only a few times larger. Bitcoin ecosystem\nexpenditures on R&D are likely even lower. Bitcoin ecosystem R&D\nis largely funded by companies (with $250m total raised so far according\nto this page), and this\nreport suggests about 57 employees; assuming fairly high salaries\nand many paid developers not being counted, that works out to about $20m\nper year.\n\nClearly, this expenditure pattern is a massive misallocation of\nresources. The last 20% of network hashpower provides\nvastly less value to the ecosystem than those same resources\nwould if they had gone into research and core protocol development. So\nwhy not just.... cut the PoW budget by 20% and redirect the funds to those\nother things instead?\n\nThe standard answer to this puzzle has to do with concepts like \"public\nchoice theory\" and \"Schelling\nfences\": even though we could easily identify some valuable public\ngoods to redirect some funding to as a one-off, making a regular\ninstitutionalized pattern of such decisions carries risks of\npolitical chaos and capture that are in the long run not worth it. But\nregardless of the reasons why, we are faced with this interesting fact\nthat the organisms that are the Bitcoin and Ethereum ecosystems\nare capable of summoning up billions of dollars of capital, but have\nstrange and hard-to-understand restrictions on where that capital can\ngo.\n\nThe powerful social force that is creating this effect is worth\nunderstanding. As we are going to see, it's also the same social force\nbehind why the Ethereum ecosystem is capable of summoning up these\nresources in the first place (and the technologically near-identical\nEthereum Classic is not). It's also a social force that is key to\nhelping a chain recover from a 51% attack. And it's a social force that\nunderlies all sorts of extremely powerful mechanisms far beyond the\nblockchain space. For reasons that will be clear in the upcoming\nsections, I will give this powerful social force a name:\nlegitimacy.\nCoins can be owned by\nsocial contracts\n\nTo better understand the force that we are getting at, another\nimportant example is the epic saga of Steem and Hive. In early 2020, Justin\nSun bought Steem-the-company,\nwhich is not the same thing as Steem-the-blockchain but did hold about\n20% of the STEEM token supply. The community, naturally, did not trust\nJustin Sun. So they made an on-chain vote to formalize what they\nconsidered to be a longstanding \"gentleman's agreement\" that\nSteem-the-company's coins were held in trust for the common good of\nSteem-the-blockchain and should not be used to vote. With the help of\ncoins held by exchanges, Justin Sun made a counterattack, and won\ncontrol of enough delegates to unilaterally control the chain. The\ncommunity saw no further in-protocol options. So instead they made a\nfork of Steem-the-blockchain, called Hive, and copied over all of the\nSTEEM token balances - except those, including Justin Sun's, which\nparticipated in the attack.\n\n And they got plenty of applications on board. If they\nhad not managed this, far more users would have either stayed on Steem\nor moved to some different project entirely. \n\nThe lesson that we can learn from this situation is this:\nSteem-the-company never actually \"owned\" the coins. If they\ndid, they would have had the practical ability to use,\nenjoy and abuse the coins in whatever way they wanted. But in\nreality, when the company tried to enjoy and abuse the coins in a way\nthat the community did not like, they were successfully\nstopped. What's going on here is a pattern of a similar type to\nwhat we saw with the not-yet-issued Bitcoin and Ethereum coin rewards:\nthe coins were ultimately owned not by a cryptographic key, but by some\nkind of social contract.\n\nWe can apply the same reasoning to many other structures in the\nblockchain space. Consider, for example, the ENS root multisig. The root\nmultisig is controlled by seven prominent ENS and Ethereum community\nmembers. But what would happen if four of them were to come together and\n\"upgrade\" the registrar to one that transfers all the best domains to\nthemselves? Within the context of ENS-the-smart-contract-system, they\nhave the complete and unchallengeable ability to do this. But if they\nactually tried to abuse their technical ability in this way, what would\nhappen is clear to anyone: they would be ostracized from the community,\nthe remaining ENS community members would make a new ENS contract that\nrestores the original domain owners, and every Ethereum application that\nuses ENS would repoint their UI to use the new one.\n\nThis goes well beyond smart contract structures. Why is it that Elon\nMusk can sell an NFT of Elon Musk's tweet, but Jeff Bezos would have a\nmuch harder time doing the same? Elon and Jeff have the same level of\nability to screenshot Elon's tweet and stick it into an NFT dapp, so\nwhat's the difference? To anyone who has even a basic intuitive\nunderstanding of human social psychology (or the fake\nart scene), the answer is obvious: Elon selling Elon's tweet is\nthe real thing, and Jeff doing the same is not. Once again,\nmillions of dollars of value are being controlled and allocated, not by\nindividuals or cryptographic keys, but by social conceptions of\nlegitimacy.\n\nAnd, going even further out, legitimacy governs all sorts of social\nstatus games, intellectual\ndiscourse, language, property rights, political systems and national\nborders. Even blockchain consensus works the same way: the only\ndifference between a soft fork that gets accepted by the community and a\n51% censorship attack after which the community coordinates an extra-protocol\nrecovery fork to take out the attacker is legitimacy.\n\n## So what is legitimacy?\n\nSee also: my earlier post on blockchain\ngovernance.\n\nTo understand the workings of legitimacy, we need to dig down into\nsome game theory. There are many situations in life that demand\ncoordinated behavior: if you act in a certain way\nalone, you are likely to get nowhere (or worse), but if everyone acts\ntogether a desired result can be achieved.\n\n  An abstract coordination game. You benefit heavily\nfrom making the same move as everyone else. \n\nOne natural example is driving on the left vs right side of the road:\nit doesn't really matter what side of the road people drive on,\nas long as they drive on the same side. If you switch sides at the same\ntime as everyone else, and most people prefer the new arrangement, there\ncan be a net benefit. But if you switch sides alone, no matter how much\nyou prefer driving on the other side, the net result for you will be\nquite negative.\n\nNow, we are ready to define legitimacy.\n\nLegitimacy is a pattern of higher-order\nacceptance. An outcome in some social context is legitimate if\nthe people in that social context broadly accept and play their part in\nenacting that outcome, and each individual person does so because they\nexpect everyone else to do the same.\n\nLegitimacy is a phenomenon that arises naturally in coordination\ngames. If you're not in a coordination game, there's no reason to act\naccording to your expectation of how other people will act, and so\nlegitimacy is not important. But as we have seen, coordination games are\neverywhere in society, and so legitimacy turns out to be quite important\nindeed. In almost any environment with coordination games that exists\nfor long enough, there inevitably emerge some mechanisms that can choose\nwhich decision to take. These mechanisms are powered by an established\nculture that everyone pays attention to these mechanisms and (usually)\ndoes what they say. Each person reasons that because everyone\nelse follows these mechanisms, if they do something different they\nwill only create conflict and suffer, or at least be left in a lonely\nforked ecosystem all by themselves. If a mechanism successfully has the\nability to make these choices, then that mechanism has legitimacy.\n\n  A Byzantine general rallying his troops forward. The\npurpose of this isn't just to make the soldiers feel brave and excited,\nbut also to reassure them that everyone else feels brave and excited and\nwill charge forward as well, so an individual soldier is not just\ncommitting suicide by charging forward alone. \n\nIn any context where there's a coordination game that has existed for\nlong enough, there's likely a conception of legitimacy. And\nblockchains are full of coordination games. Which client\nsoftware do you run? Which decentralized domain name registry do you ask\nfor which address corresponds to a .eth name? Which copy of the Uniswap\ncontract do you accept as being \"the\" Uniswap exchange? Even NFTs are a\ncoordination game. The two largest parts of an NFT's value are (i) pride\nin holding the NFT and ability to show off your ownership, and (ii) the\npossibility of selling it in the future. For both of these components,\nit's really really important that whatever NFT you buy is recognized as\nlegitimate by everyone else. In all of these cases, there's a\ngreat benefit to having the same answer as everyone else, and the\nmechanism that determines that equilibrium has a lot of power.\n\n## Theories of legitimacy\n\nThere are many different ways in which legitimacy can come about. In\ngeneral, legitimacy arises because the thing that gains legitimacy is\npsychologically appealing to most people. But of course, people's\npsychological intuitions can be quite complex. It is impossible to make\na full listing of theories of legitimacy, but we can start with a\nfew:\n\n- Legitimacy by brute force: someone convinces\neveryone that they are powerful enough to impose their will and\nresisting them will be very hard. This drives most people to submit\nbecause each person expects that everyone else will be too\nscared to resist as well.\n\n- Legitimacy by continuity: if something was\nlegitimate at time T, it is by default legitimate at time T+1.\n\n- Legitimacy by fairness: something can become\nlegitimate because it satisfies an intuitive notion of fairness. See\nalso: my post on\ncredible neutrality, though note that this is not the only kind of\nfairness.\n\n- Legitimacy by process: if a process is legitimate,\nthe outputs of that process gain legitimacy (eg. laws passed by\ndemocracies are sometimes described in this way).\n\n- Legitimacy by performance: if the outputs of a\nprocess lead to results that satisfy people, then that process can gain\nlegitimacy (eg. successful dictatorships are sometimes described in this\nway).\n\n- Legitimacy by participation: if people participate\nin choosing an outcome, they are more likely to consider it legitimate.\nThis is similar to fairness, but not quite: it rests on a psychological\ndesire to be consistent with your previous actions.\n\nNote that legitimacy is a descriptive concept; something can be\nlegitimate even if you personally think that it is horrible. That said,\nif enough people think that an outcome is horrible, there is a higher\nchance that some event will happen in the future that will cause that\nlegitimacy to go away, often at first gradually, then suddenly.\nLegitimacy\nis a powerful social technology, and we should use it\n\nThe public goods funding situation in cryptocurrency ecosystems is\nfairly poor. There are hundreds of billions of dollars of capital\nflowing around, but public goods that are key to that capital's ongoing\nsurvival are receiving only tens of millions of dollars per year of\nfunding.\n\nThere are two ways to respond to this fact. The first way is to be\nproud of these limitations and the valiant, even if not particularly\neffective, efforts that your community makes to work around them. This\nseems to be the route that the Bitcoin ecosystem often takes:\n\nThe personal self-sacrifice of the teams funding core development is\nof course admirable, but it's admirable the same way that Eliud Kipchoge\nrunning a marathon in under 2 hours is admirable: it's an impressive\nshow of human fortitude, but it's not the future of transportation (or,\nin this case, public goods funding). Much like we have much better\ntechnologies to allow people to move 42 km in under an hour without\nexceptional fortitude and years of training, we should also\nfocus on building better social technologies to fund public goods at the\nscales that we need, and as a systemic part of our economic ecology and\nnot one-off acts of philanthropic initiative.\n\nNow, let us get back to cryptocurrency. A major power of\ncryptocurrency (and other digital assets such as domain names, virtual\nland and NFTs) is that it allows communities to summon up large amounts\nof capital without any individual person needing to personally donate\nthat capital. However, this capital is constrained by conceptions of\nlegitimacy: you cannot simply allocate it to a centralized team\nwithout compromising on what makes it valuable. While Bitcoin and\nEthereum do already rely on conceptions of legitimacy to respond\nto 51% attacks, using conceptions of legitimacy to guide in-protocol\nfunding of public goods is much harder. But at the increasingly\nrich\napplication layer where new protocols are constantly being created, we\nhave quite a bit more flexibility in where that funding could go.\n\n## Legitimacy in Bitshares\n\nOne of the long-forgotten, but in my opinion very innovative, ideas\nfrom the early cryptocurrency space was the Bitshares\nsocial\nconsensus model. Essentially, Bitshares described itself as a\ncommunity of people (PTS and AGS\nholders) who were willing to help collectively support an ecosystem\nof new projects, but for a project to be welcomed into the ecosystem, it\nwould have to allocate 10% of its token supply to existing PTS and AGS\nholders.\n\nNow, of course anyone can make a project that does not allocate any\ncoins to PTS/AGS holders, or even fork a project that did make an\nallocation and take the allocation out. But, as Dan Larimer says:\n\nYou cannot force anyone to do anything, but in this market is is all\nnetwork effect. If someone comes up with a compelling implementation\nthen you can adopt the entire PTS community for the cost of generating a\nnew genesis block. The individual who decided to start from scratch\nwould have to build an entire new community around his system.\nConsidering the network effect, I suspect that the coin that honors\nProtoShares will win.\n\nThis is also a conception of legitimacy: any project that makes the\nallocation to PTS/AGS holders will get the attention and support of the\ncommunity (and it will be worthwhile for each individual community\nmember to take an interest in the project because the rest of the\ncommunity is doing so as well), and any project that does not make the\nallocation will not. Now, this is certainly not a conception of\nlegitimacy that we want to replicate verbatim - there is little appetite\nin the Ethereum community for enriching a small group of early adopters\n- but the core concept can be adapted into something much more socially\nvaluable.\nExtending the model to\nEthereum\n\nBlockchain ecosystems, Ethereum included, value freedom and\ndecentralization. But the public goods ecology of most of these\nblockchains is, regrettably, still quite authority-driven and\ncentralized: whether it's Ethereum, Zcash or any other major blockchain,\nthere is typically one (or at most 2-3) entities that far outspend\neveryone else, giving independent teams that want to build public goods\nfew options. I call this model of public goods funding \"Central Capital\nCoordinators for Public-goods\" (CCCPs).\n\nThis state of affairs is not the fault of the organizations\nthemselves, who are typically valiantly doing their best to support the\necosystem. Rather, it's the rules of the ecosystem that are being\nunfair to that organization, because they hold the organization\nto an unfairly high standard. Any single centralized\norganization will inevitably have blindspots and at least a few\ncategories and teams whose value that it fails to understand; this is\nnot because anyone involved is doing anything wrong, but because such\nperfection is beyond the reach of small groups of humans. So there is\ngreat value in creating a more diversified and resilient approach to\npublic goods funding to take the pressure off any single\norganization.\n\nFortunately, we already have the seed of such an alternative! The\nEthereum application-layer ecosystem exists, is growing increasingly\npowerful, and is already showing its public-spiritedness. Companies like\nGnosis have been contributing to Ethereum client development, and\nvarious Ethereum DeFi projects have donated hundreds of thousands of\ndollars to the Gitcoin Grants matching pool.\n\n  \n\nGitcoin Grants has already achieved a high level of legitimacy: its\npublic goods funding mechanism, quadratic funding, has\nproven itself to be credibly neutral\nand effective at reflecting the community's priorities and values and\nplugging the holes left by existing funding mechanisms. Sometimes, top\nGitcoin Grants matching recipients are even used as inspiration for\ngrants by other and more centralized grant-giving entities. The Ethereum\nFoundation itself has played a key role in supporting this\nexperimentation and diversity, incubating efforts like Gitcoin Grants,\nalong with MolochDAO and others, that then go on to get broader\ncommunity support.\n\nWe can make this nascent public goods-funding ecosystem even stronger\nby taking the Bitshares model, and making a modification: instead of\ngiving the strongest community support to projects who allocate tokens\nto a small oligarchy who bought PTS or AGS back in 2013, we\nsupport projects that contribute a small portion of their treasuries\ntoward the public goods that make them and the ecosystem that they\ndepend on possible. And, crucially, we can deny these benefits\nto projects that fork an existing project and do not give back value to\nthe broader ecosystem.\n\nThere are many ways to do support public goods: making a long-term\ncommitment to support the Gitcoin Grants matching pool, supporting\nEthereum client development (also a reasonably credibly-neutral task as\nthere's a clear definition of what an Ethereum client is), or\neven running one's own grant program whose scope goes beyond that\nparticular application-layer project itself. The easiest way to agree on\nwhat counts as sufficient support is to agree on how much - for example,\n5% of a project's spending going to support the broader ecosystem and\nanother 1% going to public goods that go beyond the blockchain space -\nand rely on good faith to choose where that funding would go.\nDoes the\ncommunity actually have that much leverage?\n\nOf course, there are limits to the value of this kind of community\nsupport. If a competing project (or even a fork of an existing project)\ngives its users a much better offering, then users are going to flock to\nit, regardless of how many people yell at them to instead use some\nalternative that they consider to be more pro-social.\n\nBut these limits are different in different contexts; sometimes the\ncommunity's leverage is weak, but at other times it's quite strong. An\ninteresting case study in this regard is the case of Tether vs DAI. Tether has\nmany\nscandals,\nbut despite this traders use Tether to hold and move around dollars all\nthe time. The more decentralized and transparent DAI, despite its benefits, is unable to\ntake away much of Tether's market share, at least as far as traders go.\nBut where DAI excels is applications: Augur uses DAI, xDai uses DAI, PoolTogether uses DAI, zk.money plans to use DAI, and the list goes\non. What dapps use USDT? Far fewer.\n\nHence, though the power of community-driven legitimacy effects is not\ninfinite, there is nevertheless considerable room for leverage, enough\nto encourage projects to direct at least a few percent of their budgets\nto the broader ecosystem. There's even a selfish reason to participate\nin this equilibrium: if you were the developer of an Ethereum wallet, or\nan author of a podcast or newsletter, and you saw two competing\nprojects, one of which contributes significantly to ecosystem-level\npublic goods including yourself and one which does not, for which one\nwould you do your utmost to help them secure more market share?\nNFTs: supporting\npublic goods beyond Ethereum\n\nThe concept of supporting public goods through value generated \"out\nof the ether\" by publicly supported conceptions of legitimacy has value\ngoing far beyond the Ethereum ecosystem. An important and immediate\nchallenge and opportunity is NFTs. NFTs stand a great chance of\nsignificantly helping many kinds of public goods, especially of the\ncreative variety, at least partially solve their chronic and\nsystemic funding deficiencies.\n\n Actually a very admirable first step.\n\nBut they could also be a missed opportunity: there is little social\nvalue in helping Elon Musk earn yet another $1 million by selling his\ntweet when, as far as we can tell, the money is just going to himself\n(and, to his credit, he eventually decided\nnot to sell). If NFTs simply become a casino that largely benefits\nalready-wealthy celebrities, that would be a far less interesting\noutcome.\n\nFortunately, we have the ability to help shape the\noutcome. Which NFTs people find attractive to buy, and which\nones they do not, is a question of legitimacy: if everyone agrees that\none NFT is interesting and another NFT is lame, then people will\nstrongly prefer buying the first, because it would have both higher\nvalue for bragging rights and personal pride in holding it, and because\nit could be resold for more because everyone else is thinking in the\nsame way. If the conception of legitimacy for NFTs can be pulled in a\ngood direction, there is an opportunity to establish a solid channel of\nfunding to artists, charities and others.\n\nHere are two potential ideas:\n\n- Some institution (or even DAO) could \"bless\" NFTs in exchange for a\nguarantee that some portion of the revenues goes toward a charitable\ncause, ensuring that multiple groups benefit at the same time. This\nblessing could even come with an official categorization: is the NFT\ndedicated to global poverty relief, scientific research, creative arts,\nlocal journalism, open source software development, empowering\nmarginalized communities, or something else?\n\n- We can work with social media platforms to make NFTs more visible on\npeople's profiles, giving buyers a way to show the values that they\ncommitted not just their words but their hard-earned money to. This\ncould be combined with (1) to nudge users toward NFTs that contribute to\nvaluable social causes.\n\nThere are definitely more ideas, but this is an area that certainly\ndeserves more active coordination and thought.\n\n## In summary\n\n- The concept of legitimacy (higher-order acceptance) is very\npowerful. Legitimacy appears in any context where there is\ncoordination,\nand especially on the internet, coordination is everywhere.\n\n- There are different ways in which legitimacy comes to be:\nbrute force, continuity, fairness, process, performance\nand participation are among the important ones.\n\n- Cryptocurrency is powerful because it lets us summon up large pools\nof capital by collective economic will, and these pools of capital are,\nat the beginning, not controlled by any person. Rather, these\npools of capital are controlled directly by concepts of\nlegitimacy.\n\n- It's too risky to start doing public goods funding by printing\ntokens at the base layer. Fortunately, however, Ethereum has a very rich\napplication-layer ecosystem, where we have much more\nflexibility. This is in part because there's an opportunity not just to\ninfluence existing projects, but also shape new ones that will come into\nexistence in the future.\n\n- Application-layer projects that support public goods in the\ncommunity should get the support of the community, and this is\na big deal. The example of DAI shows that this support really\nmatters!\n\n- The Etherem ecosystem cares about mechanism design and innovating at\nthe social layer. The Ethereum ecosystem's own public goods\nfunding challenges are a great place to start!\n\n- But this goes far beyond just Ethereum itself. NFTs are one example\nof a large pool of capital that depends on concepts of legitimacy.\nThe NFT industry could be a significant boon to\nartists, charities and other public goods providers far beyond our own\nvirtual corner of the world, but this outcome is not\npredetermined; it depends on active coordination and\nsupport.",
    "contentLength": 25196,
    "summary": "Vitalik argues that legitimacy\u2014coordinated social acceptance\u2014is crypto's most scarce resource, not hashpower or code.",
    "detailedSummary": {
      "theme": "Legitimacy as a social force drives resource allocation in blockchain ecosystems and should be leveraged to improve public goods funding.",
      "summary": "Vitalik argues that legitimacy - defined as 'higher-order acceptance' where people act according to their expectations of how others will act - is the most important scarce resource in coordination games, particularly in blockchain ecosystems. He demonstrates how Bitcoin and Ethereum spend billions on mining security but only tens of millions on development and public goods, representing a massive resource misallocation driven by legitimacy constraints rather than technical limitations. Vitalik proposes that the Ethereum ecosystem can harness legitimacy to improve public goods funding by encouraging application-layer projects to contribute a portion of their treasuries to ecosystem-wide public goods, similar to how the Bitshares model worked but directed toward social value rather than enriching early adopters. He extends this concept to NFTs, arguing that legitimacy determines which NFTs people find valuable, and this social force could be directed toward supporting artists, charities, and other public goods rather than just enriching celebrities.",
      "takeaways": [
        "Blockchain ecosystems massively overspend on security (mining) while underfunding critical public goods like research and development",
        "Legitimacy is a pattern of higher-order acceptance that emerges in coordination games and controls allocation of resources that aren't owned by individuals or cryptographic keys",
        "The Ethereum application layer offers flexibility to create new funding mechanisms where projects contributing to public goods receive community support and legitimacy",
        "NFTs represent a major opportunity to fund public goods like art and charity, but only if legitimacy can be directed toward socially valuable outcomes rather than celebrity enrichment",
        "Examples like DAI vs Tether show that community legitimacy has real market power, especially in applications even when it may not affect pure trading behavior"
      ],
      "controversial": [
        "The proposal to essentially create social pressure for projects to donate to public goods could be seen as coercive or creating unfair competitive advantages",
        "The suggestion that community coordination should influence which projects succeed based on their charitable contributions rather than purely on merit or user value",
        "The characterization of proof-of-work mining spending as largely wasteful 'misallocation' may be disputed by Bitcoin advocates who see security as paramount"
      ]
    }
  },
  {
    "id": "general-2021-02-18-election",
    "title": "Prediction Markets: Tales from the Election",
    "date": "2021-02-18",
    "category": "applications",
    "url": "https://vitalik.eth.limo/general/2021/02/18/election.html",
    "path": "general/2021/02/18/election.html",
    "content": "- \n\n- \n\n  \n    \n\n  \n  \n  Dark Mode Toggle\n  \n    \n\n  \n\n##  Prediction Markets: Tales from the Election \n\n 2021 Feb 18 \nSee all posts\n\n \n \n\n Prediction Markets: Tales from the Election \n\nSpecial thanks to Jeff Coleman, Karl Floersch and Robin Hanson\nfor critical feedback and review.\n\nTrigger warning: I express some political opinions.\n\nPrediction markets are a subject that has interested me for many\nyears. The idea of allowing anyone in the public to make bets about\nfuture events, and using the odds at which these bets are made as a credibly neutral\nsource of predicted probabilities of these events, is a fascinating\napplication of mechanism design. Closely related ideas, like futarchy,\nhave always interested me as innovative tools that could improve\ngovernance and decision-making. And as Augur and Omen, and more recently PolyMarket, have shown, prediction\nmarkets are a fascinating application of blockchains (in all three\ncases, Ethereum) as well.\n\nAnd the 2020 US presidential election, it seems like prediction\nmarkets are finally entering the limelight, with blockchain-based\nmarkets in particular growing from near-zero in 2016 to millions\nof dollars of volume in 2020. As someone who is closely interested\nin seeing Ethereum applications cross the chasm into widespread\nadoption, this of course aroused my interest. At first, I was inclined\nto simply watch, and not participate myself: I am not an expert on US\nelectoral politics, so why should I expect my opinion to be more correct\nthan that of everyone else who was already trading? But in my\nTwitter-sphere, I saw more and more arguments from Very Smart People\nwhom I respected arguing that the markets were in fact being\nirrational and I should participate and bet against them if I\ncan. Eventually, I was convinced.\n\nI decided to make an experiment on the blockchain that I helped to\ncreate: I bought $2,000 worth of NTRUMP (tokens that pay $1 if Trump\nloses) on Augur. Little did I know then that my position would\neventually increase to $308,249, earning me a profit of over $56,803,\nand that I would make all of these remaining bets, against willing\ncounterparties, after Trump had already lost the election. What\nwould transpire over the next two months would prove to be a fascinating\ncase study in social psychology, expertise, arbitrage, and the limits of\nmarket efficiency, with important ramifications to anyone who is deeply\ninterested in the possibilities of economic institution design.\n\n## Before the Election\n\nMy first bet on this election was actually not on a blockchain at\nall. When Kanye\nannounced his presidential bid in July, a political theorist whom I\nordinarily quite respect for his high-quality and original thinking\nimmediately claimed on Twitter that he was confident that this would\nsplit the anti-Trump vote and lead to a Trump victory. I remember\nthinking at the time that this particular opinion of his was\nover-confident, perhaps even a result of over-internalizing the\nheuristic that if a viewpoint seems clever and contrarian then it is\nlikely to be correct. So of\ncourse I offered to make a $200 bet, myself betting the boring\nconventional pro-Biden view, and he honorably accepted.\n\nThe election came up again on my radar in September, and this time it\nwas the prediction markets that caught my attention. The markets gave\nTrump a nearly 50% chance of winning, but I saw many Very Smart People\nin my Twitter-sphere whom I respected pointing out that this number\nseemed far too high. This of course led to the familiar \"efficient\nmarkets debate\": if you can buy a token that gives you $1 if Trump loses\nfor $0.52, and Trump's actual chance of losing is much higher, why\nwouldn't people just come in and buy the token until the price rises\nmore? And if nobody has done this, who are you to think that you're\nsmarter than everyone else?\n\nNe0liberal's Twitter thread\njust before Election Day does an excellent job summarizing his case\nagainst prediction markets being accurate at that time. In short, the\n(non-blockchain) prediction markets that most people used at least prior\nto 2020 have all sorts of restrictions that make it difficult for people\nto participate with more than a small amount of cash. As a result, if a\nvery smart individual or a professional organization saw a probability\nthat they believed was wrong, they would only have a very limited\nability to push the price in the direction that they believe to be\ncorrect.\n\nThe most important restrictions that the paper points out are:\n\n- Low limits (well under $1,000) on how much each person can bet\n\n- High fees (eg. a 5% withdrawal fee on PredictIt)\n\nAnd this is where I pushed\nback against ne0liberal in September: although the stodgy old-world\ncentralized prediction markets may have low limits and high fees, the\ncrypto markets do not! On Augur or Omen, there's no limit to how much\nsomeone can buy or sell if they think the price of some outcome token is\ntoo low or too high. And the blockchain-based prediction markets were\nfollowing the same prices as PredictIt. If the markets really were\nover-estimating Trump because high fees and low trading limits were\npreventing the more cool-headed traders from outbidding the overly\noptimistic ones, then why would blockchain-based markets, which don't\nhave those issues, show the same prices?\n\nPredictIt\n\nAugur\n\nThe main response my Twitter friends gave to this was that\nblockchain-based markets are highly niche, and very few people,\nparticularly very few people who know much about politics, have easy\naccess to cryptocurrency. That seemed plausible, but I was not\ntoo confident in that argument. And so I bet $2,000 against\nTrump and went no further.\n\n## The Election\n\nThen the election happened. After an initial scare where Trump at\nfirst won more seats than we expected, Biden turned out to be the\neventual winner. Whether or not the election itself validated or refuted\nthe efficiency of prediction markets is a topic that, as far as I can\ntell, is quite open to interpretation. On the one hand, by a standard Bayes rule\napplication, I should decrease my confidence of prediction markets, at\nleast relative to Nate Silver. Prediction markets gave a 60% chance of\nBiden winning, Nate Silver gave a 90%\nchance of Biden winning. Since Biden in fact won, this is one piece\nof evidence that I live in a world where Nate gives the more correct\nanswers.\n\nBut on the other hand, you can make a case that the prediction\nmarkets bettter estimated the margin of victory. The median of\nNate's probability distribution was somewhere around 370 of 538\nelectoral college votes going to Biden:\n\nThe Trump markets didn't give a probability distribution, but if you\nhad to guess a probability distribution from the statistic \"40%\nchance Trump will win\", you would probably give one with a median\nsomewhere around 300 EC votes for Biden. The actual result: 306. So the\nnet score for prediction markets vs Nate seems to me, on reflection,\nambiguous.\n\n## After the election\n\nBut what I could not have imagined at the time was that the election\nitself was just the beginning. A few days after the election, Biden was\ndeclared the winner by various major organizations and even a few\nforeign governments. Trump mounted various legal challenges to the\nelection results, as was expected, but each of these challenges quickly\nfailed. But for over a month, the price of the NTRUMP tokens\nstayed at 85 cents!\n\nAt the beginning, it seemed reasonable to guess that Trump had a 15%\nchance of overturning the results; after all, he had appointed three\njudges to the Supreme Court, at a time of heightened partisanship\nwhere many have come to favor team over principle. Over the next three\nweeks, however, it became more and more clear that the challenges were\nfailing, and Trump's hopes continued to look grimmer with each passing\nday, but the NTRUMP price did not budge; in fact, it even briefly\ndecreased to around $0.82. On December 11, more than five weeks\nafter the election, the Supreme\nCourt decisively and unanimously rejected Trump's attempts to\noverturn the vote, and the NTRUMP price finally rose.... to $0.88.\n\nIt was in November that I was finally convinced that the market\nskeptics were right, and I plunged in and bet against Trump myself. The\ndecision was not so much about the money; after all, barely two months\nlater I would earn\nand donate to GiveDirectly a far larger amount simply from holding\ndogecoin. Rather, it was to take part in the experiment not just as an\nobserver, but as an active participant, and improve my personal\nunderstanding of why everyone else hadn't already plunged in to buy\nNTRUMP tokens before me.\n\n## Dipping in\n\nI bought my NTRUMP on Catnip, a front-end user\ninterface that combines together the Augur prediction market with Balancer, a Uniswap-style\nconstant-function market maker. Catnip was by far the easiest\ninterface for making these trades, and in my opinion contributed\nsignificantly to Augur's usability.\n\nThere are two ways to bet against Trump with Catnip:\n\n- Use DAI to buy NTRUMP on\nCatnip directly\n\n- Use Foundry to access an Augur\nfeature that allows you to convert 1 DAI into 1 NTRUMP + 1 YTUMP +\n1ITRUMP (the \"I\" stands for \"invalid\", more on this later), and sell the\nYTRUMP on Catnip\n\nAt first, I only knew about the first option. But then I discovered\nthat Balancer has far more liquidity for YTRUMP, and so I switched to\nthe second option.\n\nThere was also another problem: I did not have any DAI. I had ETH,\nand I could have sold my ETH to get DAI, but I did not want to sacrifice\nmy ETH exposure; it would have been a shame if I earned $50,000 betting\nagainst Trump but simultaneously lost $500,000 missing out on ETH price\nchanges. So I decided to keep my ETH price exposure the same by opening\nup a collateralized debt position (CDP, now also called a \"vault\")\non MakerDAO.\n\nA CDP is how all DAI is generated: users deposit their ETH into a\nsmart contract, and are allowed to withdraw an amount of newly-generated\nDAI up to 2/3 of the value of ETH that they put in. They can get their\nETH back by sending back the same amount of DAI that they withdrew plus\nan extra interest fee (currently 3.5%). If the value of the ETH\ncollateral that you deposited drops to less than 150% the value of the\nDAI you withdrew, anyone can come in and \"liquidate\"\nthe vault, forcibly selling the ETH to buy back the DAI and charging you\na high penalty. Hence, it's a good idea to have a high collateralization\nratio in case of sudden price movements; I had over $3 worth of ETH in\nmy CDP for every $1 that I withdrew.\n\nRecapping the above, here's the pipeline in diagram form:\n\nI did this many times; the slippage on Catnip meant that I could\nnormally make trades only up to about $5,000 to $10,000 at a time\nwithout prices becoming too unfavorable (when I had skipped Foundry and\nbought NTRUMP with DAI directly, the limit was closer to $1,000). And\nafter two months, I had accumulated over 367,000 NTRUMP.\n\n## Why not everyone else?\n\nBefore I went in, I had four main hypotheses about why so few others\nwere buying up dollars for 85 cents:\n\n- Fear that either the Augur smart contracts would break or Trump\nsupporters would manipulate the oracle (a decentralized mechanism\nwhere holders of Augur's REP token vote by staking their tokens on one\noutcome or the other) to make it return a false result\n\n- Capital costs: to buy these tokens, you have to lock up funds for\nover two months, and this removes your ability to spend those funds or\nmake other profitable trades for that duration\n\n- It's too technically complicated for almost everyone to trade\n\n- There just really are far fewer people than I thought who are\nactually motivated enough to take a weird opportunity even when it\npresents them straight in the face\n\nAll four have reasonable arguments going for them. Smart contracts\nbreaking is a\nreal\nrisk,\nand the Augur oracle had not before been tested in such a contentious\nenvironment. Capital costs are real, and while betting against something\nis easier in a prediction market than in a stock market because you know\nthat prices will never go above $1, locking up capital nevertheless\ncompetes with other lucrative opportunities in the crypto markets.\nMaking transactions things in dapps is technically complicated,\nand it's rational to have some degree of fear-of-the-unknown.\n\nBut my experience actually going into the financial trenches, and\nwatching the prices on this market evolve, taught me a lot about each of\nthese hypotheses.\nFear of smart contract\nexploits\n\nAt first, I thought that \"fear of smart contract exploits\" must have\nbeen a significant part of the explanation. But over time, I have become\nmore convinced that it is probably not a dominant factor. One\nway to see why I think this is the case is to compare the prices for\nYTRUMP and ITRUMP. ITRUMP stands for \"Invalid Trump\"; \"Invalid\" is an\nevent outcome that is intended to be triggered in some\nexceptional cases: when the description of the event is ambiguous,\nwhen the outcome of the event is not yet known when the market is\nresolved, when the market is unethical (eg. assassination markets), and\na few other similar situations. In this market, the price of ITRUMP\nconsistently stayed under $0.02. If someone wanted to earn a profit by\nattacking the market, it would be far more lucrative for them to not buy\nYTRUMP at $0.15, but instead buy ITRUMP at $0.02. If they buy a large\namount of ITRUMP, they could earn a 50x return if they can force the\n\"invalid\" outcome to actually trigger. So if you fear an attack, buying\nITRUMP is by far the most rational response. And yet, very few people\ndid.\n\nA further argument against fear of smart contract exploits, of\ncourse, is the fact that in every crypto application except\nprediction markets (eg. Compound, the various yield farming schemes)\npeople are surprisingly blas\u00e9 about smart contract risks. If people are\nwilling to put their money into all sorts of risky and untested schemes\neven for a promise of mere 5-8% annual gains, why would they suddenly\nbecome over-cautious here?\n\n## Capital costs\n\nCapital costs - the inconvenience and opportunity cost of locking up\nlarge amounts of money - are a challenge that I have come to appreciate\nmuch more than I did before. Just looking at the Augur side of things, I\nneeded to lock up 308,249 DAI for an average of about two months to make\na $56,803 profit. This works out to about a 175% annualized interest\nrate; so far, quite a good deal, even compared to the various yield\nfarming crazes of the summer of 2020. But this becomes worse when you\ntake into account what I needed to do on MakerDAO. Because I wanted to\nkeep my exposure to ETH the same, I needed to get my DAI through a CDP,\nand safely using a CDP required a collateral ratio of over 3x. Hence,\nthe total amount of capital I actually needed to lock up was\nsomewhere around a million dollars.\n\nNow, the interest rates are looking less favorable. And if you add to\nthat the possibility, however remote, that a smart contract hack, or a\ntruly unprecedented political event, actually will happen, it\nlooks less favorable still.\n\nBut even still, assuming a 3x lockup and a 3% chance of Augur\nbreaking (I had bought ITRUMP to cover the possibility that it breaks in\nthe \"invalid\" direction, so I needed only worry about the risk of breaks\nin the \"yes\" direction or the the funds being stolen outright), that\nworks out to a risk-neutral rate of about 35%, and even lower once you\ntake real human beings' views on risk into account. The deal is still\nvery attractive, but on the other hand, it now looks very understandable\nthat such numbers are unimpressive to people who live and breathe\ncryptocurrency with its frequent 100x ups and downs.\n\nTrump supporters, on the other hand, faced none of these\nchallenges: they cancelled out my $308,249 bet by throwing in a mere\n$60,000 (my winnings are less than this because of fees). When\nprobabilities are close to 0 or 1, as is the case here, the game is\nvery lopsided in favor of those who are trying to push the\nprobability away from the extreme value. And this explains not just\nTrump; it's also the reason why all sorts of popular-among-a-niche\ncandidates with no real chance of victory frequently get winning\nprobabilities as high as 5%.\n\n## Technical complexity\n\nI had at first tried buying NTRUMP on Augur, but technical glitches\nin the user interface prevented me from being able to make orders on\nAugur directly (other people I talked to did not have this issue... I am\nstill not sure what happened there). Catnip's UI is much simpler and\nworked excellently. However, automated market makers like Balancer (and\nUniswap) work best for smaller trades; for larger trades, the slippage\nis too high. This is a good microcosm of the broader \"AMM vs order book\"\ndebate: AMMs are more convenient but order books really do work better\nfor large trades. Uniswap v3 is introducing an AMM design that has\nbetter capital efficiency; we shall see if that improves things.\n\nThere were other technical complexities too, though fortunately they\nall seem to be easily solvable. There is no reason why an interface like\nCatnip could not integrate the \"DAI -> Foundry -> sell YTRUMP\"\npath into a contract so that you could buy NTRUMP that way in a single\ntransaction. In fact, the interface could even check the price and\nliquidity properties of the \"DAI -> NTRUMP\" path and the \"DAI ->\nFoundry -> sell YTRUMP\" path and give you the better trade\nautomatically. Even withdrawing DAI from a MakerDAO CDP can be included\nin that path. My conclusion here is optimistic: technical complexity\nissues were a real barrier to participation this round, but things will\nbe much easier in future rounds as technology improves.\n\n## Intellectual underconfidence\n\nAnd now we have the final possibility: that many people (and smart\npeople in particular) have a pathology that they suffer from excessive\nhumility, and too easily conclude that if no one else has taken some\naction, then there must therefore be a good reason why that action is\nnot worth taking.\n\nEliezer Yudkowsky spends the second half of his excellent book Inadequate Equilibria making this\ncase, arguing that too many people overuse \"modest epistemology\", and we\nshould be much more willing to act on the results of our reasoning, even\nwhen the result suggests that the great majority of the population is\nirrational or lazy or wrong about something. When I read those sections\nfor the first time, I was unconvinced; it seemed like Eliezer was simply\nbeing overly arrogant. But having gone through this experience, I have\ncome to see some wisdom in his position.\n\nThis was not my first time seeing the virtues of trusting one's own\nreasoning first hand. When I had originally started working on Ethereum,\nI was at first beset by fear that there must be some very good reason\nthe project was doomed to fail. A fully programmable\nsmart-contract-capable blockchain, I reasoned, was clearly such a great\nimprovement over what came before, that surely many other people must\nhave thought of it before I did. And so I fully expected that, as soon\nas I publish the idea, many very smart cryptographers would tell me the\nvery good reasons why something like Ethereum was fundamentally\nimpossible. And yet, no one ever did.\n\nOf course, not everyone suffers from excessive modesty. Many of the\npeople making predictions in favor of Trump winning the\nelection were arguably fooled by their own excessive contrarianism.\nEthereum benefited from my youthful suppression of my own modesty and\nfears, but there are plenty of other projects that could have benefited\nfrom more intellectual humility and avoided failures.\n\nNot a sufferer of excessive modesty.\n\nBut nevertheless it seems to me more true than ever that, as goes the\nfamous\nYeats quote, \"the best lack all conviction, while the worst are full\nof passionate intensity.\" Whatever the faults of overconfidence or\ncontrarianism sometimes may be, it seems clear to me that spreading a\nsociety-wide message that the solution is to simply trust the existing\noutputs of society, whether those come in the form of academic\ninstitutions, media, governments or markets, is not the\nsolution. All of these institutions can only work precisely because of\nthe presence of individuals who think that they do not work, or who at\nleast think that they can be wrong at least some of the time.\n\n## Lessons for futarchy\n\nSeeing the importance of capital costs and their interplay with risks\nfirst hand is also important evidence for judging systems like futarchy.\nFutarchy, and \"decision markets\" more generally are an important and\npotentially very socially useful application of prediction markets.\nThere is not much social value in having slightly more accurate\npredictions of who will be the next president. But there is a\nlot of social value in having conditional predictions:\nif we do A, what's the chance it will lead to some good thing X, and\nif we do B instead what are the chances then? Conditional\npredictions are important because they do not just satisfy our\ncuriosity; they can also help us make decisions.\n\nThough electoral prediction markets are much less useful than\nconditional predictions, they can help shed light on an important\nquestion: how robust are they to manipulation or even just biased and\nwrong opinions? We can answer this question by looking at how difficult\narbitrage is: suppose that a conditional prediction market currently\ngives probabilities that (in your opinion) are wrong (could be\nbecause of ill-informed traders or an explicit manipulation attempt; we\ndon't really care). How much of an impact can you have, and how much\nprofit can you make, by setting things right?\n\nLet's start with a concrete example. Suppose that we are trying to\nuse a prediction market to choose between decision A and decision B,\nwhere each decision has some probability of achieving some desirable\noutcome. Suppose that your opinion is that decision A has a 50%\nchance of achieving the goal, and decision B has a 45% chance. The\nmarket, however, (in your opinion wrongly) thinks decision B has a 55%\nchance and decision A has a 40% chance.\n\nProbability of good outcome if we choose strategy...\nCurrent market position\nYour opinion\n\nA\n40%\n50%\n\nB\n55%\n45%\n\nSuppose that you are a small participant, so your individual bets\nwon't affect the outcome; only many bettors acting together could. How\nmuch of your money should you bet?\n\nThe standard theory here relies on the Kelly\ncriterion. Essentially, you should act to maximize the expected\nlogarithm of your assets. In this case, we can solve the resulting\nequation. Suppose you invest portion \\(r\\) of your money into buying A-token for\n$0.4. Your expected new log-wealth, from your point of view, would\nbe:\n\n\\(0.5 * log((1-r) + \\frac{r}{0.4}) + 0.5 *\nlog(1-r)\\)\n\nThe first term is the 50% chance (from your point of view) that the\nbet pays off, and the portion \\(r\\)\nthat you invest grows by 2.5x (as you bought dollars at 40 cents). The\nsecond term is the 50% chance that the bet does not pay off, and you\nlose the portion you bet. We can use calculus to find the \\(r\\) that maximizes this; for the lazy, here's\nWolframAlpha. The answer is \\(r =\n\\frac{1}{6}\\). If other people buy and the price for A on the\nmarket gets up to 47% (and B gets down to 48%), we can redo the\ncalculation for the last trader who would flip the market over to make\nit correctly favor A:\n\n\\(0.5 * log((1-r) + \\frac{r}{0.47}) + 0.5 *\nlog(1-r)\\)\n\nHere, the expected-log-wealth-maximizing \\(r\\) is a mere 0.0566. The conclusion is\nclear: when decisions are close and when there is a lot of noise, it\nturns out that it only makes sense to invest a small portion of your\nmoney in a market. And this is assuming rationality; most people invest\nless into uncertain gambles than the Kelly criterion says they\nshould. Capital costs stack on top even further. But if an attacker\nreally wants to force outcome B through because they want it to\nhappen for personal reasons, they can simply put all of their\ncapital toward buying that token. All in all, the game can easily be\nlopsided more than 20:1 in favor of the attacker.\n\nOf course, in reality attackers are rarely willing to stake all their\nfunds on one decision. And futarchy is not the only mechanism that is\nvulerable to attacks: stock markets are similarly vulnerable, and\nnon-market decision mechanisms can also be manipulated by determined\nwealthy attackers in all sorts of ways. But nevertheless, we should be\nwary of assuming that futarchy will propel us to new heights of\ndecision-making accuracy.\n\nInterestingly enough, the math seems to suggest that futarchy would\nwork best when the expected manipulators would want to push the outcome\ntoward an extreme value. An example of this might be liability\ninsurance, as someone wishing to improperly obtain insurance would\neffectively be trying to force the market-estimated probability that an\nunfavorable event will happen down to zero. And as it turns out,\nliability insurance is futarchy inventor Robin Hanson's new\nfavorite policy prescription.\nCan prediction markets\nbecome better?\n\nThe final question to ask is: are prediction markets doomed to repeat\nerrors as grave as giving Trump a 15% chance of overturning the election\nin early December, and a 12% chance of overturning it even after the\nSupreme Court including three judges whom he appointed telling him to\nscrew off? Or could the markets improve over time? My answer is,\nsurprisingly, emphatically on the optimistic side, and I see a few\nreasons for optimism.\n\n## Markets as natural selection\n\nFirst, these events have given me a new perspective on how market\nefficiency and rationality might actually come about. Too often,\nproponents of market efficiency theories claim that market efficiency\nresults because most participants are rational (or at least the\nrationals outweigh any coherent group of deluded people), and this is\ntrue as an axiom. But instead, we could take an evolutionary\nperspective on what is going on.\n\nCrypto is a young ecosystem. It is an ecosystem that is still quite\ndisconnected from the mainstream, Elon's recent tweets notwithstanding,\nand that does not yet have much expertise in the minutiae of electoral\npolitics. Those who are experts in electoral politics have a\nhard time getting into crypto, and crypto has a large presence of\nnot-always-correct forms of contrarianism especially when it comes to\npolitics. But what happened this year is that within the crypto space,\nprediction market users who correctly expected Biden to win got an 18%\nincrease to their capital, and prediction market users who incorrectly\nexpected Trump to win got a 100% decrease to their capital (or at least\nthe portion they put into the bet).\n\nThus, there is a selection pressure in favor of the type of\npeople who make bets that turn out to be correct. After ten rounds of\nthis, good predictors will have more capital to bet with, and bad\npredictors will have less capital to bet with. This does not\nrely on anyone \"getting wiser\" or \"learning their lesson\" or any other\nassumption about humans' capacity to reason and learn. It is simply a\nresult of selection dynamics that over time, participants that are good\nat making correct guesses will come to dominate the ecosystem.\n\nNote that prediction markets fare better than stock markets in this\nregard: the \"nouveau riche\" of stock markets often arise from getting\nlucky on a single thousandfold gain, adding a lot of noise to the\nsignal, but in prediction markets, prices are bounded between 0 and 1,\nlimiting the impact of any one single event.\nBetter participants\nand better technology\n\nSecond, prediction markets themselves will improve. User interfaces\nhave greatly improved already, and will continue to improve further. The\ncomplexity of the MakerDAO -> Foundry -> Catnip cycle will be\nabstracted away into a single transaction. Blockchain scaling technology\nwill improve, reducing fees for participants (The ZK-rollup Loopring with a built-in AMM is already\nlive on the Ethereum mainnet, and a prediction market could\ntheoretically run on it).\n\nThird, the demonstration that we saw of the prediction market working\ncorrectly will ease participants' fears. Users will see that the Augur\noracle is capable of giving correct outputs even in very contentious\nsituations (this time, there were two rounds of disputes, but the no\nside nevertheless cleanly won). People from outside the crypto space\nwill see that the process works and be more inclined to participate.\nPerhaps even Nate Silver himself might get some DAI and use Augur, Omen,\nPolymarket and other markets to supplement his income in 2022 and\nbeyond.\n\nFourth, prediction market tech itself could improve. Here\nis a proposal from myself on a market design that could make it more\ncapital-efficient to simultaneously bet against many unlikely events,\nhelping to prevent unlikely outcomes from getting irrationally high\nodds. Other ideas will surely spring up, and I look forward to seeing\nmore experimentation in this direction.\n\n## Conclusion\n\nThis whole saga has proven to be an incredibly interesting direct\ntrial-by-first test of prediction markets and how they collide with the\ncomplexities of individual and social psychology. It shows a lot about\nhow market efficiency actually works in practice, what are the limits of\nit and what could be done to improve it.\n\nIt has also been an excellent demonstration of the power of\nblockchains; in fact, it is one of the Ethereum applications that have\nprovided to me the most concrete value. Blockchains are often criticized\nfor being speculative toys and not doing anything meaningful except for\nself-referential games (tokens, with yield farming, whose returns are\npowered by... the launch of other tokens). There are certainly exceptions\nthat the critics fail to recognize; I personally have benefited from ENS\nand even from using ETH for payments on several occasions where all\ncredit card options failed. But over the last few months, it seems like\nwe have seen a rapid burst in Ethereum applications being concretely\nuseful for people and interacting with the real world, and prediction\nmarkets are a key example of this.\n\nI expect prediction markets to become an increasingly important\nEthereum application in the years to come. The 2020 election was only\nthe beginning; I expect more interest in prediction markets going\nforward, not just for elections but for conditional predictions,\ndecision-making and other applications as well. The amazing promises of\nwhat prediction markets could bring if they work mathematically\noptimally will, of course, continue to collide with the limits of human\nreality, and hopefully, over time, we will get a much clearer view of\nexactly where this new social technology can provide the most value.",
    "contentLength": 30823,
    "summary": "Vitalik details betting against Trump on blockchain prediction markets, earning $56,803 by exploiting post-election market inefficiencies.",
    "detailedSummary": {
      "theme": "Vitalik's experience betting against Trump in blockchain-based prediction markets reveals insights about market efficiency, technical barriers, and the future potential of prediction markets on Ethereum.",
      "summary": "Vitalik documents his journey from skeptical observer to active participant in blockchain prediction markets during the 2020 US election, ultimately betting $308,249 against Trump and earning over $56,000 in profit. Initially hesitant about his political expertise, Vitalik was convinced by arguments that prediction markets were being irrational, showing Trump with nearly 50% winning odds despite what many experts considered overly optimistic. His experience revealed four key barriers preventing more rational arbitrage: fears about smart contract risks, significant capital costs requiring collateralized positions, technical complexity of using decentralized applications, and intellectual underconfidence where smart people assume others must know something they don't. Most surprisingly, even after Trump clearly lost the election, NTRUMP tokens remained underpriced for months, only reaching fair value after the Supreme Court definitively rejected challenges.\n\nVitalik argues this experience demonstrates both the limitations and potential of prediction markets, particularly for futarchy applications in governance. While markets can be inefficient due to capital constraints and user barriers, he sees reasons for optimism: natural selection will favor successful predictors over time, user interfaces and blockchain technology continue improving, and the demonstrated success of the oracle system during contentious events will build confidence. He concludes that prediction markets represent one of Ethereum's most practically valuable applications, moving beyond speculative trading to provide real-world utility, though their mathematical promises will continue to collide with human psychological limitations.",
      "takeaways": [
        "Blockchain prediction markets suffered from the same inefficiencies as traditional platforms despite lacking betting limits and high fees, suggesting barriers beyond just structural restrictions",
        "Capital costs create significant advantages for contrarians betting on unlikely outcomes, as they need much less capital to move prices than rational arbitrageurs trying to correct them",
        "Technical complexity and intellectual underconfidence (assuming others must know better) were major factors preventing more people from taking advantage of obvious arbitrage opportunities",
        "Market efficiency may emerge through evolutionary selection pressure rather than participant rationality, as successful predictors accumulate capital while unsuccessful ones lose it",
        "Prediction markets show promise for governance applications like futarchy, but are vulnerable to manipulation when decisions are close and attackers have strong personal incentives"
      ],
      "controversial": [
        "Vitalik's political opinion that Biden had a much higher chance of winning than prediction markets suggested, and his criticism of Trump supporters' continued betting after the election was decided",
        "His argument that smart people suffer from 'excessive humility' and should trust their own reasoning more, even when it contradicts market consensus",
        "The suggestion that prediction markets could be superior to traditional expert analysis like Nate Silver's statistical models for certain types of predictions"
      ]
    }
  },
  {
    "id": "general-2021-01-26-snarks",
    "title": "An approximate introduction to how zk-SNARKs are possible",
    "date": "2021-01-26",
    "category": "math",
    "url": "https://vitalik.eth.limo/general/2021/01/26/snarks.html",
    "path": "general/2021/01/26/snarks.html",
    "content": "- \n\n- \n\n  \n    \n\n  \n  \n  Dark Mode Toggle\n  \n    \n\n  \n\n##  An approximate introduction to how zk-SNARKs are possible \n\n 2021 Jan 26 \nSee all posts\n\n \n \n\n An approximate introduction to how zk-SNARKs are possible \n\nSpecial thanks to Dankrad Feist, Karl Floersch and Hsiao-wei Wang\nfor feedback and review.\n\nPerhaps the most powerful cryptographic technology to come out of the\nlast decade is general-purpose succinct zero knowledge proofs, usually\ncalled zk-SNARKs (\"zero knowledge succinct arguments of knowledge\"). A\nzk-SNARK allows you to generate a proof that some computation has some\nparticular output, in such a way that the proof can be verified\nextremely quickly even if the underlying computation takes a very long\ntime to run. The \"ZK\" (\"zero knowledge\") part adds an additional\nfeature: the proof can keep some of the inputs to the computation\nhidden.\n\nFor example, you can make a proof for the statement \"I know a secret\nnumber such that if you take the word \u2018cow', add the number to the end,\nand SHA256 hash it 100 million times, the output starts with\n0x57d00485aa\". The verifier can verify the proof far more\nquickly than it would take for them to run 100 million hashes\nthemselves, and the proof would also not reveal what the secret number\nis.\n\nIn the context of blockchains, this has two very powerful\napplications:\n\n- Scalability: if a block takes a long time to\nverify, one person can verify it and generate a proof, and everyone else\ncan just quickly verify the proof instead\n\n- Privacy: you can prove that you have the right to\ntransfer some asset (you received it, and you didn't already transfer\nit) without revealing the link to which asset you received. This ensures\nsecurity without unduly leaking information about who is transacting\nwith whom to the public.\n\nBut zk-SNARKs are quite complex; indeed, as recently as in 2014-17\nthey were still frequently called \"moon math\". The good news is that\nsince then, the protocols have become simpler and our understanding of\nthem has become much better. This post will try to explain how ZK-SNARKs\nwork, in a way that should be understandable to someone with a medium\nlevel of understanding of mathematics.\n\nNote that we will focus on scalability; privacy for these\nprotocols is actually relatively easy once the scalability is there, so\nwe will get back to that topic at the end.\nWhy ZK-SNARKs \"should\" be\nhard\n\nLet us take the example that we started with: we have a number (we\ncan encode \"cow\" followed by the secret input as an integer), we take\nthe SHA256 hash of that number, then we do that again another 99,999,999\ntimes, we get the output, and we check what its starting digits are.\nThis is a huge computation.\n\nA \"succinct\" proof is one where both the size of the proof and the\ntime required to verify it grow much more slowly than the computation to\nbe verified. If we want a \"succinct\" proof, we cannot require the\nverifier to do some work per round of hashing (because then the\nverification time would be proportional to the computation). Instead,\nthe verifier must somehow check the whole computation without peeking\ninto each individual piece of the computation.\n\nOne natural technique is random sampling: how about we just\nhave the verifier peek into the computation in 500 different places,\ncheck that those parts are correct, and if all 500 checks pass then\nassume that the rest of the computation must with high probability be\nfine, too?\n\nSuch a procedure could even be turned into a non-interactive proof\nusing the Fiat-Shamir heuristic: the prover computes a\nMerkle root of the computation, uses the Merkle root to pseudorandomly\nchoose 500 indices, and provides the 500 corresponding Merkle branches\nof the data. The key idea is that the prover does not know which\nbranches they will need to reveal until they have already \"committed to\"\nthe data. If a malicious prover tries to fudge the data after learning\nwhich indices are going to be checked, that would change the Merkle\nroot, which would result in a new set of random indices, which would\nrequire fudging the data again... trapping the malicious prover in an\nendless cycle.\n\nBut unfortunately there is a fatal flaw in naively applying random\nsampling to spot-check a computation in this way: computation is\ninherently fragile. If a malicious prover flips one bit\nsomewhere in the middle of a computation, they can make it give a\ncompletely different result, and a random sampling verifier would almost\nnever find out.\n\n It only takes one deliberately inserted error, that a\nrandom check would almost never catch, to make a computation give a\ncompletely incorrect result.\n\nIf tasked with the problem of coming up with a zk-SNARK protocol,\nmany people would make their way to this point and then get stuck and\ngive up. How can a verifier possibly check every single piece of the\ncomputation, without looking at each piece of the computation\nindividually? But it turns out that there is a clever solution.\n\n## Polynomials\n\nPolynomials are a special class of algebraic expressions of the\nform:\n\n- \\(x + 5\\)\n\n- \\(x^4\\)\n\n- \\(x^3 + 3x^2 + 3x + 1\\)\n\n- \\(628x^{271} + 318x^{270} + 530x^{269} +\n... + 69x + 381\\)\n\ni.e.\u00a0they are a sum of any (finite!) number of terms of the form\n\\(c x^k\\).\n\nThere are many things that are fascinating about polynomials. But\nhere we are going to zoom in on a particular one: polynomials\nare a single mathematical object that can contain an unbounded amount of\ninformation (think of them as a list of integers and this is\nobvious). The fourth example above contained 816 digits of tau, and one can\neasily imagine a polynomial that contains far more.\n\nFurthermore, a single equation between polynomials can\nrepresent an unbounded number of equations between numbers. For\nexample, consider the equation \\(A(x) + B(x) =\nC(x)\\). If this equation is true, then it's also true that:\n\n- \\(A(0) + B(0) = C(0)\\)\n\n- \\(A(1) + B(1) = C(1)\\)\n\n- \\(A(2) + B(2) = C(2)\\)\n\n- \\(A(3) + B(3) = C(3)\\)\n\nAnd so on for every possible coordinate. You can even construct\npolynomials to deliberately represent sets of numbers so you can check\nmany equations all at once. For example, suppose that you wanted to\ncheck:\n\n- 12 + 1 = 13\n\n- 10 + 8 = 18\n\n- 15 + 8 = 23\n\n- 15 + 13 = 28\n\nYou can use a procedure called Lagrange\ninterpolation to construct polynomials \\(A(x)\\) that give\n(12, 10, 15, 15) as outputs at some specific set of\ncoordinates (eg. (0, 1, 2, 3)), \\(B(x)\\) the outputs\n(1, 8, 8, 13) on those same coordinates, and so forth. In\nfact, here are the polynomials:\n\n- \\(A(x) = -2x^3 + \\frac{19}{2}x^2 -\n\\frac{19}{2}x + 12\\)\n\n- \\(B(x) = 2x^3 - \\frac{19}{2}x^2 +\n\\frac{29}{2}x + 1\\)\n\n- \\(C(x) = 5x + 13\\)\n\nChecking the equation \\(A(x) + B(x) =\nC(x)\\) with these polynomials checks all four above equations at\nthe same time.\nComparing a polynomial to\nitself\n\nYou can even check relationships between a large number of\nadjacent evaluations of the same polynomial using a simple\npolynomial equation. This is slightly more advanced. Suppose that you\nwant to check that, for a given polynomial \\(F\\), \\(F(x+2) =\nF(x) + F(x+1)\\) within the integer range \\(\\{0, 1 ... 98\\}\\) (so if you also\ncheck \\(F(0) = F(1) = 1\\), then \\(F(100)\\) would be the 100th Fibonacci\nnumber).\n\nAs polynomials, \\(F(x+2) - F(x+1) -\nF(x)\\) would not be exactly zero, as it could give arbitrary\nanswers outside the range \\(x = \\{0,\n1 ... 98\\}\\). But we can do something clever. In general, there\nis a rule that if a polynomial \\(P\\) is\nzero across some set \\(S=\\{x_1, x_2 ...\nx_n\\}\\) then it can be expressed as \\(P(x) = Z(x) * H(x)\\), where \\(Z(x) =\\) \\((x -\nx_1) * (x - x_2) * ... * (x - x_n)\\) and \\(H(x)\\) is also a polynomial. In other\nwords, any polynomial that equals zero across some set is a\n(polynomial) multiple of the simplest (lowest-degree) polynomial that\nequals zero across that same set.\n\nWhy is this the case? It is a nice corollary of polynomial long\ndivision: the\nfactor theorem. We know that, when dividing \\(P(x)\\) by \\(Z(x)\\), we will get a quotient \\(Q(x)\\) and a remainer \\(R(x)\\) which satisfy \\(P(x) = Z(x) * Q(x) + R(x)\\), where the\ndegree of the remainder \\(R(x)\\) is\nstrictly less than that of \\(Z(x)\\).\nSince we know that \\(P\\) is zero on all\nof \\(S\\), it means that \\(R\\) has to be zero on all of \\(S\\) as well. So we can simply compute \\(R(x)\\) via polynomial interpolation, since\nit's a polynomial of degree at most \\(n-1\\) and we know \\(n\\) values (the zeroes at \\(S\\)). Interpolating a polynomial with all\nzeroes gives the zero polynomial, thus \\(R(x)\n= 0\\) and \\(H(x)= Q(x)\\).\n\nGoing back to our example, if we have a polynomial \\(F\\) that encodes Fibonacci numbers (so\n\\(F(x+2) = F(x) + F(x+1)\\) across \\(x = \\{0, 1 ... 98\\}\\)), then I can convince\nyou that \\(F\\) actually satisfies\nthis condition by proving that the polynomial \\(P(x) =\\) \\(F(x+2)\n- F(x+1) - F(x)\\) is zero over that range, by giving you the\nquotient:\n\n\\(H(x) = \\frac{F(x+2) - F(x+1) -\nF(x)}{Z(x)}\\)\n\nWhere \\(Z(x) = (x - 0) * (x - 1) * ... * (x\n- 98)\\).\n\nYou can calculate \\(Z(x)\\) yourself\n(ideally you would have it precomputed), check the equation, and if the\ncheck passes then \\(F(x)\\) satisfies\nthe condition!\n\nNow, step back and notice what we did here. We converted a\n100-step-long computation (computing the 100th Fibonacci number) into a\nsingle equation with polynomials. Of course, proving the N'th Fibonacci\nnumber is not an especially useful task, especially since Fibonacci\nnumbers have\na closed form. But you can use exactly the same basic technique,\njust with some extra polynomials and some more complicated equations, to\nencode arbitrary computations with an arbitrarily large number of\nsteps.\n\nNow, if only there was a way to verify equations with polynomials\nthat's much faster than checking each coefficient...\n\n## Polynomial commitments\n\nAnd once again, it turns out that there is an answer:\npolynomial commitments. A polynomial commitment is best\nviewed as a special way to \"hash\" a polynomial, where the hash has the\nadditional property that you can check equations between polynomials by\nchecking equations between their hashes. Different polynomial commitment\nschemes have different properties in terms of exactly what kinds of\nequations you can check.\n\nHere are some common examples of things you can do with various\npolynomial commitment schemes (we use \\(com(P)\\) to mean \"the commitment to the\npolynomial \\(P\\)\"):\n\n- Add them: given \\(com(P)\\), \\(com(Q)\\) and \\(com(R)\\) check if \\(P + Q = R\\)\n\n- Multiply them: given \\(com(P)\\), \\(com(Q)\\) and \\(com(R)\\) check if \\(P * Q = R\\)\n\n- Evaluate at a point: given \\(com(P)\\), \\(w\\), \\(z\\)\nand a supplemental proof (or \"witness\") \\(Q\\), verify that \\(P(w) = z\\)\n\nIt's worth noting that these primitives can be constructed from each\nother. If you can add and multiply, then you can evaluate: to prove that\n\\(P(w) = z\\), you can construct \\(Q(x) = \\frac{P(x) - z}{x - w}\\), and the\nverifier can check if \\(Q(x) * (x - w) + z\n\\stackrel{?}{=} P(x)\\). This works because if such a polynomial\n\\(Q(x)\\) exists, then \\(P(x) - z = Q(x) * (x - w)\\), which means\nthat \\(P(x) - z\\) equals zero at \\(w\\) (as \\(x -\nw\\) equals zero at \\(w\\)) and so\n\\(P(x)\\) equals \\(z\\) at \\(w\\).\n\nAnd if you can evaluate, you can do all kinds of checks. This is\nbecause there is a mathematical\ntheorem that says, approximately, that if some equation involving\nsome polynomials holds true at a randomly selected coordinate,\nthen it almost certainly holds true for the polynomials as a whole. So\nif all we have is a mechanism to prove evaluations, we can check eg. our\nequation \\(P(x + 2) - P(x + 1) - P(x) = Z(x) *\nH(x)\\) using an interactive game:\n\nAs I alluded to earlier, we can make this non-interactive\nusing the Fiat-Shamir heuristic: the prover can compute\nr themselves by setting\nr = hash(com(P), com(H)) (where hash is any\ncryptographic hash function; it does not need any special properties).\nThe prover cannot \"cheat\" by picking P and H\nthat \"fit\" at that particular r but not elsewhere, because\nthey do not know r at the time that they are picking\nP and H!\n\n## A quick recap so far\n\n- ZK-SNARKs are hard because the verifier needs to somehow check\nmillions of steps in a computation, without doing a piece of work to\ncheck each individual step directly (as that would take too long).\n\n- We get around this by encoding the computation into\npolynomials.\n\n- A single polynomial can contain an unboundedly large amount of\ninformation, and a single polynomial expression (eg. \\(P(x+2) - P(x+1) - P(x) = Z(x) * H(x)\\)) can\n\"stand in\" for an unboundedly large number of equations between\nnumbers.\n\n- If you can verify the equation with polynomials, you are implicitly\nverifying all of the number equations (replace \\(x\\) with any actual x-coordinate)\nsimultaneously.\n\n- We use a special type of \"hash\" of a polynomial, called a\npolynomial commitment, to allow us to actually verify the\nequation between polynomials in a very short amount of time, even if the\nunderlying polynomials are very large.\n\nSo, how do these\nfancy polynomial hashes work?\n\nThere are three major schemes that are widely used at the moment:\nbulletproofs, Kate and FRI.\n\n- Here is a description of Kate commitments by Dankrad Feist: https://dankradfeist.de/ethereum/2020/06/16/kate-polynomial-commitments.html\n\n- Here is a description of bulletproofs by the curve25519-dalek team:\nhttps://doc-internal.dalek.rs/bulletproofs/notes/inner_product_proof/index.html,\nand here is an explanation-in-pictures by myself: https://twitter.com/VitalikButerin/status/1371844878968176647\n\n- Here is a description of FRI by... myself: ../../../2017/11/22/starks_part_2.html\n\nWhoa,\nwhoa, take it easy. Try to explain one of them simply, without shipping\nme off to even more scary links\n\nTo be honest, they're not that simple. There's a reason why\nall this math did not really take off until 2015 or so.\n\n## Please?\n\nIn my opinion, the easiest one to understand fully is FRI (Kate is\neasier if you're willing to accept elliptic\ncurve pairings as a \"black box\", but pairings are really\ncomplicated, so altogether I find FRI simpler).\n\nHere is how a simplified version of FRI works (the real protocol has\nmany tricks and optimizations that are missing here for simplicity).\nSuppose that you have a polynomial \\(P\\) with degree \\(< n\\). The commitment to \\(P\\) is a Merkle root of a set of\nevaluations to \\(P\\) at some set of\npre-selected coordinates (eg. \\(\\{0, 1 ....\n8n-1\\}\\), though this is not the most efficient choice). Now, we\nneed to add something extra to prove that this set of evaluations\nactually is a degree \\(< n\\)\npolynomial.\n\nLet \\(Q\\) be the polynomial only\ncontaining the even coefficients of \\(P\\), and \\(R\\) be the polynomial only containing the\nodd coefficients of \\(P\\). So if \\(P(x) = x^4 + 4x^3 + 6x^2 + 4x + 1\\), then\n\\(Q(x) = x^2 + 6x + 1\\) and \\(R(x) = 4x + 4\\) (note that the degrees of\nthe coefficients get \"collapsed down\" to the range \\([0...\\frac{n}{2})\\)).\n\nNotice that \\(P(x) = Q(x^2) + x *\nR(x^2)\\) (if this isn't immediately obvious to you, stop and\nthink and look at the example above until it is).\n\nWe ask the prover to provide Merkle roots for \\(Q(x)\\) and \\(R(x)\\). We then generate a random number\n\\(r\\) and ask the prover to provide a\n\"random linear combination\" \\(S(x) = Q(x) + r\n* R(x)\\).\n\nWe pseudorandomly sample a large set of indices (using the\nalready-provided Merkle roots as the seed for the randomness as before),\nand ask the prover to provide the Merkle branches for \\(P\\), \\(Q\\), \\(R\\)\nand \\(S\\) at these indices. At each of\nthese provided coordinates, we check that:\n\n- \\(P(x)\\) actually does\nequal \\(Q(x^2) + x * R(x^2)\\)\n\n- \\(S(x)\\) actually does\nequal \\(Q(x) + r * R(x)\\)\n\nIf we do enough checks, then we can be convinced that the \"expected\"\nvalues of \\(S(x)\\) are different from\nthe \"provided\" values in at most, say, 1% of cases.\n\nNotice that \\(Q\\) and \\(R\\) both have degree \\(< \\frac{n}{2}\\). Because \\(S\\) is a linear combination of \\(Q\\) and \\(R\\), \\(S\\)\nalso has degree \\(<\n\\frac{n}{2}\\). And this works in reverse: if we can prove \\(S\\) has degree \\(< \\frac{n}{2}\\), then the fact that it's\na randomly chosen combination prevents the prover from choosing\nmalicious \\(Q\\) and \\(R\\) with hidden high-degree coefficients\nthat \"cancel out\", so \\(Q\\) and \\(R\\) must both be degree \\(< \\frac{n}{2}\\), and because \\(P(x) = Q(x^2) + x * R(x^2)\\), we know that\n\\(P\\) must have degree \\(< n\\).\n\nFrom here, we simply repeat the game with \\(S\\), progressively \"reducing\" the\npolynomial we care about to a lower and lower degree, until it's at a\nsufficiently low degree that we can check it directly.\n\nAs in the previous examples, \"Bob\" here is an abstraction, useful for\ncryptographers to mentally reason about the protocol. In reality, Alice\nis generating the entire proof herself, and to prevent her from cheating\nwe use Fiat-Shamir: we choose each randomly samples coordinate or\nr value based on the hash of the data generated in the\nproof up until that point.\n\nA full \"FRI commitment\" to \\(P\\) (in\nthis simplified protocol) would consist of:\n\n- The Merkle root of evaluations of \\(P\\)\n\n- The Merkle roots of evaluations of \\(Q\\), \\(R\\), \\(S_1\\)\n\n- The randomly selected branches of \\(P\\), \\(Q\\), \\(R\\), \\(S_1\\) to check \\(S_1\\) is correctly \"reduced from\" \\(P\\)\n\n- The Merkle roots and randomly selected branches just as in steps (2)\nand (3) for successively lower-degree reductions \\(S_2\\) reduced from \\(S_1\\), \\(S_3\\) reduced from \\(S_2\\), all the way down to a low-degree\n\\(S_k\\) (this gets repeated \\(\\approx log_2(n)\\) times in total)\n\n- The full Merkle tree of the evaluations of \\(S_k\\) (so we can check it directly)\n\nEach step in the process can introduce a bit of \"error\", but if you\nadd enough checks, then the total error will be low enough that you can\nprove that \\(P(x)\\) equals a degree\n\\(< n\\) polynomial in at least, say,\n80% of positions. And this is sufficient for our use cases. If you want\nto cheat in a zk-SNARK, you would need to make a polynomial commitment\nfor a fractional expression (eg. to \"prove\" the false claim that \\(x^2 + 2x + 3\\) evaluated at \\(4\\) equals \\(5\\), you would need to provide a polynomial\ncommitment for \\(\\frac{x^2 + 2x + 3 - 5}{x -\n4} = x + 6 + \\frac{22}{x - 4}\\)). The set of evaluations for such\na fractional expression would differ from the evaluations for\nany real degree \\(< n\\) polynomial\nin so many positions that any attempt to make a FRI commitment to them\nwould fail at some step.\n\nAlso, you can check carefully that the total number and size of the\nobjects in the FRI commitment is logarithmic in the degree, so for large\npolynomials, the commitment really is much smaller than the polynomial\nitself.\n\nTo check equations between different polynomial commitments of this\ntype (eg. check \\(A(x) + B(x) = C(x)\\)\ngiven FRI commitments to \\(A\\), \\(B\\) and \\(C\\)), simply randomly select many indices,\nask the prover for Merkle branches at each of those indices for each\npolynomial, and verify that the equation actually holds true at each of\nthose positions.\n\nThe above description is a highly inefficient protocol; there\nis a whole host of algebraic tricks that can increase its\nefficiency by a factor of something like a hundred, and you\nneed these tricks if you want a protocol that is actually viable for,\nsay, use inside a blockchain transaction. In particular, for example,\n\\(Q\\) and \\(R\\) are not actually necessary, because if\nyou choose your evaluation points very cleverly, you can reconstruct the\nevaluations of \\(Q\\) and \\(R\\) that you need directly from evaluations\nof \\(P\\). But the above description\nshould be enough to convince you that a polynomial commitment is\nfundamentally possible.\n\n## Finite fields\n\nIn the descriptions above, there was a hidden assumption: that each\nindividual \"evaluation\" of a polynomial was small. But when we are\ndealing with polynomials that are big, this is clearly not true. If we\ntake our example from above, \\(628x^{271} +\n318x^{270} + 530x^{269} + ... + 69x + 381\\), that encodes 816\ndigits of tau, and evaluate it at \\(x=1000\\), you get.... an 816-digit number\ncontaining all of those digits of tau. And so there is one more thing\nthat we need to add. In a real implementation, all of the arithmetic\nthat we are doing here would not be done using \"regular\" arithmetic over\nreal numbers. Instead, it would be done using modular\narithmetic.\n\nWe redefine all of our arithmetic operations as follows. We pick some\nprime \"modulus\" p. The % operator means \"take the remainder\nof\": \\(15\\ \\%\\ 7 = 1\\), \\(53\\ \\%\\ 10 = 3\\), etc (note that the answer\nis always non-negative, so for example \\(-1\\\n\\%\\ 10 = 9\\)). We redefine\n\n\\(x + y \\Rightarrow (x + y)\\) %\n\\(p\\)\n\n\\(x * y \\Rightarrow (x * y)\\) %\n\\(p\\)\n\n\\(x^y \\Rightarrow (x^y)\\) % \\(p\\)\n\n\\(x - y \\Rightarrow (x - y)\\) %\n\\(p\\)\n\n\\(x / y \\Rightarrow (x * y ^{p-2})\\)\n% \\(p\\)\n\nThe above rules are all self-consistent. For example, if \\(p = 7\\), then:\n\n- \\(5 + 3 = 1\\) (as \\(8\\) % \\(7 =\n1\\))\n\n- \\(1 - 3 = 5\\) (as \\(-2\\) % \\(7 =\n5\\))\n\n- \\(2 \\cdot 5 = 3\\)\n\n- \\(3 / 5 = 2\\) (as (\\(3 \\cdot 5^5\\)) % \\(7 = 9375\\) % \\(7\n= 2\\))\n\nMore complex identities such as the distributive law also hold: \\((2 + 4) \\cdot 3\\) and \\(2 \\cdot 3 + 4 \\cdot 3\\) both evaluate to\n\\(4\\). Even formulas like \\((a^2 - b^2)\\) = \\((a - b) \\cdot (a + b)\\) are still true in\nthis new kind of arithmetic.\n\nDivision is the hardest part; we can't use regular division because\nwe want the values to always remain integers, and regular division often\ngives non-integer results (as in the case of \\(3/5\\)). We get around this problem using Fermat's\nlittle theorem, which states that for any nonzero \\(x < p\\), it holds that \\(x^{p-1}\\) % \\(p =\n1\\). This implies that \\(x^{p-2}\\) gives a number which, if\nmultiplied by \\(x\\) one more time,\ngives \\(1\\), and so we can say that\n\\(x^{p-2}\\) (which is an integer)\nequals \\(\\frac{1}{x}\\). A somewhat more\ncomplicated but faster way to evaluate this modular division operator is\nthe extended\nEuclidean algorithm, implemented in python here.\n\nBecause of how the numbers \"wrap around\", modular arithmetic is\nsometimes called \"clock math\"\n\nWith modular math we've created an entirely new system of arithmetic,\nand it's self-consistent in all the same ways traditional arithmetic is\nself-consistent. Hence, we can talk about all of the same kinds of\nstructures over this field, including polynomials, that we talk about in\n\"regular math\". Cryptographers love working in modular math (or, more\ngenerally, \"finite fields\") because there is a bound on the size of a\nnumber that can arise as a result of any modular math calculation - no\nmatter what you do, the values will not \"escape\" the set \\(\\{0, 1, 2 ... p-1\\}\\). Even evaluating a\ndegree-1-million polynomial in a finite field will never give an answer\noutside that set.\nWhat's\na slightly more useful example of a computation being converted into a\nset of polynomial equations?\n\nLet's say we want to prove that, for some polynomial \\(P\\), \\(0 \\le P(n)\n< 2^{64}\\), without revealing the exact value of \\(P(n)\\). This is a common use case in\nblockchain transactions, where you want to prove that a transaction\nleaves a balance non-negative without revealing what that balance\nis.\n\nWe can construct a proof for this with the following polynomial\nequations (assuming for simplicity \\(n =\n64\\)):\n\n- \\(P(0) = 0\\)\n\n- \\(P(x+1) = P(x) * 2 + R(x)\\) across\nthe range \\(\\{0...63\\}\\)\n\n- \\(R(x) \\in \\{0,1\\}\\) across the\nrange \\(\\{0...63\\}\\)\n\nThe latter two statements can be restated as \"pure\" polynomial\nequations as follows (in this context \\(Z(x) =\n(x - 0) * (x - 1) * ... * (x - 63)\\)):\n\n- \\(P(x+1) - P(x) * 2 - R(x) = Z(x) *\nH_1(x)\\)\n\n- \\(R(x) * (1 - R(x)) = Z(x) *\nH_2(x)\\) (notice the clever trick: \\(y\n* (1-y) = 0\\) if and only if \\(y \\in\n\\{0, 1\\}\\))\n\nThe idea is that successive evaluations of \\(P(i)\\) build up the number bit-by-bit: if\n\\(P(4) = 13\\), then the sequence of\nevaluations going up to that point would be: \\(\\{0, 1, 3, 6, 13\\}\\). In binary, 1 is\n1, 3 is 11, 6 is 110, 13 is\n1101; notice how \\(P(x+1) = P(x)\n* 2 + R(x)\\) keeps adding one bit to the end as long as \\(R(x)\\) is zero or one. Any number within\nthe range \\(0 \\le x < 2^{64}\\) can\nbe built up over 64 steps in this way, any number outside that range\ncannot.\n\n## Privacy\n\nBut there is a problem: how do we know that the commitments to \\(P(x)\\) and \\(R(x)\\) don't \"leak\" information that allows\nus to uncover the exact value of \\(P(64)\\), which we are trying to keep\nhidden?\n\nThere is some good news: these proofs are small proofs that\ncan make statements about a large amount of data and computation. So in\ngeneral, the proof will very often simply not be big enough to\nleak more than a little bit of information. But can we go from\n\"only a little bit\" to \"zero\"? Fortunately, we can.\n\nHere, one fairly general trick is to add some \"fudge factors\" into\nthe polynomials. When we choose \\(P\\),\nadd a small multiple of \\(Z(x)\\) into\nthe polynomial (that is, set \\(P'(x) =\nP(x) + Z(x) * E(x)\\) for some random \\(E(x)\\)). This does not affect the\ncorrectness of the statement (in fact, \\(P'\\) evaluates to the same values as\n\\(P\\) on the coordinates that \"the\ncomputation is happening in\", so it's still a valid transcript), but it\ncan add enough extra \"noise\" into the commitments to make any remaining\ninformation unrecoverable. Additionally, in the case of FRI, it's\nimportant to not sample random points that are within the domain that\ncomputation is happening in (in this case \\(\\{0...64\\}\\)).\nCan we have one more recap,\nplease??\n\n- The three most prominent types of polynomial commitments are FRI,\nKate and bulletproofs.\n\n- Kate is the simplest conceptually but depends on the really\ncomplicated \"black box\" of elliptic curve pairings.\n\n- FRI is cool because it relies only on hashes; it works by\nsuccessively reducing a polynomial to a lower and lower-degree\npolynomial and doing random sample checks with Merkle branches to prove\nequivalence at each step.\n\n- To prevent the size of individual numbers from blowing up, instead\nof doing arithmetic and polynomials over the integers, we do\neverything over a finite field (usually integers modulo some\nprime p)\n\n- Polynomial commitments lend themselves naturally to privacy\npreservation because the proof is already much smaller than the\npolynomial, so a polynomial commitment can't reveal more than a little\nbit of the information in the polynomial anyway. But we can add some\nrandomness to the polynomials we're committing to to reduce the\ninformation revealed from \"a little bit\" to \"zero\".\n\nWhat research\nquestions are still being worked on?\n\n- Optimizing FRI: there are already quite a few\noptimizations involving carefully selected evaluation domains, \"DEEP-FRI\", and a whole host\nof other tricks to make FRI more efficient. Starkware and others are\nworking on this.\n\n- Better ways to encode computation into polynomials:\nfiguring out the most efficient way to encode complicated computations\ninvolving hash functions, memory access and other features into\npolynomial equations is still a challenge. There has been great progress\non this (eg. see PLOOKUP), but we still need\nmore, especially if we want to encode general-purpose virtual machine\nexecution into polynomials.\n\n- Incrementally verifiable computation: it would be\nnice to be able to efficiently keep \"extending\" a proof while a\ncomputation continues. This is valuable in the \"single-prover\" case, but\nalso in the \"multi-prover\" case, particularly a blockchain where a\ndifferent participant creates each block. See Halo for some recent\nwork on this.\n\n## I wanna learn more!\n\n## My materials\n\n- STARKs: part 1,\npart 2, part 3\n\n- Specific protocols for encoding computation into polynomials: PLONK\n\n- Some key mathematical optimizations I didn't talk about here: Fast Fourier transforms\n\n## Other people's materials\n\n- Starkware's\nonline course\n\n- Dankrad\nFeist on Kate commitments\n\n- Bulletproofs",
    "contentLength": 27852,
    "summary": "zk-SNARKs use polynomial equations to encode large computations, enabling verification through polynomial commitments rather than checking each step individually.",
    "detailedSummary": {
      "theme": "An accessible mathematical explanation of how zk-SNARKs enable succinct verification of complex computations through polynomial encoding and cryptographic commitments.",
      "summary": "Vitalik provides a comprehensive introduction to zero-knowledge succinct non-interactive arguments of knowledge (zk-SNARKs), explaining how they solve the fundamental challenge of verifying large computations quickly without examining every step. He demonstrates how polynomials serve as the mathematical foundation, showing that a single polynomial equation can represent an unbounded number of numerical equations simultaneously. The key insight is that computations can be encoded into polynomial relationships, and polynomial commitments allow verifiers to check these relationships efficiently. Vitalik walks through three major polynomial commitment schemes (FRI, Kate, and bulletproofs), with particular focus on FRI's hash-based approach that progressively reduces polynomial degrees through random sampling and Merkle proofs. He also explains the necessity of finite field arithmetic to keep numbers bounded and discusses how privacy is achieved by adding randomness to prevent information leakage from the commitments.",
      "takeaways": [
        "zk-SNARKs solve the scalability problem by encoding entire computations into polynomial equations that can be verified much faster than running the original computation",
        "Polynomial commitments act as special 'hashes' of polynomials that preserve the ability to verify mathematical relationships between the underlying polynomials",
        "The FRI protocol achieves polynomial commitments using only hash functions by recursively reducing polynomial degrees and using Merkle trees for verification",
        "All arithmetic operations are performed in finite fields (modular arithmetic) to prevent number sizes from growing unboundedly during computation",
        "Privacy in zk-SNARKs is achieved by adding random 'fudge factors' to polynomials, which preserves correctness while hiding the actual values being computed"
      ],
      "controversial": []
    }
  },
  {
    "id": "general-2021-01-11-recovery",
    "title": "Why we need wide adoption of social recovery wallets",
    "date": "2021-01-11",
    "category": "applications",
    "url": "https://vitalik.eth.limo/general/2021/01/11/recovery.html",
    "path": "general/2021/01/11/recovery.html",
    "content": "- \n\n- \n\n  \n    \n\n  \n  \n  Dark Mode Toggle\n  \n    \n\n  \n\n##  Why we need wide adoption of social recovery wallets \n\n 2021 Jan 11 \nSee all posts\n\n \n \n\n Why we need wide adoption of social recovery wallets \n\nSpecial thanks to Itamar Lesuisse from Argent and Daniel Wang\nfrom Loopring for feedback.\n\nOne of the great challenges with making cryptocurrency and blockchain\napplications usable for average users is security: how do we prevent\nusers' funds from being lost or stolen? Losses and thefts are a serious\nissue, often costing innocent blockchain users thousands of dollars or\neven in some cases the majority of their entire net worth.\n\nThere have been many solutions proposed over the years: paper\nwallets, hardware wallets, and my own one-time favorite: multisig\nwallets. And indeed they have led to significant improvements in\nsecurity. However, these solutions have all suffered from various\ndefects - sometimes providing far less extra protection against theft\nand loss than is actually needed, sometimes being cumbersome and\ndifficult to use leading to very low adoption, and sometimes both. But\nrecently, there is an emerging better alternative: a newer type of smart\ncontract wallet called a social recovery wallet. These wallets\ncan potentially provide a high level of security and much better\nusability than previous options, but there is still a way to go before\nthey can be easily and widely deployed. This post will go\nthrough what social recovery wallets are, why they matter, and how we\ncan and should move toward much broader adoption of them throughout the\necosystem.\nWallet security is a\nreally big problem\n\nWallet security issues have been a thorn in the side of the\nblockchain ecosystem almost since the beginning. Cryptocurrency losses\nand thefts were rampant even back in 2011 when Bitcoin was almost the\nonly cryptocurrency out there; indeed, in my pre-Ethereum role as a\ncofounder and writer of Bitcoin\nMagazine, I wrote an entire article detailing the horrors of hacks\nand losses and thefts that were already happening at the time.\n\nHere is one sample:\n\nLast night around 9PM PDT, I clicked a link to go to\nCoinChat[.]freetzi[.]com \u2013 and I was prompted to run java. I did\n(thinking this was a legitimate chatoom), and nothing happened. I closed\nthe window and thought nothing of it. I opened my bitcoin-qt wallet\napprox 14 minutes later, and saw a transaction that I did NOT approve go\nto wallet 1Es3QVvKN1qA2p6me7jLCVMZpQXVXWPNTC for almost my entire\nwallet...\n\nThis person's losses were 2.07 BTC, worth $300 at the time, and over\n$70000 today. Here's another one:\n\nIn June 2011, the Bitcointalk member \"allinvain\" lost 25,000 BTC\n(worth $500,000 at the time) after an unknown intruder somehow gained\ndirect access to his computer. The attacker was able to access\nallinvain's wallet.dat file, and quickly empty out the wallet \u2013 either\nby sending a transaction from allinvain's computer itself, or by simply\nuploading the wallet.dat file and emptying it on his own machine.\n\nIn present-day value, that's a loss of nearly one billion\ndollars. But theft is not the only concern; there are also losses\nfrom losing one's private keys. Here's Stefan Thomas:\n\nBitcoin developer Stefan Thomas had three backups of his wallet \u2013 an\nencrypted USB stick, a Dropbox account and a Virtualbox virtual machine.\nHowever, he managed to erase two of them and forget the password to the\nthird, forever losing access to 7,000 BTC (worth $125,000 at the time).\nThomas's reaction: \"[I'm] pretty dedicated to creating better clients\nsince then.\"\n\nOne analysis of the Bitcoin ecosystem suggests that 1500\nBTC may be lost every day - over ten times more than what Bitcoin\nusers spend\non transaction fees, and over the years adding up to as much as 20%\nof the total supply. The stories and the numbers alike point to the\nsame inescapable truth: the importance of the wallet security\nproblem is great, and it should not be underestimated.\n\nIt's easy to see the social and psychological reasons why wallet\nsecurity is easy to underestimate: people naturally worry about\nappearing uncareful or dumb in front of an always judgemental public,\nand so many keep their experiences with their funds getting hacked to\nthemselves. Loss of funds is even worse, as there is a pervasive (though\nin my opinion very incorrect) feeling that \"there is no one to blame but\nyourself\". But the reality is that the whole point of\ndigital technology, blockchains included, is to make it easier for\nhumans to engage in very complicated tasks without having to exert\nextreme mental effort or live in constant fear of making\nmistakes. An ecosystem whose only answer to losses and thefts\nis a combination of 12-step tutorials, not-very-secure half-measures and\nthe not-so-occasional semi-sarcastic \"sorry for your loss\" is going to\nhave a hard time getting broad adoption.\n\nSo solutions that reduce the quantity of losses and thefts taking\nplace, without requiring all cryptocurrency users to turn personal\nsecurity into a full-time hobby, are highly valuable for the\nindustry.\nHardware wallets\nalone are not good enough\n\nHardware wallets are often touted as the best-in-class technology for\ncryptocurrency funds management. A hardware wallet is a specialized\nhardware device which can be connected to your computer or phone (eg.\nthrough USB), and which contains a specialized chip that can only\ngenerate private keys and sign transactions. A transaction would be\ninitiated on your computer or phone, must be confirmed on the hardware\nwallet before it can be sent. The private key stays on your hardware\nwallet, so an attacker that hacks into your computer or phone could\nnot drain the funds.\n\nHardware wallets are a significant improvement, and they certainly\nwould have protected the Java chatroom victim, but they are not perfect.\nI see two main problems with hardware wallets:\n\n- Supply chain attacks: if you buy a hardware wallet,\nyou are trusting a number of actors that were involved in producing it -\nthe company that designed the wallet, the factory that produced it, and\neveryone involved in shipping it who could have replaced it with a fake.\nHardware wallets are potentially a magnet for such attacks: the ratio of\nfunds stolen to number of devices compromised is very high. To their\ncredit, hardware wallet manufacturers such as Ledger have put in many\nsafeguards to protect against these risks, but some risks still remain.\nA hardware device fundamentally cannot be audited the same way a piece\nof open source software can.\n\n- Still a single point of failure: if someone steals\nyour hardware wallet right after they stand behind your shoulder and\ncatch you typing in the PIN, they can steal your funds. If you lose your\nhardware wallet, then you lose your funds - unless the hardware wallet\ngenerates and outputs a backup at setup time, but as we will see those\nhave problems of their own...\n\nMnemonic phrases are not\ngood enough\n\nMany wallets, hardware and software alike, have a setup procedure\nduring which they output a mnemonic phrase, which is a\nhuman-readable 12 to 24-word encoding of the wallet's root private key.\nA mnemonic phrase looks like this:\n\n vote    dance   type    subject valley  fall    usage   silk\n essay   lunch   endorse lunar   obvious race    ribbon  key\n already arrow   enable  drama   keen    survey  lesson  cruel\n\nIf you lose your wallet but you have the mnemonic phrase, you can\ninput the phrase when setting up a new wallet to recover your account,\nas the mnemonic phrase contains the root key from which all of your\nother keys can be generated.\n\nMnemonic phrases are good for protecting against loss, but they do\nnothing against theft. Even worse, they add a new vector for\ntheft: if you have the standard hardware wallet + mnemonic backup combo,\nthen someone stealing either your hardware wallet + PIN\nor your mnemonic backup can steal your funds. Furthermore,\nmaintaining a mnemonic phrase and not accidentally throwing it away is\nitself a non-trivial mental effort.\n\nThe problems with theft can be alleviated if you split the phrase in\nhalf and give half to your friend, but (i) almost no one actually\npromotes this, (ii) there are security issues, as if the phrase is short\n(128 bits) then a sophisticated and motivated attacker who steals one\npiece may be able to brute-force through all \\(2^{64}\\) possible combinations to find the\nother, and (iii) it increases the mental overhead even further.\n\n## So what do we need?\n\nWhat we need is a wallet design which satisfies three key\ncriteria:\n\n- No single point of failure: there is no single\nthing (and ideally, no collection of things which travel together)\nwhich, if stolen, can give an attacker access to your funds, or if lost,\ncan deny you access to your funds.\n\n- Low mental overhead: as much as possible, it should\nnot require users to learn strange new habits or exert mental effort to\nalways remember to follow certain patterns of behavior.\n\n- Maximum ease of transacting: most normal activities\nshould not require much more effort than they do in regular wallets (eg.\nStatus, Metamask...)\n\n## Multisig is good!\n\nThe best-in-class technology for solving these problems back\nin 2013 was multisig. You could have a wallet that has three keys,\nwhere any two of them are needed to send a transaction.\n\nThis technology was originally developed within the Bitcoin\necosystem, but excellent multisig wallets (eg. see Gnosis Safe) now exist for Ethereum\ntoo. Multisig wallets have been highly successful within organizations:\nthe Ethereum Foundation uses a 4-of-7 multisig wallet to store its\nfunds, as do many other orgs in the Ethereum ecosystem.\n\nFor a multisig wallet to hold the funds for an individual,\nthe main challenge is: who holds the funds, and how are transactions\napproved? The most common formula is some variant of \"two easily\naccessible, but separate, keys, held by you (eg. laptop and phone) and a\nthird more secure but less accessible a backup, held offline or by a\nfriend or institution\".\n\nThis is reasonably secure: there is no single device that can be lost\nor stolen that would lead to you losing access to your funds. But the\nsecurity is far from perfect: if you can steal someone's laptop, it's\noften not that hard to steal their phone as well. The usability is also\na challenge, as every transaction now requires two confirmations with\ntwo devices.\n\n## Social recovery is better\n\nThis gets us to my preferred method for securing a wallet: social\nrecovery. A social recovery system works as follows:\n\n- There is a single \"signing key\" that can be used to approve\ntransactions\n\n- There is a set of at least 3 (or a much higher number) of\n\"guardians\", of which a majority can cooperate to change the signing key\nof the account.\n\nThe signing key has the ability to add or remove guardians, though\nonly after a delay (often 1-3 days).\n\nUnder all normal circumstances, the user can simply use their social\nrecovery wallet like a regular wallet, signing messages with their\nsigning key so that each transaction signed can fly off with a single\nconfirmation click much like it would in a \"traditional\" wallet like\nMetamask.\n\nIf a user loses their signing key, that is when the social\nrecovery functionality would kick in. The user can simply reach out to\ntheir guardians and ask them to sign a special transaction to change the\nsigning pubkey registered in the wallet contract to a new one. This is\neasy: they can simply go to a webpage such as security.loopring.io, sign in,\nsee a recovery request and sign it. About as easy for each guardian as\nmaking a Uniswap trade.\n\nThere are many possible choices for whom to select as a guardian. The\nthree most common choices are:\n\n- Other devices (or paper mnemonics) owned by the wallet holder\nthemselves\n\n- Friends and family members\n\n- Institutions, which would sign a recovery message if they get a\nconfirmation of your phone number or email or perhaps in high value\ncases verify you personally by video call\n\nGuardians are easy to add: you can add a guardian simply by typing in\ntheir ENS name or ETH address, though most social recovery wallets will\nrequire the guardian to sign a transaction in the recovery webpage to\nagree to be added. In any sanely designed social recovery wallet, the\nguardian does NOT need to download and use the same wallet; they can\nsimply use their existing Ethereum wallet, whichever type of wallet it\nis. Given the high convenience of adding guardians, if you are lucky\nenough that your social circles are already made up of Ethereum users, I\npersonally favor high guardian counts (ideally 7+) for increased\nsecurity. If you already have a wallet, there is no ongoing mental\neffort required to be a guardian: any recovery operations that you do\nwould be done through your existing wallet. If you not know many other\nactive Ethereum users, then a smaller number of guardians that you trust\nto be technically competent is best.\n\nTo reduce the risk of attacks on guardians and collusion,\nyour guardians do not have to be publicly known: in fact, they\ndo not need to know each other's identities. This can\nbe accomplished in two ways. First, instead of the guardians' addresses\nbeing stored directly on chain, a hash of the list of addresses can be\nstored on chain, and the wallet owner would only need to publish the\nfull list at recovery time. Second, each guardian can be asked to\ndeterministically generate a new single-purpose address that they would\nuse just for that particular recovery; they would not need to actually\nsend any transactions with that address unless a recovery is actually\nrequired. To complement these technical protections, it's\nrecommended to choose a diverse collection of guardians from different\nsocial circles (including ideally one institutional guardian);\nthese recommendations together would make it extremely difficult for the\nguardians to be attacked simultaneously or collude.\n\nIn the event that you die or are permanently incapacitated, it would\nbe a socially agreed standard protocol that guardians can publicly\nannounce themselves, so in that case they can find each other and\nrecover your funds.\nSocial\nrecovery wallets are not a betrayal, but rather an expression,\nof \"crypto values\"\n\nOne common response to suggestions to use any form of multisig,\nsocial recovery or otherwise, is the idea that this solution goes back\nto \"trusting people\", and so is a betrayal of the values of the\nblockchain and cryptocurrency industry. While I understand why one may\nthink this at first glance, I would argue that this criticism stems from\na fundamental misunderstanding of what crypto should be about.\n\nTo me, the goal of crypto was never to remove the need for\nall trust. Rather, the goal of crypto is to give people\naccess to cryptographic and economic building blocks that give people\nmore choice in whom to trust, and furthermore allow people to\nbuild more constrained forms of trust: giving someone\nthe power to do some things on your behalf without giving them the power\nto do everything. Viewed in this way, multisig and social\nrecovery are a perfect expression of this principle: each\nparticipant has some influence over the ability to accept or\nreject transactions, but no one can move funds unilaterally. This more\ncomplex logic allows for a setup far more secure than what would be\npossible if there had to be one person or key that unilaterally\ncontrolled the funds.\n\nThis fundamental idea, that human inputs should be wielded carefully\nbut not thrown away outright, is powerful because it works well with the\nstrengths and weaknesses of the human brain. The human brain is quite\npoorly suited for remembering passwords and tracking paper wallets, but\nit's an ASIC for keeping track of relationships with other people. This\neffect is even stronger for less technical users: they may have a harder\ntime with wallets and passwords, but they are just as adept at social\ntasks like \"choose 7 people who won't all gang up on me\". If we\ncan extract at least some information from human inputs into a\nmechanism, without those inputs turning into a vector for attack and\nexploitation, then we should figure out how. And social recovery is very\nrobust: for a wallet with 7 guardians to be compromised, 4 of the 7\nguardians would need to somehow discover each other and agree to steal\nthe funds, without any of them tipping the owner off: certainly\na much tougher challenge than attacking a wallet\nprotected purely by a single individuals.\nHow can social\nrecovery protect against theft?\n\nSocial recovery as explained above deals with the risk that you\nlose your wallet. But there is still the risk that your signing\nkey gets stolen: someone hacks into your computer, sneaks up\nbehind you while you're already logged in and hits you over the head, or\neven just uses some user interface glitch to trick you into signing a\ntransaction that you did not intend to sign.\n\nWe can extend social recovery to deal with such issues by adding a vault.\nEvery social recovery wallet can come with an automatically generated\nvault. Assets can be moved to the vault just by sending them to the\nvault's address, but they can be moved out of the vault only with a 1\nweek delay. During that delay, the signing key (or, by extension, the\nguardians) can cancel the transaction. If desired, the vault could also\nbe programmed so that some limited financial operations (eg. Uniswap\ntrades between some whitelisted tokens) can be done without delay.\nExisting social recovery\nwallets\n\nCurrently, the two major wallets that have implemented social\nrecovery are the Argent wallet and\nthe Loopring wallet:\n\nThe Argent wallet is the first major, and still the most popular,\n\"smart contract wallet\" currently in use, and social recovery is one of\nits main selling points. The Argent wallet includes an interface by\nwhich guardians can be added and removed:\n\nTo protect against theft, the wallet has a daily limit: transactions\nup to that amount are instant but transactions above that amount require\nguardians to approve to finalize the withdrawal.\n\nThe Loopring wallet is most known for being built by the developers\nof (and of course including support for) the Loopring protocol, a ZK rollup for payments and\ndecentralized exchange. But the Loopring wallet also has a social\nrecovery feature, which works very similarly to that in Argent. In both\ncases, the wallet companies provide one guardian for free, which relies\non a confirmation code sent by mobile phone to authenticate you. For the\nother guardians, you can add either other users of the same wallet, or\nany Ethereum user by providing their Ethereum address.\n\nThe user experience in both cases is surprisingly smooth. There were\ntwo main challenges. First, the smoothness in both cases relies on a\ncentral \"relayer\" run by the wallet maker that re-publishes signed\nmessages as transactions. Second, the fees are high. Fortunately, both\nof these problems are surmountable.\nMigration\nto Layer 2 (rollups) can solve the remaining challenges\n\nAs mentioned above, there are two key challenges: (i) the\ndependence on relayers to solve transactions, and (ii) high transaction\nfees. The first challenge, dependence on relayers, is an\nincreasingly common problem in Ethereum applications. The issue arises\nbecause there are two types of accounts in Ethereum: externally\nowned accounts (EOAs), which are accounts controlled by a\nsingle private key, and contracts. In Ethereum, there\nis a rule that every transaction must start from an EOA; the original\nintention was that EOAs represent \"users\" and contracts represent\n\"applications\", and an application can only run if a user talks to the\napplication. If we want wallets with more complex policies, like\nmultisig and social recovery, we need to use contracts to represent\nusers. But this poses a challenge: if your funds are in a contract, you\nneed to have some other account that has ETH that can pay to\nstart each transaction, and it needs quite a lot of ETH just in case\ntransaction fees get really high.\n\nArgent and Loopring get around this problem by personally running a\n\"relayer\". The relayer listens for off-chain digitally signed \"messages\"\nsubmitted by users, and wraps these messages in a transaction and\npublishes them to chain. But for the long run, this is a poor solution;\nit adds an extra point of centralization. If the relayer is down and a\nuser really needs to send a transaction, they can always just send it\nfrom their own EOA, but it is nevertheless the case that a new tradeoff\nbetween centralization and inconvenience is introduced. There are\nefforts to solve this problem and get convenience without\ncentralization; the main two categories revolve around either making a\ngeneralized decentralized relayer\nnetwork or modifying the Ethereum protocol itself to allow\ntransactions to begin from contracts. But neither of these solutions\nsolve transaction fees, and in fact, they make the problem worse due to\nsmart contracts' inherently greater complexity.\n\nFortunately, we can solve both of these problems at\nthe same time, by looking toward a third solution: moving the ecosystem\nonto layer 2 protocols\nsuch as optimistic rollups and ZK rollups. Optimistic and ZK\nrollups can both be designed with account abstraction built in,\ncircumventing any need for relayers. Existing wallet developers are\nalready looking into rollups, but ultimately migrating to rollups en\nmasse is an ecosystem-wide challenge.\n\nAn ecosystem-wide mass migration to rollups is as good an opportunity\nas any to reverse the Ethereum ecosystem's earlier mistakes and give\nmultisig and smart contract wallets a much more central role in helping\nto secure users' funds. But this requires broader recognition that\nwallet security is a challenge, and that we have not gone nearly as far\nin trying to meet and challenge as we should. Multisig and social\nrecovery need not be the end of the story; there may well be designs\nthat work even better. But the simple reform of moving to rollups and\nmaking sure that these rollups treat smart contract wallets as first\nclass citizens is an important step toward making that happen.",
    "contentLength": 22085,
    "summary": "Social recovery wallets offer better security than hardware wallets/multisig by using trusted contacts for backup instead of single devices.",
    "detailedSummary": {
      "theme": "Vitalik advocates for widespread adoption of social recovery wallets as a superior solution to current cryptocurrency wallet security problems, combining cryptographic security with human social networks.",
      "summary": "Vitalik argues that cryptocurrency wallet security remains a critical unsolved problem, with existing solutions like hardware wallets and mnemonic phrases having significant flaws including single points of failure and poor usability. He proposes social recovery wallets as the optimal solution, which combine a single signing key for normal transactions with a set of guardians (friends, family, or institutions) who can collectively change the signing key if it's lost. This approach eliminates single points of failure while maintaining ease of use for daily transactions. Vitalik addresses the criticism that social recovery betrays crypto values by arguing that crypto's goal was never to eliminate all trust, but rather to provide more constrained and flexible forms of trust. He concludes that migration to Layer 2 rollups presents an opportunity to make social recovery wallets first-class citizens in the ecosystem, solving current challenges around relayer dependence and high transaction fees.",
      "takeaways": [
        "Current wallet security solutions like hardware wallets and mnemonic phrases have critical flaws including single points of failure and poor user experience",
        "Social recovery wallets use a combination of a signing key for daily use and multiple guardians who can recover the wallet, eliminating single points of failure",
        "The human brain is well-suited for managing social relationships but poorly suited for remembering passwords, making social recovery a natural fit for human capabilities",
        "Social recovery doesn't betray crypto values but actually expresses them by providing more constrained and flexible forms of trust rather than eliminating trust entirely",
        "Migration to Layer 2 rollups with built-in account abstraction can solve the remaining technical challenges of social recovery wallets including relayer dependence and high fees"
      ],
      "controversial": [
        "The claim that relying on social networks for wallet security doesn't contradict cryptocurrency's decentralization principles may be debated by crypto purists who prefer trustless solutions",
        "The assertion that hardware wallets are insufficient despite being widely considered best practice could be controversial among security-focused users"
      ]
    }
  },
  {
    "id": "general-2021-01-05-rollup",
    "title": "An Incomplete Guide to Rollups",
    "date": "2021-01-05",
    "category": "protocol",
    "url": "https://vitalik.eth.limo/general/2021/01/05/rollup.html",
    "path": "general/2021/01/05/rollup.html",
    "content": "- \n\n- \n\n  \n    \n\n  \n  \n  Dark Mode Toggle\n  \n    \n\n  \n\n##  An Incomplete Guide to Rollups \n\n 2021 Jan 05 \nSee all posts\n\n \n \n\n An Incomplete Guide to Rollups \n\nRollups are all the rage in the Ethereum community, and are poised\nto be the key scalability solution for Ethereum for the foreseeable\nfuture. But what exactly is this technology, what can you expect from it\nand how will you be able to use it? This post will attempt to answer\nsome of those key questions.\nBackground: what\nis layer-1 and layer-2 scaling?\n\nThere are two ways to scale a blockchain ecosystem. First,\nyou can make the blockchain itself have a higher transaction\ncapacity. The main challenge with this technique is that\nblockchains with \"bigger blocks\" are inherently more difficult to verify\nand likely to become more centralized. To avoid such risks, developers\ncan either increase the efficiency of client software or, more\nsustainably, use techniques such as sharding to\nallow the work of building and verifying the chain to be split up across\nmany nodes; the effort known as\n\"eth2\" is currently building this upgrade to Ethereum.\n\nSecond, you can change the way that you use the\nblockchain. Instead of putting all activity on the\nblockchain directly, users perform the bulk of their activity off-chain\nin a \"layer 2\" protocol. There is a smart contract on-chain, which only\nhas two tasks: processing deposits and withdrawals, and verifying proofs\nthat everything happening off-chain is following the rules. There are\nmultiple ways to do these proofs, but they all share the property that\nverifying the proofs on-chain is much cheaper than doing the original\ncomputation off-chain.\nState channels vs plasma vs\nrollups\n\nThe three major types of layer-2 scaling are state channels, Plasma and rollups. They are three\ndifferent paradigms, with different strengths and weaknesses, and at\nthis point we are fairly confident that all layer-2 scaling falls into\nroughly these three categories (though naming controversies exist at the\nedges, eg. see \"validium\").\n\n## How do channels work?\n\nSee also: https://www.jeffcoleman.ca/state-channels\nand statechannels.org\n\nImagine that Alice is offering an internet connection to Bob, in\nexchange for Bob paying her $0.001 per megabyte. Instead of making a\ntransaction for each payment, Alice and Bob use the following layer-2\nscheme.\n\nFirst, Bob puts $1 (or some ETH or stablecoin equivalent) into a\nsmart contract. To make his first payment to Alice, Bob signs a \"ticket\"\n(an off-chain message), that simply says \"$0.001\", and sends it to\nAlice. To make his second payment, Bob would sign another ticket that\nsays \"$0.002\", and send it to Alice. And so on and so forth for as many\npayments as needed. When Alice and Bob are done transacting, Alice can\npublish the highest-value ticket to chain, wrapped in another signature\nfrom herself. The smart contract verifies Alice and Bob's signatures,\npays Alice the amount on Bob's ticket and returns the rest to Bob. If\nAlice is unwilling to close the channel (due to malice or technical\nfailure), Bob can initiate a withdrawal period (eg. 7 days); if Alice\ndoes not provide a ticket within that time, then Bob gets all his money\nback.\n\nThis technique is powerful: it can be adjusted to handle\nbidirectional payments, smart contract relationships (eg. Alice and Bob\nmaking a financial contract inside the channel), and composition (if\nAlice and Bob have an open channel and so do Bob and Charlie, Alice can\ntrustlessly interact with Charlie). But there are limits to what\nchannels can do. Channels cannot be used to send funds off-chain to\npeople who are not yet participants. Channels cannot be used to\nrepresent objects that do not have a clear logical owner (eg. Uniswap).\nAnd channels, especially if used to do things more complex than simple\nrecurring payments, require a large amount of capital to be locked\nup.\n\n## How does plasma work?\n\nSee also: the original Plasma paper,\nand Plasma\nCash.\n\nTo deposit an asset, a user sends it to the smart contract managing\nthe Plasma chain. The Plasma chain assigns that asset a new unique ID\n(eg. 537). Each Plasma chain has an operator (this could be a\ncentralized actor, or a multisig, or something more complex like PoS or\nDPoS). Every interval (this could be 15 seconds, or an hour, or anything\nin between), the operator generates a \"batch\" consisting of all of the\nPlasma transactions they have received off-chain. They generate a Merkle\ntree, where at each index X in the tree, there is a\ntransaction transferring asset ID X if such a transaction\nexists, and otherwise that leaf is zero. They publish the Merkle root of\nthis tree to chain. They also send the Merkle branch of each index\nX to the current owner of that asset. To withdraw an asset,\na user publishes the Merkle branch of the most recent transaction\nsending the asset to them. The contract starts a challenge period,\nduring which anyone can try to use other Merkle branches to invalidate\nthe exit by proving that either (i) the sender did not own the asset at\nthe time they sent it, or (ii) they sent the asset to someone else at\nsome later point in time. If no one proves that the exit is fraudulent\nfor (eg.) 7 days, the user can withdraw the asset.\n\nPlasma provides stronger properties than channels: you can send\nassets to participants who were never part of the system, and the\ncapital requirements are much lower. But it comes at a cost: channels\nrequire no data whatsoever to go on chain during \"normal operation\", but\nPlasma requires each chain to publish one hash at regular intervals.\nAdditionally, Plasma transfers are not instant: you have to wait for the\ninterval to end and for the block to be published.\n\nAdditionally, Plasma and channels share a key weakness in common: the\ngame theory behind why they are secure relies on the idea that each\nobject controlled by both systems has some logical \"owner\". If that\nowner does not care about their asset, then an \"invalid\" outcome\ninvolving that asset may result. This is okay for many applications, but\nit is a deal breaker for many others (eg. Uniswap). Even systems where\nthe state of an object can be changed without the owner's consent (eg.\naccount-based systems, where you can increase someone's balance\nwithout their consent) do not work well with Plasma. This all means that\na large amount of \"application-specific reasoning\" is required in any\nrealistic plasma or channels deployment, and it is not possible to make\na plasma or channel system that just simulates the full ethereum\nenvironment (or \"the EVM\"). To get around this problem, we get to...\nrollups.\n\n## Rollups\n\nSee also: EthHub\non optimistic rollups and ZK\nrollups.\n\nPlasma and channels are \"full\" layer 2 schemes, in that they try to\nmove both data and computation off-chain. However, fundamental game\ntheory issues around data availability means that it is impossible\nto safely do this for all applications. Plasma and channels get around\nthis by relying on an explicit notion of owners, but this prevents them\nfrom being fully general. Rollups, on the other hand, are a \"hybrid\"\nlayer 2 scheme. Rollups move computation (and state storage)\noff-chain, but keep some data per transaction on-chain. To\nimprove efficiency, they use a whole host of fancy compression tricks to\nreplace data with computation wherever possible. The result is\na system where scalability is still limited by the data bandwidth of the\nunderlying blockchain, but at a very favorable ratio: whereas an\nEthereum base-layer ERC20 token transfer costs ~45000 gas, an ERC20\ntoken transfer in a rollup takes up 16 bytes of on-chain space and costs\nunder 300 gas.\n\nThe fact that data is on-chain is key (note: putting data \"on IPFS\"\ndoes not work, because IPFS does not provide consensus\non whether or not any given piece of data is available; the data\nmust go on a blockchain). Putting data on-chain and having\nconsensus on that fact allows anyone to locally process all the\noperations in the rollup if they wish to, allowing them to detect fraud,\ninitiate withdrawals, or personally start producing transaction batches.\nThe lack of data availability issues means that a malicious or offline\noperator can do even less harm (eg. they cannot cause a 1 week\ndelay), opening up a much larger design space for who has the right to\npublish batches and making rollups vastly easier to reason about. And\nmost importantly, the lack of data availability issues means that there\nis no longer any need to map assets to owners, leading to the key reason\nwhy the Ethereum community is so much more excited about rollups than\nprevious forms of layer 2 scaling: rollups are fully\ngeneral-purpose, and one can even run an EVM inside a rollup, allowing\nexisting Ethereum applications to migrate to rollups with almost no need\nto write any new code.\nOK, so how exactly does a\nrollup work?\n\nThere is a smart contract on-chain which maintains a state\nroot: the Merkle root of the state of the rollup (meaning, the\naccount balances, contract code, etc, that are \"inside\" the rollup).\n\nAnyone can publish a batch, a collection of\ntransactions in a highly compressed form together with the previous\nstate root and the new state root (the Merkle root after\nprocessing the transactions). The contract checks that the previous\nstate root in the batch matches its current state root; if it does, it\nswitches the state root to the new state root.\n\nTo support depositing and withdrawing, we add the ability to have\ntransactions whose input or output is \"outside\" the rollup state. If a\nbatch has inputs from the outside, the transaction submitting the batch\nneeds to also transfer these assets to the rollup contract. If a batch\nhas outputs to the outside, then upon processing the batch the smart\ncontract initiates those withdrawals.\n\nAnd that's it! Except for one major detail: how to do know\nthat the post-state roots in the batches are correct? If\nsomeone can submit a batch with any post-state root with no\nconsequences, they could just transfer all the coins inside the rollup\nto themselves. This question is key because there are two very different\nfamilies of solutions to the problem, and these two families of\nsolutions lead to the two flavors of rollups.\nOptimistic rollups vs ZK\nrollups\n\nThe two types of rollups are:\n\n- Optimistic rollups, which use fraud\nproofs: the rollup contract keeps track of its entire history\nof state roots and the hash of each batch. If anyone discovers that one\nbatch had an incorrect post-state root, they can publish a proof to\nchain, proving that the batch was computed incorrectly. The contract\nverifies the proof, and reverts that batch and all batches after\nit.\n\n- ZK rollups, which use validity\nproofs: every batch includes a cryptographic proof called a\nZK-SNARK (eg. using the PLONK protocol), which proves\nthat the post-state root is the correct result of executing the batch.\nNo matter how large the computation, the proof can be very quickly\nverified on-chain.\n\nThere are complex tradeoffs between the two flavors of rollups:\n\nProperty\nOptimistic rollups\nZK rollups\n\nFixed gas cost per batch\n~40,000 (a lightweight transaction that mainly just\nchanges the value of the state root)\n~500,000 (verification of a ZK-SNARK is quite computationally\nintensive)\n\nWithdrawal period\n~1 week (withdrawals need to be delayed to give time for someone to\npublish a fraud proof and cancel the withdrawal if it is\nfraudulent)\nVery fast (just wait for the next batch)\n\nComplexity of technology\nLow\nHigh (ZK-SNARKs are very new and mathematically complex\ntechnology)\n\nGeneralizability\nEasier (general-purpose EVM rollups are already\nclose to mainnet)\nHarder (ZK-SNARK proving general-purpose EVM execution is much\nharder than proving simple computations, though there are efforts (eg.\nCairo)\nworking to improve on this)\n\nPer-transaction on-chain gas costs\nHigher\nLower (if data in a transaction is only used to\nverify, and not to cause state changes, then this data can be left out,\nwhereas in an optimistic rollup it would need to be published in case it\nneeds to be checked in a fraud proof)\n\nOff-chain computation costs\nLower (though there is more need for many full\nnodes to redo the computation)\nHigher (ZK-SNARK proving especially for general-purpose computation\ncan be expensive, potentially many thousands of times more expensive\nthan running the computation directly)\n\nIn general, my own view is that in the short term, optimistic rollups\nare likely to win out for general-purpose EVM computation and ZK rollups\nare likely to win out for simple payments, exchange and other\napplication-specific use cases, but in the medium to long term ZK\nrollups will win out in all use cases as ZK-SNARK technology\nimproves.\n\n## Anatomy of a fraud proof\n\nThe security of an optimistic rollup depends on the idea that if\nsomeone publishes an invalid batch into the rollup, anyone else\nwho was keeping up with the chain and detected the fraud can publish a\nfraud proof, proving to the contract that that batch is invalid and\nshould be reverted.\n\nA fraud proof claiming that a batch was invalid would contain the\ndata in green: the batch itself (which could be checked against a hash\nstored on chain) and the parts of the Merkle tree needed to prove just\nthe specific accounts that were read and/or modified by the batch. The\nnodes in the tree in yellow can be reconstructed from the nodes in green\nand so do not need to be provided. This data is sufficient to execute\nthe batch and compute the post-state root (note that this is exactly the\nsame as how stateless\nclients verify individual blocks). If the computed post-state root\nand the provided post-state root in the batch are not the same, then the\nbatch is fraudulent.\n\nIt is guaranteed that if a batch was constructed incorrectly, and\nall previous batches were constructed correctly, then it is\npossible to create a fraud proof showing the the batch was constructed\nincorrectly. Note the claim about previous batches: if there was more\nthan one invalid batch published to the rollup, then it is best to try\nto prove the earliest one invalid. It is also, of course, guaranteed\nthat if a batch was constructed correctly, then it is never possible to\ncreate a fraud proof showing that the batch is invalid.\n\n## How does compression work?\n\nA simple Ethereum transaction (to send ETH) takes ~110 bytes. An ETH\ntransfer on a rollup, however, takes only ~12 bytes:\n\nParameter\nEthereum\nRollup\n\nNonce\n~3\n0\n\nGasprice\n~8\n0-0.5\n\nGas\n3\n0-0.5\n\nTo\n21\n4\n\nValue\n~9\n~3\n\nSignature\n~68 (2 + 33 + 33)\n~0.5\n\nFrom\n0 (recovered from sig)\n4\n\nTotal\n~112\n~12\n\nPart of this is simply superior encoding: Ethereum's RLP wastes 1\nbyte per value on the length of each value. But there are also some very\nclever compression tricks that are going on:\n\n- Nonce: the purpose of this parameter is to prevent\nreplays. If the current nonce of an account is 5, the next transaction\nfrom that account must have nonce 5, but once the transaction is\nprocessed the nonce in the account will be incremented to 6 so the\ntransaction cannot be processed again. In the rollup, we can omit the\nnonce entirely, because we just recover the nonce from the pre-state; if\nsomeone tries replaying a transaction with an earlier nonce, the\nsignature would fail to verify, as the signature would be checked\nagainst data that contains the new higher nonce.\n\n- Gasprice: we can allow users to pay with a fixed\nrange of gasprices, eg. a choice of 16 consecutive powers of two.\nAlternatively, we could just have a fixed fee level in each batch, or\neven move gas payment outside the rollup protocol entirely and have\ntransactors pay batch creators for inclusion through a channel.\n\n- Gas: we could similarly restrict the total gas to a\nchoice of consecutive powers of two. Alternatively, we could just have a\ngas limit only at the batch level.\n\n- To: we can replace the 20-byte address with an\nindex (eg. if an address is the 4527th address added to the\ntree, we just use the index 4527 to refer to it. We would add a subtree\nto the state to store the mapping of indices to addresses).\n\n- Value: we can store value in scientific notation.\nIn most cases, transfers only need 1-3 significant digits.\n\n- Signature: we can use BLS\naggregate signatures, which allows many signatures to be aggregated\ninto a single ~32-96 byte (depending on protocol) signature. This\nsignature can then be checked against the entire set of messages and\nsenders in a batch all at once. The ~0.5 in the table represents the\nfact that there is a limit on how many signatures can be combined in an\naggregate that can be verified in a single block, and so large batches\nwould need one signature per ~100 transactions.\n\nOne important compression trick that is specific to ZK rollups is\nthat if a part of a transaction is only used for verification, and is\nnot relevant to computing the state update, then that part can be left\noff-chain. This cannot be done in an optimistic rollup because that data\nwould still need to be included on-chain in case it needs to be later\nchecked in a fraud proof, whereas in a ZK rollup the SNARK proving\ncorrectness of the batch already proves that any data needed for\nverification was provided. An important example of this is\nprivacy-preserving rollups: in an optimistic rollup the ~500 byte\nZK-SNARK used for privacy in each transaction needs to go on chain,\nwhereas in a ZK rollup the ZK-SNARK covering the entire batch already\nleaves no doubt that the \"inner\" ZK-SNARKs are valid.\n\nThese compression tricks are key to the scalability of rollups;\nwithout them, rollups would be perhaps only a ~10x improvement on the\nscalability of the base chain (though there are some specific\ncomputation-heavy applications where even simple rollups are powerful),\nwhereas with compression tricks the scaling factor can go over 100x for\nalmost all applications.\n\n## Who can submit a batch?\n\nThere are a number of schools of thought for who can submit a batch\nin an optimistic or ZK rollup. Generally, everyone agrees that in order\nto be able to submit a batch, a user must put down a large deposit; if\nthat user ever submits a fraudulent batch (eg. with an invalid state\nroot), that deposit would be part burned and part given as a reward to\nthe fraud prover. But beyond that, there are many possibilities:\n\n- Total anarchy: anyone can submit a batch at any\ntime. This is the simplest approach, but it has some important\ndrawbacks. Particularly, there is a risk that multiple participants will\ngenerate and attempt to submit batches in parallel, and only one of\nthose batches can be successfully included. This leads to a large amount\nof wasted effort in generating proofs and/or wasted gas in publishing\nbatches to chain.\n\n- Centralized sequencer: there is a single actor, the\nsequencer, who can submit batches (with an exception\nfor withdrawals: the usual technique is that a user can first submit a\nwithdrawal request, and then if the sequencer does not process that\nwithdrawal in the next batch, then the user can submit a\nsingle-operation batch themselves). This is the most \"efficient\", but it\nis reliant on a central actor for liveness.\n\n- Sequencer auction: an auction is held (eg. every\nday) to determine who has the right to be the sequencer for the next\nday. This technique has the advantage that it raises funds which could\nbe distributed by eg. a DAO controlled by the rollup (see: MEV\nauctions)\n\n- Random selection from PoS set: anyone can deposit\nETH (or perhaps the rollup's own protocol token) into the rollup\ncontract, and the sequencer of each batch is randomly selected from one\nof the depositors, with the probability of being selected being\nproportional to the amount deposited. The main drawback of this\ntechnique is that it leads to large amounts of needless capital\nlockup.\n\n- DPoS voting: there is a single sequencer selected\nwith an auction but if they perform poorly token holders can vote to\nkick them out and hold a new auction to replace them.\n\nSplit batching and\nstate root provision\n\nSome of the rollups being currently developed are using a \"split\nbatch\" paradigm, where the action of submitting a batch of layer-2\ntransactions and the action of submitting a state root are done\nseparately. This has some key advantages:\n\n- You can allow many sequencers in parallel to publish batches in\norder to improve censorship resistance, without worrying that some\nbatches will be invalid because some other batch got included\nfirst.\n\n- If a state root is fraudulent, you don't need to revert the entire\nbatch; you can revert just the state root, and wait for someone to\nprovide a new state root for the same batch. This gives transaction\nsenders a better guarantee that their transactions will not be\nreverted.\n\nSo all in all, there is a fairly complex zoo of techniques that are\ntrying to balance between complicated tradeoffs involving efficiency,\nsimplicity, censorship resistance and other goals. It's still too early\nto say which combination of these ideas works best; time will tell.\nHow much scaling do\nrollups give you?\n\nOn the existing Ethereum chain, the gas limit is 12.5 million, and\neach byte of data in a transaction costs 16 gas. This means that if a\nblock contains nothing but a single batch (we'll say a ZK rollup is\nused, spending 500k gas on proof verification), that batch can have (12\nmillion / 16) = 750,000 bytes of data. As shown above, a rollup for ETH\ntransfers requires only 12 bytes per user operation, meaning that the\nbatch can contain up to 62,500 transactions. At an average block time of\n13 seconds, this\ntranslates to ~4807 TPS (compared to 12.5 million / 21000 / 13 ~= 45 TPS\nfor ETH transfers directly on Ethereum itself).\n\nHere's a chart for some other example use cases:\n\nApplication\nBytes in rollup\nGas cost on layer 1\nMax scalability gain\n\nETH transfer\n12\n21,000\n105x\n\nERC20 transfer\n16 (4 more bytes to specify which token)\n~50,000\n187x\n\nUniswap trade\n~14 (4 bytes sender + 4 bytes recipient + 3 bytes\nvalue + 1 byte max price + 1 byte misc)\n~100,000\n428x\n\nPrivacy-preserving withdrawal (Optimistic rollup)\n296 (4 bytes index of root + 32 bytes nullifier + 4\nbytes recipient + 256 bytes ZK-SNARK proof)\n~380,000\n77x\n\nPrivacy-preserving withdrawal (ZK rollup)\n40 (4 bytes index of root + 32 bytes nullifier + 4\nbytes recipient)\n~380,000\n570x\n\nMax scalability gain is calculated as (L1 gas cost) /\n(bytes in rollup * 16) * 12 million / 12.5 million.\n\nNow, it is worth keeping in mind that these figures are overly\noptimistic for a few reasons. Most importantly, a block would almost\nnever just contain one batch, at the very least because there are and\nwill be multiple rollups. Second, deposits and withdrawals will continue\nto exist. Third, in the short term usage will be low, and so\nfixed costs will dominate. But even with these factors taken into\naccount, scalability gains of over 100x are expected to be the norm.\n\nNow what if we want to go above ~1000-4000 TPS (depending on the\nspecific use case)? Here is where eth2\ndata sharding comes in. The sharding proposal opens up a space of 16\nMB every 12 seconds that can be filled with any data, and the system\nguarantees consensus on the availability of that data. This data space\ncan be used by rollups. This ~1398k bytes per sec is a 23x improvement\non the ~60 kB/sec of the existing Ethereum chain, and in the longer term\nthe data capacity is expected to grow even further. Hence, rollups that\nuse eth2 sharded data can collectively process as much as ~100k TPS, and\neven more in the future.\nWhat\nare some not-yet-fully-solved challenges in rollups?\n\nWhile the basic concept of a rollup is now well-understood, we are\nquite certain that they are fundamentally feasible and secure, and\nmultiple rollups have already been deployed to mainnet, there are still\nmany areas of rollup design that have not been well explored, and quite\na few challenges in fully bringing large parts of the Ethereum ecosystem\nonto rollups to take advantage of their scalability. Some key challenges\ninclude:\n\n- User and ecosystem onboarding - not many\napplications use rollups, rollups are unfamiliar to users, and few\nwallets have started integrating rollups. Merchants and charities do not\nyet accept them for payments.\n\n- Cross-rollup transactions - efficiently moving\nassets and data (eg. oracle outputs) from one rollup into another\nwithout incurring the expense of going through the base layer.\n\n- Auditing incentives - how to maximize the chance\nthat at least one honest node actually will be fully verifying an\noptimistic rollup so they can publish a fraud proof if something goes\nwrong? For small-scale rollups (up to a few hundred TPS) this is not a\nsignificant issue and one can simply rely on altruism, but for\nlarger-scale rollups more explicit reasoning about this is needed.\n\n- Exploring the design space in between plasma and\nrollups - are there techniques that put some\nstate-update-relevant data on chain but not all of it, and is\nthere anything useful that could come out of that?\n\n- Maximizing security of pre-confirmations - many\nrollups provide a notion of \"pre-confirmation\" for faster UX, where the\nsequencer immediately provides a promise that a transaction will be\nincluded in the next batch, and the sequencer's deposit is destroyed if\nthey break their word. But the economy security of this scheme is\nlimited, because of the possibility of making many promises to very many\nactors at the same time. Can this mechanism be improved?\n\n- Improving speed of response to absent sequencers -\nif the sequencer of a rollup suddenly goes offline, it would be valuable\nto recover from that situation maximally quickly and cheaply, either\nquickly and cheaply mass-exiting to a different rollup or replacing the\nsequencer.\n\n- Efficient ZK-VM - generating a ZK-SNARK proof that\ngeneral-purpose EVM code (or some different VM that existing smart\ncontracts can be compiled to) has been executed correctly and has a\ngiven result.\n\n## Conclusions\n\nRollups are a powerful new layer-2 scaling paradigm, and are expected\nto be a cornerstone of Ethereum scaling in the short and medium-term\nfuture (and possibly long-term as well). They have seen a large amount\nof excitement from the Ethereum community because unlike previous\nattempts at layer-2 scaling, they can support general-purpose EVM code,\nallowing existing applications to easily migrate over. They do this by\nmaking a key compromise: not trying to go fully off-chain, but instead\nleaving a small amount of data per transaction on-chain.\n\nThere are many kinds of rollups, and many choices in the design\nspace: one can have an optimistic rollup using fraud proofs, or a ZK\nrollup using validity proofs (aka. ZK-SNARKs). The sequencer (the user\nthat can publish transaction batches to chain) can be either a\ncentralized actor, or a free-for-all, or many other choices in between.\nRollups are still an early-stage technology, and development is\ncontinuing rapidly, but they work and some (notably Loopring, ZKSync and DeversiFi) have already been\nrunning for months. Expect much more exciting work to come out of the\nrollup space in the years to come.",
    "contentLength": 27070,
    "summary": "This blog post explains rollups as Ethereum's key scaling solution that moves computation off-chain while keeping transaction data on-chain.",
    "detailedSummary": {
      "theme": "Rollups are the most promising layer-2 scaling solution for Ethereum, offering significant scalability improvements while maintaining security and general-purpose functionality.",
      "summary": "Vitalik explains that rollups represent a hybrid layer-2 scaling approach that moves computation off-chain while keeping transaction data on-chain, solving fundamental limitations of previous solutions like state channels and Plasma. Unlike these earlier approaches, rollups can support general-purpose EVM code without requiring application-specific reasoning or clear asset ownership models. Vitalik distinguishes between optimistic rollups (using fraud proofs with ~1 week withdrawal delays) and ZK rollups (using validity proofs with faster withdrawals but higher computational complexity), arguing that optimistic rollups will likely dominate general-purpose computing in the short term while ZK rollups will eventually win out as the technology matures. Through clever compression techniques, rollups can achieve 100x+ scalability improvements, potentially reaching ~4,000 TPS on current Ethereum and up to ~100,000 TPS when combined with eth2 data sharding.",
      "takeaways": [
        "Rollups solve the data availability problem that plagued Plasma and channels by keeping transaction data on-chain while moving computation off-chain",
        "Optimistic rollups are better for general-purpose EVM computation in the short term, while ZK rollups excel at simple payments and will likely dominate long-term",
        "Compression techniques are crucial for rollup efficiency, reducing transaction sizes from ~110 bytes to ~12 bytes for ETH transfers",
        "Rollups can achieve 100x+ scalability gains on current Ethereum, with potential for much higher throughput using eth2 data sharding",
        "Key challenges remain including user onboarding, cross-rollup transactions, auditing incentives, and improving ZK-VM efficiency"
      ],
      "controversial": [
        "Vitalik's prediction that ZK rollups will eventually win out in all use cases may be debatable given the ongoing computational overhead and complexity challenges",
        "The emphasis on centralized sequencers as the most 'efficient' approach may concern those prioritizing decentralization over efficiency"
      ]
    }
  },
  {
    "id": "general-2020-12-28-endnotes",
    "title": "Endnotes on 2020: Crypto and Beyond",
    "date": "2020-12-28",
    "category": "general",
    "url": "https://vitalik.eth.limo/general/2020/12/28/endnotes.html",
    "path": "general/2020/12/28/endnotes.html",
    "content": "- \n\n- \n\n  \n    \n\n  \n  \n  Dark Mode Toggle\n  \n    \n\n  \n\n##  Endnotes on 2020: Crypto and Beyond \n\n 2020 Dec 28 \nSee all posts\n\n \n \n\n Endnotes on 2020: Crypto and Beyond \n\nI'm writing this sitting in Singapore, the city in which I've now\nspent nearly half a year of uninterrupted time - an unremarkable\nduration for many, but for myself the longest I've stayed in any one\nplace for nearly a decade. After months of fighting what may perhaps\neven be humanity's first boss-level enemy since 1945, the city itself is\nclose to normal, though the world as a whole and its 7.8 billion\ninhabitants, normally so close by, continue to be so far away. Many\nother parts of the world have done less well and suffered more, though\nthere is now a light at the end of the tunnel, as hopefully rapid deployment of\nvaccines will help humanity as a whole overcome this great\nchallenge.\n\n2020 has been a strange year because of these events, but also\nothers. As life \"away\nfrom keyboard (AFK)\" has gotten much more constrained and\nchallenging, the internet has been supercharged, with consequences both\ngood and bad. Politics around the world has gone in strange directions,\nand I am continually worried by the many political factions that are so\neasily abandoning their most basic principles because they seem to have\ndecided that their (often mutually contradictory) personal causes are\njust too important. And yet at the same time, there are rays of hope\ncoming from unusual corners, with new\ntechnological discoveries\nin transportation,\nmedicine,\nartificial\nintelligence - and, of course, blockchains and cryptography\n- that could open up a new chapter for humanity finally coming to\nfruition.\n\nWhere we started\n\nWhere we're going\n\nAnd so, 2020 is as good a year as any to ponder a key question: how\nshould we re-evaluate our models of the world? What ways of seeing,\nunderstanding and reasoning about the world are going to be more useful\nin the decades to come, and what paths are no longer as valuable? What\npaths did we not see before that were valuable all along? In this post,\nI will give some of my own answers, covering far from everything but\ndigging into a few specifics that seem particularly interesting. It's\nsometimes hard to tell which of these ideas are a recognition of a\nchanging reality, and which are just myself finally seeing what has\nalways been there; often enough it's some of both. The answers to these\nquestions have a deep relevance both to the crypto space that I call\nhome as well as to the wider world.\nThe Changing Role of\nEconomics\n\nEconomics has historically focused on \"goods\" in the form of\nphysical objects: production of food, manufacturing of widgets,\nbuying and selling houses, and the like. Physical objects have some\nparticular properties: they can be transferred, destroyed, bought and\nsold, but not copied. If one person is using a physical object, it's\nusually impractical for another person to use it simultaneously. Many\nobjects are only valuable if \"consumed\" outright. Making ten copies of\nan object requires something close to ten times the resources that it\ntakes to make one (not quite ten\ntimes, but surprisingly close, especially at larger scales).\nBut on the internet, very different rules apply.\nCopying is cheap. I can write an article or a piece of code once, and it\nusually takes quite a bit of effort to write it once, but once that work\nis done, an unlimited number of people can download and enjoy it. Very\nfew things are \"consumable\"; often products are superseded by better\nones, but if that does not happen, something produced today may continue\nto provide value to people until the end of time.\n\nOn the internet, \"public goods\" take center stage.\nCertainly, private goods exist, particularly in the form of individuals'\nscarce attention and time and virtual assets that command that\nattention, but the average interaction is one-to-many, not\none-to-one. Confounding the situation even further, the \"many\" rarely\nmaps easily to our traditional structures for structuring one-to-many\ninteractions, such as companies, cities or countries;. Instead, these\npublic goods are typically public across a widely scattered collection\nof people all around the world. Many online platforms serving wide\ngroups of people need governance, to decide on features, content\nmoderation policies or other challenges important to their user\ncommunity, though there too, the user community rarely maps cleanly to\nanything but itself. How is it fair for the US government to govern\nTwitter, when Twitter is often a platform for public debates between US\npoliticians and representatives of its geopolitical rivals? But clearly,\ngovernance challenges exist - and so we need more creative\nsolutions.\n\nThis is not merely of interest to \"pure\" online services. Though\ngoods in the physical world - food, houses, healthcare, transportation -\ncontinue to be as important as ever, improvements in these\ngoods depend even more than before on technology, and technological\nprogress does happen over the internet.\n\n Examples of important public goods in the Ethereum\necosystem that were funded by the\nrecent Gitcoin quadratic\nfunding round. Open source software ecosystems, including\nblockchains, are hugely dependent on public goods.\n\nBut also, economics itself seems to be a less powerful tool\nin dealing with these issues. Out of all the challenges of\n2020, how many can be understood by looking at supply and demand curves?\nOne way to see what is going on here is by looking at the relationship\nbetween economics and politics. In the 19th century,\nthe two were frequently viewed as being tied together, a subject called\n\"political\neconomy\". In the 20th century, the two are more typically split\napart. But in the 21st century, the lines between \"private\" and \"public\"\nare once again rapidly blurring. Governments are behaving\nmore like market actors, and corporations are behaving\nmore like governments.\n\nWe see this merge happening in the crypto space as well, as the\nresearchers' eye of attention is increasingly switching focus to the\nchallenge of governance. Five years ago, the main economic topics being\nconsidered in the crypto space had to do with consensus theory. This is\na tractable economics problem with clear goals, and so we would on\nseveral occasions obtain nice clean results like the\nselfish mining paper. Some points of subjectivity, like quantifying\ndecentralization, exist, but they could be easily encapsulated and\ntreated separately from the formal math of the mechanism design. But in\nthe last few years, we have seen the rise of increasingly complicated\nfinancial protocols and DAOs on top of blockchains, and at the same time\ngovernance challenges within blockchains. Should Bitcoin Cash\nredirect\n12.5% of its block reward toward paying a developer team? If so, who\ndecides who that developer team is? Should Zcash extend\nits 20% developer reward for another four years? These problems\ncertainly can be analyzed economically to some extent, but the analysis\ninevitably gets stuck at concepts like coordination, flipping between equilibria,\n\"Schelling points\" and \"legitimacy\", that are much more difficult to\nexpress with numbers. And so, a hybrid discipline, combining formal\nmathematical reasoning with the softer style of humanistic reasoning, is\nrequired.\nWe\nwanted digital nations, instead we got digital nationalism\n\nOne of the most fascinating things that I noticed fairly early in the\ncrypto space starting from around 2014 is just how quickly it started\nreplicating the political patterns of the world at large. I don't mean\nthis just in some broad abstract sense of \"people are forming tribes and\nattacking each other\", I mean similarities that are surprisingly deep\nand specific.\n\nFirst, the story. From 2009 to about 2013, the\nBitcoin world was a relatively innocent happy place. The community was\nrapidly growing, prices were rising, and disagreements over block size\nor long-term direction, while present, were largely academic and took up\nlittle attention compared to the shared broader goal of helping Bitcoin\ngrow and prosper.\n\nBut in 2014, the schisms started to arise. Transaction volumes on the Bitcoin\nblockchain hit 250 kilobytes per block and kept rising, for the\nfirst time raising fears that blockchain usage might actually hit the 1\nMB limit before the limit could be increased. Non-Bitcoin blockchains,\nup until this point a minor sideshow, suddenly became a major part of\nthe space, with Ethereum itself arguably leading the charge. And it was\nduring these events that disagreements that were before politely hidden\nbeneath the surface suddenly blew up. \"Bitcoin maximalism\", the idea\nthat the goal of the crypto space should not be a diverse ecosystem of\ncryptocurrencies generally but Bitcoin and Bitcoin alone specifically,\ngrew from a\nniche curiosity into a prominent and angry movement that Dominic\nWilliams and I quickly saw for what it is and gave its\ncurrent name. The small block ideology, arguing that the block size\nshould be increased very slowly or even never increased at all\nregardless of how high transaction fees go, began to take root.\n\nThe disagreements within Bitcoin would soon turn into an all-out\ncivil war. Theymos, the operator of the /r/bitcoin subreddit and several\nother key public Bitcoin discussion spaces, resorted to extreme\ncensorship to impose his (small-block-leaning) views on the\ncommunity. In response, the big-blockers moved to a new subreddit, /r/btc. Some valiantly attempted to\ndefuse tensions with diplomatic conferences including a\nfamous one in Hong Kong, and a seeming consensus was reached, though\none year later the small block side would end up reneging on its part of\nthe deal. By 2017, the big block faction was firmly on its way to\ndefeat, and in August of that year they would secede (or \"fork off\") to\nimplement their own vision on their own separate continuation of the\nBitcoin blockchain, which they called \"Bitcoin Cash\" (symbol BCH).\n\nThe community split was chaotic, and one can see this in how the\nchannels of communication were split up in the divorce: /r/bitcoin stayed under the\ncontrol of supporters of Bitcoin (BTC). /r/btc was controlled by supporters\nof Bitcoin Cash (BCH). Bitcoin.org was\ncontrolled by supporters of Bitcoin (BTC). Bitcoin.com on the other hand was\ncontrolled by supporters of Bitcoin Cash (BCH). Each side claimed\nthemselves to be the true Bitcoin. The result looked remarkably similar\nto one of those civil wars that happens from time to time that results\nin a country splitting in half, the two halves calling themselves almost\nidentical names that differ only in which subset of the words\n\"democratic\", \"people's\" and \"republic\" appears on each side. Neither\nside had the ability to destroy the other, and of course there was no\nhigher authority to adjudicate the dispute.\n\n Major Bitcoin forks, as of 2020. Does not include\nBitcoin Diamond, Bitcoin Rhodium, Bitcoin Private, or any of the other\nlong list of Bitcoin forks that I would highly recommend you just ignore\ncompletely, except to sell (and perhaps you should sell some of the\nforks listed above too, eg.\nBSV is definitely a\nscam!)\n\nAround the same time, Ethereum had its own chaotic\nsplit, in the form of the\nDAO fork, a highly controversial resolution to a theft in which over\n$50 million was stolen from the first major smart contract application\non Ethereum. Just like in the Bitcoin case, there was first a civil war\n- though only lasting four weeks - and then a chain split, followed by\nan online war between the two now-separate chains, Ethereum (ETH) and\nEthereum Classic (ETC). The naming split was as fun as in Bitcoin: the\nEthereum Foundation held ethereumproject on\nTwitter but Ethereum Classic supporters held ethereumproject on\nGithub.\n\nSome on the Ethereum side would argue that Ethereum Classic had very\nfew \"real\" supporters, and the whole thing was mostly a social attack by\nBitcoin supporters: either to support the version of Ethereum that\naligned with their values, or to cause chaos and destroy Ethereum\noutright. I myself believed these claims somewhat at the beginning,\nthough over time I came to realize that they were overhyped. It is true\nthat some Bitcoin supporters had certainly tried to shape the outcome in\ntheir own image. But to a large extent, as is the case in many\nconflicts, the \"foreign interference\" card was simply a psychological\ndefense that many Ethereum supporters, myself included, subconsciously\nused to shield ourselves from the fact that many people within our own\ncommunity really did have different values. Fortunately relations\nbetween the two currencies have since improved - in part thanks to the\nexcellent diplomatic skills of Virgil\nGriffith - and Ethereum Classic developers have even agreed to move\nto a different Github page.\n\nCivil wars, alliances, blocs,\nalliances with participants in civil wars, you can all find it in\ncrypto. Though fortunately, the conflict is all virtual and online,\nwithout the extremely harmful in-person consequences that often come\nwith such things happening in real life. So what can we learn from all\nthis? One important takeaway is this: if phenomena like this happen in\ncontexts as widely different from each other as conflicts between\ncountries, conflicts between religions and relations within and between\npurely digital cryptocurrencies, then perhaps what we're looking at is\nthe indelible epiphenomena of human nature - something much more\ndifficult to resolve than by changing what kinds of groups we organize\nin. So we should expect situations like this to continue to play out in\nmany contexts over the decades to come. And perhaps it's harder than we\nthought to separate the good that may come out of this from the bad:\nthose same energies that drive us to fight also drive us to\ncontribute.\n\n## What motivates us anyway?\n\nOne of the key intellectual undercurrents of the 2000s era was the\nrecognition of the importance of non-monetary motivations. People are\nmotivated not just by earning as much money as possible in the work and\nextracting enjoyment from their money in their family lives; even at\nwork we are motivated by social status, honor, altruism, reciprocity, a\nfeeling of contribution, different social conceptions of what is good\nand valuable, and much more.\n\nThese differences are very meaningful and measurable. For one\nexample, see this Swiss\nstudy on compensating differentials for immoral work - how much\nextra do employers have to pay to convince someone to do a job if that\njob is considered morally unsavory?\n\nAs we can see, the effects are massive: if a job is widely considered\nimmoral, you need to pay employees almost twice as much for them to be\nwilling to do it. From personal experience, I would even argue that this\nunderstates the case: in many cases, top-quality workers would not be\nwilling to work for a company that they think is bad for the world at\nalmost any price. \"Work\" that is difficult to formalize (eg.\nword-of-mouth marketing) functions similarly: if people think a project\nis good, they will do it for free, if they do not, they will not do it\nat all. This is also likely why blockchain projects that raise a lot of\nmoney but are unscrupulous, or even just corporate-controlled\nprofit-oriented \"VC chains\", tend to fail: even a billion dollars of\ncapital cannot compete with a project having a soul.\n\nThat said, it is possible to be overly idealistic about this fact, in\nseveral ways. First of all, while this decentralized,\nnon-market, non-governmental subsidy toward projects that are socially\nconsidered to be good is massive, likely amounting to tens of trillions\nof dollars per year globally, its effect is not infinite. If a\ndeveloper has a choice between earning $30,000 per year by being\n\"ideologically pure\", and making a $30 million ICO by sticking a\nneedless token into their project, they will do the latter.\nSecond, idealistic motivations are uneven in what they\nmotivate. Rick Falkvinge's Swarmwise\nplayed up the possibility of decentralized non-market organization in\npart by pointing to political activism as a key example. And this is\ntrue, political activism does not require getting paid. But longer and\nmore grueling tasks, even something as simple as making\ngood user interfaces, are not so easily intrinsically motivated. And\nso if you rely on intrinsic motivation too much, you get projects where\nsome tasks are overdone and other tasks are done poorly, or even ignored\nentirely. And third, perceptions of what people find intrinsically\nattractive to work on may change, and may even be manipulated.\n\nOne important conclusion for me from this is the importance of\nculture (and that oh-so-important word that crypto influencers have\nunfortunately ruined for me, \"narrative\"). If a project having\na high moral standing is equivalent to that project having twice as much\nmoney, or even more, then culture and narrative are extremely powerful\nforces that command the equivalent of tens of trillions of dollars of\nvalue. And this does not even begin to cover the role of such concepts\nin shaping our perceptions of legitimacy and coordination. And so\nanything that influences the culture can have a great impact on the\nworld and on people's financial interests, and we're going to see more\nand more sophisticated efforts from all kinds of actors to do so\nsystematically and deliberately. This is the darker conclusion of the\nimportance of non-monetary social motivations - they create the\nbattlefield for the permanent and final frontier of war, the war that is\nfortunately not usually deadly but unfortunately impossible to create\npeace treaties for because of how inextricably subjective it is to\ndetermine what even counts as a battle: the culture war.\nBig X is here to stay, for all\nX\n\nOne of the great debates of the 20th century is that between \"Big\nGovernment\" and \"Big Business\" - with various permutations of each: Big\nBrother, Big Banks, Big Tech, also at times joining the stage. In this\nenvironment, the Great Ideologies were typically defined by trying to\nabolish the Big X that they disliked: communism focusing on\ncorporations, anarcho-capitalism on governments, and so forth. Looking\nback in 2020, one may ask: which of the Great Ideologies succeeded, and\nwhich failed?\n\nLet us zoom into one specific example: the 1996 Declaration of\nIndependence of Cyberspace:\n\nGovernments of the Industrial World, you weary giants of flesh and\nsteel, I come from Cyberspace, the new home of Mind. On behalf of the\nfuture, I ask you of the past to leave us alone. You are not welcome\namong us. You have no sovereignty where we gather.\n\nAnd the similarly-spirited Crypto-Anarchist\nManifesto:\n\nComputer technology is on the verge of providing the ability for\nindividuals and groups to communicate and interact with each other in a\ntotally anonymous manner. Two persons may exchange messages, conduct\nbusiness, and negotiate electronic contracts without ever knowing the\nTrue Name, or legal identity, of the other. Interactions over networks\nwill be untraceable, via extensive re-routing of encrypted packets and\ntamper-proof boxes which implement cryptographic protocols with nearly\nperfect assurance against any tampering. Reputations will be of central\nimportance, far more important in dealings than even the credit ratings\nof today. These developments will alter completely the nature of\ngovernment regulation, the ability to tax and control economic\ninteractions, the ability to keep information secret, and will even\nalter the nature of trust and reputation.\n\nHow have these predictions fared? The answer is interesting: I would\nsay that they succeeded in one part and failed in the other. What\nsucceeded? We have interactions over networks, we have powerful\ncryptography that is difficult for even state actors to break, we even\nhave powerful cryptocurrency, with smart contract capabilities\nthat the thinkers of the 1990s mostly did not even anticipate, and we're\nincreasingly moving toward anonymized reputation systems with\nzero knowledge proofs. What failed? Well, the government did not go\naway. And what just proved to be totally unexpected? Perhaps the most\ninteresting plot twist is that the two forces are, a few exceptions\nnotwithstanding, by and large not acting like mortal enemies,\nand there are even many people within governments that are earnestly\ntrying to find ways to be friendly to blockchains and cryptocurrency and\nnew forms of cryptographic trust.\n\nWhat we see in 2020 is this: Big Government is as powerful as ever,\nbut Big Business is also as powerful as ever. \"Big\nProtest Mob\" is as powerful as ever too, as is Big Tech, and soon\nenough perhaps Big Cryptography. It's a densely populated jungle, with\nan uneasy peace between many complicated actors. If you define success\nas the total absence of a category of powerful actor or even a category\nof activity that you dislike, then you will probably leave the 21st\ncentury disappointed. But if you define success more through what\nhappens than through what doesn't happen, and you are okay with\nimperfect outcomes, there is enough space to make everyone happy.\n\n Often, the boundary between multiple intersecting\nworlds is the most interesting place to be. The monkeys get\nit.\n\nProspering in the dense\njungle\n\nSo we have a world where:\n\n- One-to-one interactions are less important, one-to-many and\nmany-to-many interactions are more important.\n\n- The environment is much more chaotic, and difficult to\nmodel with clean and simple equations. Many-to-many interactions\nparticularly follow strange rules that we still do not understand\nwell.\n\n- The environment is dense, and different categories of\npowerful actors are forced to live quite closely side by side with each\nother.\n\nIn some ways, this is a world that is less convenient for someone\nlike myself. I grew up with a form of economics that is focused on\nanalyzing simpler physical objects and buying and selling, and am now\nforced to contend with a world where such analysis, while not\nirrelevant, is significantly less relevant than before. That said,\ntransitions are always challenging. In fact, transitions are\nparticularly challenging for those who think that they are not\nchallenging because they think that the transition merely confirms what\nthey believed all along. If you are still operating today precisely\naccording to a script that was created in 2009, when the Great Financial\nCrisis was the most recent pivotal event on anyone's mind, then there\nare almost certainly important things that happened in the last decade\nthat you are missing. An ideology that's finished is an ideology that's\ndead.\n\nIt's a world where blockchains and cryptocurrencies are well poised\nto play an important part, though for reasons much more complex than\nmany people think, and having as much to do with cultural forces as\nanything financial (one of the more underrated bull cases for\ncryptocurrency that I have always believed is simply the fact that\ngold is lame, the younger generations realize that it's lame,\nand that $9\ntrillion has to go somewhere). Similarly complex forces are\nwhat will lead to blockchains and cryptocurrencies being\nuseful. It's easy to say that any application can be\ndone more efficiently with a centralized service, but in practice social\ncoordination problems are very real, and unwillingness to sign onto a\nsystem that has even a perception of non-neutrality or ongoing\ndependence on a third party is real too. And so the centralized and even\nconsortium-based approaches claiming to replace blockchains don't get\nanywhere, while \"dumb and inefficient\" public-blockchain-based solutions\njust keep quietly moving forward and gaining actual adoption.\n\nAnd finally it's a very multidisciplinary world, one that is much\nharder to break up into layers and analyze each layer separately. You\nmay need to switch from one style of analysis to another style of\nanalysis in mid-sentence. Things happen for strange and inscrutable\nreasons, and there are always surprises. The question that remains is:\nhow do we adapt to it?",
    "contentLength": 24055,
    "summary": "Vitalik reflects on 2020 showing how internet economics center on public goods rather than private goods, reshaping crypto governance.",
    "detailedSummary": {
      "theme": "Vitalik reflects on how 2020 revealed fundamental shifts in economics, politics, and technology that require new multidisciplinary approaches to understanding our increasingly complex digital world.",
      "summary": "Writing from Singapore during the pandemic, Vitalik examines how 2020 exposed the limitations of traditional economic models in an internet-dominated world where public goods and one-to-many interactions have become central. Vitalik argues that the internet economy operates by fundamentally different rules than physical economies - copying is cheap, products aren't consumed, and public goods serve scattered global communities that don't map to traditional governance structures. This has blurred the lines between economics and politics, forcing crypto researchers to grapple with messy concepts like legitimacy and coordination rather than clean mathematical models. Vitalik draws fascinating parallels between cryptocurrency conflicts (like the Bitcoin/Bitcoin Cash split and Ethereum's DAO fork) and real-world civil wars, suggesting these patterns reflect unchangeable aspects of human nature rather than flaws in specific organizational structures. Vitalik concludes that we now live in a 'dense jungle' where multiple powerful actors (governments, corporations, tech platforms, crypto networks) must coexist, requiring us to abandon idealistic dreams of eliminating the 'Big X' we dislike and instead focus on navigating complexity through multidisciplinary thinking that combines formal analysis with humanistic reasoning.",
      "takeaways": [
        "Traditional economics focused on physical goods is less relevant in an internet economy dominated by public goods and one-to-many interactions",
        "Cryptocurrency communities replicate real-world political patterns, including civil wars and splits, suggesting these conflicts reflect fundamental human nature rather than fixable organizational problems",
        "Non-monetary motivations like moral purpose are incredibly powerful - projects perceived as good can effectively have twice the resources of morally questionable ones",
        "Rather than any single 'Big X' (government, business, tech) disappearing, we're seeing a dense ecosystem where multiple powerful actors must coexist",
        "Success in this complex world requires multidisciplinary thinking that combines mathematical analysis with cultural understanding and humanistic reasoning"
      ],
      "controversial": [
        "Vitalik's suggestion that Bitcoin maximalists and small-block ideology represent 'angry movements' that damaged Bitcoin's development",
        "The claim that projects with moral purpose can be equivalent to having 'twice as much money' may overstate the power of idealistic motivations",
        "Vitalik's assertion that gold is 'lame' and younger generations recognize this, driving them toward cryptocurrency instead"
      ]
    }
  },
  {
    "id": "general-2020-11-08-concave",
    "title": "Convex and Concave Dispositions",
    "date": "2020-11-08",
    "category": "society",
    "url": "https://vitalik.eth.limo/general/2020/11/08/concave.html",
    "path": "general/2020/11/08/concave.html",
    "content": "- \n\n- \n\n  \n    \n\n  \n  \n  Dark Mode Toggle\n  \n    \n\n  \n\n##  Convex and Concave Dispositions \n\n 2020 Nov 08 \nSee all posts\n\n \n \n\n Convex and Concave Dispositions \n\nOne of the major philosophical differences that I have noticed in how\npeople approach making large-scale decisions in the world is how they\napproach the age-old tradeoff of compromise versus purity. Given a\nchoice between two alternatives, often both expressed as deep principled\nphilosophies, do you naturally gravitate toward the idea that one of the\ntwo paths should be correct and we should stick to it, or do you prefer\nto find a way in the middle between the two extremes?\n\nIn mathematical terms, we can rephrase this as follows: do you expect\nthe world that we are living in, and in particular the way that it\nresponds to the actions that we take, to fundamentally be concave\nor convex?\n\nSomeone with a concave disposition might say things like this:\n\n- \"Going to the extremes has never been good for us; you can die from\nbeing too hot or too cold. We need to find the balance between the two\nthat's just right\"\n\n- \"If you implement only a little bit of a philosophy, you can pick\nthe parts that have the highest benefits and the lowest risks, and avoid\nthe parts that are more risky. But if you insist on going to the\nextremes, once you've picked the low-hanging fruit, you'll be forced to\nlook harder and harder for smaller and smaller benefits, and before you\nknow it the growing risks might outweigh the benefit of the whole\nthing\"\n\n- \"The opposing philosophy probably has some value too, so we should\ntry to combine the best parts of both, and definitely avoid doing things\nthat the opposing philosophy considers to be extremely\nterrible, just in case\"\n\nSomeone with a convex disposition might say things like this:\n\n- \"We need to focus. Otherwise, we risk becoming a jack of all trades,\nmaster of none\"\n\n- \"If we take even a few steps down that road, it will become slippery\nslope and only pull us down ever further until we end up in the abyss.\nThere's only two stable positions on the slope: either we're down there,\nor we stay up here\"\n\n- \"If you give an inch, they will take a mile\"\n\n- \"Whether we're following this philosophy or that philosophy, we\nshould be following some philosophy and just stick to it.\nMaking a wishy-washy mix of everything doesn't make sense\"\n\nI personally find myself perenially more sympathetic to the concave\napproach than the convex approach, across a wide variety of contexts. If\nI had to choose either (i) a coin-flip between anarcho-capitalism and\nSoviet communism or (ii) a 50/50 compromise between the two, I would\npick the latter in a heartbeat. I argued\nfor moderation in Bitcoin block size debates, arguing against both\n1-2 MB small blocks and\n128 MB \"very big blocks\". I've argued\nagainst the idea that freedom and decentralization are \"you either\nhave it or you don't\" properties with no middle ground. I argued\nin favor\nof the DAO fork, but to many people's surprise I've argued since then\nagainst similar \"state-intervention\" hard forks that were proposed more\nrecently. As I said in\n2019, \"support for Szabo's law [blockchain immutability] is a\nspectrum, not a binary\".\n\nBut as you can probably tell by the fact that I needed to make those\nstatements at all, not everyone seems to share the same broad intuition.\nI would particularly argue that the Ethereum ecosystem in\ngeneral has a fundamentally concave temperament, while the Bitcoin\necosystem's temperament is much more fundamentally convex. In\nBitcoin land, you can frequently hear arguments that, for example, either\nyou have self-sovereignty or you don't, or that any system must have\neither a fundamentally\ncentralizing or a fundamentally decentralizing tendency, with no\npossibility halfway in between.\n\nThe occasional half-joking support\nfor Tron is a key example: from my own concave point of view, if you\nvalue decentralization and immutability, you should recognize that while\nthe Ethereum ecosystem does sometimes violate purist conceptions of\nthese values, Tron violates them far more egregiously and without\nremorse, and so Ethereum is still by far the more palatable of the two\noptions. But from a convex point of view, the extremeness of Tron's\nviolations of these norms is a virtue: while Ethereum half-heartedly\npretends to be decentralized, Tron is centralized but at least it's\nproud and honest about it.\n\nThis difference between concave and convex mindsets is not at all\nlimited to arcane points about efficiency/decentralization tradeoffs in\ncryptocurrencies. It applies to politics (guess which side has more\noutright anarcho-capitalists), other choices in technology, and even\nwhat food you eat.\n\nBut in all of these questions too, I personally find myself fairly\nconsistently coming out on the side of balance.\nBeing concave about\nconcavity\n\nBut it's worth noting that even on the\nmeta-level, concave temperament is something that one must take\ngreat care to avoid being extreme about. There are certainly situations\nwhere policy A gives a good result, policy B gives a worse but still\ntolerable result, but a half-hearted mix between the two is worst of\nall. The coronavirus is perhaps an excellent example: a 100% effective\ntravel ban is far\nmore than twice as useful as a 50% effective travel ban. An\neffective lockdown that pushes the R0 of the virus down below 1 can\neradicate the virus, leading to a quick recovery, but a half-hearted\nlockdown that only pushes the R0 down to 1.3 leads to months of agony\nwith little to show for it. This is one possible explanation for why\nmany Western countries responded poorly to it: political systems\ndesigned for compromise risk falling into middle approaches even when\nthey are not effective.\n\nAnother example is a war: if you invade country A, you conquer\ncountry A, if you invade country B, you conquer country B, but if you\ninvade both at the same time sending half your soldiers to each one, the\npower of the two combined will crush you. In general, problems where the\neffect of a response is convex are often places where you can find\nbenefits of some degree of centralization.\n\nBut there are also many places where a mix is clearly better than\neither extreme. A common example is the question of setting tax rates.\nIn economics there is the general principle that deadweight loss is\nquadratic: that is, the harms from the inefficiency of a tax are\nproportional to the square of the tax rate. The reason why this\nis the case can be seen as follows. A tax rate of 2% deters very few\ntransactions, and even the transactions it deters are not very valuable\n- how valuable can a transaction be if a mere 2% tax is enough to\ndiscourage the participants from making it? A tax rate of 20% would\ndeter perhaps ten times more transactions, but each individual\ntransaction that was deterred is itself ten times more valuable to its\nparticipants than in the 2% case. Hence, a 10x higher tax may cause\n100x higher economic harm. And for this reason, a low tax is generally\nbetter than a coin flip between high tax and no tax.\n\nBy similar economic logic, an outright prohibition on some behavior\nmay cause more than twice as much harm as a tax set high enough to only\ndeter half of people from participating. Replacing existing prohibitions\nwith medium-high punitive taxes (a very concave-temperamental thing to\ndo) could increase efficiency, increase freedom and provide\nvaluable revenue to build public goods or help the impoverished.\n\nAnother example of effects like this in Laffer curve: a\ntax rate of zero raises no revenue, a tax rate of 100% raises no revenue\nbecause no one bothers to work, but some tax rate in the middle raises\nthe most revenue. There are debates about what that revenue-maximizing\nrate is, but in general there's broad agreement that the chart looks\nsomething like this:\n\nIf you had to pick either the average of two proposed tax plans, or a\ncoin-flip between them, it's obvious that the average is usually best.\nAnd taxes are not the only phenomenon that are like this; economics\nstudies a wide array of\n\"diminishing returns\" phenomena which occur everywhere in\nproduction, consumption and many other aspects of regular day-to-day\nbehavior. Finally, a common flip-side of diminishing returns is\naccelerating costs: to give one notable example, if you take standard\neconomic models of utility of money, they directly imply that double\nthe economic inequality can cause four times the harm.\nThe world has more than\none dimension\n\nAnother point of complexity is that in the real world, policies are\nnot just single-dimensional numbers. There are many ways to average\nbetween two different policies, or two different philosophies. One easy\nexample to see this is: suppose that you and your friend want to live\ntogether, but you want to live in Toronto and your friend wants to live\nin New York. How would you compromise between these two options?\n\nWell, you could take the geographic compromise, and enjoy your\npeaceful existence at the arithmetic midpoint between the two lovely\ncities at....\n\nThis\nAssembly of God church about 29km southwest of Ithaca, NY.\n\nOr you could be even more mathematically pure, and take the\nstraight-line midpoint between Toronto and New York without even\nbothering to stay on the Earth's surface. Then, you're still pretty\nclose to that church, but you're six kilometers under it. A different\nway to compromise is spending six months every year in Toronto and six\nmonths in New York - and this may well be an actually reasonable path\nfor some people to take.\n\nThe point is, when the options being presented to you are more\ncomplicated than simple single-dimensional numbers, figuring out\nhow to compromise between the options well, and really take the\nbest parts of both and not the worst parts of both, is an art, and a\nchallenging one.\n\nAnd this is to be expected: \"convex\" and \"concave\" are terms best\nsuited to mathematical functions where the input and the output are both\none-dimensional. The real world is high-dimensional - and as\nmachine-learning researchers have now\nwell established, in high-dimensional environments the most common\nsetting that you can expect to find yourself in is not a universally\nconvex or universally concave one, but rather a saddle point: a\npoint where the local region is convex in some directions but concave in\nother directions.\n\nA\nsaddle point. Convex left-to-right, concave forward-to-backward.\n\nThis is probably the best mathematical explanation for why both of\nthese dispositions are to some extent necessary: the world is not\nentirely convex, but it is not entirely concave either. But the\nexistence of some concave path between any two distant\npositions A and B is very likely, and if you can find that path then you\ncan often find a synthesis between the two positions that is better than\nboth.",
    "contentLength": 10831,
    "summary": "This blog explores \"concave\" vs \"convex\" mindsets: preferring balanced compromises between extremes vs believing pure approaches work best.",
    "detailedSummary": {
      "theme": "Vitalik explores the philosophical difference between convex and concave approaches to decision-making, examining when compromise versus extremism is more effective in various contexts.",
      "summary": "Vitalik presents a framework for understanding how people approach major decisions through mathematical concepts of convexity and concavity. Those with concave dispositions favor compromise and balance, believing that moderate positions often yield better results than extremes, while those with convex dispositions prefer focus and purity, arguing that half-measures lead to suboptimal outcomes. Vitalik identifies himself as having a concave temperament, preferring moderate approaches in contexts ranging from cryptocurrency design to political systems. He illustrates this divide using examples from Bitcoin versus Ethereum communities, where Bitcoin culture tends toward convex thinking while Ethereum embraces more concave approaches. However, Vitalik acknowledges that neither approach is universally correct - some situations like pandemic responses or military strategy require focused, extreme measures (convex), while others like tax policy benefit from balanced approaches due to diminishing returns (concave). He concludes that the real world is high-dimensional like a saddle point, requiring both dispositions depending on the specific direction and context of the decision.",
      "takeaways": [
        "People tend to have either convex dispositions (favoring extreme, focused approaches) or concave dispositions (favoring compromise and balance)",
        "Bitcoin culture exemplifies convex thinking while Ethereum culture tends toward concave approaches",
        "Some situations genuinely require extreme measures (like effective pandemic lockdowns) while others benefit from moderation (like tax policy)",
        "Even concave thinkers should avoid being extreme about moderation - there are times when compromise is the worst option",
        "The real world is high-dimensional like a saddle point, meaning it can be convex in some directions and concave in others, requiring situational judgment"
      ],
      "controversial": [
        "The characterization of Bitcoin community as having a 'convex temperament' versus Ethereum as 'concave' may oversimplify complex communities",
        "The suggestion that replacing prohibitions with 'medium-high punitive taxes' could increase efficiency and freedom touches on contentious policy debates",
        "The preference for a compromise between anarcho-capitalism and Soviet communism over a coin-flip between them represents a specific political philosophy that others might strongly disagree with"
      ]
    }
  },
  {
    "id": "general-2020-11-06-pos2020",
    "title": "Why Proof of Stake (Nov 2020)",
    "date": "2020-11-06",
    "category": "protocol",
    "url": "https://vitalik.eth.limo/general/2020/11/06/pos2020.html",
    "path": "general/2020/11/06/pos2020.html",
    "content": "- \n\n- \n\n  \n    \n\n  \n  \n  Dark Mode Toggle\n  \n    \n\n  \n\n##  Why Proof of Stake (Nov 2020) \n\n 2020 Nov 06 \nSee all posts\n\n \n \n\n Why Proof of Stake (Nov 2020) \n\nThere are three key reasons why PoS is a superior blockchain security\nmechanism compared to PoW.\nPoS offers more\nsecurity for the same cost\n\nThe easiest way to see this is to put proof of stake and proof of\nwork side by side, and look at how much it costs to attack a\nnetwork per $1 per day in block rewards.\n\n## GPU-based proof of work\n\nYou can rent GPUs cheaply, so the cost of attacking the network is\nsimply the cost of renting enough GPU power to outrun the existing\nminers. For every $1 of block rewards, the existing miners should be\nspending close to $1 in costs (if they're spending more, miners will\ndrop out due to being unprofitable, if they're spending less, new miners\ncan join in and take high profits). Hence, attacking the network just\nrequires temporarily spending more than $1 per day, and only for a few\nhours.\n\nTotal cost of attack: ~$0.26 (assuming 6-hour\nattack), potentially reduced to zero as the attacker receives block\nrewards\n\n## ASIC-based proof of work\n\nASICs are a capital cost: you buy an ASIC once and you can expect it\nto be useful for ~2 years before it wears out and/or is obsoleted by\nnewer and better hardware. If a chain gets 51% attacked, the community\nwill likely respond by changing the PoW algorithm and your ASIC will\nlose its value. On average, mining is ~1/3 ongoing costs and ~2/3\ncapital costs (see here\nfor some sources). Hence, per $1 per day in reward, miners will be\nspending ~$0.33 per day on electricity+maintenance and ~$0.67 per day on\ntheir ASIC. Assuming an ASIC lasts ~2 years, that's $486.67 that a miner\nwould need to spend on that quantity of ASIC hardware.\n\nTotal cost of attack: $486.67 (ASICs) + $0.08\n(electricity+maintenance) = $486.75\n\nThat said, it's worth noting that ASICs provide this heightened level\nof security against attacks at a high cost of centralization, as the barriers to entry to\njoining become very high.\n\n## Proof of stake\n\nProof of stake is almost entirely capital costs (the coins being\ndeposited); the only operating costs are the cost of running a node.\nNow, how much capital are people willing to lock up to get $1 per day of\nrewards? Unlike ASICs, deposited coins do not depreciate, and\nwhen you're done staking you get your coins back after a short delay.\nHence, participants should be willing to pay much higher capital costs\nfor the same quantity of rewards.\n\nLet's assume that a ~15% rate of return is enough to motivate people\nto stake (that is the expected eth2 rate of return). Then, $1 per day of\nrewards will attract 6.667 years' worth of returns in deposits, or\n$2433. Hardware and electricity costs of a node are small; a\nthousand-dollar computer can stake for hundreds of thousands of dollars\nin deposits, and ~$100 per months in electricity and internet is\nsufficient for such an amount. But conservatively, we can say these\nongoing costs are ~10% of the total cost of staking, so we only have\n$0.90 per day of rewards that end up corresponding to capital costs, so\nwe do need to cut the above figure by ~10%.\n\nTotal cost of attack: $0.90/day * 6.667 years =\n$2189\n\nIn the long run, this cost is expected to go even higher, as staking\nbecomes more efficient and people become comfortable with lower rates of\nreturn. I personally expect this number to eventually rise to something\nlike $10000.\n\nNote that the only \"cost\" being incurred to get this high level of\nsecurity is just the inconvenience of not being able to move your coins\naround at will while you are staking. It may even be the case that the\npublic knowledge that all these coins are locked up causes the value of\nthe coin to rise, so the total amount of money floating around in the\ncommunity, ready to make productive investments etc, remains the same!\nWhereas in PoW, the \"cost\" of maintaining consensus is real\nelectricity being burned in insanely\nlarge quantities.\nHigher security or lower\ncosts?\n\nNote that there are two ways to use this 5-20x gain in\nsecurity-per-cost. One is to keep block rewards the same but benefit\nfrom increased security. The other is to massively reduce block rewards\n(and hence the \"waste\" of the consensus mechanism) and keep the security\nlevel the same.\n\nEither way is okay. I personally prefer the latter, because as we\nwill see below, in proof of stake even a successful attack is much less\nharmful and much easier to recover from than an attack on proof of\nwork!\nAttacks\nare much easier to recover from in proof of stake\n\nIn a proof of work system, if your chain gets 51% attacked, what do\nyou even do? So far, the only response in practice has been \"wait it out\nuntil the attacker gets bored\". But this misses the possibility of a\nmuch more dangerous kind of attack called a spawn camping\nattack, where the attacker attacks the chain over and over\nagain with the explicit goal of rendering it useless.\n\nIn a GPU-based system, there is no defense, and a\npersistent attacker may quite easily render a chain permanently useless\n(or more realistically, switches to proof of stake or proof of\nauthority). In fact, after the first few days, the attacker's costs may\nbecome very low, as honest miners will drop out since they have no way\nto get rewards while the attack is going on.\n\nIn an ASIC-based system, the community can respond to the\nfirst attack, but continuing the attack from there once again becomes\ntrivial. The community would meet the first attack by\nhard-forking to change the PoW algorithm, thereby \"bricking\" all ASICs\n(the attacker's and honest miners'!). But if the attacker is\nwilling to suffer that initial expense, after that point the situation\nreverts to the GPU case (as there is not enough time to build and\ndistribute ASICs for the new algorithm), and so from there the attacker\ncan cheaply continue the spawn camp inevitably.\n\nIn the PoS case, however, things are much brighter.\nFor certain kinds of 51% attacks (particularly, reverting finalized\nblocks), there is a built-in \"slashing\" mechanism in the proof of stake consensus\nby which a large portion of the attacker's stake (and no one else's\nstake) can get automatically destroyed. For other, harder-to-detect\nattacks (notably, a 51% coalition censoring everyone else), the\ncommunity can coordinate on a minority user-activated soft fork\n(UASF) in which the attacker's funds are once again largely\ndestroyed (in Ethereum, this is done via the \"inactivity leak\nmechanism\"). No explicit \"hard fork to delete coins\" is\nrequired; with the exception of the requirement to coordinate on the\nUASF to select a minority block, everything else is automated and simply\nfollowing the execution of the protocol rules.\n\nHence, attacking the chain the first time will cost the attacker many\nmillions of dollars, and the community will be back on their feet within\ndays. Attacking the chain the second time will still cost the attacker\nmany millions of dollars, as they would need to buy new coins to replace\ntheir old coins that were burned. And the third time will... cost even\nmore millions of dollars. The game is very asymmetric, and not\nin the attacker's favor.\nProof of stake\nis more decentralized than ASICs\n\nGPU-based proof of work is reasonably decentralized; it is not too\nhard to get a GPU. But GPU-based mining largely fails on the \"security\nagainst attacks\" criterion that we mentioned above. ASIC-based mining,\non the other hand, requires millions of dollars of capital to get into\n(and if you buy an ASIC from someone else, most of the time, the\nmanufacturing company gets the far better end of the deal).\n\nThis is also the correct answer to the common \"proof of stake means\nthe rich get richer\" argument: ASIC mining also means the rich\nget richer, and that game is even more tilted in favor of the\nrich. At least in PoS the minimum needed to stake is quite low and\nwithin reach of many regular people.\n\nAdditionally, proof of stake is more censorship\nresistant. GPU mining and ASIC mining are both very easy to\ndetect: they require huge amounts of electricity consumption, expensive\nhardware purchases and large warehouses. PoS staking, on the other hand,\ncan be done on an unassuming laptop and even over a VPN.\nPossible advantages of\nproof of work\n\nThere are two primary genuine advantages of PoW that I see, though I\nsee these advantages as being fairly limited.\nProof\nof stake is more like a \"closed system\", leading to higher wealth\nconcentration over the long term\n\nIn proof of stake, if you have some coin you can stake that coin and\nget more of that coin. In proof of work, you can always earn more coins,\nbut you need some outside resource to do so. Hence, one could argue that\nover the long term, proof of stake coin distributions risk becoming more\nand more concentrated.\n\nThe main response to this that I see is simply that in PoS, the\nrewards in general (and hence validator revenues) will be quite low; in\neth2, we are expecting annual validator rewards to equal ~0.5-2% of the\ntotal ETH supply. And the more validators are staking, the lower\ninterest rates get. Hence, it would likely take over a century for the\nlevel of concentration to double, and on such time scales other\npressures (people wanting to spend their money, distributing their money\nto charity or among their children, etc.) are likely to dominate.\nProof\nof stake requires \"weak subjectivity\", proof of work does not\n\nSee here\nfor the original intro to the concept of \"weak subjectivity\".\nEssentially, the first time a node comes online, and any subsequent time\na node comes online after being offline for a very long duration (ie.\nmultiple months), that node must find some third-party source to\ndetermine the correct head of the chain. This could be their friend, it\ncould be exchanges and block explorer sites, the client developers\nthemselves, or many other actors. PoW does not have this\nrequirement.\n\nHowever, arguably this is a very weak requirement; in fact, users\nneed to trust client developers and/or \"the community\" to about this\nextent already. At the very least, users need to trust someone (usually\nclient developers) to tell them what the protocol is and what any\nupdates to the protocol have been. This is unavoidable in any software\napplication. Hence, the marginal additional trust requirement that PoS\nimposes is still quite low.\n\nBut even if these risks do turn out to be significant, they seem to\nme to be much lower than the immense gains that PoS sytems get from\ntheir far greater efficiency and their better ability to handle and\nrecover from attacks.\n\nSee also: my previous pieces on proof of stake.\n\n- Proof of\nStake FAQ\n\n- A\nProof of Stake Design Philosophy",
    "contentLength": 10709,
    "summary": "PoS provides 5-20x better security-per-cost than PoW (GPU or ASIC mining) due to capital costs vs operating expenses.",
    "detailedSummary": {
      "theme": "Vitalik argues that Proof of Stake is superior to Proof of Work as a blockchain consensus mechanism due to better security economics, easier attack recovery, and greater decentralization.",
      "summary": "Vitalik presents a detailed economic analysis comparing the cost of attacking different consensus mechanisms, demonstrating that Proof of Stake provides significantly higher security per dollar spent compared to both GPU-based and ASIC-based Proof of Work systems. He calculates that attacking a PoS network would cost around $2,189 compared to just $0.26 for GPU-based PoW, while ASIC-based systems offer better security at $486.75 but with high centralization costs. Beyond superior economics, Vitalik argues that PoS systems have built-in recovery mechanisms through slashing and community coordination that make repeated attacks prohibitively expensive, unlike PoW systems which remain vulnerable to persistent 'spawn camping' attacks. He addresses common criticisms of PoS, acknowledging potential issues like wealth concentration and weak subjectivity requirements, but argues these drawbacks are minimal compared to PoS's substantial advantages in efficiency, security, and decentralization.",
      "takeaways": [
        "Proof of Stake provides 5-20x better security per dollar spent compared to Proof of Work systems",
        "PoS networks can automatically destroy attacker funds through slashing mechanisms, making repeated attacks increasingly expensive",
        "ASIC mining creates high barriers to entry and centralization, while PoS staking is accessible to regular users with lower capital requirements",
        "PoS is more censorship resistant since it can be done on regular laptops without massive electricity consumption or specialized hardware",
        "The main criticisms of PoS (wealth concentration and weak subjectivity) are relatively minor compared to its efficiency and security advantages"
      ],
      "controversial": [
        "The claim that PoS is more decentralized than ASIC mining, as critics argue that large token holders still dominate governance",
        "The assertion that wealth concentration in PoS is not problematic due to low staking rewards, which some see as understating the compound effect over time",
        "The argument that weak subjectivity requirements are minimal, as this introduces trust assumptions that some view as compromising blockchain's trustless nature"
      ]
    }
  },
  {
    "id": "general-2020-10-18-round7",
    "title": "Gitcoin Grants Round 7 Retrospective",
    "date": "2020-10-18",
    "category": "governance",
    "url": "https://vitalik.eth.limo/general/2020/10/18/round7.html",
    "path": "general/2020/10/18/round7.html",
    "content": "- \n\n- \n\n  \n    \n\n  \n  \n  Dark Mode Toggle\n  \n    \n\n  \n\n##  Gitcoin Grants Round 7 Retrospective \n\n 2020 Oct 18 \nSee all posts\n\n \n \n\n Gitcoin Grants Round 7 Retrospective \n\nRound 7 of Gitcoin Grants has successfully completed! This round has\nseen an unprecedented growth in interest and contributions, with\n$274,830 in contributions and $450,000 in matched funds distributed\nacross 857 projects.\n\nThe category structure was once again changed; this time was had a\nsplit between \"dapp tech\", \"infrastructure tech\" and \"community\". Here\nare the results:\n\n## Defi joins the matching!\n\nIn this round, we were able to have much higher matching values than\nbefore. This was because the usual matchings, provided by the Ethereum\nFoundation and a few other actors, were supplemented for the first time\nby a high level of participation from various defi projects:\n\nThe matchers were:\n\n- Chainlink, a smart contract oracle\nproject\n\n- Optimism, a layer-2 optimistic\nrollup\n\n- The Ethereum Foundation\n\n- Balancer, a decentralized\nexchange\n\n- Synthetix, a synthetic assets\nplatform\n\n- Yearn, a collateralized-lending\nplatform\n\n- Three Arrows\nCapital, an investment fund\n\n- Defiance Capital,\nanother investment fund\n\n- Future Fund, which is\ntotally not an investment fund! (/s)\n\n- $MEME, a memecoin\n\n- Yam, a defi project\n\n- Some individual contributors: ferretpatrol, bantg, Mariano Conti, Robert Leshner, Eric Conner, 10b576da0\n\nThe projects together contributed a large amount of matching funding,\nsome of which was used this round and some of which is reserved as a\n\"rainy day fund\" for future rounds in case future matchers are less\nforthcoming.\n\nThis is a significant milestone for the ecosystem because it shows\nthat Gitcoin Grants is expanding beyond reliance on a very small number\nof funders, and is moving toward something more sustainable. But it is\nworth exploring, what exactly is driving these matchers to contribute,\nand is it sustainable?\n\nThere are a few possibile motivations that are likely all in play to\nvarious extents:\n\n- People are naturally altruistic to some extent, and this round defi\nprojects got unexpectedly wealthy for the first time due to a rapid rise\nin interest and token prices, and so donating some of that windfall felt\nlike a natural \"good thing to do\"\n\n- Many in the community are critical of defi projects by default,\nviewing them as unproductive casinos that create a negative image of\nwhat Ethereum is supposed to be about. Contributing to public goods is\nan easy way for a defi project to show that they want to be a positive\ncontributor to the ecosystem and make it better\n\n- Even in the absence of such negative perceptions, defi is a\ncompetitive market that is heavily dependent on community support and\nnetwork effects, and so it's very valuable to a project to win friends\nin the ecosystem\n\n- The largest defi projects capture enough of the benefit from these\npublic goods that it's in their own interest to contribute\n\n- There's a high degree of common-ownership between defi projects\n(holders of one token also hold other tokens and hold ETH), and so even\nif it's not strictly in a project's interest to donate a large\namount, token holders of that project push the project to\ncontribute because they as holders benefit from the gains to both that\nproject but also to the other projects whose tokens they hold.\n\nThe remaining question is, of course: how sustainable will these\nincentives be? Are the altruistic and public-relations incentives only\nlarge enough for a one-time burst of donations of this size, or could it\nbecome more sustainable? Could we reliably expect to see, say, $2-3\nmillion per year spent on quadratic funding matching from here on? If\nso, it would be excellent news for public goods funding diversification\nand democratization in the Ethereum ecosystem.\nWhere did the troublemakers\ngo?\n\nOne curious result from the previous round and this round is that the\n\"controversial\" community grant recipients from previous rounds seem to\nhave dropped in prominence on their own. In theory, we should have seen\nthem continue to get support from their supporters with their detractors\nbeing able to do nothing about it. In practice, though, the top media\nrecipients this round appear to be relatively uncontroversial and\nuniversally beloved mainstays of the Ethereum ecosystem. Even the Zero Knowledge Podcast, an\nexcellent podcast but one aimed for a relatively smaller and more highly\ntechnical audience, has received a large contribution this round.\n\nWhat happened? Why did the distribution of media recipients improve\nin quality all on its own? Is the mechanism perhaps more self-correcting\nthan we had thought?\n\n## Overpayment\n\nThis round is the first round where top recipients on all sides\nreceived quite a large amount. On the infrastructure side, the White Hat\nHacking project (basically a fund to donate to samczsun) received a total of $39,258,\nand the Bankless podcast\ngot $47,620. We could ask the question: are the top recipients getting\ntoo much funding?\n\nTo be clear, I do think that it's very improper to try to create a\nmoral norm that public goods contributors should only be earning\nsalaries up to a certain level and should not be able to earn much more\nthan that. People launching coins earn huge windfalls all the time; it\nis completely natural and fair for public goods contributors to also get\nthat possibility (and furthermore, the numbers from this round translate\nto about ~$200,000 per year, which is not even that high).\n\nHowever, one can ask a more limited and pragmatic question: given the\ncurrent reward structure, is putting an extra $1 into the hands of a top\ncontributor less valuable than putting $1 into the hands of one\nof the other very valuable projects that's still underfunded? Turbogeth,\nNethermind, RadicalXChange and many other projects could still do quite\na lot with a marginal dollar. For the first time, the matching amounts\nare high enough that this is actually a significant issue.\n\nEspecially if matching amounts increase even further, is the\necosystem going to be able to correctly allocate funds and avoid\noverfunding projects? Alternatively, if it fails to avoid\nover-concentrating funds, is that all that bad? Perhaps the possibility\nof becoming the center of attention for one round and earning a $500,000\nwindfall will be part of the incentive that motivates independent public\ngoods contributors!\n\nWe don't know; but these are the yet-unknown facts that running the\nexperiment at its new increased scale is for the first time going to\nreveal.\n\n## Let's talk about categories...\n\nThe concept of categories as it is currently implemented in Gitcoin\nGrants is a somewhat strange one. Each category has a fixed total\nmatching amount that is split between projects within that category.\nWhat this mechanism basically says is that the community can be trusted\nto choose between projects within a category, but we need a\nseparate technocratic judgement to judge how the funds are split between\nthe different categories in the first place.\n\nBut it gets more paradoxical from here. In Round 7, a \"collections\" feature\nwas introduced halfway through the round:\n\nIf you click \"Add to Cart\" on a collection, you immediately add\neverything in the collection to your cart. This is strange because this\nmechanism seems to send the exact opposite message: users that\ndon't understand the details well can choose to allocate funds to entire\ncategories, but (unless they manually edit the amounts) they should not\nbe making many active decisions within each category.\n\nWhich is it? Do we trust the radical quadratic fancy democracy to\nallocate within categories but not between them, do we trust it to\nallocate between categories but nudge people away from making\nfine-grained decisions within them, or something else entirely? I\nrecommend that for Round 8 we think harder about the philosophical\nchallenges here and come up with a more principled approach.\n\nOne option would be to have one matching pool and have all\nthe categories just be a voluntary UI layer. Another would be to\nexperiment with even more \"affirmative action\" to bootstrap\nparticular categories: for example, we could split the Community\nmatching into a $25,000 matching pool for each major world region (eg.\nNorth America + Oceania, Latin America, Europe, Africa, Middle East,\nIndia, East + Southeast Asia) to try to give projects in more neglected\nareas a leg up. There are many possibilities here! One hybrid route is\nthat the \"focused\" pools could themselves be quadratic funded\nin the previous round!\n\n## Identity verification\n\nAs collusion, fake accounts and other attacks on Gitcoin Grants have\nbeen recently increasing, Round 7 added an additional verification\noption with the decentralized social-graph-based BrightID, and single-handedly\nboosted the project's userbase by a factor of ten:\n\nThis is good, because along with helping BrightID's growth, it also\nsubjects the project to a trial-by-fire: there's now a large incentive\nto try to create a large number of fake accounts on it! BrightID is\ngoing to face a tough challenge making it reasonably easy for regular\nusers to join but at the same time resist attacks from fake and\nduplicate accounts. I look forward to seeing them try to meet the\nchallenge!\n\n## ZK rollups for scalability\n\nFinally, Round 7 was the first round where Gitcoin Grants\nexperimented with using the ZkSync ZK rollup to decrease fees\nfor payments:\n\nThe main thing to report here is simply that the ZK rollup\nsuccessfully did decrease fees! The user experience worked well. Many\noptimistic and ZK rollup projects are now looking at collaborating\nwith wallets on direct integrations, which should increase the\nusability and security of such techniques further.\n\n## Conclusions\n\nRound 7 has been a pivotal round for Gitcoin Grants. The matching\nfunding has become much more sustainable. The levels of funding are now\nlarge enough to successfully fund quadratic freelancers to the point\nwhere a project getting \"too much funding\" is a conceivable thing to\nworry about! Identity verification is taking steps forward. Payments\nhave become much more efficient with the introduction of the ZkSync ZK rollup. I look forward to\nseeing the grants continue for many more rounds in the future.",
    "contentLength": 10279,
    "summary": "Gitcoin Grants Round 7 achieved record $274,830 contributions + $450,000 matched funds across 857 projects, with DeFi projects joining as major funders.",
    "detailedSummary": {
      "theme": "Vitalik analyzes the successes and challenges of Gitcoin Grants Round 7, which achieved unprecedented funding levels through DeFi project participation but raised new questions about sustainability and fund allocation.",
      "summary": "Vitalik examines Round 7 of Gitcoin Grants, which distributed $450,000 in matched funds across 857 projects with $274,830 in contributions. The most significant development was the participation of major DeFi projects as matchers, including Chainlink, Optimism, Balancer, Synthetix, and others, moving beyond traditional reliance on the Ethereum Foundation. Vitalik explores the motivations behind DeFi participation, ranging from altruism and public relations benefits to genuine ecosystem investment, while questioning the long-term sustainability of this funding model. He notes that controversial grant recipients from previous rounds naturally declined in prominence, suggesting the mechanism may be more self-correcting than expected. However, the increased funding levels created new challenges, with top recipients receiving substantial amounts that raised questions about potential overfunding versus supporting more underfunded projects. Vitalik also critiques the category system's philosophical inconsistencies and highlights positive developments in identity verification through BrightID integration and the successful implementation of ZK rollups for reduced transaction fees.",
      "takeaways": [
        "DeFi projects significantly expanded Gitcoin Grants funding sustainability by contributing matching funds, driven by a mix of altruistic, reputational, and strategic incentives",
        "The mechanism showed self-correcting properties as controversial recipients from previous rounds naturally lost prominence without external intervention",
        "High funding levels created new 'overpayment' concerns, with top recipients receiving substantial amounts that may exceed optimal allocation efficiency",
        "The current category system contains philosophical contradictions about whether to trust community allocation within or between categories",
        "Technical infrastructure improvements through BrightID identity verification and ZkSync rollup integration successfully enhanced security and reduced costs"
      ],
      "controversial": [
        "The suggestion that some public goods contributors may be receiving 'too much' funding, potentially questioning whether popular projects deserve their full quadratic funding allocation",
        "The implication that DeFi projects' motivations for contributing may be primarily self-interested rather than altruistic, driven by reputation management and competitive positioning"
      ]
    }
  },
  {
    "id": "general-2020-09-11-coordination",
    "title": "Coordination, Good and Bad",
    "date": "2020-09-11",
    "category": "governance",
    "url": "https://vitalik.eth.limo/general/2020/09/11/coordination.html",
    "path": "general/2020/09/11/coordination.html",
    "content": "- \n\n- \n\n  \n    \n\n  \n  \n  Dark Mode Toggle\n  \n    \n\n  \n\n##  Coordination, Good and Bad \n\n 2020 Sep 11 \nSee all posts\n\n \n \n\n Coordination, Good and Bad \n\nSpecial thanks to Karl Floersch and Jinglan Wang for feedback and\nreview\n\nSee also:\n\n- On Collusion\n\n- Engineering\nSecurity Through Coordination Problems\n\n- Trust Models\n\n- The\nMeaning Of Decentralization\n\nCoordination, the ability for large groups of actors to work together\nfor their common interest, is one of the most powerful forces in the\nuniverse. It is the difference between a king comfortably ruling a\ncountry as an oppressive dictatorship, and the people coming together\nand overthrowing him. It is the difference between the global\ntemperature going up 3-5'C\nand the temperature going up by a much smaller amount if we work\ntogether to stop it. And it is the factor that makes companies,\ncountries and any social organization larger than a few people possible\nat all.\n\nCoordination can be improved in many ways: faster spread of\ninformation, better norms that identify what behaviors are classified as\ncheating along with more effective punishments, stronger and more\npowerful organizations, tools like smart contracts that allow\ninteractions with reduced levels of trust, governance technologies\n(voting, shares, decision markets...), and much more. And indeed, we as a\nspecies are getting better at all of these things with each passing\ndecade.\n\nBut there is also a very philosophically counterintuitive dark side\nto coordination. While it is emphatically true that \"everyone\ncoordinating with everyone\" leads to much better outcomes than \"every\nman for himself\", what that does NOT imply is that each individual step\ntoward more coordination is necessarily beneficial. If\ncoordination is improved in an unbalanced way, the results can easily be\nharmful.\n\nWe can think about this visually as a map, though in reality the map\nhas many billions of \"dimensions\" rather than two:\n\nThe bottom-left corner, \"every man for himself\", is where we don't\nwant to be. The top-right corner, total coordination, is ideal, but\nlikely unachievable. But the landscape in the middle is far from an even\nslope up, with many reasonably safe and productive places that it might\nbe best to settle down in and many deep dark caves to avoid.\n\nNow what are these dangerous forms of partial coordination, where\nsomeone coordinating with some fellow humans but not\nothers leads to a deep dark hole? It's best to describe them by\ngiving examples:\n\n- Citizens of a nation valiantly sacrificing themselves for the\ngreater good of their country in a war.... when that country turns out to\nbe WW2-era Germany or Japan\n\n- A lobbyist giving a politician a bribe in exchange for that\npolitician adopting the lobbyist's preferred policies\n\n- Someone selling their vote in an election\n\n- All sellers of a product in a market colluding to raise their prices\nat the same time\n\n- Large miners of a blockchain colluding to launch a 51% attack\n\nIn all of the above cases, we see a group of people coming together\nand cooperating with each other, but to the great detriment of some\ngroup that is outside the circle of coordination, and thus to the net\ndetriment of the world as a whole. In the first case, it's all the\npeople that were the victims of the aforementioned nations' aggression\nthat are outside the circle of coordination and suffer heavily as a\nresult; in the second and third cases, it's the people affected by the\ndecisions that the corrupted voter and politician are making, in the\nfourth case it's the customers, and in the fifth case it's the\nnon-participating miners and the blockchain's users. It's not an\nindividual defecting against the group, it's a group defecting against a\nbroader group, often the world as a whole.\n\nThis type of partial coordination is often called \"collusion\", but\nit's important to note that the range of behaviors that we are talking\nabout is quite broad. In normal speech, the word \"collusion\" tends to be\nused more often to describe relatively symmetrical relationships, but in\nthe above cases there are plenty of examples with a strong asymmetric\ncharacter. Even extortionate relationships (\"vote for my\npreferred policies or I'll publicly reveal your affair\") are a form of\ncollusion in this sense. In the rest of this post, we'll use \"collusion\"\nto refer to \"undesired coordination\" generally.\nEvaluate Intentions, Not\nActions (!!)\n\nOne important property of especially the milder cases of collusion is\nthat one cannot determine whether or not an action is part of an\nundesired collusion just by looking at the action itself. The reason is\nthat the actions that a person takes are a combination of that person's\ninternal knowledge, goals and preferences together with externally\nimposed incentives on that person, and so the actions that people take\nwhen colluding, versus the actions that people take on their own\nvolition (or coordinating in benign ways) often overlap.\n\nFor example, consider the case of collusion between sellers (a type\nof antitrust\nviolation). If operating independently, each of three sellers might\nset a price for some product between $5 and $10; the differences within\nthe range reflect difficult-to-see factors such as the seller's internal\ncosts, their own willingness to work at different wages, supply-chain\nissues and the like. But if the sellers collude, they might set a price\nbetween $8 and $13. Once again, the range reflects different\npossibilities regarding internal costs and other difficult-to-see\nfactors. If you see someone selling that product for $8.75, are they\ndoing something wrong? Without knowing whether or not they coordinated\nwith other sellers, you can't tell! Making a law that says that selling\nthat product for more than $8 would be a bad idea; maybe there are\nlegitimate reasons why prices have to be high at the current time. But\nmaking a law against collusion, and successfully enforcing it, gives the\nideal outcome - you get the $8.75 price if the price has to be that high\nto cover sellers' costs, but you don't get that price if the factors\ndriving prices up naturally are low.\n\nThis applies in the bribery and vote selling cases too: it may well\nbe the case that some people vote for the Orange Party legitimately, but\nothers vote for the Orange Party because they were paid to. From the\npoint of view of someone determining the rules for the voting mechanism,\nthey don't know ahead of time whether the Orange Party is good or bad.\nBut what they do know is that a vote where people vote based on\ntheir honest internal feelings works reasonably well, but a vote where\nvoters can freely buy and sell their votes works terribly. This is\nbecause vote selling has a tragedy-of-the-commons: each voter only gains\na small portion of the benefit from voting correctly, but would gain the\nfull bribe if they vote the way the briber wants, and so the required\nbribe to lure each individual voter is far smaller than the bribe that\nwould actually compensate the population for the costs of whatever\npolicy the briber wants. Hence, votes where vote selling is permitted\nquickly collapse into\nplutocracy.\nUnderstanding the Game\nTheory\n\nWe can zoom further out and look at this from the perspective of game\ntheory. In the version of game theory that focuses on individual choice\n- that is, the version that assumes that each participant makes\ndecisions independently and that does not allow for the possibility of\ngroups of agents working as one for their mutual benefit, there are mathematical\nproofs that at least one stable Nash equilibrium must exist in any\ngame. In fact, mechanism designers have a very wide latitude to \"engineer\"\ngames to achieve specific outcomes. But in the version of game\ntheory that allows for the possibility of coalitions working together\n(ie. \"colluding\"), called cooperative game theory, we\ncan prove that there are large classes of games that do not have any\nstable outcome (called a \"core\"). In\nsuch games, whatever the current state of affairs is, there is always\nsome coalition that can profitably deviate from it.\n\nOne important part of that set of inherently unstable games is\nmajority games. A majority game is\nformally described as a game of agents where any subset of more than\nhalf of them can capture a fixed reward and split it among themselves -\na setup eerily similar to many situations in corporate governance,\npolitics and many other situations in human life. That is to say, if\nthere is a situation with some fixed pool of resources and some\ncurrently established mechanism for distributing those resources, and\nit's unavoidably possible for 51% of the participants can conspire to\nseize control of the resources, no matter what the current configuration\nis there is always some conspiracy that can emerge that would be\nprofitable for the participants. However, that conspiracy would then in\nturn be vulnerable to potential new conspiracies, possibly including a\ncombination of previous conspirators and victims... and so on and so\nforth.\n\nRound\n\nA\n\nB\n\nC\n\n1\n\n1/3\n\n1/3\n\n1/3\n\n2\n\n1/2\n\n1/2\n\n0\n\n3\n\n2/3\n\n0\n\n1/3\n\n4\n\n0\n\n1/3\n\n2/3\n\nThis fact, the instability of majority games under\ncooperative game theory, is arguably highly underrated as a simplified\ngeneral mathematical model of why there may well be no \"end of history\"\nin politics and no system that proves fully satisfactory; I personally\nbelieve it's much more useful than the more famous Arrow's\ntheorem, for example.\n\nNote once again that the core dichotomy here is not \"individual\nversus group\"; for a mechanism designer, \"individual versus group\" is\nsurprisingly easy to handle. It's \"group versus broader group\" that\npresents the challenge.\nDecentralization as\nAnti-Collusion\n\nBut there is another, brighter and more actionable, conclusion from\nthis line of thinking: if we want to create mechanisms that are stable,\nthen we know that one important ingredient in doing so is finding ways\nto make it more difficult for collusions, especially large-scale\ncollusions, to happen and to maintain themselves. In the case of voting,\nwe have the secret\nballot - a mechanism that ensures that voters have no way to prove\nto third parties how they voted, even if they want to prove it (MACI is one project trying\nto use cryptography to extend secret-ballot principles to an online\ncontext). This disrupts trust between voters and bribers, heavily\nrestricting undesired collusions that can happen. In that case of\nantitrust and other corporate malfeasance, we often rely on\nwhistleblowers and even give\nthem rewards, explicitly incentivizing participants in a harmful\ncollusion to defect. And in the case of public infrastructure more\nbroadly, we have that oh-so-important concept:\ndecentralization.\n\nOne naive view of why decentralization is valuable is that it's about\nreducing risk from single points of technical failure. In traditional\n\"enterprise\" distributed systems, this is often actually true, but in\nmany other cases we know that this is not sufficient to explain what's\ngoing on. It's instructive here to look at blockchains. A large mining\npool publicly showing how they have internally distributed their nodes\nand network dependencies doesn't do much to calm community members\nscared of mining centralization. And pictures like these, showing 90% of\nBitcoin hashpower at the time being capable of showing up to the same\nconference panel, do quite a bit to scare people:\n\nBut why is this image scary? From a \"decentralization as fault\ntolerance\" view, large miners being able to talk to each other causes no\nharm. But if we look at \"decentralization\" as being the presence of\nbarriers to harmful collusion, then the picture becomes quite scary,\nbecause it shows that those barriers are not nearly as strong as we\nthought. Now, in reality, the barriers are still far from zero; the fact\nthat those miners can easily perform technical coordination and likely\nare all in the same Wechat groups does not, in fact, mean that\nBitcoin is \"in practice little better than a centralized company\".\n\nSo what are the remaining barriers to collusion? Some major ones\ninclude:\n\n- Moral Barriers. In Liars\nand Outliers, Bruce Schneier reminds us that many \"security\nsystems\" (locks on doors, warning signs reminding people of\npunishments...) also serve a moral function, reminding potential\nmisbehavers that they are about to conduct a serious transgression and\nif they want to be a good person they should not do that.\nDecentralization arguably serves that function.\n\n- Internal negotiation failure. The individual\ncompanies may start demanding concessions in exchange for participating\nin the collusion, and this could lead to negotiation stalling outright\n(see \"holdout\nproblems\" in economics).\n\n- Counter-coordination. The fact that a system is\ndecentralized makes it easy for participants not participating in the\ncollusion to make a fork that strips out the colluding attackers and\ncontinue the system from there. Barriers for users to join the fork are\nlow, and the intention of decentralization creates moral\npressure in favor of participating in the fork.\n\n- Risk of defection. It still is much harder for five\ncompanies to join together to do something widely considered to be bad\nthan it is for them to join together for a non-controversial or benign\npurpose. The five companies do not know each other too well, so\nthere is a risk that one of them will refuse to participate and blow the\nwhistle quickly, and the participants have a hard time judging the risk.\nIndividual employees within the companies may blow the whistle too.\n\nTaken together, these barriers are substantial indeed - often\nsubstantial enough to stop potential attacks in their tracks, even when\nthose five companies are simultaneously perfectly capable of quickly\ncoordinating to do something legitimate. Ethereum blockchain miners, for\nexample, are perfectly capable of coordinating increases to the gas\nlimit, but that does not mean that they can so easily collude to\nattack the chain.\n\nThe blockchain experience shows how designing protocols as\ninstitutionally decentralized architectures, even when it's well-known\nahead of time that the bulk of the activity will be dominated by a few\ncompanies, can often be a very valuable thing. This idea is not limited\nto blockchains; it can be applied in other contexts as well (eg. see here\nfor applications to antitrust).\nForking as\nCounter-Coordination\n\nBut we cannot always effectively prevent harmful collusions from\ntaking place. And to handle those cases where a harmful collusion does\ntake place, it would be nice to make systems that are more robust\nagainst them - more expensive for those colluding, and easier to recover\nfor the system.\n\nThere are two core operating principles that we can use to achieve\nthis end: (1) supporting counter-coordination and (2)\nskin-in-the-game. The idea behind counter-coordination\nis this: we know that we cannot design systems to be passively\nrobust to collusions, in large part because there is an extremely large\nnumber of ways to organize a collusion and there is no passive mechanism\nthat can detect them, but what we can do is actively respond to\ncollusions and strike back.\n\nIn digital systems such as blockchains (this could also be applied to\nmore mainstream systems, eg. DNS), a major and crucially important form\nof counter-coordination is forking.\n\nIf a system gets taken over by a harmful coalition, the dissidents\ncan come together and create an alternative version of the system, which\nhas (mostly) the same rules except that it removes the power of the\nattacking coalition to control the system. Forking is very easy in an\nopen-source software context; the main challenge in creating a\nsuccessful fork is usually gathering the legitimacy\n(game-theoretically viewed as a form of \"common\nknowledge\") needed to get all those who disagree with the main\ncoalition's direction to follow along with you.\n\nThis is not just theory; it has been accomplished successfully, most\nnotably in the Steem\ncommunity's rebellion against a hostile takeover attempt, leading to\na new blockchain called Hive in which the original antagonists have no\npower.\n\n## Markets and Skin in the Game\n\nAnother class of collusion-resistance strategy is the idea of\nskin in the game. Skin in the game, in this context,\nbasically means any mechanism that holds individual contributors in a\ndecision individually accountable for their contributions. If a group\nmakes a bad decision, those who approved the decision must suffer more\nthan those who attempted to dissent. This avoids the \"tragedy of the\ncommons\" inherent in voting systems.\n\nForking is a powerful form of counter-coordination precisely because\nit introduces skin in the game. In Hive, the community fork of Steem\nthat threw off the hostile takeover attempt, the coins that were used to\nvote in favor of the hostile takeover were largely deleted in the new\nfork. The key individuals who participated in the attack individually\nsuffered as a result.\n\nMarkets are in general very powerful tools precisely\nbecause they maximize skin in the game. Decision\nmarkets (prediction\nmarkets used to guide decisions; also called futarchy)\nare an attempt to extend this benefit of markets to organizational\ndecision-making. That said, decision markets can only solve some\nproblems; in particular, they cannot tell us what variables we should be\noptimizing for in the first place.\n\n## Structuring Coordination\n\nThis all leads us to an interesting view of what it is that people\nbuilding social systems do. One of the goals of building an\neffective social system is, in large part, determining the structure\nof coordination: which groups of people and in what configurations\ncan come together to further their group goals, and which groups\ncannot?\n\n \n Different coordination structures, different\noutcomes \n\nSometimes, more coordination is good: it's better when people can\nwork together to collectively solve their problems. At other times, more\ncoordination is dangerous: a subset of participants could coordinate to\ndisenfranchise everyone else. And at still other times, more\ncoordination is necessary for another reason: to enable the broader\ncommunity to \"strike back\" against a collusion attacking the system.\n\nIn all three of those cases, there are different mechanisms that can\nbe used to achieve these ends. Of course, it is very difficult to\nprevent communication outright, and it is very difficult to make\ncoordination perfect. But there are many options in between that can\nnevertheless have powerful effects.\n\nHere are a few possible coordination structuring techniques:\n\n- Technologies and norms that protect privacy\n\n- Technological means that make it difficult to prove how you behaved\n(secret ballots, MACI and similar tech)\n\n- Deliberate decentralization, distributing control of some mechanism\nto a wide group of people that are known to not be well-coordinated\n\n- Decentralization in physical space, separating out different\nfunctions (or different shares of the same function) to different\nlocations (eg. see Samo\nBurja on connections between urban decentralization and political\ndecentralization)\n\n- Decentralization between role-based constituencies, separating out\ndifferent functions (or different shares of the same function) to\ndifferent types of participants (eg. in a blockchain: \"core developers\",\n\"miners\", \"coin holders\", \"application developers\", \"users\")\n\n- Schelling\npoints, allowing large groups of people to quickly coordinate around\na single path forward. Complex Schelling points could potentially even\nbe implemented in code (eg. recovery\nfrom 51% attacks can benefit from this).\n\n- Speaking a common language (or alternatively, splitting control\nbetween multiple constituencies who speak different languages)\n\n- Using per-person voting instead of per-(coin/share) voting to\ngreatly increase the number of people who would need to collude to\naffect a decision\n\n- Encouraging and relying on defectors to alert the public about\nupcoming collusions\n\nNone of these strategies are perfect, but they can be used in various\ncontexts with differing levels of success. Additionally, these\ntechniques can and should be combined with mechanism design that\nattempts to make harmful collusions less profitable and more risky to\nthe extent possible; skin in the game is a very powerful tool in this\nregard. Which combination works best ultimately depends on your specific\nuse case.",
    "contentLength": 20415,
    "summary": "The blog explains how coordination can be harmful when groups collude against broader groups, not just beneficial.",
    "detailedSummary": {
      "theme": "Coordination among groups can be beneficial for society overall, but partial coordination (collusion) by subgroups can be harmful, requiring careful design of systems to promote good coordination while preventing bad coordination.",
      "summary": "Vitalik argues that while coordination is one of the most powerful forces enabling human cooperation and progress, not all coordination is beneficial. He distinguishes between good coordination that benefits everyone and bad coordination (collusion) where subgroups work together at the expense of broader society - such as price-fixing cartels, political bribery, or hostile takeovers of blockchain networks. The key insight is that the same actions can be legitimate or harmful depending on the coordination context, making it crucial to evaluate intentions rather than just outcomes. Vitalik explains that game theory shows majority games are inherently unstable when coalitions can form, leading to perpetual power struggles. However, decentralization can serve as an anti-collusion mechanism by creating barriers to harmful coordination through moral constraints, negotiation difficulties, counter-coordination possibilities, and defection risks. He advocates for designing systems that structure coordination appropriately - enabling beneficial cooperation while preventing harmful collusion through techniques like secret ballots, deliberate decentralization, skin-in-the-game mechanisms, and forking capabilities that allow communities to recover from attacks.",
      "takeaways": [
        "Coordination is powerful but dangerous when it creates subgroups that benefit at the expense of the broader community",
        "The same action can be legitimate or harmful depending on whether it results from collusion, making intentions more important than outcomes",
        "Game theory proves that majority games with coalition possibilities are inherently unstable, explaining perpetual political instability",
        "Decentralization serves as an anti-collusion tool by creating barriers to harmful coordination while preserving beneficial cooperation",
        "Effective social systems require careful design to structure coordination - promoting good cooperation while preventing harmful collusion through techniques like secret voting, skin-in-the-game, and counter-coordination mechanisms"
      ],
      "controversial": [
        "The characterization of citizens sacrificing for their country in WW2-era Germany and Japan as an example of harmful coordination may be seen as oversimplifying complex historical and moral contexts",
        "The argument that decentralization primarily serves to prevent collusion rather than technical fault tolerance challenges conventional wisdom about distributed systems"
      ]
    }
  },
  {
    "id": "general-2020-08-20-trust",
    "title": "Trust Models",
    "date": "2020-08-20",
    "category": "math",
    "url": "https://vitalik.eth.limo/general/2020/08/20/trust.html",
    "path": "general/2020/08/20/trust.html",
    "content": "- \n\n- \n\n  \n    \n\n  \n  \n  Dark Mode Toggle\n  \n    \n\n  \n\n##  Trust Models \n\n 2020 Aug 20 \nSee all posts\n\n \n \n\n Trust Models \n\nOne of the most valuable properties of many blockchain applications\nis trustlessness: the ability of the application to continue\noperating in an expected way without needing to rely on a specific actor\nto behave in a specific way even when their interests might change and\npush them to act in some different unexpected way in the future.\nBlockchain applications are never fully trustless, but some\napplications are much closer to being trustless than others. If we want\nto make practical moves toward trust minimization, we want to have the\nability to compare different degrees of trust.\n\nFirst, my simple one-sentence definition of trust: trust is\nthe use of any assumptions about the behavior of other people.\nIf before the pandemic you would walk down the street without making\nsure to keep two meters' distance from strangers so that they could not\nsuddenly take out a knife and stab you, that's a kind of trust: both\ntrust that people are very rarely completely deranged, and trust that\nthe people managing the legal system continue to provide strong\nincentives against that kind of behavior. When you run a piece of code\nwritten by someone else, you trust that they wrote the code honestly\n(whether due to their own sense of decency or due to an economic\ninterest in maintaining their reputations), or at least that there\nexist enough people checking the code that a bug would be found.\nNot growing your own food is another kind of trust: trust that enough\npeople will realize that it's in their interests to grow food\nso they can sell it to you. You can trust different sizes of groups of\npeople, and there are different kinds of trust.\n\nFor the purposes of analyzing blockchain protocols, I tend to break\ndown trust into four dimensions:\n\n- How many people do you need to behave as you expect?\n\n- Out of how many?\n\n- What kinds of motivations are needed for those people to behave? Do\nthey need to be altruistic, or just profit seeking? Do they need to be\nuncoordinated?\n\n- How badly will the system fail if the assumptions are violated?\n\nFor now, let us focus on the first two. We can draw a graph:\n\nThe more green, the better. Let us explore the categories in more\ndetail:\n\n- 1 of 1: there is exactly one actor, and the system\nworks if (and only if) that one actor does what you expect them to. This\nis the traditional \"centralized\" model, and it is what we are trying to\ndo better than.\n\n- N of N: the \"dystopian\" world. You rely on a whole\nbunch of actors, all of whom need to act as expected for\neverything to work, with no backups if any of them fail.\n\n- N/2 of N: this is how blockchains work - they work\nif the majority of the miners (or PoS validators) are honest. Notice\nthat N/2 of N becomes significantly more valuable the larger the N gets;\na blockchain with a few miners/validators dominating the network is much\nless interesting than a blockchain with its miners/validators widely\ndistributed. That said, we want to improve on even this level of\nsecurity, hence the concern around surviving 51%\nattacks.\n\n- 1 of N: there are many actors, and the system works\nas long as at least one of them does what you expect them to. Any system\nbased on fraud proofs falls into this category, as do trusted setups\nthough in that case the N is often smaller. Note that you do want the N\nto be as large as possible!\n\n- Few of N: there are many actors, and the system\nworks as long as at least some small fixed number of them do what you\nexpect them do. Data\navailability checks fall into this category.\n\n- 0 of N: the systems works as expected without any\ndependence whatsoever on external actors. Validating a block by checking\nit yourself falls into this category.\n\nWhile all buckets other than \"0 of N\" can be considered \"trust\", they\nare very different from each other! Trusting that one particular person\n(or organization) will work as expected is very different from trusting\nthat some single person anywhere will do what you expect them\nto. \"1 of N\" is arguably much closer to \"0 of N\" than it is to \"N/2 of\nN\" or \"1 of 1\". A 1-of-N model might perhaps feel like a 1-of-1 model\nbecause it feels like you're going through a single actor, but the\nreality of the two is very different: in a 1-of-N system, if\nthe actor you're working with at the moment disappears or turns evil,\nyou can just switch to another one, whereas in a 1-of-1 system you're\nscrewed.\n\nParticularly, note that even the correctness of the software you're\nrunning typically depends on a \"few of N\" trust model to ensure that if\nthere's bugs in the code someone will catch them. With that fact in\nmind, trying really hard to go from 1 of N to 0 of N on some other\naspect of an application is often like making a reinforced steel door\nfor your house when the windows are open.\n\nAnother important distinction is: how does the system fail if your\ntrust assumption is violated? In blockchains, two most common types of\nfailure are liveness failure and safety\nfailure. A liveness failure is an event in which you are\ntemporarily unable to do something you want to do (eg. withdraw coins,\nget a transaction included in a block, read information from the\nblockchain). A safety failure is an event in which something actively\nhappens that the system was meant to prevent (eg. an invalid block gets\nincluded in a blockchain).\n\nHere are a few examples of trust models of a few blockchain layer 2\nprotocols. I use \"small N\" to refer to the set of\nparticipants of the layer 2 system itself, and \"big N\"\nto refer to the participants of the blockchain; the assumption is always\nthat the layer 2 protocol has a smaller community than the blockchain\nitself. I also limit my use of the word \"liveness failure\" to cases\nwhere coins are stuck for a significant amount of time; no longer being\nable to use the system but being able to near-instantly withdraw does\nnot count as a liveness failure.\n\n- Channels (incl state channels, lightning network):\n1 of 1 trust for liveness (your counterparty can temporarily freeze your\nfunds, though the harms of this can be mitigated if you split coins\nbetween multiple counterparties), N/2 of big-N trust for safety (a\nblockchain 51% attack can steal your coins)\n\n- Plasma (assuming centralized operator): 1 of 1\ntrust for liveness (the operator can temporarily freeze your funds), N/2\nof big-N trust for safety (blockchain 51% attack)\n\n- Plasma (assuming semi-decentralized operator, eg.\nDPOS): N/2 of small-N trust for liveness, N/2 of big-N trust for\nsafety\n\n- Optimistic rollup: 1 of 1 or N/2 of small-N trust\nfor liveness (depends on operator type), N/2 of big-N trust for\nsafety\n\n- ZK rollup: 1 of small-N trust for liveness (if the\noperator fails to include your transaction, you can withdraw, and if the\noperator fails to include your withdrawal immediately they cannot\nproduce more batches and you can self-withdraw with the help of any full\nnode of the rollup system); no safety failure risks\n\n- ZK rollup (with light-withdrawal\nenhancement): no liveness failure risks, no safety failure\nrisks\n\nFinally, there is the question of incentives: does the actor you're\ntrusting need to be very altruistic to act as expected, only slightly\naltruistic, or is being rational enough? Searching for fraud proofs is\n\"by default\" slightly altruistic, though just how altruistic it is\ndepends on the complexity of the computation (see the verifier's dilemma),\nand there are ways to modify the game to make it rational.\n\nAssisting others with withdrawing from a ZK rollup is rational if we\nadd a way to micro-pay for the service, so there is really\nlittle cause for concern that you won't be able to exit from a rollup\nwith any significant use. Meanwhile, the greater risks of the other\nsystems can be alleviated if we agree as a community to\nnot\naccept 51% attack chains that revert too far in history or censor\nblocks for too long.\n\nConclusion: when someone says that a system \"depends on trust\", ask\nthem in more detail what they mean! Do they mean 1 of 1, or 1 of N, or\nN/2 of N? Are they demanding these participants be altruistic or just\nrational? If altruistic, is it a tiny expense or a huge expense? And\nwhat if the assumption is violated - do you just need to wait a few\nhours or days, or do you have assets that are stuck forever? Depending\non the answers, your own answer to whether or not you want to use that\nsystem might be very different.",
    "contentLength": 8475,
    "summary": "Blockchain applications exist on a spectrum of trust models from \"1 of 1\" (centralized) to \"0 of N\" (fully trustless), not just \"trusted\" vs \"trustless\".",
    "detailedSummary": {
      "theme": "Trust in blockchain systems exists on a spectrum with different dimensions that should be analyzed systematically rather than treated as a binary trustless/trusted distinction.",
      "summary": "Vitalik argues that blockchain applications are never fully trustless, but exist on a spectrum of trust that can be systematically analyzed. He defines trust as 'the use of any assumptions about the behavior of other people' and proposes breaking down trust models into four key dimensions: how many people need to behave as expected, out of how many total people, what motivations are required, and how badly the system fails if assumptions are violated. Vitalik categorizes trust models from '1 of 1' (traditional centralized systems) to '0 of N' (no external dependencies), with blockchain consensus operating on 'N/2 of N' majority trust. He demonstrates that '1 of N' systems (where only one actor out of many needs to behave correctly) are much closer to trustless than they initially appear, since users can switch between actors if one fails. Using layer 2 scaling solutions as examples, Vitalik shows how different systems have varying trust assumptions for liveness failures (temporary inability to act) versus safety failures (system prevents what it shouldn't). He concludes that rather than dismissing systems as simply 'trusted,' we should examine the specific trust model, required incentives, and failure modes to make informed decisions about which systems to use.",
      "takeaways": [
        "Trust should be analyzed across multiple dimensions rather than viewed as a binary trustless/trusted classification",
        "The '1 of N' trust model (requiring only one honest actor among many) is much stronger than '1 of 1' centralized trust because users can switch between actors",
        "Different blockchain layer 2 solutions have vastly different trust assumptions, from ZK rollups with minimal trust requirements to channels requiring counterparty cooperation",
        "System failures can be categorized as liveness failures (temporary inability to act) or safety failures (system actively does something it shouldn't), each with different severity implications",
        "The motivation required from trusted actors matters significantly - whether they need to be altruistic or can simply act rationally affects the system's robustness"
      ],
      "controversial": [
        "The claim that community agreement to reject 51% attack chains could effectively mitigate blockchain security risks may be seen as overly optimistic about social coordination",
        "The assertion that '1 of N' trust models are much closer to '0 of N' than to '1 of 1' might be disputed by those who view any dependency on external actors as fundamentally similar"
      ]
    }
  },
  {
    "id": "general-2020-08-17-philosophy",
    "title": "A Philosophy of Blockchain Validation",
    "date": "2020-08-17",
    "category": "general",
    "url": "https://vitalik.eth.limo/general/2020/08/17/philosophy.html",
    "path": "general/2020/08/17/philosophy.html",
    "content": "- \n\n- \n\n  \n    \n\n  \n  \n  Dark Mode Toggle\n  \n    \n\n  \n\n##  A Philosophy of Blockchain Validation \n\n 2020 Aug 17 \nSee all posts\n\n \n \n\n A Philosophy of Blockchain Validation \n\nSee also:\n\n- A\nProof of Stake Design Philosophy\n\n- The\nMeaning of Decentralization\n\n- Engineering\nSecurity through Coordination Problems\n\nOne of the most powerful properties of a blockchain is the fact that\nevery single part of the blockchain's execution can be independently\nvalidated. Even if a great majority of a blockchain's miners (or\nvalidators in PoS) get taken over by an attacker, if that attacker tries\nto push through invalid blocks, the network will simply reject them.\nEven those users that were not verifying blocks at that time can be\n(potentially automatically) warned by those who were, at which point\nthey can check that the attacker's chain is invalid, and automatically\nreject it and coordinate on accepting a chain that follows the\nrules.\n\nBut how much validation do we actually need? Do we need a hundred\nindependent validating nodes, a thousand? Do we need a culture where the\naverage person in the world runs software that checks every transaction?\nIt's these questions that are a challenge, and a very important\nchallenge to resolve especially if we want to build blockchains with\nconsensus mechanisms better than the single-chain \"Nakamoto\" proof of\nwork that the blockchain space originally started with.\n\n## Why validate?\n\nA 51% attack pushing through an invalid block. We want\nthe network to reject the chain!\n\nThere are two main reasons why it's beneficial for a user to validate\nthe chain. First, it maximizes the chance that the node can correctly\ndetermine and say on the canonical chain - the chain\nthat the community accepts as legitimate. Typically, the canonical chain\nis defined as something like \"the valid chain that has the most\nminers/validators supporting it\" (eg. the \"longest valid chain\" in\nBitcoin). Invalid chains are rejected by definition, and if there is a\nchoice between multiple valid chains, the chain that has the most\nsupport from miners/validators wins out. And so if you have a node that\nverifies all the validity conditions, and hence detects which chains are\nvalid and which chains are not, that maximizes your chances of correctly\ndetecting what the canonical chain actually is.\n\nBut there is also another deeper reason why validating the chain is\nbeneficial. Suppose that a powerful actor tries to push through a change\nto the protocol (eg. changing the issuance), and has the support of the\nmajority of miners. If no one else validates the chain, this attack can\nvery easily succeed: everyone's clients will, by default,\naccept the new chain, and by the time anyone sees what is going on, it\nwill be up to the dissenters to try to coordinate a rejection\nof that chain. But if average users are validating, then the\ncoordination problem falls on the other side: it's now the\nresponsibility of whoever is trying to change the protocol to convince\nthe users to actively download the software patch to accept the protocol\nchange.\n\nIf enough users are validating, then instead of defaulting to\nvictory, a contentious attempt to force a change of the protocol will\ndefault to chaos. Defaulting to chaos still causes a lot of\ndisruption, and would require out-of-band social coordination to\nresolve, but it places a much larger barrier in front of the attacker,\nand makes attackers much less confident that they will be able to get\naway with a clean victory, making them much less motivated to even try\nto start an attack. If most users are validating (directly or\nindirectly), and an attack has only the support of the majority\nof miners, then the attack will outright default to\nfailure - the best outcome of all.\nThe definition\nview versus the coordination view\n\nNote that this reasoning is very different from a different line of\nreasoning that we often hear: that a chain that changes the rules is\nsomehow \"by definition\" not the correct chain, and that no matter how\nmany other users accept some new set of rules, what matters is that you\npersonally can stay on the chain with the old rules that you favor.\n\nHere is one example of the \"by definition\" perspective from Gavin\nAndresen:\n\nHere's another from the\nWasabi wallet; this one comes even more directly from the\nperspective of explaining why full nodes are valuable:\n\nNotice two core components of this view:\n\n- A version of the chain that does not accept the rules that you\nconsider fundamental and non-negotiable is by definition not\nbitcoin (or not ethereum or whatever other chain), not matter how many\nother people accept that chain.\n\n- What matters is that you remain on a chain that has rules\nthat you consider acceptable.\n\nHowever, I believe this \"individualist\" view to be very wrong. To see\nwhy, let us take a look at the scenario that we are worried about: the\nvast majority of participants accept some change to protocol rules that\nyou find unacceptable. For example, imagine a future where transaction\nfees are very low, and to keep the chain secure, almost everyone else\nagrees to change to a new set of rules that increases issuance. You\nstubbornly keep running a node that continues to enforce the old rules,\nand you fork off to a different chain than the majority.\n\nFrom your point of view, you still have your coins in a system that\nruns on rules that you accept. But so what? Other users will not accept\nyour coins. Exchanges will not accept your coins. Public websites may\nshow the price of the new coin as being some high value, but they're\nreferring to the coins on the majority chain; your coins are\nvalueless. Cryptocurrencies and blockchains are fundamentally social\nconstructs; without other people believing in them, they mean\nnothing.\n\nSo what is the alternative view? The core idea is to look at\nblockchains as engineering\nsecurity through coordination problems.\n\nNormally, coordination problems in the world are a bad thing: while\nit would be better for most people if the English language got rid of\nits highly complex and irregular spelling system and made a phonetic\none, or if the United States switched to metric, or if we could\nimmediately drop all\nprices and wages by ten percent in the event of a recession, in\npractice this requires everyone to agree on the switch at the same time,\nand this is often very very hard.\n\nWith blockchain applications, however, we are using coordination\nproblems to our advantage. We are using the friction that\ncoordination problems create as a bulwark against malfeasance by\ncentralized actors. We can build systems that have property X, and we\ncan guarantee that they will preserve property X because changing the\nrules from X to not-X would require a whole bunch of people to agree to\nupdate their software at the same time. Even if there is an actor that\ncould force the change, doing so would be hard - much much harder than\nit would be if it were instead the responsibility of users to\nactively coordinate dissent to resist a change.\n\nNote one particular consequence of this view: it's emphatically\nnot the case that the purpose of your full node is just to\nprotect you, and in the case of a contentious hard fork, people\nwith full nodes are safe and people without full nodes are vulnerable.\nRather, the perspective here is much more one of herd\nimmunity: the more people are validating, the more safe\neveryone is, and even if only some portion of people are\nvalidating, everyone gets a high level of protection as a result.\nLooking deeper into\nvalidation\n\nWe now get to the next topic, and one that is very relevant to topics\nsuch as light clients and sharding: what are we actually accomplishing\nby validating? To understand this, let us go back to an earlier point.\nIf an attack happens, I would argue that we have the following\npreference order over how the attack goes:\n\ndefault to failure > default to chaos > default to victory\n\nThe \">\" here of course means \"better than\". The best is if an\nattack outright fails; the second best is if an attack leads to\nconfusion, with everyone disagreeing on what the correct chain is, and\nthe worst is if an attack succeeds. Why is chaos so much better than\nvictory? This is a matter of incentives: chaos raises costs for the\nattacker, and denies them the certainty that they will even win,\ndiscouraging attacks from being attempted in the first place. A\ndefault-to-chaos environment means that an attacker needs to win\nboth the blockchain war of making a 51% attack and the\n\"social war\" of convincing the community to follow along. This is much\nmore difficult, and much less attractive, than just launching a 51%\nattack and claiming victory right there.\n\nThe goal of validation is then to move away from default to victory\nto (ideally) default to failure or (less ideally) default to chaos. If\nyou have a fully validating node, and an attacker tries to push through\na chain with different rules, then the attack fails. If some people have\na fully validating node but many others don't, the attack leads to\nchaos. But now we can think: are there other ways of achieving the same\neffect?\nLight clients and fraud\nproofs\n\nOne natural advancement in this regard is light clients with\nfraud proofs. Most blockchain light clients that exist today\nwork by simply validating that the majority of miners support a\nparticular block, and not bothering to check if the other protocol rules\nare being enforced. The client runs on the trust assumption that the\nmajority of miners is honest. If a contentious fork happens, the client\nfollows the majority chain by default, and it's up to users to take an\nactive step if they want to follow the minority chain with the old\nrules; hence, today's light clients under attack default to victory. But\nwith fraud proof technology, the situation starts to look very\ndifferent.\n\nA fraud proof in its simplest form works as follows. Typically, a\nsingle block in a blockchain only touches a small portion of the\nblockchain \"state\" (account balances, smart contract code....). If a fully\nverifying node processes a block and finds that it is invalid, they can\ngenerate a package (the fraud proof) containing the block along with\njust enough data from the blockchain state to process the block. They\nbroadcast this package to light clients. Light clients can then take the\npackage and use that data to verify the block themselves, even if they\nhave no other data from the chain.\n\n \nA single block in a blockchain touches\nonly a few accounts. A fraud proof would contain the data in those\naccounts along with Merkle proofs proving that that data is\ncorrect.\n\nThis technique is also sometimes known as stateless\nvalidation: instead of keeping a full database of the\nblockchain state, clients can keep only the block headers, and they can\nverify any block in real time by asking other nodes for a Merkle proof\nfor any desired state entries that block validation is accessing.\n\nThe power of this technique is that light clients can verify\nindividual blocks only if they hear an alarm (and alarms are\nverifiable, so if a light client hears a false alarm, they can just stop\nlistening to alarms from that node). Hence, under normal circumstances,\nthe light client is still light, checking only which blocks are\nsupported by the majority of miners/validators. But under those\nexceptional circumstances where the majority chain contains a block that\nthe light client would not accept, as long as there is at least\none honest node verifying the fraudulent block, that node will see that\nit is invalid, broadcast a fraud proof, and thereby cause the rest of\nthe network to reject it.\n\n## Sharding\n\nSharding is a natural extension of this: in a sharded system, there\nare too many transactions in the system for most people to be verifying\ndirectly all the time, but if the system is well designed then any\nindividual invalid block can be detected and its invalidity proven with\na fraud proof, and that proof can be broadcasted across the entire\nnetwork. A sharded network can be summarized as everyone being\na light client. And as long as each shard has some minimum threshold\nnumber of participants, the network has herd immunity.\n\nIn addition, the fact that in a sharded system block\nproduction (and not just block verification) is highly\naccessible and can be done even on consumer laptops is also very\nimportant. The lack of dependence on high-performance hardware at the\ncore of the network ensures that there is a low bar on dissenting\nminority chains being viable, making it even harder for a\nmajority-driven protocol change to \"win by default\" and bully everyone\nelse into submission.\n\nThis is what auditability usually means in the real world: not that\neveryone is verifying everything all the time, but that (i) there are\nenough eyes on each specific piece that if there is an error it will get\ndetected, and (ii) when an error is detected that fact that be made\nclear and visible to all.\n\nThat said, in the long run blockchains can certainly improve on this.\nOne particular source of improvements is ZK-SNARKs (or \"validity\nproofs\"): efficiently verifiably cryptographic proofs that allow block\nproducers to prove to clients that blocks satisfy some arbitrarily\ncomplex validity conditions. Validity\nproofs are stronger than fraud proofs because they do not depend on\nan interactive game to catch fraud. Another important technology is data availability\nchecks, which can protect against blocks whose data is not fully\npublished. Data availability checks do rely on a very conservative\nassumption that there exists at least some small number of honest nodes\nsomewhere in the network continues to apply, though the good news is\nthat this minimum honesty threshold is low, and does not grow even if\nthere is a very large number of attackers.\n\n## Timing and 51% attacks\n\nNow, let us get to the most powerful consequence of the \"default to\nchaos\" mindset: 51% attacks themselves. The current norm in many\ncommunities is that if a 51% attack wins, then that 51% attack is\nnecessarily the valid chain. This norm is often stuck to quite strictly;\nand a recent 51%\nattack on Ethereum Classic illustrated this quite well. The attacker\nreverted more than 3000 blocks (stealing 807,260 ETC in a double-spend\nin the process), which pushed the chain farther back in history than one\nof the two ETC clients (OpenEthereum) was technically able to revert; as\na result, Geth nodes went with the attacker's chain but OpenEthereum\nnodes stuck with the original chain.\n\nWe can say that the attack did in fact default to chaos, though this\nwas an accident and not a deliberate design decision of the ETC\ncommunity. Unfortunately, the community then elected to accept the\n(longer) attack chain as canonical, a move described\nby the eth_classic twitter as \"following Proof of Work as intended\".\nHence, the community norms actively helped the attacker\nwin.\n\nBut we could instead agree on a definition of the canonical chain\nthat works differently: particularly, imagine a rule that once a client\nhas accepted a block as part of the canonical chain, and that block has\nmore than 100 descendants, the client will from then on never accept a\nchain that does not include that block. Alternatively, in a\nfinality-bearing proof of stake setup (which eg. ethereum 2.0 is),\nimagine a rule that once a block is finalized it can never be\nreverted.\n\n5 block revert limit only for illustration purposes;\nin reality the limit could be something longer like 100-1000\nblocks.\n\nTo be clear, this introduces a significant change to how\ncanonicalness is determined: instead of clients just looking at the data\nthey receive by itself, clients also look at when that data was\nreceived. This introduces the possibility that, because of network\nlatency, clients disagree: what if, because of a massive attack, two\nconflicting blocks A and B finalize at the same time, and some clients\nsee A first and some see B first? But I would argue that this is good:\nit means that instead of defaulting to victory, even 51% attacks\nthat just try to revert transactions default to chaos, and\nout-of-band emergency response can choose which of the two blocks the\nchain continues with. If the protocol is well-designed, forcing an\nescalation to out-of-band emergency response should be very expensive:\nin proof of stake, such a thing would require 1/3 of validators\nsacrificing their deposits and getting slashed.\n\nPotentially, we could expand this approach. We could try to make\n51% attacks that censor transactions default to chaos too. Research\non timeliness\ndetectors pushes things further in the direction of attacks\nof all types defaulting to failure, though a little chaos remains\nbecause timeliness detectors cannot help nodes that are not\nwell-connected and online.\n\nFor a blockchain community that values immutability,\nimplementing revert limits of this kind are arguably the superior path\nto take. It is difficult to honestly claim that a blockchain is\nimmutable when no matter how long a transaction has been accepted in a\nchain, there is always the possibility that some unexpected activity by\npowerful actors can come along and revert it. Of course, I would claim\nthat even BTC and ETC do already have a revert limit at the\nextremes; if there was an attack that reverted weeks of activity, the\ncommunity would likely adopt a user-activated soft fork to reject the\nattackers' chain. But more definitively agreeing on and formalizing this\nseems like a step forward.\n\n## Conclusion\n\nThere are a few \"morals of the story\" here. First, if we accept the\nlegitimacy of social coordination, and we accept the legitimacy of\nindirect validation involving \"1-of-N\" trust models (that is, assuming\nthat there exists one honest person in the network somewhere; NOT the\nsame as assuming that one specific party, eg. Infura, is honest), then\nwe can create blockchains that are much more scalable.\n\nSecond, client-side validation is extremely important for all of this\nto work. A network where only a few people run nodes and everyone else\nreally does trust them is a network that can easily be taken over by\nspecial interests. But avoiding such a fate does not require\ngoing to the opposite extreme and having everyone always validate\neverything! Systems that allow each individual block to be verified in\nisolation, so users only validate blocks if someone else raises an\nalarm, are totally reasonable and serve the same effect. But this\nrequires accepting the \"coordination view\" of what validation is\nfor.\n\nThird, if we allow the definition of canonicalness includes timing,\nthen we open many doors in improving our ability to reject 51% attacks.\nThe easiest property to gain is weak\nsubjectivity: the idea that if clients are required to log on at\nleast once every eg. 3 months, and refuse to revert longer than that,\nthen we can add slashing to proof of stake and make attacks very\nexpensive. But we can go further: we can reject chains that revert\nfinalized blocks and thereby protect immutability, and even protect\nagainst censorship. Because the network is unpredictable, relying on\ntiming does imply attacks \"defaulting to chaos\" in some cases,\nbut the benefits are very much worth it.\n\nWith all of these ideas in mind, we can avoid the traps of (i)\nover-centralization, (ii) overly redundant verification leading to\ninefficiency and (iii) misguided norms accidentally making\nattacks easier, and better work toward building more resilient,\nperformant and secure blockchains.",
    "contentLength": 19456,
    "summary": "Blockchain validation creates coordination problems that make attacks default to failure/chaos rather than victory, protecting all users through herd immunity.",
    "detailedSummary": {
      "theme": "Vitalik explores how blockchain validation can be optimized through coordination theory, arguing that not everyone needs to validate everything if systems are designed to default attacks to failure rather than victory.",
      "summary": "Vitalik presents a philosophy of blockchain validation centered on coordination problems rather than individual protection. He argues that validation serves two purposes: helping nodes identify the canonical chain and creating coordination friction that makes attacks harder. Vitalik critiques the 'individualist' view that running a full node protects you personally, instead proposing a 'herd immunity' model where widespread validation protects everyone by making attacks default to chaos or failure rather than easy victory. He advocates for techniques like fraud proofs, light clients, and sharding that maintain security while reducing the validation burden on individual users. Vitalik also suggests controversial changes like revert limits and incorporating timing into canonicalness definitions to better resist 51% attacks. His central thesis is that blockchains derive security from coordination problems that make rule changes difficult, and this principle can guide more scalable and secure blockchain designs.",
      "takeaways": [
        "Validation creates 'herd immunity' - widespread validation protects everyone, not just individual validators",
        "Attacks should default to chaos or failure rather than victory through coordination friction",
        "Light clients with fraud proofs and sharding can maintain security while improving scalability",
        "Blockchain security comes from engineering coordination problems that make unwanted changes difficult",
        "Incorporating timing into canonicalness definitions could help resist 51% attacks through revert limits"
      ],
      "controversial": [
        "Proposing that revert limits should be implemented to make even successful 51% attacks default to chaos rather than victory",
        "Arguing against the individualist view that full nodes primarily protect their individual operators",
        "Suggesting that timing should be incorporated into how canonical chains are determined, potentially causing network splits"
      ]
    }
  },
  {
    "id": "general-2020-07-22-round6",
    "title": "Gitcoin Grants Round 6 Retrospective",
    "date": "2020-07-22",
    "category": "governance",
    "url": "https://vitalik.eth.limo/general/2020/07/22/round6.html",
    "path": "general/2020/07/22/round6.html",
    "content": "- \n\n- \n\n  \n    \n\n  \n  \n  Dark Mode Toggle\n  \n    \n\n  \n\n##  Gitcoin Grants Round 6 Retrospective \n\n 2020 Jul 22 \nSee all posts\n\n \n \n\n Gitcoin Grants Round 6 Retrospective \n\nRound 6 of Gitcoin Grants has just finished, with $227,847 in\ncontributions from 1,526 contributors and $175,000 in matched funds\ndistributed across 695 projects. This time around, we had three\ncategories: the two usual categories of \"tech\" and \"community\" (the\nlatter renamed from \"media\" to reflect a desire for a broad emphasis),\nand the round-6-special category Crypto For Black Lives.\n\nFirst of all, here are the results, starting with the tech and\ncommunity sections:\n\n## Stability of income\n\nIn the last round, one concern I raised was\nstability of income. People trying to earn a livelihood off of quadratic\nfunding grants would want to have some guarantee that their income isn't\ngoing to completely disappear in the next round just because the hive\nmind suddenly gets excited about something else.\n\nRound 6 had two mechanisms to try to provide more stability of\nincome:\n\n- A \"shopping cart\" interface for giving many contributions, with an\nexplicit \"repeat your contributions from the last round\" feature\n\n- A rule that the matching amounts are calculated using not just\ncontributions from this round, but also \"carrying over\" 1/3 of the\ncontributions from the previous round (ie. if you made a $10 grant in\nthe previous round, the matching formula would pretend you made a $10\ngrant in the previous round and also a $3.33 grant this round)\n\n- was clearly successful at one goal: increasing the total number of\ncontributions. But its effect in ensuring stability of income is hard to\nmeasure. The effect of (2), on the other hand, is easy to measure,\nbecause we have stats for the actual matching amount as well as what the\nmatching amount \"would have been\" if the 1/3 carry-over rule was not in\nplace.\n\nFirst from the tech category:\n\nNow from the community category:\n\nClearly, the rule helps reduce volatility, pretty much exactly as\nexpected. That said, one could argue that this result is trivial: you\ncould argue that all that's going on here is something very similar to\ngrabbing part of the revenue from round N (eg. see how the new EIP-1559\nCommunity Fund earned less than it otherwise would have) and moving it\ninto round N+1. Sure, numerically speaking the revenues are more\n\"stable\", but individual projects could have just provided this\nstability to themselves by only spending 2/3 of the pot from each round,\nand using the remaining third later when some future round is\nunexpectedly low. Why should the quadratic funding mechanism\nsignificantly increase its complexity just to achieve a gain in\nstability that projects could simply provide for themselves?\n\nMy instinct says that it would be best to try the next round with the\n\"repeat last round\" feature but without the 1/3 carryover, and\nsee what happens. Particularly, note that the numbers seem to show that\nthe media section would have been \"stable enough\" even without the\ncarryover. The tech section was more volatile, but only because of the\nsudden entrance of the EIP 1559 community fund; it would be part of the\nexperiment to see just how common that kind of situation is.\nAbout that EIP 1559\nCommunity fund...\n\nThe big unexpected winner of this round was the EIP 1559 community\nfund. EIP 1559 (EIP here, FAQ here,\noriginal paper here)\nis a major fee market reform proposal which far-reaching consequences;\nit aims to improve the user experience of sending Ethereum transactions,\nreduce economic inefficiencies, provide an accurate in-protocol gas\nprice oracle and burn a portion of fee revenue.\n\nMany people in the Ethereum community are very excited about this\nproposal, though so far there has been fairly little funding toward\ngetting it implemented. This gitcoin grant was a large community effort\ntoward fixing this.\n\nThe grant had quite a few very large contributions, including roughly\n$2,400 each from myself and Eric Conner, early on. Early in the round,\none could clearly see the EIP 1559 community grant having an abnormally\nlow ratio of matched funds to contributed funds; it was somewhere around\n$4k matched to $20k contributed. This was because while the amount\ncontributed was large, it came from relatively few wealthier donors, and\nso the matching amount was less than it would have been had the same\nquantity of funds come from more diverse sources - the quadratic funding\nformula working as intended. However, a social media push advertising\nthe grant then led to a large number of smaller contributors following\nalong, which then quickly raised the match to its currently very high\nvalue ($35,578).\n\n## Quadratic signaling\n\nUnexpectedly, this grant proved to have a double function. First, it\nprovided $65,473 of much-needed funding to EIP 1559 implementation.\nSecond, it served as a credible community signal of the level of demand\nfor the proposal. The Ethereum community has long been struggling to find\neffective ways to determine what \"the community\" supports, especially in\ncases of controversy.\n\nCoin votes have been used in the past, and\nhave the advantage that they come with an answer to the key problem of\ndetermining who is a \"real community member\" - the answer is, your\nmembership in the Ethereum community is proportional to how much ETH you\nhave. However, they are plutocratic; in the famous DAO coin vote, a\nsingle \"yes\" voter voted with more ETH than all \"no\" voters put together\n(~20% of the total).\n\nThe alternative, looking at github, reddit and twitter comments and\nvotes to measure sentiment (sometimes derided as \"proof of social\nmedia\") is egalitarian, but it is easily exploitable, comes with no\nskin-in-the-game, and frequently falls under criticisms of \"foreign\ninterference\" (are those really ethereum community members\ndisagreeing with the proposal, or just those dastardly bitcoiners coming\nin from across the pond to stir up trouble?).\n\nQuadratic funding falls perfectly in the middle: the need to\ncontribute monetary value to vote ensures that the votes of those who\nreally care about the project count more than the votes of\nless-concerned outsiders, and the square-root function ensures that the\nvotes of individual ultra-wealthy \"whales\" cannot beat out a poorer, but\nbroader, coalition.\n\nA\ndiagram from my post on\nquadratic payments showing how quadratic payments is \"in the middle\"\nbetween the extremes of voting-like systems and money-like systems, and\navoids the worst flaws of both.\n\nThis raises the question: might it make sense to try to use explicit\nquadratic voting (with the ability to vote \"yes\" or \"no\" to a\nproposal) as an additional signaling tool to determine community\nsentiment for ethereum protocol proposals?\nHow well are \"guest\ncategories\" working?\n\nSince round 5, Gitcoin Grants has had three categories per round:\ntech, community (called \"media\" before), and some \"guest\" category that\nappears only during that specific round. In round 5 this was COVID\nrelief; in round 6, it's Crypto For Black Lives.\n\nBy far the largest recipient was Black Girls CODE, claiming over 80%\nof the matching pot. My guess for why this happened is simple: Black\nGirls CODE is an established project that has been participating in the\ngrants for several rounds already, whereas the other projects were new\nentrants that few people in the Ethereum community knew well. In\naddition, of course, the Ethereum community \"understands\" the value of\nhelping people code more than it understands chambers of commerce and\nbail funds.\n\nThis raises the question: is Gitcoin's current approach of having a\nguest category each round actually working well? The case for \"no\" is\nbasically this: while the individual causes (empowering black\ncommunities, and fighting covid) are certainly admirable, the Ethereum\ncommunity is by and large not experts at these topics, and we're\ncertainly not experts on those specific projects working on\nthose challenges.\n\nIf the goal is to try to bring quadratic funding to causes beyond\nEthereum, the natural alternative is a separate funding round marketed\nspecifically to those communities; https://downtownstimulus.com/\nis a great example of this. If the goal is to get the Ethereum community\ninterested in other causes, then perhaps running more than one round on\neach cause would work better. For example, \"guest categories\" could last\nfor three rounds (~6 months), with $8,333 matching per round (and there\ncould be two or three guest categories running simultaneously). In any\ncase, it seems like some revision of the model makes sense.\n\n## Collusion\n\nNow, the bad news. This round saw an unprecedented amount of\nattempted collusion and other forms of fraud. Here are a few of the most\negregious examples.\n\nBlatant attempted bribery:\n\nImpersonation:\n\nMany contributions with funds clearly coming from a single\naddress:\n\nThe big question is: how much fraudulent activity can be prevented in\na fully automated/technological way, without requiring detailed analysis\nof each and every case? If quadratic funding cannot survive such fraud\nwithout needing to resort to expensive case-by-case judgement, then\nregardless of its virtues in an ideal world, in reality it would not be\na very good mechanism!\n\nFortunately, there is a lot that we can do to reduce harmful\ncollusion and fraud that we are not yet doing. Stronger identity systems\nis one example; in this round, Gitcoin added optional SMS verification,\nand it seems like the in this round the detected instances of collusion\nwere mostly github-verified accounts and not SMS-verified accounts. In\nthe next round, making some form of extra verification beyond a github\naccount (whether SMS or something more decentralized, eg. BrightID)\nseems like a good idea. To limit bribery, MACI can help, by making\nit impossible for a briber to tell who actually voted for any particular\nproject.\n\nImpersonation is not really a quadratic funding-specific challenge;\nthis could be solved with manual verification, or if one wishes for a\nmore decentralized solution one could try using Kleros or some similar system. One could\neven imagine incentivized reporting: anyone can lay down a deposit and\nflag a project as fraudulent, triggering an investigation; if the\nproject turns out to be legitimate the deposit is lost but if the\nproject turns out to be fraudulent, the challenger gets half of the\nfunds that were sent to that project.\n\n## Conclusion\n\nThe best news is the unmentioned news: many of the positive behaviors\ncoming out of the quadratic funding rounds have stabilized. We're seeing\nvaluable projects get funded in the tech and community categories, there\nhas been less social media contention this round than in previous\nrounds, and people are getting better and better at understanding the\nmechanism and how to participate in it.\n\nThat said, the mechanism is definitely at a scale where we are seeing\nthe kinds of attacks and challenges that we would realistically see in a\nlarger-scale context. There are some challenges that we have not yet\nworked through (one that I am particularly watching out for is: matched\ngrants going to a project that one part of the community supports and\nanother part of the community thinks is very harmful). That said, we've\ngotten as far as we have with fewer problems than even I had been\nanticipating.\n\nI recommend holding steady, focusing on security (and scalability)\nfor the next few rounds, and coming up with ways to increase the size of\nthe matching pots. And I continue to look forward to seeing valuable\npublic goods get funded!",
    "contentLength": 11559,
    "summary": "Gitcoin Grants Round 6 raised $227K+ across 695 projects with new stability features, while EIP-1559 fund became top winner via quadratic signaling.",
    "detailedSummary": {
      "theme": "Vitalik's analysis of Gitcoin Grants Round 6 outcomes, examining income stability mechanisms, community signaling through quadratic funding, and growing challenges with fraud and collusion.",
      "summary": "Vitalik analyzes the results of Gitcoin Grants Round 6, which distributed $227,847 in contributions and $175,000 in matched funds across 695 projects in three categories: tech, community, and Crypto For Black Lives. He examines two mechanisms implemented to provide income stability - a shopping cart interface with repeat contribution features and a 1/3 carryover rule from previous rounds - concluding that while the carryover rule reduced volatility, it may be unnecessary complexity that projects could manage themselves. Vitalik highlights the EIP 1559 Community Fund as an unexpected winner that served a dual purpose of providing funding and demonstrating quadratic funding's potential as a community signaling mechanism, positioning it between plutocratic coin votes and easily manipulated social media polls. However, he expresses concerns about the effectiveness of guest categories, noting that established projects like Black Girls CODE dominated the Crypto For Black Lives category, and suggests either separate funding rounds for non-Ethereum causes or longer-duration guest categories spanning multiple rounds.",
      "takeaways": [
        "Income stability mechanisms like the 1/3 carryover rule help reduce volatility but may add unnecessary complexity that projects could manage independently",
        "Quadratic funding can serve as an effective community signaling tool that balances democratic participation with skin-in-the-game requirements, as demonstrated by the EIP 1559 Community Fund",
        "Guest categories may not be working optimally since the Ethereum community lacks expertise in evaluating projects outside their domain, with established projects dominating over newcomers",
        "Round 6 experienced unprecedented fraud and collusion attempts, but technological solutions like stronger identity verification, MACI for bribery prevention, and incentivized reporting systems could address these issues",
        "The mechanism is maturing with stabilized positive behaviors and better community understanding, though security and scalability should be the focus for upcoming rounds"
      ],
      "controversial": [
        "The suggestion that quadratic voting could be used to determine community sentiment for Ethereum protocol proposals may be controversial as it introduces a new governance mechanism",
        "The critique that guest categories aren't working well because the Ethereum community isn't expert in evaluating non-Ethereum projects could be seen as dismissive of the community's ability to support broader social causes"
      ]
    }
  },
  {
    "id": "general-2020-07-20-homomorphic",
    "title": "Exploring Fully Homomorphic Encryption",
    "date": "2020-07-20",
    "category": "math",
    "url": "https://vitalik.eth.limo/general/2020/07/20/homomorphic.html",
    "path": "general/2020/07/20/homomorphic.html",
    "content": "- \n\n- \n\n  \n    \n\n  \n  \n  Dark Mode Toggle\n  \n    \n\n  \n\n##  Exploring Fully Homomorphic Encryption \n\n 2020 Jul 20 \nSee all posts\n\n \n \n\n Exploring Fully Homomorphic Encryption \n\nSpecial thanks to Karl Floersch and Dankrad Feist for\nreview\n\nFully homomorphic encryption has for a long time been considered one\nof the holy grails of cryptography. The promise of fully homomorphic\nencryption (FHE) is powerful: it is a type of encryption that allows a\nthird party to perform computations on encrypted data, and get an\nencrypted result that they can hand back to whoever has the decryption\nkey for the original data, without the third party being able\nto decrypt the data or the result themselves.\n\nAs a simple example, imagine that you have a set of emails, and you\nwant to use a third party spam filter to check whether or not they are\nspam. The spam filter has a desire for privacy of their\nalgorithm: either the spam filter provider wants to keep their\nsource code closed, or the spam filter depends on a very large database\nthat they do not want to reveal publicly as that would make attacking\neasier, or both. However, you care about the privacy of your\ndata, and don't want to upload your unencrypted emails to a third\nparty. So here's how you do it:\n\nFully homomorphic encryption has many applications, including in the\nblockchain space. One key example is that can be used to implement\nprivacy-preserving light clients (the light client hands the server an\nencrypted index i, the server computes and returns\ndata[0] * (i = 0) + data[1] * (i = 1) + ... + data[n] * (i = n),\nwhere data[i] is the i'th piece of data in a block or state\nalong with its Merkle branch and (i = k) is an expression\nthat returns 1 if i = k and otherwise 0; the light client\ngets the data it needs and the server learns nothing about what the\nlight client asked).\n\nIt can also be used for:\n\n- More efficient stealth\naddress protocols, and more generally scalability solutions to\nprivacy-preserving protocols that today require each user to personally\nscan the entire blockchain for incoming transactions\n\n- Privacy-preserving data-sharing marketplaces that let users allow\nsome specific computation to be performed on their data while keeping\nfull control of their data for themselves\n\n- An ingredient in more powerful cryptographic primitives, such as\nmore efficient multi-party computation protocols and perhaps eventually\nobfuscation\n\nAnd it turns out that fully homomorphic encryption is, conceptually,\nnot that difficult to understand!\nPartially,\nSomewhat, Fully homomorphic encryption\n\nFirst, a note on definitions. There are different kinds of\nhomomorphic encryption, some more powerful than others, and they are\nseparated by what kinds of functions one can compute on the encrypted\ndata.\n\n- Partially homomorphic encryption allows evaluating\nonly a very limited set of operations on encrypted data: either\njust additions (so given encrypt(a) and\nencrypt(b) you can compute encrypt(a+b)), or\njust multiplications (given encrypt(a) and\nencrypt(b) you can compute encrypt(a*b)).\n\n- Somewhat homomorphic encryption allows computing\nadditions as well as a limited number of multiplications\n(alternatively, polynomials up to a limited degree). That is, if you get\nencrypt(x1) ... encrypt(xn) (assuming these are \"original\"\nencryptions and not already the result of homomorphic computation), you\ncan compute encrypt(p(x1 ... xn)), as long as\np(x1 ... xn) is a polynomial with degree\n< D for some specific degree bound D\n(D is usually very low, think 5-15).\n\n- Fully homomorphic encryption allows unlimited\nadditions and multiplications. Additions and multiplications let you\nreplicate any binary circuit gates (AND(x, y) = x*y,\nOR(x, y) = x+y-x*y, XOR(x, y) = x+y-2*x*y or\njust x+y if you only care about even vs odd,\nNOT(x) = 1-x...), so this is sufficient to do arbitrary\ncomputation on encrypted data.\n\nPartially homomorphic encryption is fairly easy; eg. RSA has a\nmultiplicative homomorphism: \\(enc(x) =\nx^e\\), \\(enc(y) = y^e\\), so\n\\(enc(x) * enc(y) = (xy)^e = enc(xy)\\).\nElliptic curves can offer similar properties with addition. Allowing\nboth addition and multiplication is, it turns out,\nsignificantly harder.\nA simple somewhat-HE\nalgorithm\n\nHere, we will go through a somewhat-homomorphic encryption algorithm\n(ie. one that supports a limited number of multiplications) that is\nsurprisingly simple. A more complex version of this category of\ntechnique was used by Craig Gentry to create the first-ever\nfully homomorphic scheme in 2009. More recent efforts have\nswitched to using different schemes based on vectors and matrices, but\nwe will still go through this technique first.\n\nWe will describe all of these encryption schemes as\nsecret-key schemes; that is, the same key is used to encrypt\nand decrypt. Any secret-key HE scheme can be turned into a public key\nscheme easily: a \"public key\" is typically just a set of many\nencryptions of zero, as well as an encryption of one (and possibly more\npowers of two). To encrypt a value, generate it by adding together the\nappropriate subset of the non-zero encryptions, and then adding a random\nsubset of the encryptions of zero to \"randomize\" the ciphertext and make\nit infeasible to tell what it represents.\n\nThe secret key here is a large prime, \\(p\\) (think of \\(p\\) as having hundreds or even thousands of\ndigits). The scheme can only encrypt 0 or 1, and \"addition\" becomes XOR,\nie. 1 + 1 = 0. To encrypt a value \\(m\\)\n(which is either 0 or 1), generate a large random value \\(R\\) (this will typically be even larger\nthan \\(p\\)) and a smaller random value\n\\(r\\) (typically much smaller than\n\\(p\\)), and output:\n\n\\[enc(m) = R * p + r * 2 + m\\]\n\nTo decrypt a ciphertext \\(ct\\),\ncompute:\n\n\\[dec(ct) = (ct\\ mod\\ p)\\ mod\\\n2\\]\n\nTo add two ciphertexts \\(ct_1\\) and\n\\(ct_2\\), you simply, well, add them:\n\\(ct_1 + ct_2\\). And to multiply two\nciphertexts, you once again... multiply them: \\(ct_1 * ct_2\\). We can prove the homomorphic\nproperty (that the sum of the encryptions is an encryption of the sum,\nand likewise for products) as follows.\n\nLet:\n\n\\[ct_1 = R_1 * p + r_1 * 2 + m_1\\]\n\\[ct_2 = R_2 * p + r_2 * 2 + m_2\\]\n\nWe add:\n\n\\[ct_1 + ct_2 = R_1 * p + R_2 * p + r_1 *\n2 + r_2 * 2 + m_1 + m_2\\]\n\nWhich can be rewritten as:\n\n\\[(R_1 + R_2) * p + (r_1 + r_2) * 2 + (m_1\n+ m_2)\\]\n\nWhich is of the exact same \"form\" as a ciphertext of \\(m_1 + m_2\\). If you decrypt it, the first\n\\(mod\\ p\\) removes the first term, the\nsecond \\(mod\\ 2\\) removes the second\nterm, and what's left is \\(m_1 + m_2\\)\n(remember that if \\(m_1 = 1\\) and \\(m_2 = 1\\) then the 2 will get absorbed into\nthe second term and you'll be left with zero). And so, voila, we have\nadditive homomorphism!\n\nNow let's check multiplication:\n\n\\[ct_1 * ct_2 = (R_1 * p + r_1 * 2 + m_1)\n* (R_2 * p + r_2 * 2 + m_2)\\]\n\nOr:\n\n\\[(R_1 * R_2 * p + r_1 * 2 + m_1 + r_2 * 2\n+ m_2) * p + \\] \\[(r_1 * r_2 * 2 + r_1\n* m_2 + r_2 * m_1) * 2 + \\] \\[(m_1 *\nm_2)\\]\n\nThis was simply a matter of expanding the product above, and grouping\ntogether all the terms that contain \\(p\\), then all the remaining terms that\ncontain \\(2\\), and finally the\nremaining term which is the product of the messages. If you decrypt,\nthen once again the \\(mod\\ p\\) removes\nthe first group, the \\(mod\\ 2\\) removes\nthe second group, and only \\(m_1 *\nm_2\\) is left.\n\nBut there are two problems here: first, the size of the ciphertext\nitself grows (the length roughly doubles when you multiply), and second,\nthe \"noise\" (also often called \"error\") in the smaller \\(\\* 2\\) term also gets quadratically bigger.\nAdding this error into the ciphertexts was necessary because the\nsecurity of this scheme is based on the approximate\nGCD problem:\n\nHad we instead used the \"exact GCD problem\", breaking the system\nwould be easy: if you just had a set of expressions of the form \\(p * R_1 + m_1\\), \\(p * R_2 + m_2\\)..., then you could use the Euclidean\nalgorithm to efficiently compute the greatest common divisor \\(p\\). But if the ciphertexts are only\napproximate multiples of \\(p\\)\nwith some \"error\", then extracting \\(p\\) quickly becomes impractical, and so the\nscheme can be secure.\n\nUnfortunately, the error introduces the inherent limitation that if\nyou multiply the ciphertexts by each other enough times, the error\neventually grows big enough that it exceeds \\(p\\), and at that point the \\(mod\\ p\\) and \\(mod\\ 2\\) steps \"interfere\" with each other,\nmaking the data unextractable. This will be an inherent tradeoff in all\nof these homomorphic encryption schemes: extracting information from\napproximate equations \"with errors\" is much harder than\nextracting information from exact equations, but any error you add\nquickly increases as you do computations on encrypted data, bounding the\namount of computation that you can do before the error becomes\noverwhelming. And this is why these schemes are only \"somewhat\"\nhomomorphic.\n\n## Bootstrapping\n\nThere are two classes of solution to this problem. First, in many\nsomewhat homomorphic encryption schemes, there are clever tricks to make\nmultiplication only increase the error by a constant factor (eg. 1000x)\ninstead of squaring it. Increasing the error by 1000x still sounds by a\nlot, but keep in mind that if \\(p\\) (or\nits equivalent in other schemes) is a 300-digit number, that means that\nyou can multiply numbers by each other 100 times, which is enough to\ncompute a very wide class of computations. Second, there is Craig\nGentry's technique of \"bootstrapping\".\n\nSuppose that you have a ciphertext \\(ct\\) that is an encryption of some \\(m\\) under a key \\(p\\), that has a lot of error. The idea is\nthat we \"refresh\" the ciphertext by turning it into a new ciphertext of\n\\(m\\) under another key \\(p_2\\), where this process \"clears out\" the\nold error (though it will introduce a fixed amount of new error). The\ntrick is quite clever. The holder of \\(p\\) and \\(p_2\\) provides a \"bootstrapping key\" that\nconsists of an encryption of the bits of \\(p\\) under the key \\(p_2\\), as well as the public key for \\(p_2\\). Whoever is doing computations on\ndata encrypted under \\(p\\) would then\ntake the bits of the ciphertext \\(ct\\),\nand individually encrypt these bits under \\(p_2\\). They would then homomorphically\ncompute the decryption under \\(p\\)\nusing these ciphertexts, and get out the single bit, which would be\n\\(m\\) encrypted under \\(p_2\\).\n\nThis is difficult to understand, so we can restate it as follows. The\ndecryption procedure \\(dec(ct, p)\\)\nis itself a computation, and so it can itself be\nimplemented as a circuit that takes as input the bits of \\(ct\\) and the bits of \\(p\\), and outputs the decrypted bit \\(m \\in {0, 1}\\). If someone has a ciphertext\n\\(ct\\) encrypted under \\(p\\), a public key for \\(p_2\\), and the bits of \\(p\\) encrypted under \\(p_2\\), then they can compute \\(dec(ct, p) = m\\) \"homomorphically\", and get\nout \\(m\\) encrypted under \\(p_2\\). Notice that the decryption procedure\nitself washes away the old error; it just outputs 0 or 1. The decryption\nprocedure is itself a circuit, which contains additions or\nmultiplications, so it will introduce new error, but this new error\ndoes not depend on the amount of error in the original\nencryption.\n\n(Note that we can avoid having a distinct new key \\(p_2\\) (and if you want to bootstrap\nmultiple times, also a \\(p_3\\), \\(p_4\\)...) by just setting \\(p_2 = p\\). However, this introduces a new\nassumption, usually called \"circular security\"; it becomes\nmore difficult to formally prove security if you do this, though\nmany cryptographers think it's fine and circular security poses no\nsignificant risk in practice)\n\nBut.... there is a catch. In the scheme as described above (using\ncircular security or not), the error blows up so quickly that even the\ndecryption circuit of the scheme itself is too much for it. That is, the\nnew \\(m\\) encrypted under \\(p_2\\) would already have so much\nerror that it is unreadable. This is because each AND gate doubles the\nbit-length of the error, so a scheme using a \\(d\\)-bit modulus \\(p\\) can only handle less than \\(log(d)\\) multiplications (in series), but\ndecryption requires computing \\(mod\\\np\\) in a circuit made up of these binary logic gates, which\nrequires... more than \\(log(d)\\)\nmultiplications.\n\nCraig Gentry came up with clever techniques to get around this\nproblem, but they are arguably too complicated to explain; instead, we\nwill skip straight to newer work from 2011 and 2013, that solves this\nproblem in a different way.\n\n## Learning with errors\n\nTo move further, we will introduce a different type of\nsomewhat-homomorphic encryption introduced by Brakerski and\nVaikuntanathan in 2011, and show how to bootstrap it. Here, we will move\naway from keys and ciphertexts being integers, and instead have\nkeys and ciphertexts be vectors. Given a key \\(k = {k_1, k_2 .... k_n}\\), to encrypt a\nmessage \\(m\\), construct a vector \\(c = {c_1, c_2 ... c_n}\\) such that the\ninner product (or \"dot product\") \\(<c, k> = c_1k_1 + c_2k_1 + ... +\nc_nk_n\\), modulo some fixed number \\(p\\), equals \\(m+2e\\) where \\(m\\) is the message (which must be 0 or 1),\nand \\(e\\) is a small (much smaller than\n\\(p\\)) \"error\" term. A \"public key\"\nthat allows encryption but not decryption can be constructed, as before,\nby making a set of encryptions of 0; an encryptor can randomly combine a\nsubset of these equations and add 1 if the message they are encrypting\nis 1. To decrypt a ciphertext \\(c\\)\nknowing the key \\(k\\), you would\ncompute \\(<c, k>\\) modulo \\(p\\), and see if the result is odd or even\n(this is the same \"mod p mod 2\" trick we used earlier). Note that here\nthe \\(mod\\ p\\) is typically a\n\"symmetric\" mod, that is, it returns a number between \\(-\\frac{p}{2}\\) and \\(\\frac{p}{2}\\) (eg. 137 mod 10 = -3, 212 mod\n10 = 2); this allows our error to be positive or negative. Additionally,\n\\(p\\) does not necessarily have to be\nprime, though it does need to be odd.\n\nKey\n\n3\n\n14\n\n15\n\n92\n\n65\n\nCiphertext\n\n2\n\n71\n\n82\n\n81\n\n8\n\nThe key and the ciphertext are both vectors, in this example\nof five elements each.\n\nIn this example, we set the modulus \\(p =\n103\\). The dot product is\n3 * 2 + 14 * 71 + 15 * 82 + 92 * 81 + 65 * 8 = 10202, and\n\\(10202 = 99 * 103 + 5\\). 5 itself is\nof course \\(2 * 2 + 1\\), so the message\nis 1. Note that in practice, the first element of the key is often set\nto \\(1\\); this makes it easier to\ngenerate ciphertexts for a particular value (see if you can figure out\nwhy).\n\nThe security of the scheme is based on an assumption known as \"learning with\nerrors\" (LWE) - or, in more jargony but also more understandable\nterms, the hardness of solving systems of equations with\nerrors.\n\nA ciphertext can itself be viewed as an equation: \\(k_1c_1 + .... + k_nc_n \\approx 0\\), where\nthe key \\(k_1 ... k_n\\) is the\nunknowns, the ciphertext \\(c_1 ...\nc_n\\) is the coefficients, and the equality is only approximate\nbecause of both the message (0 or 1) and the error (\\(2e\\) for some relatively small \\(e\\)). The LWE assumption ensures that even\nif you have access to many of these ciphertexts, you cannot recover\n\\(k\\).\n\nNote that in some descriptions of LWE, <c, k> can\nequal any value, but this value must be provided as part of the\nciphertext. This is mathematically equivalent to the <c, k> = m+2e\nformulation, because you can just add this answer to the end of the\nciphertext and add -1 to the end of the key, and get two vectors that\nwhen multiplied together just give m+2e. We'll use the formulation that\nrequires <c, k> to be near-zero (ie. just m+2e) because it is\nsimpler to work with.\n\n## Multiplying ciphertexts\n\nIt is easy to verify that the encryption is additive: if \\(<ct_1, k> = 2e_1 + m_1\\) and \\(<ct_2, k> = 2e_2 + m_2\\), then \\(<ct_1 + ct_2, k> = 2(e_1 + e_2) + m_1 +\nm_2\\) (the addition here is modulo \\(p\\)). What is harder is multiplication:\nunlike with numbers, there is no natural way to multiply two length-n\nvectors into another length-n vector. The best that we can do is the outer product: a\nvector containing the products of each possible pair where the first\nelement comes from the first vector and the second element comes from\nthe second vector. That is, \\(a \\otimes b =\na_1b_1 + a_2b_1 + ... + a_nb_1 + a_1b_2 + ... + a_nb_2 + ... +\na_nb_n\\). We can \"multiply ciphertexts\" using the convenient\nmathematical identity \\(<a \\otimes b, c\n\\otimes d> = <a, c> * <b, d>\\).\n\nGiven two ciphertexts \\(c_1\\) and\n\\(c_2\\), we compute the outer product\n\\(c_1 \\otimes c_2\\). If both \\(c_1\\) and \\(c_2\\) were encrypted with \\(k\\), then \\(<c_1, k> = 2e_1 + m_1\\) and \\(<c_2, k> = 2e_2 + m_2\\). The outer\nproduct \\(c_1 \\otimes c_2\\) can be\nviewed as an encryption of \\(m_1 *\nm_2\\) under \\(k \\otimes k\\); we\ncan see this by looking what happens when we try to decrypt with \\(k \\otimes k\\):\n\n\\[<c_1 \\otimes c_2, k \\otimes\nk>\\] \\[= <c_1, k> * <c_2,\nk>\\] \\[ = (2e_1 + m_1) * (2e_2 +\nm_2)\\] \\[ = 2(e_1m_2 + e_2m_1 +\n2e_1e_2) + m_1m_2\\]\n\nSo this outer-product approach works. But there is, as you may have\nalready noticed, a catch: the size of the ciphertext, and the key, grows\nquadratically.\n\n## Relinearization\n\nWe solve this with a relinearization procedure. The\nholder of the private key \\(k\\)\nprovides, as part of the public key, a \"relinearization key\", which you\ncan think of as \"noisy\" encryptions of \\(k\n\\otimes k\\) under \\(k\\). The\nidea is that we provide these encrypted pieces of \\(k \\otimes k\\) to anyone performing the\ncomputations, allowing them to compute the equation \\(<c_1 \\otimes c_2, k \\otimes k>\\) to\n\"decrypt\" the ciphertext, but only in such a way that the output comes\nback encrypted under \\(k\\).\n\nIt's important to understand what we mean here by \"noisy\nencryptions\". Normally, this encryption scheme only allows encrypting\n\\(m \\in \\{0,1\\}\\), and an \"encryption\nof \\(m\\)\" is a vector \\(c\\) such that \\(<c, k> = m+2e\\) for some small error\n\\(e\\). Here, we're \"encrypting\"\narbitrary \\(m \\in \\{0,1, 2....p-1\\}\\).\nNote that the error means that you can't fully recover \\(m\\) from \\(c\\); your answer will be off by some\nmultiple of 2. However, it turns out that, for this specific use case,\nthis is fine.\n\nThe relinearization key consists of a set of vectors which, when\ninner-producted (modulo \\(p\\)) with the\nkey \\(k\\), give values of the form\n\\(k_i * k_j * 2^d + 2e\\) (mod \\(p\\)), one such vector for every possible\ntriple \\((i, j, d)\\), where \\(i\\) and \\(j\\) are indices in the key and \\(d\\) is an exponent where \\(2^d < p\\) (note: if the key has length\n\\(n\\), there would be \\(n^2 * log(p)\\) values in the\nrelinearization key; make sure you understand why before\ncontinuing).\n\nExample assuming p = 15 and k has length 2. Formally, enc(x)\nhere means \"outputs x+2e if inner-producted with k\".\n\nNow, let us take a step back and look again at our goal. We have a\nciphertext which, if decrypted with \\(k\n\\otimes k\\), gives \\(m_1 *\nm_2\\). We want a ciphertext which, if decrypted with\n\\(k\\), gives \\(m_1 * m_2\\). We can do this with the\nrelinearization key. Notice that the decryption equation \\(<ct_1 \\otimes ct_2, k \\otimes k>\\) is\njust a big sum of terms of the form \\((ct_{1_i} * ct_{2_j}) * k_p * k_q\\).\n\nAnd what do we have in our relinearization key? A bunch of elements\nof the form \\(2^d * k_p * k_q\\),\nnoisy-encrypted under \\(k\\), for every\npossible combination of \\(p\\) and \\(q\\)! Having all the powers of two in our\nrelinearization key allows us to generate any \\((ct_{1_i} * ct_{2_j}) * k_p * k_q\\) by just\nadding up \\(\\le log(p)\\) powers of two\n(eg. 13 = 8 + 4 + 1) together for each \\((p,\nq)\\) pair.\n\nFor example, if \\(ct_1 = [1, 2]\\)\nand \\(ct_2 = [3, 4]\\), then \\(ct_1 \\otimes ct_2 = [3, 4, 6, 8]\\), and\n\\(enc(<ct_1 \\otimes ct_2, k \\otimes k>)\n= enc(3k_1k_1 + 4k_1k_2 + 6k_2k_1 + 8k_2k_2)\\) could be computed\nvia:\n\n\\[enc(k_1 * k_1) + enc(k_1 * k_1 * 2)  +\nenc(k_1 * k_2 * 4)  + \\]\n\n\\[enc(k_2 * k_1 * 2) + enc(k_2 * k_1 *\n4)  + enc(k_2 * k_2 * 8) \\]\n\nNote that each noisy-encryption in the relinearization key has some\neven error \\(2e\\), and the equation\n\\(<ct_1 \\otimes ct_2, k \\otimes\nk>\\) itself has some error: if \\(<ct_1, k> = 2e_1 + m_1\\) and \\(<ct_2 + k> = 2e_2 + m_2\\), then \\(<ct_1 \\otimes ct_2, k \\otimes k> =\\)\n\\(<ct_1, k> * <ct_2 + k>\n=\\) \\(2(2e_1e_2 + e_1m_2 + e_2m_1) +\nm_1m_2\\). But this total error is still (relatively) small (\\(2e_1e_2 + e_1m_2 + e_2m_1\\) plus \\(n^2 * log(p)\\) fixed-size errors from the\nrealinearization key), and the error is even, and so the result of this\ncalculation still gives a value which, when inner-producted with \\(k\\), gives \\(m_1\n* m_2 + 2e'\\) for some \"combined error\" \\(e'\\).\n\nThe broader technique we used here is a common trick in homomorphic\nencryption: provide pieces of the key encrypted under the key itself (or\na different key if you are pedantic about avoiding circular security\nassumptions), such that someone computing on the data can compute the\ndecryption equation, but only in such a way that the output itself is\nstill encrypted. It was used in bootstrapping above, and it's used here;\nit's best to make sure you mentally understand what's going on in both\ncases.\n\nThis new ciphertext has considerably more error in it: the \\(n^2 * log(p)\\) different errors from the\nportions of the relinearization key that we used, plus the \\(2(2e_1e_2 + e_1m_2 + e_2m_1)\\) from the\noriginal outer-product ciphertext. Hence, the new ciphertext still does\nhave quadratically larger error than the original ciphertexts, and so we\nstill haven't solved the problem that the error blows up too quickly. To\nsolve this, we move on to another trick...\n\n## Modulus switching\n\nHere, we need to understand an important algebraic fact. A ciphertext\nis a vector \\(ct\\), such that \\(<ct, k> = m+2e\\), where \\(m \\in \\{0,1\\}\\). But we can also look at\nthe ciphertext from a different \"perspective\": consider \\(\\frac{ct}{2}\\) (modulo \\(p\\)). \\(<\\frac{ct}{2}, k> = \\frac{m}{2} +\ne\\), where \\(\\frac{m}{2} \\in\n\\{0,\\frac{p+1}{2}\\}\\). Note that because (modulo \\(p\\)) \\((\\frac{p+1}{2})*2 = p+1 = 1\\), division by\n2 (modulo \\(p\\)) maps \\(1\\) to \\(\\frac{p+1}{2}\\); this is a very convenient\nfact for us.\n\nThe scheme in this section uses both modular division (ie.\nmultiplying by the modular\nmultiplicative inverse) and regular \"rounded down\" integer division;\nmake sure you understand how both work and how they are different from\neach other.\n\nThat is, the operation of dividing by 2 (modulo \\(p\\)) converts small even numbers into small\nnumbers, and it converts 1 into \\(\\frac{p}{2}\\) (rounded up). So if we look\nat \\(\\frac{ct}{2}\\) (modulo \\(p\\)) instead of \\(ct\\), decryption involves computing \\(<\\frac{ct}{2}, k>\\) and seeing if\nit's closer to \\(0\\) or \\(\\frac{p}{2}\\). This \"perspective\" is much\nmore robust to certain kinds of errors, where you know the error is\nsmall but can't guarantee that it's a multiple of 2.\n\nNow, here is something we can do to a ciphertext.\n\n- Start: \\(<ct, k> = \\{0\\ or\\ 1\\} +\n2e\\ (mod\\ p)\\)\n\n- Divide \\(ct\\) by 2 (modulo \\(p\\)): \\(<ct', k> = \\{0\\ or\\ \\frac{p}{2}\\} + e\\\n(mod\\ p)\\)\n\n- Multiply \\(ct'\\) by \\(\\frac{q}{p}\\) using \"regular\nrounded-down integer division\": \\(<ct'', k> = \\{0\\ or\\ \\frac{q}{2}\\} +\ne' + e_2\\ (mod\\ q)\\)\n\n- Multiply \\(ct''\\) by 2\n(modulo \\(q\\)): \\(<ct''', k> = \\{0\\ or\\ 1\\} +\n2e' + 2e_2\\ (mod\\ q)\\)\n\nStep 3 is the crucial one: it converts a ciphertext under modulus\n\\(p\\) into a ciphertext under modulus\n\\(q\\). The process just involves\n\"scaling down\" each element of \\(ct'\\) by multiplying by \\(\\frac{q}{p}\\) and rounding down, eg. \\(floor(56 * \\frac{15}{103}) = floor(8.15533..) =\n8\\).\n\nThe idea is this: if \\(<ct', k> =\nm*\\frac{p}{2} + e\\ (mod\\ p)\\), then we can interpret this as\n\\(<ct', k> = p(z + \\frac{m}{2}) +\ne\\) for some integer \\(z\\).\nTherefore, \\(<ct' * \\frac{q}{p}, k>\n= q(z + \\frac{m}{2}) + e*\\frac{p}{q}\\). Rounding adds error, but\nonly a little bit (specifically, up to the size of the values in \\(k\\), and we can make the values in \\(k\\) small without sacrificing security).\nTherefore, we can say \\(<ct' *\n\\frac{q}{p}, k> = m*\\frac{q}{2} + e' + e_2\\ (mod\\ q)\\),\nwhere \\(e' = e * \\frac{q}{p}\\), and\n\\(e_2\\) is a small error from\nrounding.\n\nWhat have we accomplished? We turned a ciphertext with modulus \\(p\\) and error \\(2e\\) into a ciphertext with modulus \\(q\\) and error \\(2(floor(e*\\frac{p}{q}) + e_2)\\), where the\nnew error is smaller than the original error.\n\nLet's go through the above with an example. Suppose:\n\n- \\(ct\\) is just one value, \\([5612]\\)\n\n- \\(k = [9]\\)\n\n- \\(p = 9999\\) and \\(q = 113\\)\n\n\\(<ct, k> = 5612 * 9 = 50508 = 9999 *\n5 + 2 * 256 + 1\\), so \\(ct\\)\nrepresents the bit 1, but the error is fairly large (\\(e = 256\\)).\n\nStep 2: \\(ct' = \\frac{ct}{2} =\n2806\\) (remember this is modular division; if \\(ct\\) were instead \\(5613\\), then we would have \\(\\frac{ct}{2} = 7806\\)). Checking: \\(<ct', k> = 2806 * 9 = 25254 = 9999 * 2.5\n+ 256.5\\)\n\nStep 3: \\(ct'' = floor(2806 *\n\\frac{113}{9999}) = floor(31.7109...) = 31\\). Checking: \\(<ct'', k> = 279 = 113 * 2.5 -\n3.5\\)\n\nStep 4: \\(ct''' = 31 * 2 =\n62\\). Checking: \\(<ct''', k> = 558 = 113 * 5 - 2 *\n4 + 1\\)\n\nAnd so the bit \\(1\\) is preserved\nthrough the transformation. The crazy thing about this procedure is:\nnone of it requires knowing \\(k\\). Now, an astute reader might\nnotice: you reduced the absolute size of the error (from 256 to\n2), but the relative size of the error remained unchanged, and\neven slightly increased: \\(\\frac{256}{9999}\n\\approx 2.5\\%\\) but \\(\\frac{4}{113}\n\\approx 3.5\\%\\). Given that it's the relative error that causes\nciphertexts to break, what have we gained here?\n\nThe answer comes from what happens to error when you multiply\nciphertexts. Suppose that we start with a ciphertext \\(x\\) with error 100, and modulus \\(p = 10^{16} - 1\\). We want to repeatedly\nsquare \\(x\\), to compute \\((((x^2)^2)^2)^2 = x^{16}\\). First, the\n\"normal way\":\n\nThe error blows up too quickly for the computation to be possible.\nNow, let's do a modulus reduction after every multiplication. We assume\nthe modulus reduction is imperfect and increases error by a factor of\n10, so a 1000x modulo reduction only reduces error from 10000 to 100\n(and not to 10):\n\nThe key mathematical idea here is that the factor by which\nerror increases in a multiplication depends on the absolute size of the\nerror, and not its relative size, and so if we keep doing modulus\nreductions to keep the error small, each multiplication only increases\nthe error by a constant factor. And so, with a \\(d\\) bit modulus (and hence \\(\\approx 2^d\\) room for \"error\"), we can do\n\\(O(d)\\) multiplications! This is\nenough to bootstrap.\n\n## Another technique: matrices\n\nAnother technique (see Gentry, Sahai, Waters\n(2013)) for fully homomorphic encryption involves matrices: instead\nof representing a ciphertext as \\(ct\\)\nwhere \\(<ct, k> = 2e + m\\), a\nciphertext is a matrix, where \\(k * CT = k * m\n+ e\\) (\\(k\\), the key, is still\na vector). The idea here is that \\(k\\)\nis a \"secret near-eigenvector\" - a secret vector which, if you multiply\nthe matrix by it, returns something very close to either zero or the key\nitself.\n\nThe fact that addition works is easy: if \\(k * CT_1 = m_1 * k + e_1\\) and \\(k * CT_2 = m_2 * k + e_2\\), then \\(k * (CT_1 + CT_2) = (m_1 + m_2) * k + (e_1 +\ne_2)\\). The fact that multiplication works is also easy:\n\n\\(k * CT_1 * CT_2\\) \\(= (m_1 * k + e_1) * CT_2\\) \\(= m_1 * k * CT_2 + e_1 * CT_2\\) \\(= m_1 * m_2 * k + m_1 * e_2 + e_1 *\nCT_2\\)\n\nThe first term is the \"intended term\"; the latter two terms are the\n\"error\". That said, notice that here error does blow up quadratically\n(see the \\(e_1 * CT_2\\) term; the size\nof the error increases by the size of each ciphertext element, and the\nciphertext elements also square in size), and you do need some clever\ntricks for avoiding this. Basically, this involves turning ciphertexts\ninto matrices containing their constituent bits before multiplying, to\navoid multiplying by anything higher than 1; if you want to see how this\nworks in detail I recommend looking at my code: https://github.com/vbuterin/research/blob/master/matrix_fhe/matrix_fhe.py#L121\n\nIn addition, the code there, and also https://github.com/vbuterin/research/blob/master/tensor_fhe/homomorphic_encryption.py#L186,\nprovides simple examples of useful circuits that you can build out of\nthese binary logical operations; the main example is for adding numbers\nthat are represented as multiple bits, but one can also make circuits\nfor comparison (\\(<\\), \\(>\\), \\(=\\)), multiplication, division, and many\nother operations.\n\nSince 2012-13, when these algorithms were created, there have been\nmany optimizations, but they all work on top of these basic frameworks.\nOften, polynomials are used instead of integers; this is called ring\nLWE. The major challenge is still efficiency: an operation involving\na single bit involves multiplying entire matrices or performing an\nentire relinearization computation, a very high overhead. There are\ntricks that allow you to perform many bit operations in a single\nciphertext operation, and this is actively being worked on and\nimproved.\n\nWe are quickly getting to the point where many of the applications of\nhomomorphic encryption in privacy-preserving computation are starting to\nbecome practical. Additionally, research in the more advanced\napplications of the lattice-based cryptography used in homomorphic\nencryption is rapidly progressing. So this is a space where some things\ncan already be done today, but we can hopefully look forward to much\nmore becoming possible over the next decade.",
    "contentLength": 29345,
    "summary": "FHE allows computation on encrypted data; early schemes used modular arithmetic with error that grows with operations.",
    "detailedSummary": {
      "theme": "Vitalik explores the mathematical foundations and practical applications of fully homomorphic encryption, explaining how it enables computation on encrypted data without revealing the underlying information.",
      "summary": "Vitalik provides a comprehensive technical overview of fully homomorphic encryption (FHE), starting with the fundamental promise that it allows third parties to perform computations on encrypted data without being able to decrypt it themselves. He explains the progression from partially homomorphic encryption (supporting only additions or multiplications) to somewhat homomorphic encryption (limited multiplications) to fully homomorphic encryption (unlimited operations). Vitalik walks through several encryption schemes, beginning with a simple somewhat-homomorphic algorithm based on the approximate GCD problem, then progressing to more advanced techniques using Learning with Errors (LWE) that represent keys and ciphertexts as vectors rather than integers.\n\nThe post delves into sophisticated techniques like bootstrapping (refreshing ciphertexts to reduce accumulated error), relinearization (managing the quadratic growth of ciphertext size during multiplication), and modulus switching (controlling error growth through careful mathematical transformations). Vitalik also covers matrix-based approaches and explains how these methods can be combined to achieve true fully homomorphic encryption. He emphasizes the practical applications in blockchain technology, including privacy-preserving light clients, stealth address protocols, and data-sharing marketplaces, while acknowledging that efficiency remains a significant challenge despite recent optimizations.",
      "takeaways": [
        "Fully homomorphic encryption allows computation on encrypted data without decryption, with applications in privacy-preserving blockchain protocols and data marketplaces",
        "The progression from partially to fully homomorphic encryption involves overcoming the challenge of error accumulation during multiplication operations",
        "Bootstrapping enables refreshing ciphertexts by homomorphically computing the decryption function, effectively resetting accumulated errors",
        "Advanced techniques like relinearization and modulus switching are essential for managing ciphertext size growth and controlling error propagation",
        "While FHE is becoming more practical, efficiency remains a major challenge as single-bit operations require computationally expensive matrix operations or relinearization procedures"
      ],
      "controversial": [
        "The assumption of 'circular security' when using the same key for bootstrapping multiple times, which Vitalik suggests is likely safe despite being difficult to formally prove"
      ]
    }
  },
  {
    "id": "general-2020-04-30-round5",
    "title": "Gitcoin Grants Round 5 Retrospective",
    "date": "2020-04-30",
    "category": "governance",
    "url": "https://vitalik.eth.limo/general/2020/04/30/round5.html",
    "path": "general/2020/04/30/round5.html",
    "content": "- \n\n- \n\n  \n    \n\n  \n  \n  Dark Mode Toggle\n  \n    \n\n  \n\n##  Gitcoin Grants Round 5 Retrospective \n\n 2020 Apr 30 \nSee all posts\n\n \n \n\n Gitcoin Grants Round 5 Retrospective \n\nSpecial thanks to Kevin Owocki and Frank Chen for help and\nreview\n\nRound 5 of Gitcoin Grants has just finished, with $250,000 of\nmatching split between tech, media, and the new (non-Ethereum-centric)\ncategory of \"public health\". In general, it seems like the mechanism and\nthe community are settling down into a regular rhythm. People know what\nit means to contribute, people know what to expect, and the results\nemerge in a relatively predictable pattern - even if which specific\ngrants get the most funds is not so easy to predict.\n\n## Stability of income\n\nSo let's go straight into the analysis. One important property worth\nlooking at is stability of income across rounds: do projects that do\nwell in round N also tend to do well in round N+1? Stability of income\nis very important if we want to support an ecosystem of \"quadratic\nfreelancers\": we want people to feel comfortable relying on their income\nknowing that it will not completely disappear the next round. On the\nother hand, it would be harmful if some recipients became completely\nentrenched, with no opportunity for new projects to come in and compete\nfor the pot, so there is a need for a balance.\n\nOn the media side, we do see some balance between stability and\ndynamism:\n\nWeek in Ethereum had the highest total amount received in both the previous round and the\ncurrent round. EthHub and Bankless are also near the top in both the\ncurrent round and the previous round. On the other hand,\nAntiprosynthesis, the (beloved? notorious? famous?) Twitter\ninfo-warrior, has decreased from $13,813 to $5,350, while Chris\nBlec's YouTube channel has increased from $5,851 to\n$12,803. So some churn, but also some continuity between rounds.\n\nOn the tech side, we see much more churn in the winners, with a less\nclear relationship between income last round and income this round:\n\nLast round, the winner was Tornado Cash, claiming $30,783; this\nround, they are down to $8,154. This round, the three roughly-even\nwinners are Samczsun\n($4,631 contributions + $15,704 match = $20,335 total), Arboreum ($16,084\ncontributions + $9,046 match = $25,128 total) and 1inch.exchange ($58,566\ncontributions + $7,893 match = $66,459 total), in the latter case the\nbulk coming from one contribution:\n\nIn the previous round, those three winners were not even in the top\nten, and in some cases not even part of Gitcoin Grants at all.\n\nThese numbers show us two things. First, large parts of the Gitcoin\ncommunity seem to be in the mindset of treating grants not as a question\nof \"how much do you deserve for your last two months of work?\", but\nrather as a one-off reward for years of contributions in the past. This\nwas one of the strongest rebuttals that I received to my criticism of Antiprosynthesis\nreceiving $13,813 in the last round: that the people who contributed\nto that award did not see it as two months' salary, but rather as a\nreward for years of dedication and work for the Ethereum ecosystem. In\nthe next round, contributors were content that the debt was sufficiently\nrepaid, and so they moved on to give a similar gift of appreciation and\ngratitude to Chris Blec.\n\nThat said, not everyone contributes in this way. For example, Prysm\ngot $7,966 last round and $8,033 this round, and Week in Ethereum is\nconsistently well-rewarded ($16,727 previous, $12,195 current), and\nEthHub saw less stability but still kept half its income ($13,515\nprevious, $6,705 current) even amid a 20% drop to the matching pool size\nas some funds were redirected to public health. So there definitely are\nsome contributors that are getting almost a reasonable monthly\nsalary from Gitcoin Grants (yes, even these amounts are all serious\nunderpayment, but remember that the pool of funds Gitcoin Grants has to\ndistribute in the first place is quite small, so there's no allocation\nthat would not seriously underpay most people; the hope is that\nin the future we will find ways to make the matching pot grow\nbigger).\nWhy didn't\nmore people use recurring contributions?\n\nOne feature that was tested this round to try to improve stability\nwas recurring contributions: users could choose to split their\ncontribution among multiple rounds. However, the feature was not used\noften: out of over 8,000 total contributions, only 120 actually made\nrecurring contributions. I can think of three possible explanations for\nthis:\n\n- People just don't want to give recurring contributions; they\ngenuinely prefer to freshly rethink who they are supporting every\nround.\n\n- People would be willing to give more recurring contributions, but\nthere is some kind of \"market failure\" stopping them; that is, it's\ncollectively optimal for everyone to give more recurring contributions,\nbut it's not any individual contributor's interest to be the first to do\nso.\n\n- There's some UI inconveniences or other \"incidental\" obstacles\npreventing recurring contributions.\n\nIn a recent call with the Gitcoin team, hypothesis (3) was mentioned\nfrequently. A specific issue was that people were worried about making\nrecurring contributions because they were concerned whether or not the\nmoney that they lock up for a recurring contribution would be safe.\nImproving the payment system and notification workflow may help with\nthis. Another option is to move away from explicit \"streaming\" and\ninstead simply have the UI provide an option for repeating the last\nround's contributions and making edits from there.\n\nHypothesis (1) also should be taken seriously; there's genuine value\nin preventing ossification and allowing space for new entrants. But I\nwant to zoom in particularly on hypothesis (2), the coordination failure\nhypothesis.\n\nMy explanation of hypothesis (2) starts, interestingly enough, with a\ndefense of (1): why ossification is genuinely a risk. Suppose that there\nare two projects, A and B, and suppose that they are equal quality. But\nA already has an established base of contributors; B does not (we'll say\nfor illustration it only has a few existing contributors). Here's how\nmuch matching you are contributing by participating in each project:\n\nContributing to A\nContributing to B\n\nClearly, you have more impact by supporting A, and so A gets even\nmore contributors and B gets fewer; the rich get richer. Even if project\nB was somewhat better, the greater impact from supporting A\ncould still create a lock-in that reinforces A's position. The current\neveryone-starts-from-zero-in-each-round mechanism greatly limits this\ntype of entrenchment, because, well, everyone's matching gets reset and\nstarts from zero.\n\nHowever, a very similar effect also is the cause behind the market\nfailure preventing stable recurring contributions, and the\nevery-round-reset actually exacerbates it. Look at the same\npicture above, except instead of thinking of A and B as two\ndifferent projects, think of them as the same project in the\ncurrent round and in the next round.\n\nWe simplify the model as follows. An individual has two choices:\ncontribute $10 in the current round, or contribute $5 in the current\nround and $5 in the next round. If the matchings in the two rounds were\nequal, then the latter option would actually be more favorable: because\nthe matching is proportional to the square root of the donation size,\nthe former might give you eg. a $200 match now, but the latter would\ngive you $141 in the current round + $141 in the next round = $282. But\nif you see a large mass of people contributing in the current round, and\nyou expect much fewer people to contribute in the second round, then the\nchoice is not $200 versus $141 + $141, it might be $200 versus $141 +\n$5. And so you're better off joining the current round's frenzy. We can\nmathematically analyze the equilibrium:\n\nSo there is a substantial region within which the bad equilibrium of\neveryone concentrating is sticky: if more than about 3/4 of contributors\nare expected to concentrate, it seems in your interest to also\nconcentrate. A mathematically astute reader may note that there is always some intermediate\nstrategy that involves splitting but at a ratio different from\n50/50, which you can prove performs better than either full\nconcentrating or the even split, but here we get back to\nhypothesis (3) above: the UI doesn't offer such a complex menu of\nchoices, it just offers the choice of a one-time contribution or a\nrecurring contribution, so people pick one or the other.\n\nHow might we fix this? One option is to add a bit of continuity to\nmatching ratios: when computing pairwise matches, match against not just\nthe current round's contributors but, say, 1/3 of the previous round's\ncontributors as well:\n\nThis makes some philosophical sense: the objective of quadratic\nfunding is to subsidize contributions to projects that are detected to\nbe public goods because multiple people have contributed to them, and\ncontributions in the previous round are certainly also evidence of a\nproject's value, so why not reuse those? So here, moving away from\neveryone-starts-from-zero toward this partial carryover of matching\nratios would mitigate the round concentration effect - but, of course,\nit would exacerbate the risk of entrenchment. Hence, some\nexperimentation and balance may be in order. A broader philosophical\nquestion is, is there really a deep inherent tradeoff between risk of\nentrenchment and stability of income, or is there some way we could get\nboth?\nResponses to negative\ncontributions\n\nThis round also introduced negative contributions, a feature proposed\nin my review of the previous\nround. But as with recurring contributions, very few people made\nnegative contributions, to the point where their impact on the results\nwas negligible. Also, there was active\nopposition\nto negative\ncontributions:\n\nSource:\nhonestly I have no idea, someone else sent it to me and they forgot\nwhere they found it. Sorry :(\n\nThe main source of opposition was basically what I predicted in the\nprevious round. Adding a mechanism that allows people to penalize\nothers, even if deservedly so, can have tricky and easily harmful social\nconsequences. Some people even opposed the negative contribution\nmechanism to the point where they took care to give positive\ncontributions to everyone who received a negative contribution.\n\nHow do we respond? To me it seems clear that, in the long run,\nsome mechanism of filtering out bad projects, and ideally\ncompensating for overexcitement into good projects, will have to exist.\nIt doesn't necessarily need to be integrated as a symmetric part of the\nQF, but there does need to be a filter of some form. And this mechanism,\nwhatever form it will take, invariably opens up the possibility of the\nsame social dynamics. So there is a challenge that will have to be\nsolved no matter how we do it.\n\nOne approach would be to hide more information: instead of just\nhiding who made a negative contribution, outright hide the fact\nthat a negative contribution was made. Many opponents of negative\ncontributions explicitly indicated that they would be okay (or at least\nmore okay) with such a model. And indeed (see the next section), this is\na direction we will have to go anyway. But it would come at a cost -\neffectively hiding negative contributions would mean not giving as much\nreal-time feedback into what projects got how much funds.\nStepping up the fight\nagainst collusion\n\nThis round saw much larger-scale attempts at collusion:\n\nIt does seem clear that, at current scales, stronger protections\nagainst manipulation are goingto be required. The first thing that can\nbe done is adding a stronger identity verification layer than Github\naccounts; this is something that the Gitcoin team is already working on.\nThere is definitely a complex tradeoff between security and\ninclusiveness to be worked through, but it is not especially difficult\nto implement a first version. And if the identity problem is solved to a\nreasonable extent, that will likely be enough to prevent collusion at\ncurrent scales. But in the longer term, we are going to need protection\nnot just against manipulating the system by making many fake accounts,\nbut also against collusion via bribes (explicit and implicit).\n\nMACI\nis the solution that I proposed (and Barry Whitehat and co are\nimplementing) to solve this problem. Essentially, MACI is a\ncryptographic construction that allows for contributions to projects to\nhappen on-chain in a privacy-preserving, encrypted form, that allows\nanyone to cryptographically verify that the mechanism is being\nimplemented correctly, but prevents participants from being\nable to prove to a third party that they made any particular\ncontribution. Unprovability means that if someone tries to bribe others\nto contribute to their project, the bribe recipients would have no way\nto prove that they actually contributed to that project, making the\nbribe unenforceable. Benign \"collusion\" in the form of friends and\nfamily supporting each other would still happen, as people would not\neasily lie to each other at such small scales, but any broader collusion\nwould be very difficult to maintain.\n\nHowever, we do need to think through some of the second-order\nconsequences that integrating MACI would introduce. The biggest\nblessing, and curse, of using MACI is that contributions become hidden.\nIdentities necessarily become hidden, but even the exact timing of\ncontributions would need to be hidden to prevent deanonymization through\ntiming (to prove that you contributed, make the total amount\njump up between 17:40 and 17:42 today). Instead, for example, totals\ncould be provided and updated once per day. Note that as a corollary\nnegative contributions would be hidden as well; they would only appear\nif they exceeded all positive contributions for an entire day (and if\neven that is not desired then the mechanism for when balances are\nupdated could be tweaked to further hide downward changes).\n\nThe challenge with hiding contributions is that we lose the \"social\nproof\" motivator for contributing: if contributions are unprovable you\ncan't as easily publicly brag about a contribution you made. My best\nproposal for solving this is for the mechanism to publish one extra\nnumber: the total amount that a particular participant\ncontributed (counting only projects that have received at least 10\ncontributors to prevent inflating one's number by self-dealing).\nIndividuals would then have a generic \"proof-of-generosity\" that they\ncontributed some specific total amount, and could publicly\nstate (without proof) what projects it was that they supported. But this\nis all a significant change to the user experience that will require\nmultiple rounds of experimentation to get right.\n\n## Conclusions\n\nAll in all, Gitcoin Grants is establishing itself as a significant\npillar of the Ethereum ecosystem that more and more projects are relying\non for some or all of their support. While it has a relatively low\namount of funding at present, and so inevitably underfunds almost\neverything it touches, we hope that over time we'll continue to see\nlarger sources of funding for the matching pools appear. One option is\nMEV\nauctions, another is that new or existing token projects looking to\ndo airdrops could provide the tokens to a matching pool. A third is\ntransaction fees of various applications. With larger amounts of\nfunding, Gitcoin Grants could serve as a more significant funding stream\n- though to get to that point, further iteration and work on fine-tuning\nthe mechanism will be required.\n\nAdditionally, this round saw Gitcoin Grants' first foray into\napplications beyond Ethereum with the health section. There is growing\ninterest in quadratic funding from local government bodies and other\nnon-blockchain groups, and it would be very valuable to see quadratic\nfunding more broadly deployed in such contexts. That said, there are\nunique challenges there too. First, there's issues around onboarding\npeople who do not already have cryptocurrency. Second, the Ethereum\ncommunity is naturally expert in the needs of the Ethereum community,\nbut neither it nor average people are expert in, eg. medical support for\nthe coronavirus pandemic. We should expect quadratic funding to perform\nworse when the participants are not experts in the domain they're being\nasked to contribute to. Will non-blockchain uses of QF focus on domains\nwhere there's a clear local community that's expert in its own needs, or\nwill people try larger-scale deployments soon? If we do see larger-scale\ndeployments, how will those turn out? There's still a lot of questions\nto be answered.",
    "contentLength": 16660,
    "summary": "Gitcoin Grants Round 5 distributed $250k with tech categories showing high winner volatility while media grants remained more stable.",
    "detailedSummary": {
      "theme": "Analysis of Gitcoin Grants Round 5 examining funding stability, contributor behavior, and mechanisms to prevent manipulation while scaling quadratic funding.",
      "summary": "Vitalik analyzes the fifth round of Gitcoin Grants, which distributed $250,000 across tech, media, and public health categories. He examines funding stability across rounds, finding that media projects show more consistent income while tech projects exhibit significant churn in winners. Vitalik identifies a coordination failure problem where contributors concentrate donations in single rounds rather than spreading them across multiple rounds, despite recurring contributions being more mathematically optimal. He proposes solutions including partial carryover of matching ratios from previous rounds. The analysis also covers the limited adoption of negative contributions due to social concerns, and the growing need for stronger anti-collusion measures as manipulation attempts scale up. Vitalik discusses implementing MACI (Minimal Anti-Collusion Infrastructure) to prevent bribery through cryptographic privacy, though this would hide contribution details and reduce social proof incentives. He concludes by noting Gitcoin's growing importance to the Ethereum ecosystem and potential expansion beyond blockchain applications, while acknowledging challenges around expertise domains and onboarding non-crypto users.",
      "takeaways": [
        "Funding stability varies significantly between categories, with media projects showing more consistency than tech projects in receiving grants across rounds",
        "A coordination failure prevents optimal use of recurring contributions, as contributors concentrate donations in single rounds rather than spreading them for better mathematical outcomes",
        "Negative contributions saw minimal adoption due to social concerns about creating punitive dynamics, despite their potential utility for filtering bad projects",
        "Scaling quadratic funding requires stronger anti-collusion measures, with MACI proposed as a cryptographic solution to prevent bribery while maintaining verification",
        "Expanding quadratic funding beyond Ethereum faces challenges including cryptocurrency onboarding and the expertise problem when participants lack domain knowledge"
      ],
      "controversial": [
        "The implementation of negative contributions as a mechanism to penalize projects, which faced active community opposition due to concerns about harmful social dynamics",
        "The proposal to use MACI (cryptographic privacy) which would hide contribution details and reduce social proof, potentially diminishing participation incentives",
        "The suggestion that some grant recipients like Antiprosynthesis were overfunded, which Vitalik notes received pushback from the community"
      ]
    }
  },
  {
    "id": "general-2020-03-21-garbled",
    "title": "A Quick Garbled Circuits Primer",
    "date": "2020-03-21",
    "category": "math",
    "url": "https://vitalik.eth.limo/general/2020/03/21/garbled.html",
    "path": "general/2020/03/21/garbled.html",
    "content": "- \n\n- \n\n  \n    \n\n  \n  \n  Dark Mode Toggle\n  \n    \n\n  \n\n##  A Quick Garbled Circuits Primer \n\n 2020 Mar 21 \nSee all posts\n\n \n \n\n A Quick Garbled Circuits Primer \n\nSpecial thanks to Dankrad Feist for review\n\nGarbled\ncircuits are a quite old, and surprisingly simple, cryptographic\nprimitive; they are quite possibly the simplest form of general-purpose\n\"multi-party computation\" (MPC) to wrap your head around.\n\nHere is the usual setup for the scheme:\n\n- Suppose that there are two parties, Alice and Bob, who want to\ncompute some function f(alice_inputs, bob_inputs), which\ntakes inputs from both parties. Alice and Bob want to both learn the\nresult of computing f, but Alice does not want Bob to learn\nher inputs, and Bob does not want Alice to learn his inputs. Ideally,\nthey would both learn nothing except for just the output of\nf.\n\n- Alice performs a special procedure (\"garbling\") to encrypt a circuit\n(meaning, a set of AND, OR... gates) which evaluates the function\nf. She passes along inputs, also encrypted in a way that's\ncompatible with the encrypted circuit, to Bob.\n\n- Bob uses a technique called \"1-of-2 oblivious transfer\" to learn the\nencrypted form of his own inputs, without letting Alice know which\ninputs he obtained.\n\n- Bob runs the encrypted circuit on the encrypted data and gets the\nanswer, and passes it along to Alice.\n\nExtra cryptographic wrappings can be used to protect the scheme\nagainst Alice and Bob sending wrong info and giving each other an\nincorrect answer; we won't go into those here for simplicity, though it\nsuffices to say \"wrap a ZK-SNARK around everything\" is one (quite heavy\nduty and suboptimal!) solution that works fine.\n\nSo how does the basic scheme work? Let's start with a circuit:\n\nThis is one of the simplest examples of a not-completely-trivial\ncircuit that actually does something: it's a two-bit adder. It takes as\ninput two numbers in binary, each with two bits, and outputs the\nthree-bit binary number that is the sum.\n\nNow, let's encrypt the circuit. First, for every input, we randomly\ngenerate two \"labels\" (think: 256-bit numbers): one to represent that\ninput being 0 and the other to represent that input being 1. Then we\nalso do the same for every intermediate wire, not including the output\nwires. Note that this data is not part of the \"garbling\" that Alice\nsends to Bob; so far this is just setup.\n\nNow, for every gate in the circuit, we do the following. For every\ncombination of inputs, we include in the \"garbling\" that Alice provides\nto Bob the label of the output (or if the label of the output is a\n\"final\" output, the output directly) encrypted with a key generated by\nhashing the input labels that lead to that output together. For\nsimplicity, our encryption algorithm can just be\nenc(out, in1, in2) = out + hash(k, in1, in2) where\nk is the index of the gate (is it the first gate in the\ncircuit, the second, the third?). If you know the labels of both inputs,\nand you have the garbling, then you can learn the label of the\ncorresponding output, because you can just compute the corresponding\nhash and subtract it out.\n\nHere's the garbling of the first XOR gate:\n\nInputs\nOutput\nEncoding of output\n\n00\n0\n0 + hash(1, 6816, 6529)\n\n01\n1\n1 + hash(1, 6816, 4872)\n\n10\n1\n1 + hash(1, 8677, 6529)\n\n11\n0\n0 + hash(1, 8677, 4872)\n\nNotice that we are including the (encrypted forms of) 0 and 1\ndirectly, because this XOR gate's outputs are directly final outputs of\nthe program. Now, let's look at the leftmost AND gate:\n\nInputs\nOutput\nEncoding of output\n\n00\n0\n5990 + hash(2, 6816, 6529)\n\n01\n0\n5990 + hash(2, 6816, 4872)\n\n10\n0\n5990 + hash(2, 8677, 6529)\n\n11\n1\n1921 + hash(2, 8677, 4872)\n\nHere, the gate's outputs are just used as inputs to other gates, so\nwe use labels instead of bits to hide these intermediate bits from the\nevaluator.\n\nThe \"garbling\" that Alice would provide to Bob is just everything in\nthe third column for each gate, with the rows of each gate re-ordered\n(to avoid revealing whether a given row corresponds to a 0 or a 1 in any\nwire). To help Bob learn which value to decrypt for each gate, we'll use\na particular order: for each gate, the first row becomes the row where\nboth input labels are even, in the second row the second label is odd,\nin the third row the first label is odd, and in the fourth row both\nlabels are odd (we deliberately chose labels earlier so that each gate\nwould have an even label for one output and an odd label for the other).\nWe garble every other gate in the circuit in the same way.\n\nAll in all, Alice sends to Bob four ~256 bit numbers for each gate in\nthe circuit. It turns out that four is far from optimal; see here for\nsome optimizations on how to reduce this to three or even two numbers\nfor an AND gate and zero (!!) for an XOR gate. Note that these\noptimizations do rely on some changes, eg. using XOR instead of addition\nand subtraction, though this should be done anyway for security.\n\nWhen Bob receives the circuit, he asks Alice for the labels\ncorresponding to her input, and he uses a protocol called \"1-of-2\noblivious transfer\" to ask Alice for the labels corresponding to his own\ninput without revealing to Alice what his input is. He then goes through\nthe gates in the circuit one by one, uncovering the output wires of each\nintermediate gate.\n\nSuppose Alice's input is the two left wires and she gives (0, 1), and\nBob's input is the two right wires and he gives (1, 1). Here's the\ncircuit with labels again:\n\n- At the start, Bob knows the labels 6816, 3621, 4872, 5851\n\n- Bob evaluates the first gate. He knows 6816 and 4872, so he can\nextract the output value corresponding to (1, 6816, 4872) (see the table\nabove) and extracts the first output bit, 1\n\n- Bob evaluates the second gate. He knows 6816 and 4872, so he can\nextract the output value corresponding to (2, 6816, 4872) (see the table\nabove) and extracts the label 5990\n\n- Bob evaluates the third gate (XOR). He knows 3621 and 5851, and\nlearns 7504\n\n- Bob evaluates the fourth gate (OR). He knows 3621 and 5851, and\nlearns 6638\n\n- Bob evaluates the fifth gate (AND). He knows 3621 and 5851, and\nlearns 7684\n\n- Bob evaluates the sixth gate (XOR). He knows 5990 and 7504, and\nlearns the second output bit, 0\n\n- Bob evaluates the seventh gate (AND). He knows 5990 and 6638, and\nlearns 8674\n\n- Bob evaluates the eighth gate (OR). He knows 8674 and 7684, and\nlearns the third output bit, 1\n\nAnd so Bob learns the output: 101. And in binary 10 + 11 actually\nequals 101 (the input and output bits are both given in\nsmallest-to-greatest order in the circuit, which is why Alice's input 10\nis represented as (0, 1) in the circuit), so it worked!\n\nNote that addition is a fairly pointless use of garbled circuits,\nbecause Bob knowing 101 can just subtract out his own input and get 101\n- 11 = 10 (Alice's input), breaking privacy. However, in general garbled\ncircuits can be used for computations that are not reversible, and so\ndon't break privacy in this way (eg. one might imagine a computation\nwhere Alice's input and Bob's input are their answers to a personality\nquiz, and the output is a single bit that determines whether or not the\nalgorithm thinks they are compatible; that one bit of information won't\nlet Alice or Bob know anything about each other's individual quiz\nanswers).\n\n## 1 of 2 Oblivious Transfer\n\nNow let us talk more about 1-of-2 oblivious transfer, this technique\nthat Bob used to obtain the labels from Alice corresponding to his own\ninput. The problem is this. Focusing on Bob's first input bit (the\nalgorithm for the second input bit is the same), Alice has a label\ncorresponding to 0 (6529), and a label corresponding to 1 (4872). Bob\nhas his desired input bit: 1. Bob wants to learn the correct label\n(4872) without letting Alice know that his input bit is 1. The trivial\nsolution (Alice just sends Bob both 6529 and 4872) doesn't work because\nAlice only wants to give up one of the two input labels; if Bob receives\nboth input labels this could leak data that Alice doesn't want to give\nup.\n\nHere is a fairly\nsimple protocol using elliptic curves:\n\n- Alice generates a random elliptic curve point, H.\n\n- Bob generates two points, P1 and P2, with\nthe requirement that P1 + P2 sums to H. Bob\nchooses either P1 or P2 to be\nG * k (ie. a point that he knows the corresponding private\nkey for). Note that the requirement that P1 + P2 = H\nensures that Bob has no way to generate P1 and\nP2 such that he knows the corresponding private key for.\nThis is because if P1 = G * k1 and P2 = G * k2\nwhere Bob knows both k1 and k2, then\nH = G * (k1 + k2), so that would imply Bob can extract the\ndiscrete logarithm (or \"corresponding private key\") for H,\nwhich would imply all of elliptic curve cryptography is broken.\n\n- Alice confirms P1 + P2 = H, and encrypts\nv1 under P1 and v2 under\nP2 using some standard public key encryption scheme (eg. El-Gamal).\nBob is only able to decrypt one of the two values, because he knows the\nprivate key corresponding to at most one of (P1, P2), but\nAlice does not know which one.\n\nThis solves the problem; Bob learns one of the two wire labels\n(either 6529 or 4872), depending on what his input bit is, and Alice\ndoes not know which label Bob learned.\n\n## Applications\n\nGarbled circuits are potentially useful for many more things than\njust 2-of-2 computation. For example, you can use them to make\nmulti-party computations of arbitrary complexity with an arbitrary\nnumber of participants providing inputs, that can run in a constant\nnumber of rounds of interaction. Generating a garbled circuit is\ncompletely parallelizable; you don't need to finish garbling one gate\nbefore you can start garbling gates that depend on it. Hence, you can\nsimply have a large multi-party computation with many participants\ncompute a garbling of all gates of a circuit and publish the labels\ncorresponding to their inputs. The labels themselves are random and so\nreveal nothing about the inputs, but anyone can then execute the\npublished garbled circuit and learn the output \"in the clear\". See here for a recent\nexample of an MPC protocol that uses garbling as an ingredient.\n\nMulti-party computation is not the only context where this technique\nof splitting up a computation into a parallelizable part that operates\non secret data followed by a sequential part that can be run in the\nclear is useful, and garbled circuits are not the only technique for\naccomplishing this. In general, the literature on randomized encodings\nincludes many more sophisticated techniques. This branch of math is also\nuseful in technologies such as functional\nencryption and obfuscation.",
    "contentLength": 10581,
    "summary": "Garbled circuits enable two parties to compute a function together while keeping their inputs private through encryption of circuit gates and oblivious transfer.",
    "detailedSummary": {
      "theme": "An accessible technical explanation of garbled circuits, a cryptographic primitive that enables secure multi-party computation where parties can jointly compute a function without revealing their private inputs to each other.",
      "summary": "Vitalik explains garbled circuits as a surprisingly simple form of multi-party computation (MPC) where two parties, Alice and Bob, can compute a function using both their inputs without revealing those inputs to each other. The process involves Alice 'garbling' (encrypting) a circuit by creating random labels for each wire's 0 and 1 values, then encrypting the outputs of each gate using hashes of the input labels. Bob uses a technique called '1-of-2 oblivious transfer' to obtain the encrypted labels corresponding to his inputs without Alice knowing which ones he received, then executes the garbled circuit to compute the final result. Vitalik walks through a detailed example using a two-bit adder circuit, showing how Bob can learn that 10 + 11 = 101 in binary without either party learning the other's input values. The post also covers the oblivious transfer protocol using elliptic curves and discusses broader applications beyond simple two-party computation, including how garbled circuits can enable complex multi-party computations that run in constant rounds of interaction and connect to other cryptographic techniques like functional encryption and obfuscation.",
      "takeaways": [
        "Garbled circuits enable secure multi-party computation by allowing parties to jointly compute functions without revealing their private inputs to each other",
        "The technique works by having one party encrypt a circuit with random labels for wire values, while the other party uses oblivious transfer to obtain only the labels corresponding to their actual inputs",
        "The garbling process is completely parallelizable, making it suitable for large-scale multi-party computations with many participants",
        "1-of-2 oblivious transfer using elliptic curves allows a party to learn one of two secret values without the sender knowing which one was learned",
        "Garbled circuits connect to broader cryptographic research areas including randomized encodings, functional encryption, and obfuscation"
      ],
      "controversial": []
    }
  },
  {
    "id": "general-2020-01-28-round4",
    "title": "Review of Gitcoin Quadratic Funding Round 4",
    "date": "2020-01-28",
    "category": "governance",
    "url": "https://vitalik.eth.limo/general/2020/01/28/round4.html",
    "path": "general/2020/01/28/round4.html",
    "content": "- \n\n- \n\n  \n    \n\n  \n  \n  Dark Mode Toggle\n  \n    \n\n  \n\n##  Review of Gitcoin Quadratic Funding Round 4 \n\n 2020 Jan 28 \nSee all posts\n\n \n \n\n Review of Gitcoin Quadratic Funding Round 4 \n\nRound 4 of Gitcoin Grants quadratic funding has just completed, and\nhere are the results:\n\nThe main distinction between round 3 and round 4 was that while round\n3 had only one category, with mostly tech projects and a few outliers\nsuch as EthHub, in round 4 there were two separate categories, one with\na $125,000 matching pool for tech projects, and the other with a $75,000\nmatching pool for \"media\" projects. Media could include documentation,\ntranslation, community activities, news reporting, theoretically pretty\nmuch anything in that category. And while the tech section went about\nlargely without incident, in the new media section the results\nproved to be much more interesting than I could have possibly imagined,\nshedding a new light on deep questions in institutional design and\npolitical science.\nTech: quadratic\nfunding worked great as usual\n\nIn the tech section, the main changes that we see compared to round 3\nare (i) the rise of Tornado Cash and\n(ii) the decline in importance of eth2 clients and the rise of \"utility\napplications\" of various forms. Tornado Cash is a trustless smart\ncontract-based Ethereum mixer. It became popular quickly in recent\nmonths, as the Ethereum community was swept by worries about the\nblockchain's current low\nlevels of privacy and wanted solutions. Tornado Cash amassed an\nincredible $31,200. If they continue receiving such an amount every two\nmonths then this would allow them to pay two people $7,800 per month\neach - meaning that the hoped-for milestone of seeing the first\n\"quadratic freelancer\" may have already been reached! The other major\nwinners included tools like Dappnode,\na software package to help people run nodes, Sablier, a payment streaming\nservice, and DefiZap, which makes\nDeFi services easy to use. The Gitcoin\nSustainability Fund got over $13,000, conclusively resolving my\ncomplaint from last round\nthat they were under-supported. All in all, valuable grants for valuable\nprojects that provide services that the community genuinely needs.\n\nWe can see one major shift this round compared to the previous\nrounds. Whereas in previous rounds, the grants went largely to projects\nlike eth2 clients that were already well-supported, this time the\nlargest grants shifted toward having a different focus from the grants\ngiven by the Ethereum Foundation. The EF has not given grants to\ntornado.cash, and generally limits its grants to application-specific\ntools, Uniswap being a notable exception. The Gitcoin Grants quadratic\nfund, on the other hand, is supporting DeFiZap, Sablier, and many other\ntools that are valuable to the community. This is arguably a positive\ndevelopment, as it allows Gitcoin Grants and the Ethereum Foundation to\ncomplement each other rather than focusing on the same things.\n\nThe one proposed change to the quadratic funding implementation for\ntech that I would favor is a user interface change, that makes it easier\nfor users to commit funds for multiple rounds. This would increase the\nstability of contributions, thereby increasing the stability of\nprojects' income - very important if we want \"quadratic freelancer\" to\nactually be a viable job category!\nMedia: The First\nQuadratic Twitter Freelancer\n\nNow, we get to the new media section. In the first few days of the\nround, the leading recipient of the grants was \"@antiprosynth Twitter account\nactivity\": an Ethereum community member who is very active on twitter\npromoting Ethereum and refuting misinformation from Bitcoin maximalists,\nasking for help from the Gitcoin QF crowd to.... fund his tweeting\nactivities. At its peak, the projected matching going to @antiprosynth exceeded\n$20,000. This naturally proved to be controversial, with many\ncriticizing this move and questioning whether or not a Twitter account\nis a legitimate public good:\n\nOn the surface, it does indeed seem like someone getting paid $20,000\nfor operating a Twitter account is ridiculous. But it's worth digging in\nand questioning exactly what, if anything, is actually wrong\nwith this outcome. After all, maybe this is what effective marketing in\n2020 actually looks like, and it's our expectations that need to\nadapt.\n\nThere are two main objections that I heard, and both lead to\ninteresting criticisms of quadratic funding in its current\nimplementation. First, there was criticism of\noverpayment. Twittering is a fairly \"trivial\" activity;\nit does not require that much work, lots of people do it for\nfree, and it doesn't provide nearly as much long-term value as something\nmore substantive like EthHub or the Zero\nKnowledge Podcast. Hence, it feels wrong to pay a full-time salary\nfor it.\n\n Examples of @antiprosynth's recent\ntweets \n\nIf we accept the metaphor of quadratic funding as being like a market for public goods,\nthen one could simply extend the metaphor, and reply to the concern with\nthe usual free-market argument. People voluntarily paid their own money\nto support @antiprosynth's twitter activity, and\nthat itself signals that it's valuable. Why should we trust you with\nyour mere words and protestations over a costly signal of real money on\nthe table from dozens of people?\n\nThe most plausible answer is actually quite similar to one that you\noften hear in discussions about financial markets: markets can give\nskewed results when you can express an opinion in favor of\nsomething but cannot express an opinion against it. When short\nselling is not possible, financial markets are often\nmuch more inefficient, because instead of reflecting the\naverage opinion on an asset's true value, a market may instead\nreflect the inflated expectations of an asset's few rabid supporters. In\nthis version of quadratic funding, there too is an asymmetry, as\nyou can donate in support of a project but you cannot donate to oppose\nit. Might this be the root of the problem?\n\nOne can go further and ask, why might overpayment happen to this\nparticular project, and not others? I have heard a common answer:\ntwitter accounts already have a high exposure. A client\ndevelopment team like Nethermind\ndoes not gain much publicity through their work directly, so they need\nto separately market themselves, whereas a twitter account's \"work\" is\nself-marketing by its very nature. Furthermore, the most prominent\ntwitterers get quadratically more matching out of their exposure,\namplifying their outsized advantage further - a problem I alluded to in\nmy review of round 3.\n\nInterestingly, in the case of vanilla quadratic voting there was\nan argument made by Glen Weyl for why economies-of-scale effects of\ntraditional voting, such as Duverger's\nlaw, don't apply to quadratic voting: a project becoming more\nprominent increases the incentive to give it both positive and negative\nvotes, so on net the effects cancel out. But notice once again, that\nthis argument relies on negative votes being a\npossibility.\nGood for the\ntribe, but is it good for the world?\n\nThe particular story of @antiprosynth had what is in my opinion\na happy ending: over the next ten days, more contributions came in to\nother candidates, and @antiprosynth's match reduced to\n$11,316, still a respectably high amount but on par with EthHub and\nbelow Week in Ethereum. However, even a quadratic matching grant of this\nsize still raises to the next criticism: is twittering a public\ngood or public bad anyway?\n\nTraditionally, public goods of the type that Gitcoin Grants quadratic\nfunding is trying to support were selected and funded by governments.\nThe motivation of @antiprosynth's tweets is \"aggregating\nEthereum-related news, fighting information asymmetry and\nfine-tuning/signaling a consistent narrative for Ethereum (and ETH)\":\nessentially, fighting the good fight against anti-Ethereum\nmisinformation by bitcoin\nmaximalists. And, lo and behold, governments too have a rich history\nof sponsoring\nsocial media participants to argue on their behalf. And it seems\nlikely that most of these governments see themselves as \"fighting the\ngood fight against anti-[X] misinformation by [Y] {extremists,\nimperialists, totalitarians}\", just as the Ethereum community feels a\nneed to fight the good fight against maximalist trolls. From the inside\nview of each individual country (and in our case the Ethereum community)\norganized social media participation seems to be a clear public good\n(ignoring the possibility of blowback effects, which are real and\nimportant). But from the outside view of the entire world, it can be\nviewed as a zero-sum game.\n\nThis is actually a common pattern to see in politics, and indeed\nthere are many instances of larger-scale coordination that are precisely\nintended to undermine smaller-scale coordination that is seen as \"good\nfor the tribe but bad for the world\": antitrust law, free trade\nagreements, state-level pre-emption of local zoning codes,\nanti-militarization agreements... the list goes on. A broad environment\nwhere public subsidies are generally viewed suspiciously also does quite\na good job of limiting many kinds of malign local coordination. But as\npublic goods become more important, and we discover better and better\nways for communities to coordinate on producing them, that strategy's\nefficacy becomes more limited, and properly grappling with these\ndiscrepancies between what is good for the tribe and what is good for\nthe world becomes more important.\n\nThat said, internet marketing and debate is not a zero-sum\ngame, and there are plenty of ways to engage in internet marketing and\ndebate that are good for the world. Internet debate in general\nserves to help the public learn what things are true, what things are\nnot true, what causes to support, and what causes to oppose. Some\ntactics are clearly not truth-favoring, but other tactics are quite\ntruth-favoring. Some tactics are clearly offensive, but others are\ndefensive. And in the ethereum community, there is\nwidespread\nsentiment\nthat there is not enough resources going into marketing of some\nkind, and I personally agree with this sentiment.\n\nWhat kind of marketing is positive-sum (good for tribe and good for\nworld) and what kind of marketing is zero-sum (good for tribe but bad\nfor world) is another question, and one that's worth the community\ndebating. I naturally hope that the Ethereum community continues to\nvalue maintaining a moral high ground. Regarding the case of @antiprosynth himself,\nI cannot find any tactics that I would classify as bad-for-world,\nespecially when compared to outright misinformation (\"it's impossible to\nrun a full node\") that we often see used against Ethereum - but I am\npro-ethereum and hence biased, hence the need to be careful.\nUniversal mechanisms,\nparticular goals\n\nThe story has another plot twist, which reveals yet another feature\n(or bug?) or quadratic funding. Quadratic funding was originally described as\n\"Formal Rules for a Society Neutral among Communities\", the intention\nbeing to use it at a very large, potentially even global, scale. Anyone\ncan participate as a project or as a participant, and projects that\nsupport public goods that are good for any \"public\" would be\nsupported. In the case of Gitcoin Grants, however, the matching funds\nare coming from Ethereum organizations, and so there is an expectation\nthat the system is there to support Ethereum projects. But there is\nnothing in the rules of quadratic funding that privileges Ethereum\nprojects and prevents, say, Ethereum Classic projects from seeking\nfunding using the same platform! And, of course, this is exactly what\nhappened:\n\n \n\nSo now the result is, $24 of funding from Ethereum organizations will\nbe going toward supporting an Ethereum Classic promoter's twitter\nactivity. To give people outside of the crypto space a feeling for what\nthis is like, imagine the USA holding a quadratic funding raise, using\ngovernment funding to match donations, and the result is that some of\nthe funding goes to someone explicitly planning to use the money to talk\non Twitter about how great Russia is (or vice versa). The matching funds\nare coming from Ethereum sources, and there's an implied expectation\nthat the funds should support Ethereum, but nothing actually prevents,\nor even discourages, non-Ethereum projects from organizing to get a\nshare of the matched funds on the platform!\n\n## Solutions\n\nThere are two solutions to these problems. One is to modify the\nquadratic funding mechanism to support negative votes in addition to\npositive votes. The mathematical theory behind quadratic voting already\nimplies that it is the \"right thing\" to do to allow such a possibility\n(every positive number has a negative square root as well as a positive\nsquare root). On the other hand, there are social concerns that allowing\nfor negative voting would cause more animosity and lead to other kinds\nof harms. After all, mob mentality is at its worst when it is against\nsomething rather than for something. Hence, it's my view that it's not\ncertain that allowing negative contributions will work out well, but\nthere is enough evidence that it might that it is definitely worth\ntrying out in a future round.\n\nThe second solution is to use two separate mechanisms for identifying\nrelative goodness of good projects and for screening out bad projects.\nFor example, one could use a challenge mechanism followed by a majority\nETH coin vote, or even at first just a centralized appointed board, to\nscreen out bad projects, and then use quadratic funding as before to\nchoose between good projects. This is less mathematically elegant, but\nit would solve the problem, and it would at the same time provide an\nopportunity to mix in a separate mechanism to ensure that chosen\nprojects benefit Ethereum specifically.\n\nBut even if we adopt the first solution, defining boundaries for the\nquadratic funding itself may also be a good idea. There is intellectual\nprecedent for this. In Elinor Ostrom's eight\nprinciples for governing the commons, defining clear boundaries\nabout who has the right to access the commons is the first one. Without\nclear boundaries, Ostrom writes, \"local appropriators face the risk that\nany benefits they produce by their efforts will be reaped by others who\nhave not contributed to those efforts.\" In the case of Gitcoin Grants\nquadratic funding, one possibility would be to set the maximum matching\ncoefficient for any pair of users to be proportional to the geometric\naverage of their ETH holdings, using that as a proxy for measuring\nmembership in the Ethereum community (note that this avoids being\nplutocratic because 1000 users with 1 ETH each would have a maximum\nmatching of \\(\\approx k * 500,000\\)\nETH, whereas 2 users with 500 ETH each would only have a maximum\nmatching of \\(k * 1,000\\) ETH).\n\n## Collusion\n\nAnother issue that came to the forefront this round was the issue of\ncollusion. The math behind quadratic funding, which compensates for\ntragedies of the commons by magnifying individual contributions based on\nthe total number and size of other contributions to the same project,\nonly works if there is an actual tragedy of the commons limiting natural\ndonations to the project. If there is a \"quid pro quo\", where people get\nsomething individually in exchange for their contributions, the\nmechanism can easily over-compensate. The long-run solution to this is\nsomething like MACI,\na cryptographic system that ensures that contributors have no way to\nprove their contributions to third parties, so any such collusion would\nhave to be done by honor system. In the short run, however, the rules\nand enforcement has not yet been set, and this has led to vigorous\ndebate about what kinds of quid pro quo are legitimate:\n\n[Update 2020.01.29: the above was ultimately\na result of a miscommunication from Gitcoin; a member of the Gitcoin\nteam had okayed Richard Burton's proposal to give rewards to donors\nwithout realizing the implications. So Richard himself is blameless\nhere; though the broader point that we underestimated the need for\nexplicit guidance about what kinds of quid pro quos are acceptable is\nvery much real.]\n\nCurrently, the position is that quid pro\nquos are disallowed, though there is a more nuanced feeling that\ninformal social quid pro quos (\"thank yous\" of different forms) are\nokay, whereas formal and especially monetary or product rewards are a\nno-no. This seems like a reasonable approach, though it does put Gitcoin\nfurther into the uncomfortable position of being a central arbiter,\ncompromising credible neutrality\nsomewhat. One positive byproduct of this whole discussion is that it has\nled to much more awareness in the Ethereum community of what actually is\na public good (as opposed to a \"private good\" or a \"club good\"), and\nmore generally brought public goods much further into the public\ndiscourse.\n\n## Conclusions\n\nWhereas round 3 was the first round with enough participants to have\nany kind of interesting effects, round 4 felt like a true \"coming-out\nparty\" for the cause of decentralized public goods funding. The round\nattracted a large amount of attention from the community, and even from\noutside actors such as the Bitcoin community. It is part of a broader\ntrend in the last few months where public goods funding has become a dominant\npart of the crypto community discourse. Along with this, we have\nalso seen much more discussion\nof strategies\nabout long-term sources of funding for quadratic matching pools of\nlarger sizes.\n\nDiscussions about funding will be important going forward: donations\nfrom large Ethereum organizations are enough to sustain quadratic\nmatching at its current scale, but not enough to allow it to grow much\nfurther, to the point where we can have hundreds of quadratic\nfreelancers instead of about five. At those scales, sources of funding\nfor Ethereum public goods must rely on network effect lockin to some\nextent, or else they will have little more staying power than individual\ndonations, but there are strong reasons not to embed these funding\nsources too deeply into Ethereum (eg. into the protocol itself, a la the\nrecent BCH proposal), to avoid risking the protocol's\nneutrality.\n\nApproaches based on capturing transaction fees at layer 2 are\nsurprisingly viable: currently, there are about $50,000-100,000 per day\n(~$18-35m per year) of transaction fees happening on Ethereum, roughly\nequal to the entire budget of the Ethereum Foundation. And there is\nevidence that miner-extractable value is\neven higher. There are all discussions that we need to have, and\nchallenges that we need to address, if we want the Ethereum community to\nbe a leader in implementing decentralized, credibly neutral and\nmarket-based solutions to public goods funding challenges.",
    "contentLength": 18766,
    "summary": "Gitcoin Round 4 split into tech ($125K) and media ($75K) categories, with controversy over funding a Twitter account $11K for pro-Ethereum posts.",
    "detailedSummary": {
      "theme": "Vitalik analyzes the results and challenges of Gitcoin's fourth quadratic funding round, examining both successful tech project funding and controversial issues that emerged in the new media category.",
      "summary": "Vitalik evaluates Gitcoin Grants Round 4, which introduced separate categories for tech ($125k matching pool) and media projects ($75k matching pool). While the tech section functioned well, funding projects like Tornado Cash and various DeFi tools that complement rather than duplicate Ethereum Foundation grants, the media section revealed significant institutional design challenges. The controversial case of @antiprosynth receiving substantial matching funds for Twitter activity sparked debates about what constitutes legitimate public goods, the problem of overpayment due to asymmetric voting (only positive contributions allowed), and whether community-focused advocacy is good for the tribe but potentially harmful globally. Vitalik also discusses the boundary problem illustrated when an Ethereum Classic promoter received Ethereum-sourced matching funds, and addresses collusion issues where quid pro quo arrangements could game the system.",
      "takeaways": [
        "Quadratic funding worked effectively for tech projects, with funding shifting toward application-specific tools that complement rather than duplicate Ethereum Foundation grants",
        "The absence of negative voting in quadratic funding creates market inefficiencies similar to financial markets without short selling, potentially leading to overpayment",
        "Clear community boundaries are essential for quadratic funding systems, as illustrated by Elinor Ostrom's principles for governing commons",
        "There's a fundamental tension between what's good for a specific community (tribe) versus what's good globally, particularly in social media advocacy",
        "Scaling quadratic funding beyond current levels will require sustainable funding sources, potentially through capturing layer 2 transaction fees rather than relying on individual donations"
      ],
      "controversial": [
        "Funding Twitter activity as a legitimate public good - whether social media advocacy deserves substantial financial support",
        "The tribal versus global good dilemma - whether community-focused marketing and advocacy creates zero-sum dynamics that benefit one group at the expense of others",
        "Allowing negative voting in quadratic funding systems - balancing mathematical elegance against potential for increased animosity and mob mentality"
      ]
    }
  },
  {
    "id": "general-2019-12-26-mvb",
    "title": "Base Layers And Functionality Escape Velocity",
    "date": "2019-12-26",
    "category": "general",
    "url": "https://vitalik.eth.limo/general/2019/12/26/mvb.html",
    "path": "general/2019/12/26/mvb.html",
    "content": "- \n\n- \n\n  \n    \n\n  \n  \n  Dark Mode Toggle\n  \n    \n\n  \n\n##  Base Layers And Functionality Escape Velocity \n\n 2019 Dec 26 \nSee all posts\n\n \n \n\n Base Layers And Functionality Escape Velocity \n\nOne common strand of thinking in blockchain land goes as follows:\nblockchains should be maximally simple, because they are a piece of\ninfrastructure that is difficult to change and would lead to great harms\nif it breaks, and more complex functionality should be built on top, in\nthe form of layer 2 protocols: state channels, Plasma, rollup,\nand so forth. Layer 2 should be the site of ongoing innovation, layer 1\nshould be the site of stability and maintenance, with large changes only\nin emergencies (eg. a one-time set of serious breaking changes to\nprevent the base protocol's cryptography from falling to quantum\ncomputers would be okay).\n\nThis kind of layer separation is a very nice idea, and in the long\nterm I strongly support this idea. However, this kind of thinking misses\nan important point: while layer 1 cannot be too powerful, as\ngreater power implies greater complexity and hence greater brittleness,\nlayer 1 must also be powerful enough for the layer 2\nprotocols-on-top that people want to build to actually be possible in\nthe first place. Once a layer 1 protocol has achieved a certain level of\nfunctionality, which I will term \"functionality escape velocity\", then\nyes, you can do everything else on top without further changing the\nbase. But if layer 1 is not powerful enough, then you can talk about\nfilling in the gap with layer 2 systems, but the reality is that there\nis no way to actually build those systems, without reintroducing a whole\nset of trust assumptions that the layer 1 was trying to get away from.\nThis post will talk about some of what this minimal functionality that\nconstitutes \"functionality escape velocity\" is.\n\n## A programming language\n\nIt must be possible to execute custom user-generated scripts\non-chain. This programming language can be simple, and actually does not\nneed to be high-performance, but it needs to at least have the level of\nfunctionality required to be able to verify arbitrary things that might\nneed to be verified. This is important because the layer 2 protocols\nthat are going to be built on top need to have some kind of verification\nlogic, and this verification logic must be executed by the blockchain\nsomehow.\n\nYou may have heard of Turing\ncompleteness; the \"layman's intuition\" for the term being that if a\nprogramming language is Turing complete then it can do anything that a\ncomputer theoretically could do. Any program in one Turing-complete\nlanguage can be translated into an equivalent program in any other\nTuring-complete language. However, it turns out that we only need\nsomething slightly lighter: it's okay to restrict to programs without\nloops, or programs which are guaranteed\nto terminate in a specific number of steps.\n\n## Rich Statefulness\n\nIt doesn't just matter that a programming language exists,\nit also matters precisely how that programming language is integrated\ninto the blockchain. Among the more constricted ways that a language\ncould be integrated is if it is used for pure transaction verification:\nwhen you send coins to some address, that address represents a computer\nprogram P which would be used to verify a transaction that\nsends coins from that address. That is, if you send a\ntransaction whose hash is h, then you would supply a\nsignature S, and the blockchain would run\nP(h, S), and if that outputs TRUE then the transaction is\nvalid. Often, P is a verifier for a cryptographic signature\nscheme, but it could do more complex operations. Note particularly that\nin this model P does not have access to the\ndestination of the transaction.\n\nHowever, this \"pure function\" approach is not enough. This is because\nthis pure function-based approach is not powerful enough to implement\nmany kinds of layer 2 protocols that people actually want to implement.\nIt can do channels (and channel-based systems like the Lightning\nNetwork), but it cannot implement other scaling techniques with stronger\nproperties, it cannot be used to bootstrap systems that do have more\ncomplicated notions of state, and so forth.\n\nTo give a simple example of what the pure function paradigm cannot\ndo, consider a savings account with the following feature: there is a\ncryptographic key k which can initiate a withdrawal, and if\na withdrawal is initiated, within the next 24 hours that same key\nk can cancel the withdrawal. If a withdrawal remains\nuncancelled within 24 hours, then anyone can \"poke\" the account to\nfinalize that withdrawal. The goal is that if the key is stolen, the\naccount holder can prevent the thief from withdrawing the funds. The\nthief could of course prevent the legitimate owner from getting the\nfunds, but the attack would not be profitable for the thief and so they\nwould probably not bother with it (see the\noriginal paper for an explanation of this technique).\n\nUnfortunately this technique cannot be implemented with just pure\nfunctions. The problem is this: there needs to be some way to move coins\nfrom a \"normal\" state to an \"awaiting withdrawal\" state. But the program\nP does not have access to the destination! Hence, any\ntransaction that could authorize moving the coins to an awaiting\nwithdrawal state could also authorize just stealing those coins\nimmediately; P can't tell the difference. The ability to\nchange the state of coins, without completely setting them free, is\nimportant to many kinds of applications, including layer 2 protocols.\nPlasma itself fits into this \"authorize, finalize, cancel\" paradigm: an\nexit from Plasma must be approved, then there is a 7 day challenge\nperiod, and within that challenge period the exit could be cancelled if\nthe right evidence is provided. Rollup also needs this property: coins\ninside a rollup must be controlled by a program that keeps track of a\nstate root R, and changes from R to\nR' if some verifier P(R, R', data) returns\nTRUE - but it only changes the state to R' in that case, it\ndoes not set the coins free.\n\nThis ability to authorize state changes without completely setting\nall coins in an account free, is what I mean by \"rich statefulness\". It\ncan be implemented in many ways, some UTXO-based, but without it a\nblockchain is not powerful enough to implement most layer 2 protocols,\nwithout including trust assumptions (eg. a set of functionaries who are\ncollectively trusted to execute those richly-stateful programs).\n\nNote: yes, I know that if P has access to\nh then you can just include the destination address as part\nof S and check it against h, and restrict\nstate changes that way. But it is possible to have a programming\nlanguage that is too resource-limited or otherwise restricted to\nactually do this; and surprisingly this often actually is the case in\nblockchain scripting languages.\nSufficient data\nscalability and low latency\n\nIt turns out that plasma and channels, and other layer 2 protocols\nthat are fully off-chain have some fundamental weaknesses that prevent\nthem from fully replicating the capabilities of layer 1. I go into this\nin detail here;\nthe summary is that these protocols need to have a way of adjudicating\nsituations where some parties maliciously fail to provide data that they\npromised to provide, and because data publication is not globally\nverifiable (you don't know when data was published unless you already\ndownloaded it yourself) these adjudication games are not\ngame-theoretically stable. Channels and Plasma cleverly get around this\ninstability by adding additional assumptions, particularly assuming that\nfor every piece of state, there is a single actor that is interested in\nthat state not being incorrectly modified (usually because it represents\ncoins that they own) and so can be trusted to fight on its behalf.\nHowever, this is far from general-purpose; systems like Uniswap, for example, include a large\n\"central\" contract that is not owned by anyone, and so they cannot\neffectively be protected by this paradigm.\n\nThere is one way to get around this, which is layer 2 protocols that\npublish very small amounts of data on-chain, but do computation entirely\noff-chain. If data is guaranteed to be available, then computation being\ndone off-chain is okay, because games for adjudicating who did\ncomputation correctly and who did it incorrectly are\ngame-theoretically stable (or could be replaced entirely by SNARKs or STARKs). This is the\nlogic behind ZK\nrollup and optimistic\nrollup. If a blockchain allows for the publication and guarantees\nthe availability of a reasonably large amount of data, even if its\ncapacity for computation remains very limited, then the\nblockchain can support these layer-2 protocols and achieve a high level\nof scalability and functionality.\n\nJust how much data does the blockchain need to be able to process and\nguarantee? Well, it depends on what TPS you want. With a rollup, you can\ncompress most activity to ~10-20 bytes per transaction, so 1 kB/sec\ngives you 50-100 TPS, 1 MB/sec gives you 50,000-100,000 TPS, and so\nforth. Fortunately, internet bandwidth continues\nto grow quickly, and does not seem to be slowing down the way\nMoore's law for computation is, so increasing scaling for data without\nincreasing computational load is quite a viable path for blockchains to\ntake!\n\nNote also that it is not just data capacity that matters, it is also\ndata latency (ie. having low block times). Layer 2 protocols like rollup\n(or for that matter Plasma) only give any guarantees of security when\nthe data actually is published to chain; hence, the time it takes for\ndata to be reliably included (ideally \"finalized\") on chain is the time\nthat it takes between when Alice sends Bob a payment and Bob can be\nconfident that this payment will be included. The block time of the base\nlayer sets the latency for anything whose confirmation depends things\nbeing included in the base layer. This could be worked around with\non-chain security deposits, aka \"bonds\", at the cost of high capital\ninefficiency, but such an approach is inherently imperfect because a\nmalicious actor could trick an unlimited number of different people by\nsacrificing one deposit.\n\n## Conclusions\n\n\"Keep layer 1 simple, make up for it on layer 2\" is NOT a universal\nanswer to blockchain scalability and functionality problems, because it\nfails to take into account that layer 1 blockchains themselves must have\na sufficient level of scalability and functionality for this \"building\non top\" to actually be possible (unless your so-called \"layer 2\nprotocols\" are just trusted intermediaries). However, it is true that\nbeyond a certain point, any layer 1 functionality can be\nreplicated on layer 2, and in many cases it's a good idea to do this to\nimprove upgradeability. Hence, we need layer 1 development in parallel\nwith layer 2 development in the short term, and more focus on layer 2 in\nthe long term.",
    "contentLength": 10916,
    "summary": "Blockchains need \"functionality escape velocity\" - sufficient programming capability, rich state management, and data capacity - to enable secure layer 2 protocols like rollups.",
    "detailedSummary": {
      "theme": "Blockchain base layers must achieve sufficient functionality before layer 2 solutions can effectively operate without reintroducing trust assumptions.",
      "summary": "Vitalik argues against the common belief that blockchain base layers should remain maximally simple while relegating all complex functionality to layer 2 protocols. He introduces the concept of 'functionality escape velocity' - the minimum level of capability that layer 1 must possess for layer 2 solutions to actually work without compromising the trust model. Vitalik identifies three essential requirements for achieving this escape velocity: a programming language capable of executing custom scripts for verification logic, rich statefulness that allows state changes without completely freeing funds, and sufficient data scalability with low latency to support rollup-style solutions. He emphasizes that while keeping layer 1 simple is desirable, it cannot be so restrictive that it prevents legitimate layer 2 innovations from functioning properly. Once a blockchain reaches this functionality threshold, then additional features can indeed be built on layer 2, but attempting to build sophisticated protocols on an underpowered base layer simply forces developers to reintroduce the very trust assumptions that blockchain technology aims to eliminate.",
      "takeaways": [
        "Layer 1 blockchains must reach 'functionality escape velocity' before layer 2 solutions can work without trust assumptions",
        "Three core requirements are needed: programmable verification logic, rich statefulness for controlled state changes, and adequate data throughput with low latency",
        "Pure function-based programming models are insufficient for most layer 2 protocols like Plasma and rollups",
        "Data availability and publication guarantees are crucial for rollup-based scaling solutions to maintain security",
        "The strategy should be layer 1 development in parallel with layer 2 in the short term, then focus on layer 2 once escape velocity is achieved"
      ],
      "controversial": [
        "Challenges the popular 'keep layer 1 simple at all costs' philosophy prevalent in blockchain development",
        "Suggests that many proposed layer 2 solutions may actually be 'trusted intermediaries' if built on insufficiently powerful base layers"
      ]
    }
  },
  {
    "id": "general-2019-12-24-christmas",
    "title": "Christmas Special",
    "date": "2019-12-24",
    "category": "general",
    "url": "https://vitalik.eth.limo/general/2019/12/24/christmas.html",
    "path": "general/2019/12/24/christmas.html",
    "content": "- \n\n- \n\n  \n    \n\n  \n  \n  Dark Mode Toggle\n  \n    \n\n  \n\n##  Christmas Special \n\n 2019 Dec 24 \nSee all posts\n\n \n \n\n Christmas Special \n\nSince it's Christmas time now, and we're theoretically supposed to be\nenjoying ourselves and spending time with our families instead of waging\nendless holy wars on Twitter, this blog post will offer some games that\nyou can play with your friends that will help you have fun and\nat the same time understand some spooky mathematical concepts!\n\n## 1.58 dimensional chess\n\n  \n\nThis is a variant of chess where the board is set up like this:\n\n  \n\nThe board is still a normal 8x8 board, but there are only 27 open\nsquares. The other 37 squares should be covered up by checkers or Go\npieces or anything else to denote that they are inaccessible. The rules\nare the same as chess, with a few exceptions:\n\n- White pawns move up, black pawns move left. White pawns take going\nleft-and-up or right-and-up, black pawns take going left-and-down or\nleft-and-up. White pawns promote upon reaching the top, black pawns\npromote upon reaching the left.\n\n- No en passant, castling, or two-step-forward pawn jumps.\n\n- Chess pieces cannot move onto or through the 37 covered\nsquares. Knights cannot move onto the 37 covered squares, but don't care\nwhat they move \"through\".\n\nThe game is called 1.58 dimensional chess because the 27 open squares\nare chosen according to a pattern based on the Sierpinski\ntriangle. You start off with a single open square, and then every\ntime you double the width, you take the shape at the end of the previous\nstep, and copy it to the top left, top right and bottom left corners,\nbut leave the bottom right corner inaccessible. Whereas in a\none-dimensional structure, doubling the width increases the space by 2x,\nand in a two-dimensional structure, doubling the width increases the\nspace by 4x (4 = 22), and in a three-dimensional structure,\ndoubling the width increases the space by 8x (8 = 23), here\ndoubling the width increases the space by 3x (3 = 21.58496),\nhence \"1.58 dimensional\" (see Hausdorff\ndimension for details).\n\nThe game is substantially simpler and more \"tractable\" than full-on\nchess, and it's an interesting exercise in showing how in lower-dimensional\nspaces defense becomes much easier than offense. Note that the\nrelative value of different pieces may change here, and new kinds of\nendings become possible (eg. you can checkmate with just a bishop).\n\n## 3 dimensional tic tac toe\n\n  \n\nThe goal here is to get 4 in a straight line, where the line can go\nin any direction, along an axis or diagonal, including between planes.\nFor example in this configuration X wins:\n\n  \n\nIt's considerably harder than traditional\n2D tic tac toe, and hopefully much more fun!\n\n## Modular tic-tac-toe\n\nHere, we go back down to having two dimensions, except we allow lines\nto wrap around:\n\n \n\nX wins\n\nNote that we allow diagonal lines with any slope, as long as they\npass through all four points. Particularly, this means that lines with\nslope +/- 2 and +/- 1/2 are admissible:\n\n \n\nMathematically, the board can be interpreted as a 2-dimensional\nvector space over integers modulo\n4, and the goal being to fill in a line that passes through four\npoints over this space. Note that there exists at least one line passing\nthrough any two points.\nTic tac toe over\nthe 4-element binary field\n\n  \n\nHere, we have the same concept as above, except we use an even\nspookier mathematical structure, the 4-element\nfield of polynomials over \\(Z_2\\)\nmodulo \\(x^2 + x + 1\\). This structure\nhas pretty much no reasonable geometric interpretation, so I'll just\ngive you the addition and multiplication tables:\n\n  \n\nOK fine, here are all possible lines, excluding the horizontal and\nthe vertical lines (which are also admissible) for brevity:\n\nThe lack of geometric interpretation does make the game harder to\nplay; you pretty much have to memorize the twenty winning combinations,\nthough note that they are basically rotations and reflections\nof the same four basic shapes (axial line, diagonal line, diagonal line\nstarting in the middle, that weird thing that doesn't look like a\nline).\nNow play\n1.77 dimensional connect four. I dare you.\n\n## Modular poker\n\nEveryone is dealt five (you can use whatever variant poker rules you\nwant here in terms of how these cards are dealt and whether or not\nplayers have the right to swap cards out). The cards are interpreted as:\njack = 11, queen = 12, king = 0, ace = 1. A hand is stronger than\nanother hand, if it contains a longer sequence, with any constant\ndifference between consecutive cards (allowing wraparound), than the\nother hand.\n\nMathametically, this can be represented as, a hand is stronger if the\nplayer can come up with a line \\(L(x) =\nmx+b\\) such that they have cards for the numbers \\(L(0)\\), \\(L(1)\\) ... \\(L(k)\\) for the highest \\(k\\).\n\n \n\nExample of a full five-card winning hand. y = 4x +\n5.\n\nTo break ties between equal maximum-length sequences, count the\nnumber of distinct length-three sequences they have; the hand with more\ndistinct length-three sequences wins.\n\n \n\nThis hand has four length-three sequences: K 2 4, K 4 8, 2\n3 4, 3 8 K. This is rare.\n\nOnly consider lines of length three or higher. If a hand has three or\nmore of the same denomination, that counts as a sequence, but if a hand\nhas two of the same denomination, any sequences passing through that\ndenomination only count as one sequence.\n\n \n\nThis hand has no length-three sequences.\n\nIf two hands are completely tied, the hand with the higher highest\ncard (using J = 11, Q = 12, K = 0, A = 1 as above) wins.\n\nEnjoy!",
    "contentLength": 5577,
    "summary": "This Christmas blog post presents festive math games including Sierpinski triangle-based \"1.58 dimensional chess\" and modular tic-tac-toe variants.",
    "detailedSummary": {
      "theme": "A playful exploration of mathematical concepts through modified games that demonstrate principles like fractional dimensions, modular arithmetic, and field theory.",
      "summary": "In this Christmas-themed blog post, Vitalik presents several mathematically-inspired game variants designed to be both entertaining and educational. He introduces '1.58 dimensional chess' played on a Sierpinski triangle pattern with 27 squares instead of 64, demonstrating the concept of Hausdorff dimension where doubling the width increases space by 3x rather than the typical 4x of two-dimensional structures. Vitalik also describes 3D tic-tac-toe requiring 4-in-a-row across any direction including between planes, and modular tic-tac-toe where lines can wrap around the board edges with various slopes allowed. His most mathematically complex variant involves tic-tac-toe over a 4-element binary field with polynomial operations, requiring players to memorize winning combinations due to the lack of geometric intuition. Finally, he presents modular poker where hands are evaluated based on arithmetic sequences with constant differences between cards, treating face cards as numbers in modular 13 arithmetic. Vitalik frames these games as fun ways to explore 'spooky mathematical concepts' during the holiday season while spending time with friends and family.",
      "takeaways": [
        "Mathematical concepts like fractional dimensions and modular arithmetic can be made accessible and engaging through game mechanics",
        "Lower-dimensional spaces fundamentally change strategic dynamics, making defense easier than offense in games like 1.58 dimensional chess",
        "Abstract mathematical structures like finite fields can be incorporated into familiar games, though they may require memorization rather than intuitive play",
        "Game modifications can demonstrate complex mathematical principles while remaining playable and entertaining",
        "Creative applications of mathematics can serve as both educational tools and social activities for the holiday season"
      ],
      "controversial": []
    }
  },
  {
    "id": "general-2019-12-07-quadratic",
    "title": "Quadratic Payments: A Primer",
    "date": "2019-12-07",
    "category": "governance",
    "url": "https://vitalik.eth.limo/general/2019/12/07/quadratic.html",
    "path": "general/2019/12/07/quadratic.html",
    "content": "- \n\n- \n\n  \n    \n\n  \n  \n  Dark Mode Toggle\n  \n    \n\n  \n\n##  Quadratic Payments: A Primer \n\n 2019 Dec 07 \nSee all posts\n\n \n \n\n Quadratic Payments: A Primer \n\nSpecial thanks to Karl Floersch and Jinglan Wang for\nfeedback\n\nIf you follow applied mechanism design or decentralized governance at\nall, you may have recently heard one of a few buzzwords: quadratic\nvoting, quadratic\nfunding and quadratic\nattention purchase. These ideas have been gaining popularity rapidly\nover the last few years, and small-scale tests have already been\ndeployed: the Taiwanese\npresidential hackathon used quadratic voting to vote on winning\nprojects, Gitcoin Grants used\nquadratic funding to fund public goods in the Ethereum ecosystem,\nand the Colorado Democratic party also\nexperimented with quadratic voting to determine their party\nplatform.\n\nTo the proponents of these voting schemes, this is not just another\nslight improvement to what exists. Rather, it's an initial foray into a\nfundamentally new class of social technology which, has the potential to\noverturn how we make many public decisions, large and small. The\nultimate effect of these schemes rolled out in their full form could\nbe as deeply transformative as the industrial-era advent of mostly-free\nmarkets and constitutional democracy. But now, you may be thinking:\n\"These are large promises. What do these new governance technologies\nhave that justifies such claims?\"\nPrivate goods, private\nmarkets...\n\nTo understand what is going on, let us first consider an existing\nsocial technology: money, and property rights - the invisible social\ntechnology that generally hides behind money. Money and private property\nare extremely powerful social technologies, for all the reasons\nclassical economists have been stating for over a hundred years. If Bob\nis producing apples, and Alice wants to buy apples, we can economically\nmodel the interaction between the two, and the results seem to make\nsense:\n\nAlice keeps buying apples until the marginal value of the next apple\nto her is less than the cost of producing it, which is pretty much\nexactly the optimal thing that could happen. This is all formalized in\nresults such as the \"fundamental\ntheorems of welfare economics\". Now, those of you who have learned\nsome economics may be screaming, but what about imperfect\ncompetition? Asymmetric\ninformation? Economic\ninequality? Public goods? Externalities? Many\nactivities in the real world, including those that are key to the\nprogress of human civilization, benefit (or harm) many people in\ncomplicated ways. These activities and the consequences that arise from\nthem often cannot be neatly decomposed into sequences of distinct trades\nbetween two parties.\n\nBut since when do we expect a single package of technologies to solve\nevery problem anyway? \"What about oceans?\" isn't an argument against\ncars, it's an argument against car maximalism, the\nposition that we need cars and nothing else. Much like how private\nproperty and markets deal with private goods, can we try to use economic\nmeans to deduce what kind of social technologies would work well for\nencouraging production of the public goods that we need?\n\n## ... Public goods, public markets\n\nPrivate goods (eg. apples) and public goods (eg. public parks, air\nquality, scientific research, this article...) are different in some key\nways. When we are talking about private goods, production for multiple\npeople (eg. the same farmer makes apples for both Alice and Bob) can be\ndecomposed into (i) the farmer making some apples for Alice, and (ii)\nthe farmer making some other apples for Bob. If Alice wants apples but\nBob does not, then the farmer makes Alice's apples, collects payment\nfrom Alice, and leaves Bob alone. Even complex collaborations (the \"I, Pencil\" essay popular\nin libertarian circles comes to mind) can be decomposed into a series of\nsuch interactions. When we are talking about public goods, however,\nthis kind of decomposition is not possible. When I write this\nblog article, it can be read by both Alice and Bob (and everyone else).\nI could put it behind a paywall, but if it's popular enough it\nwill inevitably get mirrored on third-party sites, and paywalls are in\nany case annoying and not very effective. Furthermore, making an article\navailable to ten people is not ten times cheaper than making the article\navailable to a hundred people; rather, the cost is exactly the\nsame. So I either produce the article for everyone, or I do not\nproduce it for anyone at all.\n\nSo here comes the challenge: how do we aggregate together people's\npreferences? Some private and public goods are worth producing, others\nare not. In the case of private goods, the question is easy, because we\ncan just decompose it into a series of decisions for each individual.\nWhatever amount each person is willing to pay for, that much gets\nproduced for them; the economics is not especially complex. In the case\nof public goods, however, you cannot \"decompose\", and so we need to add\nup people's preferences in a different way.\n\nFirst of all, let's see what happens if we just put up a plain old\nregular market: I offer to write an article as long as at least $1000 of\nmoney gets donated to me (fun fact: I\nliterally did this back in 2011). Every dollar donated increases the\nprobability that the goal will be reached and the article will be\npublished; let us call this \"marginal probability\" p. At a\ncost of $k, you can increase the probability that the\narticle will be published by k * p (though eventually the\ngains will decrease as the probability approaches 100%). Let's say to\nyou personally, the article being published is worth $V.\nWould you donate? Well, donating a dollar increases the probability it\nwill be published by p, and so gives you an expected\n$p * V of value. If p * V > 1, you donate,\nand quite a lot, and if p * V < 1 you don't donate at\nall.\n\nPhrased less mathematically, either you value the article enough\n(and/or are rich enough) to pay, and if that's the case it's in your\ninterest to keep paying (and influencing) quite a lot, or you don't\nvalue the article enough and you contribute nothing. Hence, the only\nblog articles that get published would be articles where some single\nperson is willing to basically pay for it\nthemselves (in my experiment in 2011, this prediction was\nexperimentally verified: in most\nrounds,\nover half of the total contribution came from a single donor).\n\nNote that this reasoning applies for any kind of mechanism that\ninvolves \"buying influence\" over matters of public concern. This\nincludes paying for public goods, shareholder voting in corporations,\npublic advertising, bribing politicians, and much more. The little guy\nhas too little influence (not quite zero, because in the real world\nthings like altruism exist) and the big guy has too much. If you had an\nintuition that markets work great for buying apples, but money is\ncorrupting in \"the public sphere\", this is basically a simplified\nmathematical model that shows why.\n\nWe can also consider a different mechanism: one-person-one-vote.\nLet's say you can either vote that I deserve a reward for writing this\narticle, or you can vote that I don't, and my reward is proportional to\nthe number of votes in my favor. We can interpret this as follows: your\nfirst \"contribution\" costs only a small amount of effort, so you'll\nsupport an article if you care about it enough, but after that point\nthere is no more room to contribute further; your second contribution\n\"costs\" infinity.\n\nNow, you might notice that neither of the graphs above look quite\nright. The first graph over-privileges people who care a lot\n(or are wealthy), the second graph over-privileges people who care\nonly a little, which is also a problem. The single sheep's desire\nto live is more important than the two wolves' desire to have a tasty\ndinner.\n\nBut what do we actually want? Ultimately, we want a scheme where\nhow much influence you \"buy\" is proportional to how much you\ncare. In the mathematical lingo above, we want your k\nto be proportional to your V. But here's the problem: your\nV determines how much you're willing to pay for\none unit of influence. If Alice were willing to pay $100 for\nthe article if she had to fund it herself, then she would be willing to\npay $1 for an increased 1% chance it will get written, and if Bob were\nonly willing to pay $50 for the article then he would only be willing to\npay $0.5 for the same \"unit of influence\".\n\nSo how do we match these two up? The answer is clever: your n'th\nunit of influence costs you $n . That is, for example, you could\nbuy your first vote for $0.01, but then your second would cost $0.02,\nyour third $0.03, and so forth. Suppose you were Alice in the example\nabove; in such a system she would keep buying units of influence until\nthe cost of the next one got to $1, so she would buy 100 units. Bob\nwould similarly buy until the cost got to $0.5, so he would buy 50\nunits. Alice's 2x higher valuation turned into 2x more units of\ninfluence purchased.\n\nLet's draw this as a graph:\n\nNow let's look at all three beside each other:\n\nOne dollar one vote\nQuadratic voting\nOne person one vote\n\nNotice that only quadratic voting has this nice property that the\namount of influence you purchase is proportional to how much you care;\nthe other two mechanisms either over-privilege concentrated interests or\nover-privilege diffuse interests.\n\nNow, you might ask, where does the quadratic come from?\nWell, the marginal cost of the n'th vote is $n (or $0.01 * n),\nbut the total cost of n votes is \\(\\approx \\frac{n^2}{2}\\). You can view this\ngeometrically as follows:\n\nThe total cost is the area of a triangle, and you probably learned in\nmath class that area is base * height / 2. And since here base and\nheight are proportionate, that basically means that total cost is\nproportional to number of votes squared - hence, \"quadratic\". But\nhonestly it's easier to think \"your n'th unit of influence costs\n$n\".\n\nFinally, you might notice that above I've been vague about what \"one\nunit of influence\" actually means. This is deliberate; it can mean\ndifferent things in different contexts, and the different \"flavors\" of\nquadratic payments reflect these different perspectives.\n\n## Quadratic Voting\n\nSee also the original paper: https://papers.ssrn.com/sol3/papers.cfm?abstract%5fid=2003531\n\nLet us begin by exploring the first \"flavor\" of quadratic payments:\nquadratic voting. Imagine that some organization is trying to choose\nbetween two choices for some decision that affects all of its members.\nFor example, this could be a company or a nonprofit deciding which part\nof town to make a new office in, or a government deciding whether or not\nto implement some policy, or an internet forum deciding whether or not\nits rules should allow discussion of cryptocurrency prices. Within the\ncontext of the organization, the choice made is a public good (or public\nbad, depending on whom you talk to): everyone \"consumes\" the results of\nthe same decision, they just have different opinions about how much they\nlike the result.\n\nThis seems like a perfect target for quadratic voting. The goal is\nthat option A gets chosen if in total people like A more, and option B\ngets chosen if in total people like B more. With simple voting (\"one\nperson one vote\"), the distinction between stronger vs weaker\npreferences gets ignored, so on issues where one side is of very high\nvalue to a few people and the other side is of low value to more people,\nsimple voting is likely to give wrong answers. With a private-goods\nmarket mechanism where people can buy as many votes as they want at the\nsame price per vote, the individual with the strongest preference (or\nthe wealthiest) carries everything. Quadratic voting, where you can make\nn votes in either direction at a cost of n2, is right in the\nmiddle between these two extremes, and creates the perfect balance.\n\nNote that in the voting case, we're deciding two options, so\ndifferent people will favor A over B or B over A; hence, unlike the\ngraphs we saw earlier that start from zero, here voting and preference\ncan both be positive or negative (which option is considered positive\nand which is negative doesn't matter; the math works out the same\nway)\n\n \n\nAs shown above, because the n'th vote has a cost of n,\nthe number of votes you make is proportional to how much you value one\nunit of influence over the decision (the value of the decision\nmultiplied by the probability that one vote will tip the result), and\nhence proportional to how much you care about A being chosen over B or\nvice versa. Hence, we once again have this nice clean \"preference\nadding\" effect.\n\nWe can extend quadratic voting in multiple ways. First, we can allow\nvoting between more than two options. While traditional voting schemes\ninevitably fall prey to various kinds of \"strategic voting\" issues\nbecause of Arrow's\ntheorem and Duverger's\nlaw, quadratic voting continues\nto be optimal in contexts with more than two choices.\n\nThe intuitive argument for those interested: suppose\nthere are established candidates A and B and new candidate C. Some\npeople favor C > A > B but others C > B > A. in a regular\nvote, if both sides think C stands no chance, they decide may as well\nvote their preference between A and B, so C gets no votes, and C's\nfailure becomes a self-fulfilling prophecy. In quadratic voting the\nformer group would vote [A +10, B -10, C +1] and the latter [A -10, B\n+10, C +1], so the A and B votes cancel out and C's popularity shines\nthrough.\n\nSecond, we can look not just at voting between discrete options, but\nalso at voting on the setting of a thermostat: anyone can push the\nthermostat up or down by 0.01 degrees n times by paying a cost of\nn2.\n\nPlot\ntwist: the side wanting it colder only wins when they convince the other\nside that \"C\" stands for \"caliente\".\n\n## Quadratic funding\n\nSee also the original paper: https://papers.ssrn.com/sol3/papers.cfm?abstract%5fid=3243656\n\nQuadratic voting is optimal when you need to make some fixed number\nof collective decisions. But one weakness of quadratic voting is that it\ndoesn't come with a built-in mechanism for deciding what goes on the\nballot in the first place. Proposing votes is potentially a source of\nconsiderable power if not handled with care: a malicious actor in\ncontrol of it can repeatedly propose some decision that a majority\nweakly approves of and a minority strongly disapproves of, and keep\nproposing it until the minority runs out of voting tokens (if you do the\nmath you'll see that the minority would burn through tokens much faster\nthan the majority). Let's consider a flavor of quadratic payments that\ndoes not run into this issue, and makes the choice of decisions itself\nendogenous (ie. part of the mechanism itself). In this case, the\nmechanism is specialized for one particular use case: individual\nprovision of public goods.\n\nLet us consider an example where someone is looking to produce a\npublic good (eg. a developer writing an open source software program),\nand we want to figure out whether or not this program is worth funding.\nBut instead of just thinking about one single public good, let's create\na mechanism where anyone can raise funds for what they claim to\nbe a public good project. Anyone can make a contribution to any project;\na mechanism keeps track of these contributions and then at the end of\nsome period of time the mechanism calculates a payment to each project.\nThe way that this payment is calculated is as follows: for any given\nproject, take the square root of each contributor's contribution, add\nthese values together, and take the square of the result. Or in math\nspeak:\n\n\\[(\\sum_{i=1}^n \\sqrt{c_i})^2\\]\n\nIf that sounds complicated, here it is graphically:\n\nIn any case where there is more than one contributor, the computed\npayment is greater than the raw sum of contributions; the difference\ncomes out of a central subsidy pool (eg. if ten people each donate $1,\nthen the sum-of-square-roots is $10, and the square of that is $100, so\nthe subsidy is $90). Note that if the subsidy pool is not big enough to\nmake the full required payment to every project, we can just divide the\nsubsidies proportionately by whatever constant makes the totals add up\nto the subsidy pool's budget; you can prove that this solves the\ntragedy-of-the-commons problem as well as you can with that subsidy\nbudget.\n\nThere are two ways to intuitively interpret this formula. First, one\ncan look at it through the \"fixing market failure\" lens, a surgical fix\nto the tragedy of\nthe commons problem. In any situation where Alice contributes to a\nproject and Bob also contributes to that same project, Alice is making a\ncontribution to something that is valuable not only to herself, but also\nto Bob. When deciding how much to contribute, Alice was only\ntaking into account the benefit to herself, not Bob, whom she most\nlikely does not even know. The quadratic funding mechanism adds a\nsubsidy to compensate for this effect, determining how much Alice \"would\nhave\" contributed if she also took into account the benefit her\ncontribution brings to Bob. Furthermore, we can separately calculate the\nsubsidy for each pair of people (nb. if there are N people\nthere are N * (N-1) / 2 pairs), and add up all of these\nsubsidies together, and give Bob the combined subsidy from all pairs.\nAnd it turns out that this gives exactly the quadratic funding\nformula.\n\nSecond, one can look at the formula through a quadratic voting lens.\nWe interpret the quadratic funding as being a special case of\nquadratic voting, where the contributors to a project are voting for\nthat project and there is one imaginary participant voting against it:\nthe subsidy pool. Every \"project\" is a motion to take money from the\nsubsidy pool and give it to that project's creator. Everyone sending\n\\(c_i\\) of funds is making \\(\\sqrt{c_i}\\) votes, so there's a total of\n\\(\\sum_{i=1}^n \\sqrt{c_i}\\) votes in\nfavor of the motion. To kill the motion, the subsidy pool would need to\nmake more than \\(\\sum_{i=1}^n\n\\sqrt{c_i}\\) votes against it, which would cost it more than\n\\((\\sum_{i=1}^n \\sqrt{c_i})^2\\). Hence,\n\\((\\sum_{i=1}^n \\sqrt{c_i})^2\\) is the\nmaximum transfer from the subsidy pool to the project that the subsidy\npool would not vote to stop.\n\nQuadratic funding is starting to be explored as a mechanism for\nfunding public goods already; Gitcoin grants for funding\npublic goods in the Ethereum ecosystem is currently the biggest example,\nand the most recent round led to results that, in my own view, did a\nquite good job of making a fair allocation to support projects that the\ncommunity deems valuable.\n\nNumbers\nin white are raw contribution totals; numbers in green are the extra\nsubsidies.\n\n## Quadratic attention payments\n\nSee also the original post: https://kortina.nyc/essays/speech-is-free-distribution-is-not-a-tax-on-the-purchase-of-human-attention-and-political-power/\n\nOne of the defining features of modern capitalism that people love to\nhate is ads. Our cities have ads:\n\nSource:\nhttps://www.flickr.com/photos/argonavigo/36657795264\n\nOur subway turnstiles have ads:\n\nSource:\nhttps://commons.wikimedia.org/wiki/File:NYC,_subway_ad_on_Prince_St.jpg\n\nOur politics are dominated by ads:\n\nSource:\nhttps://upload.wikimedia.org/wikipedia/commons/e/e3/Billboard_Challenging_the_validity_of_Barack_Obama%27s_Birth_Certificate.JPG\n\nAnd even the rivers and the skies have\nads. Now, there are some places that seem to not have this\nproblem:\n\nBut really they just have a different kind of ads:\n\nNow, recently there are attempts to move beyond this in\nsome cities. And on\nTwitter. But let's look at the problem systematically and try to see\nwhat's going wrong. The answer is actually surprisingly simple: public\nadvertising is the evil twin of public goods production. In the case of\npublic goods production, there is one actor that is taking on an\nexpenditure to produce some product, and this product benefits a large\nnumber of people. Because these people cannot effectively coordinate to\npay for the public goods by themselves, we get much less public goods\nthan we need, and the ones we do get are those favored by wealthy actors\nor centralized authorities. Here, there is one actor that reaps a large\nbenefit from forcing other people to look at some image, and\nthis action harms a large number of people. Because these\npeople cannot effectively coordinate to buy out the slots for the ads,\nwe get ads we don't want to see, that are favored by... wealthy actors or\ncentralized authorities.\n\nSo how do we solve this dark mirror image of public goods production?\nWith a bright mirror image of quadratic funding: quadratic fees! Imagine\na billboard where anyone can pay $1 to put up an ad for one minute, but\nif they want to do this multiple times the prices go up: $2 for the\nsecond minute, $3 for the third minute, etc. Note that you can pay to\nextend the lifetime of someone else's ad on the billboard, and\nthis also costs you only $1 for the first minute, even if other\npeople already paid to extend the ad's lifetime many times. We can\nonce again interpret this as being a special case of quadratic voting:\nit's basically the same as the \"voting on a thermostat\" example above,\nbut where the thermostat in question is the number of seconds an ad\nstays up.\n\nThis kind of payment model could be applied in cities, on websites,\nat conferences, or in many other contexts, if the goal is to optimize\nfor putting up things that people want to see (or things that people\nwant other people to see, but even here it's much more democratic than\nsimply buying space) rather than things that wealthy people and\ncentralized institutions want people to see.\n\n## Complexities and caveats\n\nPerhaps the biggest challenge to consider with this concept of\nquadratic payments is the practical implementation issue of identity and\nbribery/collusion. Quadratic payments in any form require a model of\nidentity where individuals cannot easily get as many identities as they\nwant: if they could, then they could just keep getting new identities\nand keep paying $1 to influence some decision as many times as they\nwant, and the mechanism collapses into linear vote-buying. Note that the\nidentity system does not need to be airtight (in the sense of\npreventing multiple-identity acquisition), and indeed there are good\ncivil-liberties reasons why identity systems probably should\nnot try to be airtight. Rather, it just needs to be robust\nenough that manipulation is not worth the cost.\n\nCollusion is also tricky. If we can't prevent people from selling\ntheir votes, the mechanisms once again collapse into\none-dollar-one-vote. We don't just need votes to be anonymous and\nprivate (while still making the final result provable and public);\nwe need votes to be so private that even the person who made the\nvote can't prove to anyone else what they voted for. This is\ndifficult. Secret ballots do this well in the offline world, but secret\nballots are a nineteenth century technology, far too inefficient for the\nsheer amount of quadratic voting and funding that we want to see in the\ntwenty first century.\n\nFortunately, there are technological\nmeans that can help, combining together zero-knowledge proofs,\nencryption and other cryptographic technologies to achieve the precise\ndesired set of privacy and verifiability properties. There's also proposed\ntechniques to verify that private keys actually are in an\nindividual's possession and not in some hardware or cryptographic system\nthat can restrict how they use those keys. However, these techniques are\nall untested and require quite a bit of further work.\n\nAnother challenge is that quadratic payments, being a payment-based\nmechanism, continues to favor people with more money. Note that because\nthe cost of votes is quadratic, this effect is dampened: someone with\n100 times more money only has 10 times more influence, not 100 times, so\nthe extent of the problem goes down by 90% (and even more for\nultra-wealthy actors). That said, it may be desirable to mitigate this\ninequality of power further. This could be done either by denominating\nquadratic payments in a separate token of which everyone gets a fixed\nnumber of units, or giving each person an allocation of funds that can\nonly be used for quadratic-payments use cases: this is basically Andrew Yang's\n\"democracy dollars\" proposal.\n\nA third challenge is the \"rational\nignorance\" and \"rational\nirrationality\" problems, which is that decentralized public\ndecisions have the weakness that any single individual has very little\neffect on the outcome, and so little motivation to make sure they are\nsupporting the decision that is best for the long term; instead,\npressures such as tribal affiliation may dominate. There are many\nstrands of philosophy that emphasize the ability of large crowds to be\nvery wrong despite (or because of!) their size, and quadratic payments\nin any form do little to address this.\n\nQuadratic payments do better at mitigating this problem than\none-person-one-vote systems, and these problems can be expected to be\nless severe for medium-scale public goods than for large decisions that\naffect many millions of people, so it may not be a large challenge at\nfirst, but it's certainly an issue worth confronting. One approach is combining\nquadratic voting with elements of sortition. Another, potentially\nmore long-term durable, approach is to combine quadratic voting with\nanother economic technology that is much more specifically targeted\ntoward rewarding the \"correct contrarianism\" that can dispel mass\ndelusions: prediction\nmarkets. A simple example would be a system where quadratic funding\nis done retrospectively, so people vote on which public goods\nwere valuable some time ago (eg. even 2 years), and projects are funded\nup-front by selling shares of the results of these deferred votes; by\nbuying shares people would be both funding the projects and betting on\nwhich project would be viewed as successful in 2 years' time. There is a\nlarge design space to experiment with here.\n\n## Conclusion\n\nAs I mentioned at the beginning, quadratic payments do not solve\nevery problem. They solve the problem of governing resources that affect\nlarge numbers of people, but they do not solve many other kinds of\nproblems. A particularly important one is information asymmetry and low\nquality of information in general. For this reason, I am a fan of\ntechniques such as prediction markets (see electionbettingodds.com for\none example) to solve information-gathering problems, and many\napplications can be made most effective by combining different\nmechanisms together.\n\nOne particular cause dear to me personally is what I call\n\"entrepreneurial public goods\": public goods that in the present only a\nfew people believe are important but in the future many more people will\nvalue. In the 19th century, contributing to abolition of slavery may\nhave been one example; in the 21st century I can't give examples that\nwill satisfy every reader because it's the nature of these goods that\ntheir importance will only become common knowledge later down the road,\nbut I would point to life extension\nand AI risk research as two\npossible examples.\n\nThat said, we don't need to solve every problem today. Quadratic\npayments are an idea that has only become popular in the last few years;\nwe still have not seen more than small-scale trials of quadratic voting\nand funding, and quadratic attention payments have not been tried at\nall! There is still a long way to go. But if we can get these mechanisms\noff the ground, there is a lot that these mechanisms have to offer!",
    "contentLength": 27589,
    "summary": "Quadratic payments use increasing marginal costs (vote 1 costs $1, vote 2 costs $2, etc.) to fairly balance influence between those who care little vs. a lot.",
    "detailedSummary": {
      "theme": "Quadratic payments represent a new class of social technology that could revolutionize public decision-making by balancing individual preferences more fairly than traditional voting or market mechanisms.",
      "summary": "Vitalik introduces quadratic payments as a fundamental advancement in governance technology that addresses the shortcomings of both traditional democratic voting and market-based mechanisms. He explains that while private markets work well for individual goods, public goods require different approaches because they benefit multiple people simultaneously and cannot be easily decomposed into individual transactions. The core insight is that quadratic payments create a system where influence is proportional to how much someone cares about an outcome, achieved by making the nth unit of influence cost $n, resulting in total costs that scale quadratically. Vitalik explores three main applications: quadratic voting for collective decisions, quadratic funding for public goods provision, and quadratic attention payments for managing advertising and public attention. He demonstrates how each mechanism creates better alignment between individual preferences and collective outcomes compared to either pure democracy (one person, one vote) or pure markets (one dollar, one vote). However, Vitalik acknowledges significant implementation challenges including identity verification, preventing collusion and bribery, wealth inequality effects, and the problems of rational ignorance in public decision-making.",
      "takeaways": [
        "Quadratic payments offer a middle ground between pure democracy and pure markets by making influence proportional to preference intensity rather than wealth or simple headcount",
        "The mechanism works by charging $n for the nth unit of influence, creating quadratic total costs that balance concentrated and diffuse interests",
        "Three main applications are quadratic voting for decisions, quadratic funding for public goods, and quadratic attention payments for managing advertising",
        "Major implementation challenges include preventing identity fraud, collusion, and vote-buying while addressing wealth inequality and rational ignorance",
        "The technology requires advanced cryptographic solutions and identity systems to work effectively at scale"
      ],
      "controversial": [
        "The claim that quadratic payments could be 'as deeply transformative as the industrial-era advent of mostly-free markets and constitutional democracy' may be seen as overly ambitious",
        "The reliance on payment-based mechanisms still favors wealthy individuals, even if the effect is reduced compared to linear systems",
        "The technical requirements for preventing collusion and maintaining vote secrecy may be impractical or compromise privacy rights"
      ]
    }
  },
  {
    "id": "general-2019-11-22-progress",
    "title": "Hard Problems in Cryptocurrency: Five Years Later",
    "date": "2019-11-22",
    "category": "general",
    "url": "https://vitalik.eth.limo/general/2019/11/22/progress.html",
    "path": "general/2019/11/22/progress.html",
    "content": "- \n\n- \n\n  \n    \n\n  \n  \n  Dark Mode Toggle\n  \n    \n\n  \n\n##  Hard Problems in Cryptocurrency: Five Years Later \n\n 2019 Nov 22 \nSee all posts\n\n \n \n\n Hard Problems in Cryptocurrency: Five Years Later \n\nSpecial thanks to Justin Drake and Jinglan Wang for\nfeedback\n\nIn 2014, I made a post\nand a presentation with\na list of hard problems in math, computer science and economics that I\nthought were important for the cryptocurrency space (as I then called\nit) to be able to reach maturity. In the last five years, much has\nchanged. But exactly how much progress on what we thought then was\nimportant has been achieved? Where have we succeeded, where have we\nfailed, and where have we changed our minds about what is important? In\nthis post, I'll go through the 16 problems from 2014 one by one, and see\njust where we are today on each one. At the end, I'll include my new\npicks for hard problems of 2019.\n\nThe problems are broken down into three categories: (i)\ncryptographic, and hence expected to be solvable with purely\nmathematical techniques if they are to be solvable at all, (ii)\nconsensus theory, largely improvements to proof of work and proof of\nstake, and (iii) economic, and hence having to do with creating\nstructures involving incentives given to different participants, and\noften involving the application layer more than the protocol layer. We\nsee significant progress in all categories, though some more than\nothers.\n\n## Cryptographic problems\n\n- Blockchain Scalability\n\nOne of the largest problems facing the cryptocurrency space today is the\nissue of scalability ... The main concern with [oversized blockchains] is\ntrust: if there are only a few entities capable of running full nodes,\nthen those entities can conspire and agree to give themselves a large\nnumber of additional bitcoins, and there would be no way for other users\nto see for themselves that a block is invalid without processing an\nentire block themselves. Problem: create a blockchain\ndesign that maintains Bitcoin-like security guarantees, but where the\nmaximum size of the most powerful node that needs to exist for the\nnetwork to keep functioning is substantially sublinear in the number of\ntransactions.\n\nStatus: Great theoretical progress, pending more real-world\nevaluation.\n\nScalability is one technical problem that we have had a huge amount\nof progress on theoretically. Five years ago, almost no one was thinking\nabout sharding; now, sharding designs are commonplace. Aside from ethereum 2.0, we\nhave OmniLedger, LazyLedger, Zilliqa\nand research papers seemingly coming out every\nmonth. In my own view, further progress at this point is\nincremental. Fundamentally, we already have a number of techniques that\nallow groups of validators to securely come to consensus on much more\ndata than an individual validator can process, as well as techniques\nallow clients to indirectly verify the full validity and availability of\nblocks even under 51% attack conditions.\n\nThese are probably the most important technologies:\n\n- Random sampling, allowing a small randomly selected\ncommittee to statistically stand in for the full validator set: https://github.com/ethereum/wiki/wiki/Sharding-FAQ#how-can-we-solve-the-single-shard-takeover-attack-in-an-uncoordinated-majority-model\n\n- Fraud proofs, allowing individual nodes that learn\nof an error to broadcast its presence to everyone else: https://bitcoin.stackexchange.com/questions/49647/what-is-a-fraud-proof\n\n- Proofs of custody, allowing validators to\nprobabilistically prove that they individually downloaded and verified\nsome piece of data: https://ethresear.ch/t/1-bit-aggregation-friendly-custody-bonds/2236\n\n- Data availability proofs, allowing clients to\ndetect when the bodies of blocks that they have headers for are\nunavailable: https://arxiv.org/abs/1809.09044.\nSee also the newer coded\nMerkle trees proposal.\n\nThere are also other smaller developments like Cross-shard\ncommunication via receipts as well as \"constant-factor\" enhancements\nsuch as BLS signature aggregation.\n\nThat said, fully sharded blockchains have still not been seen in live\noperation (the partially sharded Zilliqa has recently started running).\nOn the theoretical side, there are mainly disputes about details\nremaining, along with challenges having to do with stability of sharded\nnetworking, developer experience and mitigating risks of centralization;\nfundamental technical possibility no longer seems in doubt. But the\nchallenges that do remain are challenges that cannot be solved\nby just thinking about them; only developing the system and seeing\nethereum 2.0 or some similar chain running live will suffice.\n\n- Timestamping\n\nProblem: create a distributed incentive-compatible\nsystem, whether it is an overlay on top of a blockchain or its own\nblockchain, which maintains the current time to high accuracy. All\nlegitimate users have clocks in a normal distribution around some \"real\"\ntime with standard deviation 20 seconds ... no two nodes are more than 20\nseconds apart The solution is allowed to rely on an existing concept of\n\"N nodes\"; this would in practice be enforced with proof-of-stake or\nnon-sybil tokens (see #9). The system should continuously provide a time\nwhich is within 120s (or less if possible) of the internal clock of\n>99% of honestly participating nodes. External systems may end up\nrelying on this system; hence, it should remain secure against attackers\ncontrolling < 25% of nodes regardless of incentives.\n\nStatus: Some progress.\n\nEthereum has actually survived just fine with a 13-second block time\nand no particularly advanced timestamping technology; it uses a simple\ntechnique where a client does not accept a block whose stated timestamp\nis earlier than the client's local time. That said, this has not been\ntested under serious attacks. The recent network-adjusted\ntimestamps proposal tries to improve on the status quo by allowing\nthe client to determine the consensus on the time in the case where the\nclient does not locally know the current time to high accuracy; this has\nnot yet been tested. But in general, timestamping is not currently at\nthe foreground of perceived research challenges; perhaps this will\nchange once more proof of stake chains (including Ethereum 2.0 but also\nothers) come online as real live systems and we see what the issues\nare.\n\n- Arbitrary Proof of Computation\n\nProblem: create programs\nPOC_PROVE(P,I) -> (O,Q) and\nPOC_VERIFY(P,O,Q) -> { 0, 1 } such that\nPOC_PROVE runs program P on input\nI and returns the program output O and a\nproof-of-computation Q and POC_VERIFY takes P,\nO and Q and outputs whether or not\nQ and O were legitimately produced by the\nPOC_PROVE algorithm using P.\n\nStatus: Great theoretical and practical progress.\n\nThis is basically saying, build a SNARK (or STARK, or SHARK, or...).\nAnd we've\ndone it! SNARKs are now\nincreasingly well understood, and are even already being used in\nmultiple blockchains today (including tornado.cash on Ethereum). And SNARKs\nare extremely useful, both as a privacy technology (see Zcash and\ntornado.cash) and as a scalability technology (see ZK\nRollup, STARKDEX and STARKing\nerasure coded data roots).\n\nThere are still challenges with efficiency; making\narithmetization-friendly hash functions (see here and here for bounties for breaking proposed\ncandidates) is a big one, and efficiently proving random memory accesses\nis another. Furthermore, there's the unsolved question of whether the\nO(n * log(n)) blowup in prover time is a fundamental limitation or if\nthere is some way to make a succinct proof with only linear overhead as\nin bulletproofs\n(which unfortunately take linear time to verify). There are also\never-present risks that the existing schemes have bugs. In general, the\nproblems are in the details rather than the fundamentals.\n\n- Code Obfuscation\n\nThe holy grail is to create an obfuscator O, such that given any program\nP the obfuscator can produce a second program O(P) = Q such that P and Q\nreturn the same output if given the same input and, importantly, Q\nreveals no information whatsoever about the internals of P. One can hide\ninside of Q a password, a secret encryption key, or one can simply use Q\nto hide the proprietary workings of the algorithm itself.\n\nStatus: Slow progress.\n\nIn plain English, the problem is saying that we want to come up with\na way to \"encrypt\" a program so that the encrypted program would still\ngive the same outputs for the same inputs, but the \"internals\" of the\nprogram would be hidden. An example use case for obfuscation is a\nprogram containing a private key where the program only allows the\nprivate key to sign certain messages.\n\nA solution to code obfuscation would be very useful to blockchain\nprotocols. The use cases are subtle, because one must deal with the\npossibility that an on-chain obfuscated program will be copied and run\nin an environment different from the chain itself, but there are many\npossibilities. One that personally interests me is the ability to remove\nthe centralized operator from collusion-resistance\ngadgets by replacing the operator with an obfuscated program that\ncontains some proof of work, making it very expensive to run more than\nonce with different inputs as part of an attempt to determine individual\nparticipants' actions.\n\nUnfortunately this continues to be a hard problem. There is\ncontinuing ongoing work in attacking the problem, one side making\nconstructions (eg. this)\nthat try to reduce the number of assumptions on mathematical objects\nthat we do not know practically exist (eg. general cryptographic\nmultilinear maps) and another side trying to make practical\nimplementations of the desired mathematical objects. However, all of\nthese paths are still quite far from creating something viable and known\nto be secure. See https://eprint.iacr.org/2019/463.pdf\nfor a more general overview to the problem.\n\n- Hash-Based Cryptography\n\nProblem: create a signature algorithm relying on no\nsecurity assumption but the random oracle property of hashes that\nmaintains 160 bits of security against classical computers (ie. 80\nvs.\u00a0quantum due to Grover's algorithm) with optimal size and other\nproperties.\n\nStatus: Some progress.\n\nThere have been two strands of progress on this since 2014. SPHINCS, a\n\"stateless\" (meaning, using it multiple times does not require\nremembering information like a nonce) signature scheme, was released\nsoon after this \"hard problems\" list was published, and provides a\npurely hash-based signature scheme of size around 41 kB. Additionally,\nSTARKs have been\ndeveloped, and one can create signatures of similar size based on them.\nThe fact that not just signatures, but also general-purpose zero\nknowledge proofs, are possible with just hashes was definitely something\nI did not expect five years ago; I am very happy that this is the case.\nThat said, size continues to be an issue, and ongoing progress (eg. see\nthe very recent DEEP FRI)\nis continuing to reduce the size of proofs, though it looks like further\nprogress will be incremental.\n\nThe main not-yet-solved problem with hash-based cryptography is\naggregate signatures, similar to what BLS\naggregation makes possible. It's known that we can just make a STARK\nover many Lamport signatures, but this is inefficient; a more efficient\nscheme would be welcome. (In case you're wondering if hash-based\npublic key encryption is possible, the answer is, no, you can't\ndo anything with more than a quadratic\nattack cost)\n\n## Consensus theory problems\n\n- ASIC-Resistant Proof of Work\n\nOne approach at solving the problem is creating a proof-of-work\nalgorithm based on a type of computation that is very difficult to\nspecialize ... For a more in-depth discussion on ASIC-resistant hardware,\nsee https://blog.ethereum.org/2014/06/19/mining/.\n\nStatus: Solved as far as we can.\n\nAbout six months after the \"hard problems\" list was posted, Ethereum\nsettled on its ASIC-resistant proof of work algorithm: Ethash. Ethash\nis known as a memory-hard algorithm. The theory is that random-access\nmemory in regular computers is well-optimized already and hence\ndifficult to improve on for specialized applications. Ethash aims to\nachieve ASIC resistance by making memory access the dominant part of\nrunning the PoW computation. Ethash was not the first memory-hard\nalgorithm, but it did add one innovation: it uses pseudorandom lookups\nover a two-level DAG, allowing for two ways of evaluating the function.\nFirst, one could compute it quickly if one has the entire (~2 GB) DAG;\nthis is the memory-hard \"fast path\". Second, one can compute it much\nmore slowly (still fast enough to check a single provided solution\nquickly) if one only has the top level of the DAG; this is used for\nblock verification.\n\nEthash has proven remarkably successful at ASIC resistance; after\nthree years and billions of dollars of block rewards, ASICs do exist but\nare at best 2-5\ntimes more power and cost-efficient than GPUs. ProgPoW has been\nproposed as an alternative, but there is a growing consensus that\nASIC-resistant algorithms will inevitably have a limited lifespan, and\nthat ASIC resistance has\ndownsides because it makes 51% attacks cheaper (eg. see the 51%\nattack on Ethereum Classic).\n\nI believe that PoW algorithms that provide a medium level of ASIC\nresistance can be created, but such resistance is limited-term and both\nASIC and non-ASIC PoW have disadvantages; in the long term the better\nchoice for blockchain consensus is proof of stake.\n\n- Useful Proof of Work\n\nmaking the proof of work function something which is simultaneously\nuseful; a common candidate is something like Folding@home, an existing\nprogram where users can download software onto their computers to\nsimulate protein folding and provide researchers with a large supply of\ndata to help them cure diseases.\n\nStatus: Probably not feasible, with one exception.\n\nThe challenge with useful proof of work is that a proof of work\nalgorithm requires many properties:\n\n- Hard to compute\n\n- Easy to verify\n\n- Does not depend on large amounts of external data\n\n- Can be efficiently computed in small \"bite-sized\" chunks\n\nUnfortunately, there are not many computations that are useful that\npreserve all of these properties, and most computations that do\nhave all of those properties and are \"useful\" are only \"useful\" for far\ntoo short a time to build a cryptocurrency around them.\n\nHowever, there is one possible exception: zero-knowledge-proof\ngeneration. Zero knowledge proofs of aspects of blockchain validity (eg.\ndata\navailability roots for a simple example) are difficult to compute,\nand easy to verify. Furthermore, they are durably difficult to compute;\nif proofs of \"highly structured\" computation become too easy, one can\nsimply switch to verifying a blockchain's entire state transition, which\nbecomes extremely expensive due to the need to model the virtual machine\nand random memory accesses.\n\nZero-knowledge proofs of blockchain validity provide great value to\nusers of the blockchain, as they can substitute the need to verify the\nchain directly; Coda is doing\nthis already, albeit with a simplified blockchain design that is heavily\noptimized for provability. Such proofs can significantly assist in\nimproving the blockchain's safety and scalability. That said, the total\namount of computation that realistically needs to be done is still much\nless than the amount that's currently done by proof of work miners, so\nthis would at best be an add-on for proof of stake blockchains, not a\nfull-on consensus algorithm.\n\n- Proof of Stake\n\nAnother approach to solving the mining centralization problem is to\nabolish mining entirely, and move to some other mechanism for counting\nthe weight of each node in the consensus. The most popular alternative\nunder discussion to date is \"proof of stake\" - that is to say, instead\nof treating the consensus model as \"one unit of CPU power, one vote\" it\nbecomes \"one currency unit, one vote\".\n\nStatus: Great theoretical progress, pending more real-world\nevaluation.\n\nNear the end of 2014, it became clear to the proof of stake community\nthat some form of \"weak subjectivity\" is\nunavoidable. To maintain economic security, nodes need to obtain a\nrecent checkpoint extra-protocol when they sync for the first time, and\nagain if they go offline for more than a few months. This was a\ndifficult pill to swallow; many PoW advocates still cling to PoW\nprecisely because in a PoW chain the \"head\" of the chain can be\ndiscovered with the only data coming from a trusted source being the\nblockchain client software itself. PoS advocates, however, were willing\nto swallow the pill, seeing the added trust requirements as not being\nlarge. From there the path to proof of stake through long-duration\nsecurity deposits became clear.\n\nMost interesting consensus algorithms today are fundamentally similar\nto PBFT, but\nreplace the fixed set of validators with a dynamic list that anyone can\njoin by sending tokens into a system-level smart contract with\ntime-locked withdrawals (eg. a withdrawal might in some cases take up to\n4 months to complete). In many cases (including ethereum 2.0), these\nalgorithms achieve \"economic finality\" by penalizing validators that are\ncaught performing actions that violate the protocol in certain ways (see\nhere\nfor a philosophical view on what proof of stake accomplishes).\n\nAs of today, we have (among many other algorithms):\n\n- Casper FFG: https://arxiv.org/abs/1710.09437\n\n- Tendermint: https://tendermint.com/docs/spec/consensus/consensus.html\n\n- HotStuff: https://arxiv.org/abs/1803.05069\n\n- Casper CBC: ../../../2018/12/05/cbc_casper.html\n\nThere continues to be ongoing refinement (eg. here\nand here)\n. Eth2 phase 0, the chain that will implement FFG, is currently under\nimplementation and enormous progress has been made. Additionally,\nTendermint has been running, in the form of the Cosmos chain for\nseveral months. Remaining arguments about proof of stake, in my view,\nhave to do with optimizing the economic incentives, and further\nformalizing the strategy\nfor responding to 51% attacks. Additionally, the Casper CBC\nspec could still use concrete efficiency improvements.\n\n- Proof of Storage\n\nA third approach to the problem is to use a scarce computational\nresource other than computational power or currency. In this regard, the\ntwo main alternatives that have been proposed are storage and bandwidth.\nThere is no way in principle to provide an after-the-fact cryptographic\nproof that bandwidth was given or used, so proof of bandwidth should\nmost accurately be considered a subset of social proof, discussed in\nlater problems, but proof of storage is something that certainly can be\ndone computationally. An advantage of proof-of-storage is that it is\ncompletely ASIC-resistant; the kind of storage that we have in hard\ndrives is already close to optimal.\n\nStatus: A lot of theoretical progress, though still a lot to\ngo, as well as more real-world evaluation.\n\nThere are a number of blockchains planning\nto use proof of storage protocols, including Chia and Filecoin. That said, these\nalgorithms have not been tested in the wild. My own main concern is\ncentralization: will these algorithms actually be dominated by smaller\nusers using spare storage capacity, or will they be dominated by large\nmining farms?\n\n## Economics\n\n- Stable-value cryptoassets\n\nOne of the main problems with Bitcoin is the issue of price volatility ...\nProblem: construct a cryptographic asset with a stable price.\n\nStatus: Some progress. <img\nsrc=\"../../../../images/progress-files/happy_face2.png\"\nstyle=\"width:50px; height: 50px\" ]class=\"transparent\" />\n\nMakerDAO is now live, and has\nbeen holding stable for nearly two years. It has survived a 93% drop in\nthe value of its underlying collateral asset (ETH), and there is now\nmore than $100 million in DAI issued. It has become a mainstay of the\nEthereum ecosystem, and many Ethereum projects have or are integrating\nwith it. Other synthetic token projects, such as UMA, are rapidly gaining steam as\nwell.\n\nHowever, while the MakerDAO system has survived tough economic\nconditions in 2019, the conditions were by no means the toughest that\ncould happen. In the past, Bitcoin has fallen by\n75% over the course of two days; the same may happen to ether or any\nother collateral asset some day. Attacks on the underlying blockchain\nare an even larger untested risk, especially if compounded by price\ndecreases at the same time. Another major challenge, and arguably the\nlarger one, is that the stability of MakerDAO-like systems is dependent\non some underlying oracle scheme. Different attempts at oracle systems\ndo exist (see #16), but the jury is still out on how well they can hold\nup under large amounts of economic stress. So far, the collateral\ncontrolled by MakerDAO has been lower than the value of the MKR token;\nif this relationship reverses MKR holders may have a collective\nincentive to try to \"loot\" the MakerDAO system. There are ways to try to\nprotect against such attacks, but they have not been tested in real\nlife.\n\n- Decentralized Public Goods Incentivization\n\nOne of the challenges in economic systems in general is the problem of\n\"public goods\". For example, suppose that there is a scientific research\nproject which will cost $1 million to complete, and it is known that if\nit is completed the resulting research will save one million people $5\neach. In total, the social benefit is clear ... [but] from the point of\nview of each individual person contributing does not make sense ... So\nfar, most problems to public goods have involved centralization\nAdditional Assumptions And Requirements: A fully trustworthy oracle\nexists for determining whether or not a certain public good task has\nbeen completed (in reality this is false, but this is the domain of\nanother problem)\n\nStatus: Some progress.\n\nThe problem of funding public goods is generally understood to be\nsplit into two problems: the funding problem (where to get funding for\npublic goods from) and the preference aggregation problem (how to\ndetermine what is a genuine public good, rather than some single\nindividual's pet project, in the first place). This problem focuses\nspecifically on the former, assuming the latter is solved (see the \"decentralized contribution metrics\" section\nbelow for work on that problem).\n\nIn general, there haven't been large new breakthroughs here. There's\ntwo major categories of solutions. First, we can try to elicit\nindividual contributions, giving people social rewards for doing so. My\nown proposal for charity through\nmarginal price discrimination is one example of this; another is the\nanti-malaria donation badges on Peepeth. Second, we can collect\nfunds from applications that have network effects. Within blockchain\nland there are several options for doing this:\n\n- Issuing coins\n\n- Taking a portion of transaction fees at protocol level (eg. through\nEIP\n1559)\n\n- Taking a portion of transaction fees from some layer-2 application\n(eg. Uniswap, or some scaling solution, or even state rent in an\nexecution environment in ethereum 2.0)\n\n- Taking a portion of other kinds of fees (eg. ENS registration)\n\nOutside of blockchain land, this is just the age-old question of how\nto collect taxes if you're a government, and charge fees if you're a\nbusiness or other organization.\n\n- Reputation systems\n\nProblem: design a formalized reputation system,\nincluding a score rep(A,B) -> V where V is the reputation of B from\nthe point of view of A, a mechanism for determining the probability that\none party can be trusted by another, and a mechanism for updating the\nreputation given a record of a particular open or finalized interaction.\n\nStatus: Slow progress.\n\nThere hasn't really been much work on reputation systems since 2014.\nPerhaps the best is the use of token curated registries to create\ncurated lists of trustable entities/objects; the Kleros\nERC20 TCR (yes, that's a token-curated\nregistry of legitimate ERC20 tokens) is one example, and there is\neven an alternative interface to Uniswap (http://uniswap.ninja) that uses it as\nthe backend to get the list of tokens and ticker symbols and logos from.\nReputation systems of the subjective variety have not really been tried,\nperhaps because there is just not enough information about the \"social\ngraph\" of people's connections to each other that has already been\npublished to chain in some form. If such information starts to exist for\nother reasons, then subjective reputation systems may become more\npopular.\n\n- Proof of excellence\n\nOne interesting, and largely unexplored, solution to the problem of\n[token] distribution specifically (there are reasons why it cannot be so\neasily used for mining) is using tasks that are socially useful but\nrequire original human-driven creative effort and talent. For example,\none can come up with a \"proof of proof\" currency that rewards players\nfor coming up with mathematical proofs of certain theorems\n\nStatus: No progress, problem is largely forgotten.\n\nThe main alternative approach to token distribution that has instead\nbecome popular is airdrops;\ntypically, tokens are distributed at launch either proportionately to\nexisting holdings of some other token, or based on some other metric\n(eg. as in the Handshake\nairdrop). Verifying human creativity directly has not really been\nattempted, and with recent progress on AI the problem of creating a task\nthat only humans can do but computers can verify may well be too\ndifficult.\n\n15 [sic]. Anti-Sybil systems\n\nA problem that is somewhat related to the issue of a reputation system\nis the challenge of creating a \"unique identity system\" - a system for\ngenerating tokens that prove that an identity is not part of a Sybil\nattack ... However, we would like to have a system that has nicer and more\negalitarian features than \"one-dollar-one-vote\"; arguably,\none-person-one-vote would be ideal.\n\nStatus: Some progress.\n\nThere have been quite a few attempts at solving the unique-human\nproblem. Attempts that come to mind include (incomplete list!):\n\n- HumanityDAO: https://www.humanitydao.org/\n\n- Pseudonym parties: https://bford.info/pub/net/sybil.pdf\n\n- POAP (\"proof of attendance protocol\"): https://www.poap.xyz/\n\n- BrightID: https://www.brightid.org/\n\nWith the growing interest in techniques like quadratic\nvoting and quadratic\nfunding, the need for some kind of human-based anti-sybil system\ncontinues to grow. Hopefully, ongoing development of these techniques\nand new ones can come to meet it.\n\n14 [sic]. Decentralized contribution metrics\n\nIncentivizing the production of public goods is, unfortunately, not the\nonly problem that centralization solves. The other problem is\ndetermining, first, which public goods are worth producing in the first\nplace and, second, determining to what extent a particular effort\nactually accomplished the production of the public good. This challenge\ndeals with the latter issue.\n\nStatus: Some progress, some change in focus.\n\nMore recent work on determining value of public-good contributions\ndoes not try to separate determining tasks and determining quality of\ncompletion; the reason is that in practice the two are difficult to\nseparate. Work done by specific teams tends to be non-fungible and\nsubjective enough that the most reasonable approach is to look at\nrelevance of task and quality of performance as a single package, and\nuse the same technique to evaluate both.\n\nFortunately, there has been great progress on this, particularly with\nthe discovery of quadratic\nfunding. Quadratic funding is a mechanism where individuals can make\ndonations to projects, and then based on the number of people who\ndonated and how much they donated, a formula is used to calculate how\nmuch they would have donated if they were perfectly coordinated with\neach other (ie. took each other's interests into account and did not\nfall prey to the tragedy of the commons). The difference between amount\nwould-have-donated and amount actually donated for any given project is\ngiven to that project as a subsidy from some central pool (see #11 for\nwhere the central pool funding could come from). Note that this\nmechanism focuses on satisfying the values of some community, not on\nsatisfying some given goal regardless of whether or not anyone cares\nabout it. Because of the complexity of\nvalues problem, this approach is likely to be much more robust to\nunknown unknowns.\n\nQuadratic funding has even been tried in real life with considerable\nsuccess in the recent gitcoin\nquadratic funding round. There has also been some incremental\nprogress on improving quadratic funding and similar mechanisms;\nparticularly, pairwise-bounded\nquadratic funding to mitigate collusion. There has also been work on\nspecification and implementation of bribe-resistant\nvoting technology, preventing users from proving to third parties who\nthey voted for; this prevents many kinds of collusion and bribe\nattacks.\n\n- Decentralized success metrics\n\nProblem: come up with and implement a decentralized method for measuring\nnumerical real-world variables ... the system should be able to measure\nanything that humans can currently reach a rough consensus on (eg. price\nof an asset, temperature, global CO2 concentration)\n\nStatus: Some progress.\n\nThis is now generally just called \"the oracle problem\". The largest\nknown instance of a decentralized oracle running is Augur, which has processed outcomes\nfor millions of dollars of bets. Token\ncurated registries such as the Kleros TCR for tokens are\nanother example. However, these systems still have not seen a real-world\ntest of the forking mechanism (search for \"subjectivocracy\" here)\neither due to a highly controversial question or due to an attempted 51%\nattack. There is also research on the oracle problem happening outside\nof the blockchain space in the form of the \"peer\nprediction\" literature; see here for a very recent\nadvancement in the space.\n\nAnother looming challenge is that people want to rely on these\nsystems to guide transfers of quantities of assets larger than the\neconomic value of the system's native token. In these conditions, token\nholders in theory have the incentive to collude to give wrong answers to\nsteal the funds. In such a case, the system would fork and the original\nsystem token would likely become valueless, but the original system\ntoken holders would still get away with the returns from whatever asset\ntransfer they misdirected. Stablecoins (see #10) are a particularly egregious case of this.\nOne approach to solving this would be a system that assumes that\naltruistically honest data providers do exist, and creating a mechanism\nto identify them, and only allowing them to churn slowly so that if\nmalicious ones start getting voted in the users of systems that rely on\nthe oracle can first complete an orderly exit. In any case, more\ndevelopment of oracle tech is very much an important problem.\n\n## New problems\n\nIf I were to write the hard problems list again in 2019, some would\nbe a continuation of the above problems, but there would be significant\nchanges in emphasis, as well as significant new problems. Here are a few\npicks:\n\n- Cryptographic obfuscation: same as #4 above\n\n- Ongoing work on post-quantum cryptography: both\nhash-based as well as based on post-quantum-secure \"structured\"\nmathematical objects, eg. elliptic curve isogenies, lattices...\n\n- Anti-collusion infrastructure: ongoing work and\nrefinement of https://ethresear.ch/t/minimal-anti-collusion-infrastructure/5413,\nincluding adding privacy against the operator, adding multi-party\ncomputation in a maximally practical way, etc.\n\n- Oracles: same as #16\nabove, but removing the emphasis on \"success metrics\" and focusing on\nthe general \"get real-world data\" problem\n\n- Unique-human identities (or, more realistically,\nsemi-unique-human identities): same as what was written as #15 above, but with an emphasis on a less\n\"absolute\" solution: it should be much harder to get two identities than\none, but making it impossible to get multiple identities is both\nimpossible and potentially harmful even if we do succeed\n\n- Homomorphic encryption and multi-party computation:\nongoing improvements are still required for practicality\n\n- Decentralized governance mechanisms: DAOs are cool,\nbut current DAOs are still very primitive; we can do better\n\n- Fully formalizing responses to PoS 51% attacks:\nongoing work and refinement of https://ethresear.ch/t/responding-to-51-attacks-in-casper-ffg/6363\n\n- More sources of public goods funding: the ideal is\nto charge for congestible resources inside of systems that have network\neffects (eg. transaction fees), but doing so in decentralized systems\nrequires public legitimacy; hence this is a social problem along with\nthe technical one of finding possible sources\n\n- Reputation systems: same as #12 above\n\nIn general, base-layer problems are slowly but surely decreasing, but\napplication-layer problems are only just getting started.",
    "contentLength": 32871,
    "summary": "After five years, blockchain has achieved great theoretical progress on scalability through sharding and made practical advances in SNARKs, but code obfuscation remains unsolved.",
    "detailedSummary": {
      "theme": "Vitalik evaluates five years of progress on 16 hard problems in cryptocurrency from 2014, finding significant theoretical advances in cryptography and consensus while identifying new challenges in application-layer development.",
      "summary": "In this comprehensive retrospective, Vitalik revisits his 2014 list of 16 fundamental problems facing cryptocurrency development, categorizing them into cryptographic, consensus theory, and economic challenges. He finds that many problems have seen substantial theoretical progress - particularly blockchain scalability through sharding, SNARKs for arbitrary proof of computation, and proof of stake consensus mechanisms. Practical implementations like MakerDAO for stablecoins and various anti-Sybil systems have also emerged. However, many solutions remain untested under extreme conditions or economic stress, and some problems like code obfuscation and proof of excellence have seen minimal progress. Vitalik concludes that while base-layer technical problems are gradually being solved, application-layer challenges involving governance, reputation systems, and social coordination are becoming increasingly important. He presents a new set of hard problems for 2019, emphasizing that the focus has shifted from fundamental protocol issues to more sophisticated challenges around privacy, governance, and human coordination systems.",
      "takeaways": [
        "Blockchain scalability has seen tremendous theoretical progress through sharding techniques, fraud proofs, and data availability solutions, though real-world implementation remains pending",
        "SNARKs and STARKs have successfully solved the arbitrary proof of computation problem and are now being used for both privacy and scalability applications",
        "Proof of stake has made great theoretical advances with multiple working algorithms, though it requires 'weak subjectivity' assumptions that some find controversial",
        "MakerDAO represents significant progress in stablecoin technology but remains untested under extreme market conditions and oracle attacks",
        "The cryptocurrency space is transitioning from solving base-layer technical problems to addressing more complex application-layer challenges involving human coordination and governance"
      ],
      "controversial": [
        "Vitalik's acceptance that proof of stake requires 'weak subjectivity' where nodes need recent checkpoints, abandoning the trustless ideal that many cryptocurrency advocates value",
        "His dismissal of ASIC resistance as ultimately futile and potentially harmful, arguing that proof of stake is superior despite its different trust assumptions",
        "The assertion that useful proof of work is 'probably not feasible' except for zero-knowledge proof generation, which challenges many projects built around this concept"
      ]
    }
  },
  {
    "id": "general-2019-10-24-gitcoin",
    "title": "Review of Gitcoin Quadratic Funding Round 3",
    "date": "2019-10-24",
    "category": "governance",
    "url": "https://vitalik.eth.limo/general/2019/10/24/gitcoin.html",
    "path": "general/2019/10/24/gitcoin.html",
    "content": "- \n\n- \n\n  \n    \n\n  \n  \n  Dark Mode Toggle\n  \n    \n\n  \n\n##  Review of Gitcoin Quadratic Funding Round 3 \n\n 2019 Oct 24 \nSee all posts\n\n \n \n\n Review of Gitcoin Quadratic Funding Round 3 \n\nSpecial thanks to the Gitcoin team and especially Frank Chen for\nworking with me through these numbers\n\nThe next round of Gitcoin Grants quadratic funding has just finished,\nand we the numbers for how much each project has received were just\nreleased. Here are the top ten:\n\nAltogether, $163,279 was donated to 80 projects by 477 contributors,\naugmented by a matching pool of $100,000. Nearly half came from four\ncontributions above $10,000: $37,500 to Lighthouse, and $12,500 each to\nGas Station Network, Black Girls Code and Public Health Incentives\nLayer. Out of the remainder, about half came from contributions between\n$1,000 and $10,000, and the rest came from smaller donations of various\nsizes. But what matters more here are not the raw donations, but rather\nthe subsidies that the quadratic\nfunding mechanism applied. Gitcoin Grants is there to support\nvaluable public goods in the Ethereum ecosystem, but also serve as a\ntestbed for this new quadratic donation matching mechanism, and see how\nwell it lives up to its promise of creating a democratic, market-based\nand efficient way of funding public goods. This time around, a modified\nformula based on pairwise-bounded\ncoordination subsidies was used, which has the goal of minimizing\ndistortion from large contributions from coordinated actors. And now we\nget to see how the experiment went.\n\n## Judging the Outcomes\n\nFirst, the results. Ultimately, every mechanism for allocating\nresources, whether centralized, market-based, democratic or otherwise,\nmust stand the test of delivering results, or else sooner or later it\nwill be abandoned for another mechanism that is perceived to be better,\neven if it is less philosophically clean. Judging results is inherently\na subjective exercise; any single person's analysis of a mechanism will\ninevitably be shaped by how well the results fit their own preferences\nand tastes. However, in those cases where a mechanism does output a\nsurprising result, one can and should use that as an opportunity to\nlearn, and see whether or not one missed some key information that other\nparticipants in the mechanism had.\n\nIn my own case, I found the top results very agreeable and a quite\nreasonable catalogue of projects that are good for the Ethereum\ncommunity. One of the disparities between these grants and the Ethereum\nFoundation grants is that the Ethereum Foundation grants (see recent\nrounds here\nand here)\ntend to overwhelmingly focus on technology with only a small section on\neducation and community resources, whereas in the Gitcoin grants while\ntechnology still dominates, EthHub is #2 and lower down defiprime.com is\n#14 and cryptoeconomics.study is #17. In this case my personal opinion\nis that EF has made a genuine error in undervaluing grants to\ncommunity/education organizations and Gitcoin's \"collective instinct\" is\ncorrect. Score one for new-age fancy quadratic market democracy.\n\nAnother surprising result to me was Austin Griffith getting second\nplace. I personally have never spent too much time thinking about Burner\nWallet; I knew that it existed but in my mental space I did not take it\ntoo seriously, focusing instead on client development, L2 scaling,\nprivacy and to a lesser extent smart contract wallets (the latter being\na key use case of Gas Station Network at #8). After seeing Austin's\nimpressive performance in this Gitcoin round, I asked a few people what\nwas going on.\n\nBurner Wallet (website, explainer\narticle) is an \"insta-wallet\" that's very easy to use: just load it\nup on your desktop or phone, and there you have it. It was used\nsuccessfully at EthDenver to sell food from food trucks, and generally\nmany people appreciate its convenience. Its main weaknesses are lower\nsecurity and that one of its features, support for xDAI, is dependent on\na permissioned chain.\n\nAustin's Gitcoin grant is there to fund his ongoing\nwork, and I have heard one criticism: there's many prototypes, but\ncomparatively few \"things taken to completion\". There is also the\ncritique that as great as Austin is, it's difficult to argue that he's\nas important to the success of Ethereum as, say, Lighthouse and\nPrysmatic, though one can reply that what matters is not total value,\nbut rather the marginal value of giving a given project or person an\nextra $10,000. On the whole, however, I feel like quadratic funding's\n(Glen would say deliberate!) tendency to select for things like Burner\nWallet with populist appeal is a much needed corrective to the influence\nof the Ethereum tech elite (including myself!) who often value technical\nimpressiveness and undervalue simple and quick things that make it\nreally easy for people to participate in Ethereum. This one is slightly\nmore ambiguous, but I'll say score two for new-age fancy quadratic\nmarket democracy.\n\nThe main thing that I was disappointed the Gitcoiner-ati did\nnot support more was Gitcoin maintenance itself. The Gitcoin\nSustainability Fund only got a total $1,119 in raw contributions from 18\nparticipants, plus a match of $202. The optional 5% tips that users\ncould give to Gitcoin upon donating were not included into the quadratic\nmatching calculations, but raised another ~$1,000. Given the amount of\neffort the Gitcoin people put in to making quadratic funding possible,\nthis is not nearly enough; Gitcoin clearly deserves more than 0.9% of\nthe total donations in the round. Meanwhile, the Ethereum Foundation (as\nwell as Consensys and individual donors) have been giving grants to\nGitcoin that include supporting Gitcoin itself. Hopefully in future\nrounds people will support Gitcoin itself too, but for now, score one\nfor good old-fashioned EF technocracy.\n\nOn the whole, quadratic funding, while still young and immature,\nseems to be a remarkably effective complement to the funding preferences\nof existing institutions, and it seems worthwhile to continue it and\neven increase its scope and size in the future.\nPairwise-bounded\nquadratic funding vs traditional quadratic funding\n\nRound 3 differs from previous rounds in that it uses a new flavor\nof quadratic funding, which limits the subsidy per pair of\nparticipants. For example, in traditional QF, if two people each donate\n$10, the subsidy would be $10, and if two people each donate $10,000,\nthe subsidy would be $10,000. This property of traditional QF makes it\nhighly vulnerable to collusion: two key employees of a project (or even\ntwo fake accounts owned by the same person) could each donate as much\nmoney as they have, and get back a very large subsidy. Pairwise-bounded\nQF computes the total subsidy to a project by looking through all pairs\nof contributors, and imposes a maximum bound on the total subsidy that\nany given pair of participants can trigger (combined across all\nprojects). Pairwise-bounded QF also has the property that it generally\npenalizes projects that are dominated by large contributors:\n\nThe projects that lost the most relative to traditional QF seem to be\nprojects that have a single large contribution (or sometimes two). For\nexample, \"fuzz geth and Parity for EVM consensus bugs\" got a $415 match\ncompared to the $2000 he would have gotten in traditional QF; the\ndecrease is explained by the fact that the contributions are dominated\nby two large $4500 contributions. On the other hand, cryptoeconomics.study got $1274,\nup nearly double from the $750 it would have gotten in\ntraditional QF; this is explained by the large diversity of\ncontributions that the project received and particularly the lack of\nlarge sponsors: the largest contribution to cryptoeconomics.study was\n$100.\n\nAnother desirable property of pairwise-bounded QF is that it\nprivileges cross-tribal projects. That is, if there are\nprojects that group A typically supports, and projects that group B\ntypically supports, then projects that manage to get support from both\ngroups get a more favorable subsidy (because the pairs that go between\ngroups are not as saturated). Has this incentive for building bridges\nappeared in these results?\n\nUnfortunately, my code of honor as a social scientist obliges me to\nreport the negative result: the Ethereum community just does not yet\nhave enough internal tribal structure for effects like this to\nmaterialize, and even when there are differences in correlations they\ndon't seem strongly connected to higher subsidies due to\npairwise-bounding. Here are the cross-correlations between who\ncontributed to different projects:\n\nGenerally, all projects are slightly positively correlated with each\nother, with a few exceptions with greater correlation and one exception\nwith broad roughly zero correlation: Nori (120 in this chart).\nHowever, Nori did not do well in pairwise-bounded QF, because over 94%\nof its donations came from a single $5000 donation.\n\n## Dominance of large projects\n\nOne other pattern that we saw in this round is that popular projects\ngot disproportionately large grants:\n\nTo be clear, this is not just saying \"more contributions, more\nmatch\", it's saying \"more contributions, more match per dollar\ncontributed\". Arguably, this is an intended feature of the\nmechanism. Projects that can get more people to donate to them represent\npublic goods that serve a larger public, and so tragedy of the commons\nproblems are more severe and hence contributions to them should be\nmultiplied more to compensate. However, looking at the list, it's hard\nto argue that, say, Prysm ($3,848 contributed, $8,566 matched) is a more\npublic good than Nimbus ($1,129 contributed, $496 matched; for the\nunaware, Prysm and Nimbus are both eth2 clients). The failure does not\nlook too severe; on average, projects near the top do seem to serve a\nlarger public and projects near the bottom do seem niche, but it seems\nclear that at least part of the disparity is not genuine publicness of\nthe good, but rather inequality of attention. N units of marketing\neffort can attract attention of N people, and theoretically get N^2\nresources.\n\nOf course, this could be solved via a \"layer on top\" venture-capital\nstyle: upstart new projects could get investors to support them, in\nreturn for a share of matched contributions received when they get\nlarge. Something like this would be needed eventually; predicting future\npublic goods is as important a social function as predicting future\nprivate goods. But we could also consider less winner-take-all\nalternatives; the simplest one would be adjusting the QF formula so it\nuses an exponent of eg. 1.5 instead of 2. I can see it being worthwhile\nto try a future round of Gitcoin Grants with such a formula (\\(\\left(\\sum_i\nx_i^{\\frac{2}{3}}\\right)^{\\frac{3}{2}}\\) instead of \\(\\left(\\sum_i x_i^{\\frac{1}{2}}\\right)^2\\))\nto see what the results are like.\n\n## Individual leverage curves\n\nOne key question is, if you donate $1, or $5, or $100, how big an\nimpact can you have on the amount of money that a project gets?\nFortunately, we can use the data to calculate these deltas!\n\nThe different lines are for different projects; supporting projects\nwith higher existing support will lead to you getting a bigger\nmultiplier. In all cases, the first dollar is very valuable, with a\nmatching ratio in some cases over 100:1. But the second dollar is much\nless valuable, and matching ratios quickly taper off; even for the\nlargest projects increasing one's donation from $32 to $64 will only get\na 1:1 match, and anything above $100 becomes almost a straight donation\nwith nearly no matching. However, given that it's likely possible to get\nlegitimate-looking Github accounts on the grey market for around those\ncosts, having a cap of a few hundred dollars on the amount of matched\nfunds that any particular account can direct seems like a very\nreasonable mitigation, despite its costs in limiting the bulk of the\nmatching effect to small-sized donations.\n\n## Conclusions\n\nOn the whole, this was by far the largest and the most data-rich\nGitcoin funding round to date. It successfully attracted hundreds of\ncontributors, reaching a size where we can finally see many significant\neffects in play and drown out the effects of the more naive forms of\nsmall-scale collusion. The experiment already seems to be leading to\nvaluable information that can be used by future quadratic funding\nimplementers to improve their quadratic funding implementations. The\ncase of Austin Griffith is also interesting because $23,911 in funds\nthat he received comes, in relative terms, surprisingly close to an\naverage salary for a developer if the grants can be repeated on a\nregular schedule. What this means is that if Gitcoin Grants\ndoes continue operating regularly, and attracts and expands its\npool of donations, we could be very close to seeing the first \"quadratic\nfreelancer\" - someone directly \"working for the public\", funded by\ndonations boosted by quadratic matching subsidies. And at that point we\ncould start to see more experimentation in new forms of organization\nthat live on top of quadratic funding gadgets as a base layer. All in\nall, this foretells an exciting and, err, radical public-goods funding\nfuture ahead of us.",
    "contentLength": 13215,
    "summary": "Gitcoin's third quadratic funding round distributed $163k across 80 projects, with results favoring community/education over pure tech.",
    "detailedSummary": {
      "theme": "Vitalik's comprehensive analysis of Gitcoin's third quadratic funding round, evaluating the effectiveness of this experimental mechanism for funding Ethereum public goods.",
      "summary": "Vitalik analyzes Gitcoin's third quadratic funding round, which distributed $163,279 from 477 contributors across 80 projects, augmented by a $100,000 matching pool. He evaluates the outcomes by comparing them to traditional Ethereum Foundation grants and finds that quadratic funding produces surprisingly good results, particularly in supporting community/education projects that the EF has undervalued. Vitalik highlights Austin Griffith's second-place finish with Burner Wallet as an example of how quadratic funding's 'populist appeal' serves as a valuable corrective to tech elite preferences. This round used a modified 'pairwise-bounded' formula to reduce collusion vulnerability, which penalized projects dominated by large contributors while rewarding those with diverse, smaller donations. Vitalik notes concerning patterns like the disproportionate advantage given to popular projects and the disappointing lack of support for Gitcoin's own sustainability fund. Despite these issues, he concludes that quadratic funding shows remarkable promise as a complement to existing funding mechanisms, potentially enabling the emergence of 'quadratic freelancers' who work directly for the public through donation-funded roles.",
      "takeaways": [
        "Quadratic funding successfully identified valuable public goods that traditional institutions like the Ethereum Foundation have undervalued, particularly in community and education sectors",
        "The pairwise-bounded formula effectively reduced collusion vulnerability by capping subsidies per contributor pair and penalizing projects dominated by large donations",
        "Popular projects received disproportionately large grants due to the mechanism's structure, creating a potential 'rich get richer' dynamic that may not reflect true public good value",
        "Individual donation leverage is highest for the first dollar (sometimes 100:1 matching) but rapidly diminishes, with donations over $100 receiving minimal matching",
        "The funding amounts achieved suggest we may be approaching the possibility of 'quadratic freelancers' - developers funded entirely through quadratic funding mechanisms"
      ],
      "controversial": [
        "Vitalik's assertion that the Ethereum Foundation has made a 'genuine error' in undervaluing community/education grants compared to technical development",
        "His suggestion that Austin Griffith's high ranking, despite criticisms about completing projects, represents a beneficial 'populist' corrective to tech elite preferences"
      ]
    }
  },
  {
    "id": "general-2019-10-01-story",
    "title": "In-person meatspace protocol to prove unconditional possession of a private key",
    "date": "2019-10-01",
    "category": "math",
    "url": "https://vitalik.eth.limo/general/2019/10/01/story.html",
    "path": "general/2019/10/01/story.html",
    "content": "- \n\n- \n\n  \n    \n\n  \n  \n  Dark Mode Toggle\n  \n    \n\n  \n\n##  In-person meatspace protocol to prove unconditional possession of a private key \n\n 2019 Oct 01 \nSee all posts\n\n \n \n\n In-person meatspace protocol to prove unconditional possession of a private key \n\nRecommended pre-reading: https://ethresear.ch/t/minimal-anti-collusion-infrastructure/5413\n\nAlice slowly walks down the old, dusty stairs of the building into\nthe basement. She thinks wistfully of the old days, when\nquadratic-voting in the World Collective Market was a much simpler\nprocess of linking her public key to a twitter account and opening up\nmetamask to start firing off votes. Of course back then voting in the\nWCM was used for little; there were a few internet forums that used it\nfor voting on posts, and a few million dollars donated to its quadratic\nfunding oracle. But then it grew, and then the game-theoretic\nattacks came.\n\nFirst came the exchange platforms, which started offering \"dividends\" to anyone who\nregistered a public key belonging to an exchange and thus provably\nallowed the exchange to vote on their behalf, breaking the crucial\n\"independent choice\" assumption of the quadratic voting and funding\nmechanisms. And soon after that came the fake accounts - Twitter\naccounts, Reddit accounts filtered by karma score, national government\nIDs, all proved vulnerable to either government cheating or hackers, or\nboth. Elaborate infrastructure was instituted at registration time to\nensure both that account holders were real people, and that account\nholders themselves held the keys, not a central custody service\npurchasing keys by the thousands to buy votes.\n\nAnd so today, voting is still easy, but initiation, while still not\nharder than going to a government office, is no longer exactly trivial.\nBut of course, with billions of dollars in donations from now-deceased\nbillionaires and cryptocurrency premines forming part of the WCM's\nquadratic funding pool, and elements of municipal governance using its\nquadratic voting protocols, participating is very much worth it.\n\nAfter reaching the end of the stairs, Alice opens the door and enters\nthe room. Inside the room, she sees a table. On the near side of the\ntable, she sees a single, empty chair. On the far side of the table, she\nsees four people already sitting down on chairs of their own, the\nhigh-reputation Guardians randomly selected by the WCM for Alice's\nregistration ceremony. \"Hello, Alice,\" the person sitting on the\nleftmost chair, whose name she intuits is Bob, says in a calm voice.\n\"Glad that you can make it,\" the person sitting beside Bob, whose name\nshe intuits is Charlie, adds.\n\nAlice walks over to the chair that is clearly meant for her and sits\ndown. \"Let us begin,\" the person sitting beside Charlie, whose name by\nlogical progression is David, proclaims. \"Alice, do you have your key\nshares?\"\n\nAlice takes out four pocket-sized notebooks, clearly bought from a\ndollar store, and places them on the table. The person sitting at the\nright, logically named Evan, takes out his phone, and immediately the\nothers take out theirs. They open up their ethereum wallets. \"So,\" Evan\nbegins, \"the current Ethereum beacon chain slot number is 28,205,913,\nand the block hash starts 0xbe48. Do all agree?\". \"Yes,\"\nAlice, Bob, Charlie and David exclaim in unison. Evan continues: \"so let\nus wait for the next block.\"\n\nThe five intently stare at their phones. First for ten seconds, then\ntwenty, then thirty. \"Three skipped proposers,\" Bob mutters, \"how\nunusual\". But then after another ten seconds, a new block appears. \"Slot\nnumber 28,205,917, block hash starts 0x62f9, so first digit\n6. All agreed?\"\n\n\"Yes.\"\n\n\"Six mod four is two, and as is prescribed in the Old Ways, we start\ncounting indices from zero, so this means Alice will keep the third\nbook, counting as usual from our left.\"\n\nBob takes the first, second and fourth notebooks that Alice provided,\nleaving the third untouched. Alice takes the remaining notebook and puts\nit back in her backpack. Bob opens each notebook to a page in the middle\nwith the corner folded, and sees a sequence of letters and numbers\nwritten with a pencil in the middle of each page - a standard way of\nwriting the key shares for over a decade, since camera and image\nprocessing technology got powerful enough to recognize words and numbers\nwritten on single slips of paper even inside an envelope. Bob, Charlie,\nDavid and Evan crowd around the books together, and each open up an app\non their phone and press a few buttons.\n\nBob starts reading, as all four start typing into their phones at the\nsame time:\n\n\"Alice's first key share is,\n6-b-d-7-h-k-k-l-o-e-q-q-p-3-y-s-6-x-e-f. Applying the\n100,000x iterated SHA256 hash we get e-a-6-6...,\nconfirm?\"\n\n\"Confirmed,\" the others replied. \"Checking against Alice's\nprecommitted elliptic curve point A0... match.\"\n\n\"Alice's second key share is,\nf-r-n-m-j-t-x-r-s-3-b-u-n-n-n-i-z-3-d-g. Iterated hash\n8-0-3-c..., confirm?\"\n\n\"Confirmed. Checking against Alice's precommitted elliptic curve\npoint A1... match.\"\n\n\"Alice's fourth key share is,\ni-o-f-s-a-q-f-n-w-f-6-c-e-a-m-s-6-z-z-n. Iterated hash\n6-a-5-6..., confirm?\"\n\n\"Confirmed. Checking against Alice's precommitted elliptic curve\npoint A3... match.\"\n\n\"Adding the four precommitted curve points, x coordinate begins\n3-1-8-3. Alice, confirm that that is the key you wish to\nregister?\"\n\n\"Confirm.\"\n\nBob, Charlie, David and Evan glance down at their smartphone apps one\nmore time, and each tap a few buttons. Alice catches a glance at\nCharlie's phone; she sees four yellow checkmarks, and an \"approval\ntransaction pending\" dialog. After a few seconds, the four yellow\ncheckmarks are replaced with a single green checkmark, with a\ntransaction hash ID, too small for Alice to make out the digits from a\nfew meters away, below. Alice's phone soon buzzes, with a notification\ndialog saying \"Registration confirmed\".\n\n\"Congratulations, Alice,\" Bob says. \"Unconditional possession of your\nkey has been verified. You are now free to send a transaction to the\nWorld Collective Market's MPC oracle to update your key.\"\n\n\"Only a 75% probability this would have actually caught me if I\ndidn't actually have all four parts of the key,\" Alice thought to\nherself. But it seemed to be enough for an in-person protocol in\npractice; and if it ever wasn't then they could easily switch to\nslightly more complex protocols that used low-degree polynomials to\nachieve exponentially high levels of soundness. Alice taps a few buttons\non her smartphone, and a \"transaction pending\" dialog shows up on the\nscreen. Five seconds later, the dialog disappears and is replaced by a\ngreen checkmark. She jumps up with joy and, before Bob, Charlie, David\nand Evan can say goodbye, runs out of the room, frantically tapping\nbuttons to vote on all the projects and issues in the WCM that she had\nwanted to support for months.",
    "contentLength": 6874,
    "summary": "Alice undergoes an in-person ceremony where guardians verify she possesses a private key by checking secret shares before blockchain voting.",
    "detailedSummary": {
      "theme": "Vitalik presents a fictional narrative describing an in-person cryptographic protocol to verify authentic private key ownership for preventing voting manipulation in quadratic voting systems.",
      "summary": "Vitalik crafts a speculative fiction set in a future where quadratic voting and funding mechanisms have become critical infrastructure worth billions of dollars, but face sophisticated gaming attacks from exchanges offering vote-buying services and fake account creation. The story follows Alice through a ceremonial key verification process involving four guardians who use secret sharing, random beacon selection, and cryptographic proofs to verify she genuinely controls her private key rather than allowing a third party to vote on her behalf. The protocol uses a 4-of-4 secret sharing scheme where Alice must demonstrate possession of key shares through iterated hashing and elliptic curve verification, providing approximately 75% probability of detecting fraud in this implementation, though Vitalik notes more sophisticated polynomial-based approaches could achieve exponentially higher security guarantees.",
      "takeaways": [
        "Quadratic voting and funding systems are vulnerable to centralized vote-buying schemes where exchanges or services control users' keys",
        "In-person verification protocols can help ensure individual agency in voting systems by proving actual private key possession",
        "Secret sharing schemes combined with random selection can create practical anti-collusion mechanisms",
        "The protocol described achieves 75% fraud detection probability but could be enhanced with polynomial-based approaches for exponentially better security",
        "As blockchain-based governance scales to handle significant financial and political decisions, sophisticated anti-gaming measures become essential infrastructure requirements"
      ],
      "controversial": [
        "The trade-off between voting accessibility and security, as the proposed system makes participation significantly more burdensome",
        "Whether such elaborate verification processes are practical or justified for democratic participation",
        "The assumption that in-person protocols are necessarily more secure than digital alternatives in preventing collusion"
      ]
    }
  },
  {
    "id": "general-2019-09-22-plonk",
    "title": "Understanding PLONK",
    "date": "2019-09-22",
    "category": "math",
    "url": "https://vitalik.eth.limo/general/2019/09/22/plonk.html",
    "path": "general/2019/09/22/plonk.html",
    "content": "- \n\n- \n\n  \n    \n\n  \n  \n  Dark Mode Toggle\n  \n    \n\n  \n\n##  Understanding PLONK \n\n 2019 Sep 22 \nSee all posts\n\n \n \n\n Understanding PLONK \n\nSpecial thanks to Justin Drake, Karl Floersch, Hsiao-wei Wang,\nBarry Whitehat, Dankrad Feist, Kobi Gurkan and Zac Williamson for\nreview\n\nVery recently, Ariel Gabizon, Zac Williamson and Oana Ciobotaru\nannounced a new general-purpose zero-knowledge proof scheme called PLONK, standing for the\nunwieldy quasi-backronym \"Permutations over Lagrange-bases for\nOecumenical Noninteractive arguments of Knowledge\". While improvements to\ngeneral-purpose zero-knowledge proof\nprotocols have been coming\nfor years, what PLONK\n(and the earlier but more complex SONIC\nand the more recent Marlin) bring to the\ntable is a series of enhancements that may greatly improve the usability\nand progress of these kinds of proofs in general.\n\nThe first improvement is that while PLONK still requires a trusted\nsetup procedure similar to that needed for the SNARKs in Zcash,\nit is a \"universal and updateable\" trusted setup. This means two things:\nfirst, instead of there being one separate trusted setup for every\nprogram you want to prove things about, there is one single trusted\nsetup for the whole scheme after which you can use the scheme with any\nprogram (up to some maximum size chosen when making the setup). Second,\nthere is a way for multiple parties to participate in the trusted setup\nsuch that it is secure as long as any one of them is honest, and this\nmulti-party procedure is fully sequential: first one person\nparticipates, then the second, then the third... The full set of\nparticipants does not even need to be known ahead of time; new\nparticipants could just add themselves to the end. This makes it easy\nfor the trusted setup to have a large number of participants, making it\nquite safe in practice.\n\nThe second improvement is that the \"fancy cryptography\" it relies on\nis one single standardized component, called a \"polynomial commitment\".\nPLONK uses \"Kate commitments\", based on a trusted setup and elliptic\ncurve pairings, but you can instead swap it out with other schemes, such\nas FRI (which would\nturn PLONK into a kind of\nSTARK) or DARK (based on hidden-order groups). This means the scheme\nis theoretically compatible with any (achievable) tradeoff between proof\nsize and security assumptions.\n\nWhat this means is that use cases that require different tradeoffs\nbetween proof size and security assumptions (or developers that have\ndifferent ideological positions about this question) can still share the\nbulk of the same tooling for \"arithmetization\" - the process for\nconverting a program into a set of polynomial equations that the\npolynomial commitments are then used to check. If this kind of scheme\nbecomes widely adopted, we can thus expect rapid progress in improving\nshared arithmetization techniques.\n\n## How PLONK works\n\nLet us start with an explanation of how PLONK works, in a somewhat\nabstracted format that focuses on polynomial equations without\nimmediately explaining how those equations are verified. A key\ningredient in PLONK, as is the case in the QAPs\nused in SNARKs, is a procedure for converting a problem of the form\n\"give me a value \\(X\\) such that a\nspecific program \\(P\\) that I give you,\nwhen evaluated with \\(X\\) as an input,\ngives some specific result \\(Y\\)\" into\nthe problem \"give me a set of values that satisfies a set of math\nequations\". The program \\(P\\) can\nrepresent many things; for example the problem could be \"give me a\nsolution to this sudoku\", which you would encode by setting \\(P\\) to be a sudoku verifier plus some\ninitial values encoded and setting \\(Y\\) to \\(1\\) (ie. \"yes, this solution is correct\"),\nand a satisfying input \\(X\\) would be a\nvalid solution to the sudoku. This is done by representing \\(P\\) as a circuit with logic gates for\naddition and multiplication, and converting it into a system of\nequations where the variables are the values on all the wires and there\nis one equation per gate (eg. \\(x_6 = x_4\n\\cdot x_7\\) for multiplication, \\(x_8 =\nx_5 + x_9\\) for addition).\n\nHere is an example of the problem of finding \\(x\\) such that \\(P(x) = x^3 + x + 5 = 35\\) (hint: \\(x = 3\\)):\n\nWe can label the gates and wires as follows:\n\nOn the gates and wires, we have two types of constraints:\ngate constraints (equations between wires attached to\nthe same gate, eg. \\(a_1 \\cdot b_1 =\nc_1\\)) and copy constraints (claims about\nequality of different wires anywhere in the circuit, eg. \\(a_0 = a_1 = b_1 = b_2 = a_3\\) or \\(c_0 = a_1\\)). We will need to create a\nstructured system of equations, which will ultimately reduce to a very\nsmall number of polynomial equations, to represent both.\n\nIn PLONK, the setup for these equations is as follows. Each equation\nis of the following form (think: \\(L\\)\n= left, \\(R\\) = right, \\(O\\) = output, \\(M\\) = multiplication, \\(C\\) = constant):\n\n\\[\n\\left(Q_{L_{i}}\\right) a_{i}+\\left(Q_{R_{i}}\\right)\nb_{i}+\\left(Q_{O_{i}}\\right) c_{i}+\\left(Q_{M_{i}}\\right) a_{i}\nb_{i}+Q_{C_{i}}=0\n\\]\n\nEach \\(Q\\) value is a constant; the\nconstants in each equation (and the number of equations) will be\ndifferent for each program. Each small-letter value is a variable,\nprovided by the user: \\(a_i\\) is the\nleft input wire of the \\(i\\)'th gate,\n\\(b_i\\) is the right input wire, and\n\\(c_i\\) is the output wire of the \\(i\\)'th gate. For an addition gate, we\nset:\n\n\\[\nQ_{L_{i}}=1, Q_{R_{i}}=1, Q_{M_{i}}=0, Q_{O_{i}}=-1, Q_{C_{i}}=0\n\\]\n\nPlugging these constants into the equation and simplifying gives us\n\\(a_i + b_i - c_i = 0\\), which is\nexactly the constraint that we want. For a multiplication gate, we\nset:\n\n\\[\nQ_{L_{i}}=0, Q_{R_{i}}=0, Q_{M_{i}}=1, Q_{O_{i}}=-1, Q_{C_{i}}=0\n\\]\n\nFor a constant gate setting \\(a_i\\)\nto some constant \\(x\\), we set:\n\n\\[\nQ_{L}=1, Q_{R}=0, Q_{M}=0, Q_{O}=0, Q_{C}=-x\n\\]\n\nYou may have noticed that each end of a wire, as well as each wire in\na set of wires that clearly must have the same value (eg. \\(x\\)), corresponds to a distinct variable;\nthere's nothing so far forcing the output of one gate to be the same as\nthe input of another gate (what we call \"copy constraints\"). PLONK does\nof course have a way of enforcing copy constraints, but we'll get to\nthis later. So now we have a problem where a prover wants to prove that\nthey have a bunch of \\(x_{a_i},\nx_{b_i}\\) and \\(x_{c_i}\\) values\nthat satisfy a bunch of equations that are of the same form. This is\nstill a big problem, but unlike \"find a satisfying input to this\ncomputer program\" it's a very structured big problem, and we\nhave mathematical tools to \"compress\" it.\nFrom linear systems to\npolynomials\n\nIf you have read about STARKs or QAPs,\nthe mechanism described in this next section will hopefully feel\nsomewhat familiar, but if you have not that's okay too. The main\ningredient here is to understand a polynomial as a mathematical\ntool for encapsulating a whole lot of values into a single object.\nTypically, we think of polynomials in \"coefficient form\", that is an\nexpression like:\n\n\\[\ny=x^{3}-5 x^{2}+7 x-2\n\\]\n\nBut we can also view polynomials in \"evaluation form\". For example,\nwe can think of the above as being \"the\" degree \\(< 4\\) polynomial with evaluations \\((-2, 1, 0, 1)\\) at the coordinates \\((0, 1, 2, 3)\\) respectively.\n\nNow here's the next step. Systems of many equations of the same form\ncan be re-interpreted as a single equation over polynomials. For\nexample, suppose that we have the system:\n\n\\[\n\\begin{array}{l}{2 x_{1}-x_{2}+3 x_{3}=8} \\\\ {x_{1}+4 x_{2}-5 x_{3}=5}\n\\\\ {8 x_{1}-x_{2}-x_{3}=-2}\\end{array}\n\\]\n\nLet us define four polynomials in evaluation form: \\(L(x)\\) is the degree \\(< 3\\) polynomial that evaluates to \\((2, 1, 8)\\) at the coordinates \\((0, 1, 2)\\), and at those same coordinates\n\\(M(x)\\) evaluates to \\((-1, 4, -1)\\), \\(R(x)\\) to \\((3,\n-5, -1)\\) and \\(O(x)\\) to \\((8, 5, -2)\\) (it is okay to directly define\npolynomials in this way; you can use Lagrange\ninterpolation to convert to coefficient form). Now, consider the\nequation:\n\n\\[\nL(x) \\cdot x_{1}+M(x) \\cdot x_{2}+R(x) \\cdot x_{3}-O(x)=Z(x) H(x)\n\\]\n\nHere, \\(Z(x)\\) is shorthand for\n\\((x-0) \\cdot (x-1) \\cdot (x-2)\\) - the\nminimal (nontrivial) polynomial that returns zero over the evaluation\ndomain \\((0, 1, 2)\\). A solution to\nthis equation (\\(x_1 = 1, x_2 = 6, x_3 = 4,\nH(x) = 0\\)) is also a solution to the original system of\nequations, except the original system does not need \\(H(x)\\). Notice also that in this case,\n\\(H(x)\\) is conveniently zero, but in\nmore complex cases \\(H\\) may need to be\nnonzero.\n\nSo now we know that we can represent a large set of constraints\nwithin a small number of mathematical objects (the polynomials). But in\nthe equations that we made above to represent the gate wire constraints,\nthe \\(x_1, x_2, x_3\\) variables are\ndifferent per equation. We can handle this by making the variables\nthemselves polynomials rather than constants in the same way. And so we\nget:\n\n\\[\nQ_{L}(x) a(x)+Q_{R}(x) b(x)+Q_{O}(x) c(x)+Q_{M}(x) a(x) b(x)+Q_{C}(x)=0\n\\]\n\nAs before, each \\(Q\\) polynomial is\na parameter that can be generated from the program that is being\nverified, and the \\(a\\), \\(b\\), \\(c\\)\npolynomials are the user-provided inputs.\n\n## Copy constraints\n\nNow, let us get back to \"connecting\" the wires. So far, all we have\nis a bunch of disjoint equations about disjoint values that are\nindependently easy to satisfy: constant gates can be satisfied by\nsetting the value to the constant and addition and multiplication gates\ncan simply be satisfied by setting all wires to zero! To make the\nproblem actually challenging (and actually represent the problem encoded\nin the original circuit), we need to add an equation that verifies \"copy\nconstraints\": constraints such as \\(a(5) =\nc(7)\\), \\(c(10) = c(12)\\), etc.\nThis requires some clever trickery.\n\nOur strategy will be to design a \"coordinate pair accumulator\", a\npolynomial \\(p(x)\\) which works as\nfollows. First, let \\(X(x)\\) and \\(Y(x)\\) be two polynomials representing the\n\\(x\\) and \\(y\\) coordinates of a set of points (eg. to\nrepresent the set \\(((0, -2), (1, 1), (2, 0),\n(3, 1))\\) you might set \\(X(x) =\nx\\) and \\(Y(x) = x^3 - 5x^2 + 7x -\n2)\\). Our goal will be to let \\(p(x)\\) represent all the points up to (but\nnot including) the given position, so \\(p(0)\\) starts at \\(1\\), \\(p(1)\\) represents just the first point,\n\\(p(2)\\) the first and the second, etc.\nWe will do this by \"randomly\" selecting two constants, \\(v_1\\) and \\(v_2\\), and constructing \\(p(x)\\) using the constraints \\(p(0) = 1\\) and \\(p(x+1) = p(x) \\cdot (v_1 + X(x) + v_2 \\cdot\nY(x))\\) at least within the domain \\((0, 1, 2, 3)\\).\n\nFor example, letting \\(v_1 = 3\\) and\n\\(v_2 = 2\\), we get:\n\nX(x)\n\n0\n\n1\n\n2\n\n3\n\n4\n\n\\(Y(x)\\)\n\n-2\n\n1\n\n0\n\n1\n\n\\(v_1 + X(x) + v_2 \\cdot\nY(x)\\)\n\n-1\n\n6\n\n5\n\n8\n\n\\(p(x)\\)\n\n1\n\n-1\n\n-6\n\n-30\n\n-240\n\n Notice that (aside from the first column) every \\(p(x)\\) value equals the value to the left\nof it multiplied by the value to the left and above it.\n\nThe result we care about is \\(p(4) =\n-240\\). Now, consider the case where instead of \\(X(x) = x\\), we set \\(X(x) = \\frac{2}{3} x^3 - 4x^2 +\n\\frac{19}{3}x\\) (that is, the polynomial that evaluates to \\((0, 3, 2, 1)\\) at the coordinates \\((0, 1, 2, 3)\\)). If you run the same\nprocedure, you'll find that you also get \\(p(4) = -240\\). This is not a coincidence\n(in fact, if you randomly pick \\(v_1\\)\nand \\(v_2\\) from a sufficiently large\nfield, it will almost never happen coincidentally). Rather,\nthis happens because \\(Y(1) = Y(3)\\),\nso if you \"swap the \\(X\\) coordinates\"\nof the points \\((1, 1)\\) and \\((3, 1)\\) you're not changing the\nset of points, and because the accumulator encodes a set (as\nmultiplication does not care about order) the value at the end will be\nthe same.\n\nNow we can start to see the basic technique that we will use to prove\ncopy constraints. First, consider the simple case where we only want to\nprove copy constraints within one set of wires (eg. we want to prove\n\\(a(1) = a(3)\\)). We'll make two\ncoordinate accumulators: one where \\(X(x) =\nx\\) and \\(Y(x) = a(x)\\), and the\nother where \\(Y(x) = a(x)\\) but \\(X'(x)\\) is the polynomial that\nevaluates to the permutation that flips (or otherwise rearranges) the\nvalues in each copy constraint; in the \\(a(1)\n= a(3)\\) case this would mean the permutation would start \\(0 3 2 1 4...\\). The first accumulator would\nbe compressing \\(((0, a(0)), (1, a(1)), (2,\na(2)), (3, a(3)), (4, a(4))...\\), the second \\(((0, a(0)), (3, a(1)), (2, a(2)), (1, a(3)), (4,\na(4))...\\). The only way the two can give the same result is if\n\\(a(1) = a(3)\\).\n\nTo prove constraints between \\(a\\),\n\\(b\\) and \\(c\\), we use the same procedure, but instead\n\"accumulate\" together points from all three polynomials. We assign each\nof \\(a\\), \\(b\\), \\(c\\)\na range of \\(X\\) coordinates (eg. \\(a\\) gets \\(X_a(x)\n= x\\) ie. \\(0...n-1\\), \\(b\\) gets \\(X_b(x)\n= n+x\\), ie. \\(n...2n-1\\), \\(c\\) gets \\(X_c(x)\n= 2n+x\\), ie. \\(2n...3n-1\\). To\nprove copy constraints that hop between different sets of wires, the\n\"alternate\" \\(X\\) coordinates would be\nslices of a permutation across all three sets. For example, if we want\nto prove \\(a(2) = b(4)\\) with \\(n = 5\\), then \\(X'_a(x)\\) would have evaluations \\(\\{0, 1, 9, 3, 4\\}\\) and \\(X'_b(x)\\) would have evaluations \\(\\{5, 6, 7, 8, 2\\}\\) (notice the \\(2\\) and \\(9\\) flipped, where \\(9\\) corresponds to the \\(b_4\\) wire). Often, \\(X'_a(x)\\), \\(X'_b(x)\\) and \\(X'_c(x)\\) are also called \\(\\sigma_a(x)\\), \\(\\sigma_b(x)\\) and \\(\\sigma_c(x)\\).\n\nWe would then instead of checking equality within one run of the\nprocedure (ie. checking \\(p(4) =\np'(4)\\) as before), we would check the product of\nthe three different runs on each side:\n\n\\[\np_{a}(n) \\cdot p_{b}(n) \\cdot p_{c}(n)=p_{a}^{\\prime}(n) \\cdot\np_{b}^{\\prime}(n) \\cdot p_{c}^{\\prime}(n)\n\\]\n\nThe product of the three \\(p(n)\\)\nevaluations on each side accumulates all coordinate pairs in\nthe \\(a\\), \\(b\\) and \\(c\\) runs on each side together, so this\nallows us to do the same check as before, except that we can now check\ncopy constraints not just between positions within one of the three sets\nof wires \\(a\\), \\(b\\) or \\(c\\), but also between one set of wires and\nanother (eg. as in \\(a(2) =\nb(4)\\)).\n\nAnd that's all there is to it!\n\n## Putting it all together\n\nIn reality, all of this math is done not over integers, but over a\nprime field; check the section \"A Modular Math Interlude\" here for a description\nof what prime fields are. Also, for mathematical reasons perhaps best\nappreciated by reading and understanding this article on FFT\nimplementation, instead of representing wire indices with \\(x=0....n-1\\), we'll use powers of \\(\\omega: 1, \\omega, \\omega ^2....\\omega\n^{n-1}\\) where \\(\\omega\\) is a\nhigh-order root-of-unity in the field. This changes nothing about the\nmath, except that the coordinate pair accumulator constraint checking\nequation changes from \\(p(x + 1) = p(x) \\cdot\n(v_1 + X(x) + v_2 \\cdot Y(x))\\) to \\(p(\\omega \\cdot x) = p(x) \\cdot (v_1 + X(x) + v_2\n\\cdot Y(x))\\), and instead of using \\(0..n-1\\), \\(n..2n-1\\), \\(2n..3n-1\\) as coordinates we use \\(\\omega^i, g \\cdot \\omega^i\\) and \\(g^2 \\cdot \\omega^i\\) where \\(g\\) can be some random high-order element\nin the field.\n\nNow let's write out all the equations we need to check. First, the\nmain gate-constraint satisfaction check:\n\n\\[\nQ_{L}(x) a(x)+Q_{R}(x) b(x)+Q_{O}(x) c(x)+Q_{M}(x) a(x) b(x)+Q_{C}(x)=0\n\\]\n\nThen the polynomial accumulator transition constraint (note: think of\n\"\\(= Z(x) \\cdot H(x)\\)\" as meaning\n\"equals zero for all coordinates within some particular domain that we\ncare about, but not necessarily outside of it\"):\n\n\\[\n\\begin{array}{l}{P_{a}(\\omega x)-P_{a}(x)\\left(v_{1}+x+v_{2} a(x)\\right)\n=Z(x) H_{1}(x)} \\\\ {P_{a^{\\prime}}(\\omega\nx)-P_{a^{\\prime}}(x)\\left(v_{1}+\\sigma_{a}(x)+v_{2} a(x)\\right)=Z(x)\nH_{2}(x)} \\\\ {P_{b}(\\omega x)-P_{b}(x)\\left(v_{1}+g x+v_{2}\nb(x)\\right)=Z(x) H_{3}(x)} \\\\ {P_{b^{\\prime}}(\\omega\nx)-P_{b^{\\prime}}(x)\\left(v_{1}+\\sigma_{b}(x)+v_{2} b(x)\\right)=Z(x)\nH_{4}(x)} \\\\ {P_{c}(\\omega x)-P_{c}(x)\\left(v_{1}+g^{2} x+v_{2}\nc(x)\\right)=Z(x) H_{5}(x)} \\\\ {P_{c^{\\prime}}(\\omega\nx)-P_{c^{\\prime}}(x)\\left(v_{1}+\\sigma_{c}(x)+v_{2} c(x)\\right)=Z(x)\nH_{6}(x)}\\end{array}\n\\]\n\nThen the polynomial accumulator starting and ending constraints:\n\n\\[\n\\begin{array}{l}{P_{a}(1)=P_{b}(1)=P_{c}(1)=P_{a^{\\prime}}(1)=P_{b^{\\prime}}(1)=P_{c^{\\prime}}(1)=1}\n\\\\ {P_{a}\\left(\\omega^{n}\\right) P_{b}\\left(\\omega^{n}\\right)\nP_{c}\\left(\\omega^{n}\\right)=P_{a^{\\prime}}\\left(\\omega^{n}\\right)\nP_{b^{\\prime}}\\left(\\omega^{n}\\right)\nP_{c^{\\prime}}\\left(\\omega^{n}\\right)}\\end{array}\n\\]\n\nThe user-provided polynomials are:\n\n- The wire assignments \\(a(x), b(x),\nc(x)\\)\n\n- The coordinate accumulators \\(P_a(x),\nP_b(x), P_c(x), P_{a'}(x), P_{b'}(x),\nP_{c'}(x)\\)\n\n- The quotients \\(H(x)\\) and \\(H_1(x)...H_6(x)\\)\n\nThe program-specific polynomials that the prover and verifier need to\ncompute ahead of time are:\n\n- \\(Q_L(x), Q_R(x), Q_O(x), Q_M(x),\nQ_C(x)\\), which together represent the gates in the circuit (note\nthat \\(Q_C(x)\\) encodes public inputs,\nso it may need to be computed or modified at runtime)\n\n- The \"permutation polynomials\" \\(\\sigma_a(x), \\sigma_b(x)\\) and \\(\\sigma_c(x)\\), which encode the copy\nconstraints between the \\(a\\), \\(b\\) and \\(c\\) wires\n\nNote that the verifier need only store commitments to these\npolynomials. The only remaining polynomial in the above equations is\n\\(Z(x) = (x - 1) \\cdot (x - \\omega) \\cdot ...\n\\cdot (x - \\omega ^{n-1})\\) which is designed to evaluate to zero\nat all those points. Fortunately, \\(\\omega\\) can be chosen to make this\npolynomial very easy to evaluate: the usual technique is to choose \\(\\omega\\) to satisfy \\(\\omega ^n = 1\\), in which case \\(Z(x) = x^n - 1\\).\n\nThere is one nuance here: the constraint between \\(P_a(\\omega^{i+1})\\) and \\(P_a(\\omega^i)\\) can't be true across the\nentire circle of powers of \\(\\omega\\); it's almost always false at \\(\\omega^{n-1}\\) as the next coordinate is\n\\(\\omega^n = 1\\) which brings us back\nto the start of the \"accumulator\"; to fix this, we can modify\nthe constraint to say \"either the constraint is true\nor \\(x = \\omega^{n-1}\\)\",\nwhich one can do by multiplying \\(x -\n\\omega^{n-1}\\) into the constraint so it equals zero at that\npoint.\n\nThe only constraint on \\(v_1\\) and\n\\(v_2\\) is that the user must not be\nable to choose \\(a(x), b(x)\\) or \\(c(x)\\) after \\(v_1\\) and \\(v_2\\) become known, so we can satisfy this\nby computing \\(v_1\\) and \\(v_2\\) from hashes of commitments to \\(a(x), b(x)\\) and \\(c(x)\\).\n\nSo now we've turned the program satisfaction problem into a simple\nproblem of satisfying a few equations with polynomials, and there are\nsome optimizations in PLONK that allow us to remove many of the\npolynomials in the above equations that I will not go into to preserve\nsimplicity. But the polynomials themselves, both the program-specific\nparameters and the user inputs, are big. So the next\nquestion is, how do we get around this so we can make the proof\nshort?\n\n## Polynomial commitments\n\nA polynomial\ncommitment is a short object that \"represents\" a polynomial, and\nallows you to verify evaluations of that polynomial, without needing to\nactually contain all of the data in the polynomial. That is, if someone\ngives you a commitment \\(c\\)\nrepresenting \\(P(x)\\), they can give\nyou a proof that can convince you, for some specific \\(z\\), what the value of \\(P(z)\\) is. There is a further mathematical\nresult that says that, over a sufficiently big field, if certain kinds\nof equations (chosen before \\(z\\) is\nknown) about polynomials evaluated at a random \\(z\\) are true, those same equations are true\nabout the whole polynomial as well. For example, if \\(P(z) \\cdot Q(z) + R(z) = S(z) + 5\\), then\nwe know that it's overwhelmingly likely that \\(P(x) \\cdot Q(x) + R(x) = S(x) + 5\\) in\ngeneral. Using such polynomial commitments, we could very easily check\nall of the above polynomial equations above - make the commitments, use\nthem as input to generate \\(z\\), prove\nwhat the evaluations are of each polynomial at \\(z\\), and then run the equations with these\nevaluations instead of the original polynomials. But how do these\ncommitments work?\n\nThere are two parts: the commitment to the polynomial \\(P(x) \\rightarrow c\\), and the opening to a\nvalue \\(P(z)\\) at some \\(z\\). To make a commitment, there are many\ntechniques; one example is FRI, and another is\nKate commitments which I will describe below. To prove an opening, it\nturns out that there is a simple generic \"subtract-and-divide\" trick: to\nprove that \\(P(z) = a\\), you prove\nthat\n\n\\[\n\\frac{P(x)-a}{x-z}\n\\]\n\nis also a polynomial (using another polynomial commitment). This\nworks because if the quotient is a polynomial (ie. it is not\nfractional), then \\(x - z\\) is a factor\nof \\(P(x) - a\\), so \\((P(x) - a)(z) = 0\\), so \\(P(z) = a\\). Try it with some polynomial,\neg. \\(P(x) = x^3 + 2 \\cdot x^2 + 5\\)\nwith \\((z = 6, a = 293)\\), yourself;\nand try \\((z = 6, a = 292)\\) and see\nhow it fails (if you're lazy, see WolframAlpha here\nvs here).\nNote also a generic optimization: to prove many openings of many\npolynomials at the same time, after committing to the outputs do the\nsubtract-and-divide trick on a random linear combination of the\npolynomials and the outputs.\n\nSo how do the commitments themselves work? Kate commitments are,\nfortunately, much simpler than FRI. A trusted-setup procedure generates\na set of elliptic curve points \\(G, G \\cdot s,\nG \\cdot s^2\\) .... \\(G \\cdot\ns^n\\), as well as \\(G_2 \\cdot\ns\\), where \\(G\\) and \\(G_2\\) are the generators of two elliptic\ncurve groups and \\(s\\) is a secret that\nis forgotten once the procedure is finished (note that there is a\nmulti-party version of this setup, which is secure as long as at least\none of the participants forgets their share of the secret). These points\nare published and considered to be \"the proving key\" of the scheme;\nanyone who needs to make a polynomial commitment will need to use these\npoints. A commitment to a degree-d polynomial is made by multiplying\neach of the first d+1 points in the proving key by the corresponding\ncoefficient in the polynomial, and adding the results together.\n\nNotice that this provides an \"evaluation\" of that polynomial at \\(s\\), without knowing \\(s\\). For example, \\(x^3 + 2x^2+5\\) would be represented by\n\\((G \\cdot s^3) + 2 \\cdot (G \\cdot s^2) + 5\n\\cdot G\\). We can use the notation \\([P]\\) to refer to \\(P\\) encoded in this way (ie. \\(G \\cdot P(s)\\)). When doing the\nsubtract-and-divide trick, you can prove that the two polynomials\nactually satisfy the relation by using elliptic\ncurve pairings: check that \\(e([P] - G\n\\cdot a, G_2) = e([Q], [x] - G_2 \\cdot z)\\) as a proxy for\nchecking that \\(P(x) - a = Q(x) \\cdot (x -\nz)\\).\n\nBut there are more recently other types of polynomial commitments\ncoming out too. A new scheme called DARK (\"Diophantine arguments of\nknowledge\") uses \"hidden order groups\" such as class\ngroups to implement another kind of polynomial commitment. Hidden\norder groups are unique because they allow you to compress arbitrarily\nlarge numbers into group elements, even numbers much larger than the\nsize of the group element, in a way that can't be \"spoofed\";\nconstructions from VDFs to accumulators\nto range proofs to polynomial commitments can be built on top of this.\nAnother option is to use bulletproofs, using regular elliptic curve\ngroups at the cost of the proof taking much longer to verify. Because\npolynomial commitments are much simpler than full-on zero knowledge\nproof schemes, we can expect more such schemes to get created in the\nfuture.\n\n## Recap\n\nTo finish off, let's go over the scheme again. Given a program \\(P\\), you convert it into a circuit, and\ngenerate a set of equations that look like this:\n\n\\[\n\\left(Q_{L_{i}}\\right) a_{i}+\\left(Q_{R_{i}}\\right)\nb_{i}+\\left(Q_{O_{i}}\\right) c_{i}+\\left(Q_{M_{i}}\\right) a_{i}\nb_{i}+Q_{C_{i}}=0\n\\]\n\nYou then convert this set of equations into a single polynomial\nequation:\n\n\\[\nQ_{L}(x) a(x)+Q_{R}(x) b(x)+Q_{O}(x) c(x)+Q_{M}(x) a(x) b(x)+Q_{C}(x)=0\n\\]\n\nYou also generate from the circuit a list of copy constraints. From\nthese copy constraints you generate the three polynomials representing\nthe permuted wire indices: \\(\\sigma_a(x),\n\\sigma_b(x), \\sigma_c(x)\\). To generate a proof, you compute the\nvalues of all the wires and convert them into three polynomials: \\(a(x), b(x), c(x)\\). You also compute six\n\"coordinate pair accumulator\" polynomials as part of the\npermutation-check argument. Finally you compute the cofactors \\(H_i(x)\\).\n\nThere is a set of equations between the polynomials that need to be\nchecked; you can do this by making commitments to the polynomials,\nopening them at some random \\(z\\)\n(along with proofs that the openings are correct), and running the\nequations on these evaluations instead of the original polynomials. The\nproof itself is just a few commitments and openings and can be checked\nwith a few equations. And that's all there is to it!",
    "contentLength": 24871,
    "summary": "PLONK is a zero-knowledge proof scheme with universal trusted setup and modular polynomial commitments enabling shared tooling.",
    "detailedSummary": {
      "theme": "Vitalik explains PLONK, a new general-purpose zero-knowledge proof scheme that offers significant improvements in usability through universal trusted setup and modular polynomial commitments.",
      "summary": "Vitalik provides a comprehensive technical explanation of PLONK (Permutations over Lagrange-bases for Oecumenical Noninteractive arguments of Knowledge), a zero-knowledge proof scheme that addresses key limitations of previous systems like those used in Zcash. PLONK's main innovations include a universal and updateable trusted setup that works for any program up to a maximum size, eliminating the need for separate trusted setups for each program, and a modular design based on polynomial commitments that allows swapping between different cryptographic assumptions and security tradeoffs. Vitalik walks through the technical mechanics, explaining how PLONK converts computational problems into systems of polynomial equations, handles gate constraints and copy constraints through clever mathematical techniques like coordinate pair accumulators, and uses polynomial commitments (such as Kate commitments) to create short proofs that can verify polynomial evaluations without revealing the full polynomial data. He emphasizes that this modular approach enables different use cases with varying security requirements to share the same underlying arithmetization tooling, potentially accelerating development across the zero-knowledge proof ecosystem.",
      "takeaways": [
        "PLONK features a universal trusted setup that works for any program up to a predetermined size, eliminating the need for program-specific trusted setups",
        "The scheme uses modular polynomial commitments that can be swapped out (Kate commitments, FRI, DARK) to accommodate different security assumptions and proof size requirements",
        "PLONK converts computational problems into polynomial equations through gate constraints (for circuit logic) and copy constraints (for wire connections between gates)",
        "Copy constraints are enforced using an ingenious coordinate pair accumulator technique that verifies wire equalities through polynomial multiplication properties",
        "The modular design allows different projects to share arithmetization techniques while choosing their preferred cryptographic tradeoffs, potentially accelerating ecosystem-wide progress"
      ],
      "controversial": [
        "The reliance on trusted setup procedures, even though updateable and universal, still requires trust that at least one participant in the setup ceremony is honest",
        "The choice between different polynomial commitment schemes involves fundamental tradeoffs between proof size and security assumptions that different developers may disagree on ideologically"
      ]
    }
  },
  {
    "id": "general-2019-08-28-hybrid_layer_2",
    "title": "The Dawn of Hybrid Layer 2 Protocols",
    "date": "2019-08-28",
    "category": "protocol",
    "url": "https://vitalik.eth.limo/general/2019/08/28/hybrid_layer_2.html",
    "path": "general/2019/08/28/hybrid_layer_2.html",
    "content": "- \n\n- \n\n  \n    \n\n  \n  \n  Dark Mode Toggle\n  \n    \n\n  \n\n##  The Dawn of Hybrid Layer 2 Protocols \n\n 2019 Aug 28 \nSee all posts\n\n \n \n\n The Dawn of Hybrid Layer 2 Protocols \n\nSpecial thanks to the Plasma Group team for review and\nfeedback\n\nCurrent approaches to layer 2 scaling - basically, Plasma and state\nchannels - are increasingly moving from theory to practice, but at the\nsame time it is becoming easier to see the inherent challenges in\ntreating these techniques as a fully fledged scaling solution for\nEthereum. Ethereum was arguably successful in large part because of its\nvery easy developer experience: you write a program, publish the\nprogram, and anyone can interact with it. Designing a state channel or\nPlasma application, on the other hand, relies on a lot of explicit\nreasoning about incentives and application-specific development\ncomplexity. State channels work well for specific use cases such as\nrepeated payments between the same two parties and two-player games (as\nsuccessfully implemented in Celer), but more generalized usage\nis proving challenging. Plasma, particularly Plasma Cash,\ncan work well for payments, but generalization similarly incurs\nchallenges: even implementing a decentralized exchange requires clients\nto store much more history data, and generalizing to Ethereum-style\nsmart contracts on Plasma seems extremely difficult.\n\nBut at the same time, there is a resurgence of a forgotten category\nof \"semi-layer-2\" protocols - a category which promises less extreme\ngains in scaling, but with the benefit of much easier generalization and\nmore favorable security models. A long-forgotten\nblog post from 2014 introduced the idea of \"shadow chains\", an\narchitecture where block data is published on-chain, but blocks are not\nverified by default. Rather, blocks are tentatively accepted,\nand only finalized after some period of time (eg. 2 weeks). During those\n2 weeks, a tentatively accepted block can be challenged; only then is\nthe block verified, and if the block proves to be invalid then the chain\nfrom that block on is reverted, and the original publisher's deposit is\npenalized. The contract does not keep track of the full state of the\nsystem; it only keeps track of the state root, and users themselves can\ncalculate the state by processing the data submitted to the chain from\nstart to head. A more recent proposal, ZK\nRollup, does the same thing without challenge periods, by using\nZK-SNARKs to verify blocks' validity.\n\nAnatomy of a ZK Rollup package that is published on-chain.\nHundreds of \"internal transactions\" that affect the state (ie. account\nbalances) of the ZK Rollup system are compressed into a package that\ncontains ~10 bytes per internal transaction that specifies the state\ntransitions, plus a ~100-300 byte SNARK proving that the transitions are\nall valid.\n\nIn both cases, the main chain is used to verify data\navailability, but does not (directly) verify block\nvalidity or perform any significant computation, unless\nchallenges are made. This technique is thus not a jaw-droppingly huge\nscalability gain, because the on-chain data overhead eventually presents\na bottleneck, but it is nevertheless a very significant one. Data is\ncheaper than computation, and there are ways to compress transaction\ndata very significantly, particularly because the great majority of data\nin a transaction is the signature and many signatures can be compressed\ninto one through many forms of aggregation. ZK Rollup promises 500\ntx/sec, a 30x gain over the Ethereum chain itself, by compressing each\ntransaction to a mere ~10 bytes; signatures do not need to be included\nbecause their validity is verified by the zero-knowledge proof. With BLS\naggregate signatures a similar throughput can be achieved in shadow\nchains (more recently called \"optimistic rollup\" to highlight its\nsimilarities to ZK Rollup). The upcoming Istanbul hard fork will\nreduce the gas cost of data from 68 per byte to 16 per byte, increasing\nthe throughput of these techniques by another 4x (that's over\n2000 transactions per second).\n\nSo what is the benefit of data on-chain techniques such as\nZK/optimistic rollup versus data off-chain techniques such as Plasma?\nFirst of all, there is no need for semi-trusted operators. In ZK Rollup,\nbecause validity is verified by cryptographic proofs there is literally\nno way for a package submitter to be malicious (depending on the setup,\na malicious submitter may cause the system to halt for a few seconds,\nbut this is the most harm that can be done). In optimistic rollup, a\nmalicious submitter can publish a bad block, but the next submitter will\nimmediately challenge that block before publishing their own. In both ZK\nand optimistic rollup, enough data is published on chain to allow anyone\nto compute the complete internal state, simply by processing all of the\nsubmitted deltas in order, and there is no \"data withholding attack\"\nthat can take this property away. Hence, becoming an operator can be\nfully permissionless; all that is needed is a security deposit (eg. 10\nETH) for anti-spam purposes.\n\nSecond, optimistic rollup particularly is vastly easier to\ngeneralize; the state transition function in an optimistic rollup system\ncan be literally anything that can be computed within the gas limit of a\nsingle block (including the Merkle branches providing the parts of the\nstate needed to verify the transition). ZK Rollup is theoretically\ngeneralizeable in the same way, though in practice making ZK SNARKs over\ngeneral-purpose computation (such as EVM execution) is very difficult,\nat least for now. Third, optimistic rollup is much easier to build\nclients for, as there is less need for second-layer networking\ninfrastructure; more can be done by just scanning the blockchain.\n\nBut where do these advantages come from? The answer lies in a highly\ntechnical issue known as the data availability problem (see note,\nvideo).\nBasically, there are two ways to try to cheat in a layer-2 system. The\nfirst is to publish invalid data to the blockchain. The second is to not\npublish data at all (eg. in Plasma, publishing the root hash of a new\nPlasma block to the main chain but without revealing the contents of the\nblock to anyone). Published-but-invalid data is very easy to deal with,\nbecause once the data is published on-chain there are multiple ways to\nfigure out unambiguously whether or not it's valid, and an invalid\nsubmission is unambiguously invalid so the submitter can be heavily\npenalized. Unavailable data, on the other hand, is much harder to deal\nwith, because even though unavailability can be detected if challenged,\none cannot reliably determine whose fault the non-publication is,\nespecially if data is withheld by default and revealed on-demand only\nwhen some verification mechanism tries to verify its availability. This\nis illustrated in the \"Fisherman's dilemma\", which shows how a\nchallenge-response game cannot distinguish between malicious submitters\nand malicious challengers:\n\nFisherman's dilemma. If you only start watching the given\nspecific piece of data at time T3, you have no idea whether you are\nliving in Case 1 or Case 2, and hence who is at fault.\n\nPlasma and channels both work around the fisherman's dilemma by\npushing the problem to users: if you as a user decide that another user\nyou are interacting with (a counterparty in a state channel, an operator\nin a Plasma chain) is not publishing data to you that they should be\npublishing, it's your responsibility to exit and move to a different\ncounterparty/operator. The fact that you as a user have all of the\nprevious data, and data about all of the transactions\nyou signed, allows you to prove to the chain what assets you\nheld inside the layer-2 protocol, and thus safely bring them out of the\nsystem. You prove the existence of a (previously agreed) operation that\ngave the asset to you, no one else can prove the existence of an\noperation approved by you that sent the asset to someone else, so you\nget the asset.\n\nThe technique is very elegant. However, it relies on a key\nassumption: that every state object has a logical \"owner\", and the state\nof the object cannot be changed without the owner's consent. This works\nwell for UTXO-based payments (but not account-based payments, where you\ncan edit someone else's balance upward without their\nconsent; this is why account-based Plasma is so hard), and it can even\nbe made to work for a decentralized exchange, but this \"ownership\"\nproperty is far from universal. Some applications, eg. Uniswap don't have a natural owner,\nand even in those applications that do, there are often multiple people\nthat can legitimately make edits to the object. And there is no way to\nallow arbitrary third parties to exit an asset without introducing the\npossibility of denial-of-service (DoS) attacks, precisely because one\ncannot prove whether the publisher or submitter is at fault.\n\nThere are other issues peculiar to Plasma and channels individually.\nChannels do not allow off-chain transactions to users that are not\nalready part of the channel (argument: suppose there existed a way to\nsend $1 to an arbitrary new user from inside a channel. Then this\ntechnique could be used many times in parallel to send $1 to more users\nthan there are funds in the system, already breaking its security\nguarantee). Plasma requires users to store large amounts of history\ndata, which gets even bigger when different assets can be intertwined\n(eg. when an asset is transferred conditional on transfer of another\nasset, as happens in a decentralized exchange with a single-stage order\nbook mechanism).\n\nBecause data-on-chain computation-off-chain layer 2 techniques don't\nhave data availability issues, they have none of these weaknesses. ZK\nand optimistic rollup take great care to put enough data on chain to\nallow users to calculate the full state of the layer 2 system, ensuring\nthat if any participant disappears a new one can trivially take their\nplace. The only issue that they have is verifying computation without\ndoing the computation on-chain, which is a much easier problem. And the\nscalability gains are significant: ~10 bytes per transaction in ZK\nRollup, and a similar level of scalability can be achieved in optimistic\nrollup by using BLS aggregation to aggregate signatures. This\ncorresponds to a theoretical maximum of ~500 transactions per second\ntoday, and over 2000 post-Istanbul.\n\nBut what if you want more scalability? Then there is a large middle\nground between data-on-chain layer 2 and data-off-chain layer 2\nprotocols, with many hybrid approaches that give you some of the\nbenefits of both. To give a simple example, the history storage blowup\nin a decentralized exchange implemented on Plasma Cash can be prevented\nby publishing a mapping of which orders are matched with which orders\n(that's less than 4 bytes per order) on chain:\n\nLeft: History data a Plasma Cash user needs to store if\nthey own 1 coin. Middle: History data a Plasma Cash user needs to\nstore if they own 1 coin that was exchanged with another coin using an\natomic swap. Right: History data a Plasma Cash user needs to\nstore if the order matching is published on chain.\n\nEven outside of the decentralized exchange context, the amount of\nhistory that users need to store in Plasma can be reduced by having the\nPlasma chain periodically publish some per-user data on-chain. One could\nalso imagine a platform which works like Plasma in the case where some\nstate does have a logical \"owner\" and works like ZK or\noptimistic rollup in the case where it does not. Plasma developers are\nalready starting to work on these kinds of optimizations.\n\nThere is thus a strong case to be made for developers of layer 2\nscalability solutions to move to be more willing to publish per-user\ndata on-chain at least some of the time: it greatly increases ease of\ndevelopment, generality and security and reduces per-user load (eg. no\nneed for users storing history data). The efficiency losses of doing so\nare also overstated: even in a fully off-chain layer-2 architecture,\nusers depositing, withdrawing and moving between different\ncounterparties and providers is going to be an inevitable and frequent\noccurrence, and so there will be a significant amount of per-user\non-chain data regardless. The hybrid route opens the door to a\nrelatively fast deployment of fully generalized Ethereum-style smart\ncontracts inside a quasi-layer-2 architecture.\n\nSee also:\n\n- Introducing\nthe OVM\n\n- Blog\npost by Karl Floersch\n\n- Related\nideas by John Adler",
    "contentLength": 12507,
    "summary": "Hybrid Layer 2 protocols like ZK/optimistic rollup put data on-chain but verify off-chain, offering 500+ tx/sec vs Plasma/channels.",
    "detailedSummary": {
      "theme": "Vitalik argues for hybrid Layer 2 scaling solutions that publish data on-chain while moving computation off-chain, offering better generalizability and security than purely off-chain approaches like Plasma and state channels.",
      "summary": "Vitalik examines the evolution of Ethereum Layer 2 scaling solutions, highlighting the limitations of current approaches like Plasma and state channels, which work well for specific use cases but struggle with generalization due to complex incentive structures and the data availability problem. He advocates for 'semi-layer-2' protocols like ZK Rollup and optimistic rollup that publish data on-chain while keeping computation off-chain, avoiding the data availability problem that plagues fully off-chain solutions. These hybrid approaches offer significant scalability gains (500+ transactions per second) while maintaining easier development, better security guarantees, and full generalizability to Ethereum-style smart contracts. Vitalik argues that the efficiency losses from publishing some data on-chain are overstated, and that hybrid solutions represent the most practical path forward for deploying generalized scaling solutions in the near term.",
      "takeaways": [
        "Current Layer 2 solutions like Plasma and state channels have significant limitations in generalizability and require complex application-specific development",
        "Data-on-chain, computation-off-chain approaches like ZK Rollup and optimistic rollup solve the data availability problem that plagues fully off-chain solutions",
        "Hybrid Layer 2 protocols can achieve 500+ transactions per second today and over 2000 post-Istanbul fork, representing meaningful scalability improvements",
        "The 'ownership' requirement in Plasma-style solutions severely limits which applications can be built, while hybrid approaches support arbitrary smart contracts",
        "Publishing selective data on-chain creates a spectrum of hybrid solutions that can optimize for both scalability and generalizability depending on use case requirements"
      ],
      "controversial": [
        "Vitalik's assertion that efficiency losses from publishing data on-chain are 'overstated' may be debated by those prioritizing maximum scalability",
        "The claim that ZK-SNARKs over general-purpose computation is 'very difficult' might be challenged by teams working on general-purpose ZK-EVM solutions"
      ]
    }
  },
  {
    "id": "general-2019-06-12-plasma_vs_sharding",
    "title": "Sidechains vs Plasma vs Sharding",
    "date": "2019-06-12",
    "category": "protocol",
    "url": "https://vitalik.eth.limo/general/2019/06/12/plasma_vs_sharding.html",
    "path": "general/2019/06/12/plasma_vs_sharding.html",
    "content": "- \n\n- \n\n  \n    \n\n  \n  \n  Dark Mode Toggle\n  \n    \n\n  \n\n##  Sidechains vs Plasma vs Sharding \n\n 2019 Jun 12 \nSee all posts\n\n \n \n\n Sidechains vs Plasma vs Sharding \n\nSpecial thanks to Jinglan Wang for review and feedback\n\nOne question that often comes up is: how exactly is sharding\ndifferent from sidechains or Plasma? All three architectures seem to\ninvolve a hub-and-spoke architecture with a central \"main chain\" that\nserves as the consensus backbone of the system, and a set of \"child\"\nchains containing actual user-level transactions. Hashes from the child\nchains are usually periodically published into the main chain (sharded\nchains with no hub are theoretically possible but haven't been done so\nfar; this article will not focus on them, but the arguments are\nsimilar). Given this fundamental similarity, why go with one approach\nover the others?\n\nDistinguishing sidechains from Plasma is simple. Plasma chains are\nsidechains that have a non-custodial property: if there is any error in\nthe Plasma chain, then the error can be detected, and users can safely\nexit the Plasma chain and prevent the attacker from doing any lasting\ndamage. The only cost that users suffer is that they must wait for a\nchallenge period and pay some higher transaction fees on the\n(non-scalable) base chain. Regular sidechains do not have this safety\nproperty, so they are less secure. However, designing Plasma chains is\nin many cases much harder, and one could argue that for many low-value\napplications the security is not worth the added complexity.\n\nSo what about Plasma versus sharding? The key technical difference\nhas to do with the notion of tight coupling. Tight\ncoupling is a property of sharding, but NOT a property of sidechains or\nPlasma, that says that the validity of the main chain (\"beacon chain\" in\nethereum 2.0) is inseparable from the validity of the child chains. That\nis, a child chain block that specifies an invalid main chain block as a\ndependency is by definition invalid, and more importantly a main chain\nblock that includes an invalid child chain block is by definition\ninvalid.\n\nIn non-sharded blockchains, this idea that the canonical chain (ie.\nthe chain that everyone accepts as representing the \"real\" history) is\nby definition fully available and valid also applies; for\nexample in the case of Bitcoin and Ethereum one typically says that the\ncanonical chain is the \"longest valid chain\" (or, more pedantically, the\n\"heaviest valid and available chain\"). In sharded blockchains, this idea\nthat the canonical chain is the heaviest valid and available chain\nby definition also applies, with the validity and availability\nrequirement applying to both the main chain and shard chains. The new\nchallenge that a sharded system has, however, is that users have no way\nof fully verifying the validity and availability of any given chain\ndirectly, because there is too much data. The challenge of\nengineering sharded chains is to get around this limitation by giving\nusers a maximally trustless and practical indirect means to\nverify which chains are fully available and valid, so that they can\nstill determine which chain is canonical. In practice, this includes\ntechniques like committees, SNARKs/STARKs, fisherman schemes and fraud and data availability\nproofs.\n\nIf a chain structure does not have this tight-coupling property, then\nit is arguably not a layer-1 sharding scheme, but rather a layer-2\nsystem sitting on top of a non-scalable layer-1 chain. Plasma is not a\ntightly-coupled system: an invalid Plasma block absolutely can have its\nheader be committed into the main Ethereum chain, because the Ethereum\nbase layer has no idea that it represents an invalid Plasma block, or\neven that it represents a Plasma block at all; all that it sees is a\ntransaction containing a small piece of data. However, the consequences\nof a single Plasma chain failing are localized to within that Plasma\nchain.\n\nSharding\n\nTry really hard to ensure total validity/availability of every part of\nthe system\n\nPlasma\n\nAccept local faults but try to limit their consequences\n\nHowever, if you try to analyze the process of how users\nperform the \"indirect validation\" procedure to determine if the chain\nthey are looking at is fully valid and available without downloading and\nexecuting the whole thing, one can find more similarities with how\nPlasma works. For example, a common technique used to prevent\navailability issues is fishermen: if a node sees a given piece of a\nblock as unavailable, it can publish a challenge claiming this, creating\na time period within which anyone can publish that piece of data. If a\nblock goes unchallenged for long enough, the blocks and all blocks that\ncite it as a dependency can be reverted. This seems fundamentally\nsimilar to Plasma, where if a block is unavailable users can publish a\nmessage to the main chain to exit their state in response. Both\ntechniques eventually buckle under pressure in the same way: if there\nare too many false challenges in a sharded system, then users cannot\nkeep track of whether or not all of the availability challenges have\nbeen answered, and if there are too many availability challenges in a\nPlasma system then the main chain could get overwhelmed as the exits\nfill up the chain's block size limit. In both cases, it seems like\nthere's a system that has nominally \\(O(C^2)\\) scalability (where \\(C\\) is the computing power of one node) but\nwhere scalability falls to \\(O(C)\\) in\nthe event of an attack. However, sharding has more defenses against\nthis.\n\nFirst of all, modern sharded designs use randomly sampled committees,\nso one cannot easily dominate even one committee enough to produce a\nfake block unless one has a large portion (perhaps \\(>\\frac{1}{3}\\)) of the entire validator\nset of the chain. Second, there are better strategies to handling data\navailability than fishermen: data availability proofs. In a scheme using\ndata availability proofs, if a block is unavailable, then\nclients' data availability checks will fail and clients will see that\nblock as unavailable. If the block is invalid, then even a\nsingle fraud proof will convince them of this fact for an entire block.\nAn \\(O(1)\\)-sized fraud proof can\nconvince a client of the invalidity of an \\(O(C)\\)-sized block, and so \\(O(C)\\) data suffices to convince a client\nof the invalidity of \\(O(C^2)\\) data\n(this is in the worst case where the client is dealing with \\(N\\) sister blocks all with the same parent\nof which only one is valid; in more likely cases, one single fraud proof\nsuffices to prove invalidity of an entire invalid chain). Hence, sharded\nsystems are theoretically less vulnerable to being overwhelmed by\ndenial-of-service attacks than Plasma chains.\n\nSecond, sharded chains provide stronger guarantees in the face of\nlarge and majority attackers (with more than \\(\\frac{1}{3}\\) or even \\(\\frac{1}{2}\\) of the validator set). A\nPlasma chain can always be successfully attacked by a 51% attack on the\nmain chain that censors exits; a sharded chain cannot. This is because\ndata availability proofs and fraud proofs happen inside the\nclient, rather than inside the chain, so they cannot be\ncensored by 51% attacks. Third, the defenses provided by sharded chains\nare easier to generalize; Plasma's model of exits requires state to be\nseparated into discrete pieces each of which is in the interest of any\nsingle actor to maintain, whereas sharded chains relying on data\navailability proofs, fraud proofs, fishermen and random sampling are\ntheoretically universal.\n\nSo there really is a large difference between validity and\navailability guarantees that are provided at layer 2, which are limited\nand more complex as they require explicit reasoning about incentives and\nwhich party has an interest in which pieces of state, and guarantees\nthat are provided by a layer 1 system that is committed to fully\nsatisfying them.\n\nBut Plasma chains also have large advantages too. First, they can be\niterated and new designs can be implemented more quickly, as each Plasma\nchain can be deployed separately without coordinating the rest of the\necosystem. Second, sharding is inherently more fragile, as it attempts\nto guarantee absolute and total availability and validity of some\nquantity of data, and this quantity must be set in the protocol; too\nlittle, and the system has less scalability than it could have had, too\nmuch, and the entire system risks breaking. The maximum safe level of\nscalability also depends on the number of users of the system, which is\nan unpredictable variable. Plasma chains, on the other hand, allow\ndifferent users to make different tradeoffs in this regard, and allow\nusers to adjust more flexibly to changes in circumstances.\n\nSingle-operator Plasma chains can also be used to offer more privacy\nthan sharded systems, where all data is public. Even where privacy is\nnot desired, they are potentially more efficient, because the total data\navailability requirement of sharded systems requires a large extra level\nof redundancy as a safety margin. In Plasma systems, on the other hand,\ndata requirements for each piece of data can be minimized, to the point\nwhere in the long term each individual piece of data may only need to be\nreplicated a few times, rather than a thousand times as is the case in\nsharded systems.\n\nHence, in the long term, a hybrid system where a sharded base layer\nexists, and Plasma chains exist on top of it to provide further\nscalability, seems like the most likely approach, more able to serve\ndifferent groups' of users need than sole reliance on one strategy or\nthe other. And it is unfortunately not the case that at a\nsufficient level of advancement Plasma and sharding collapse into the\nsame design; the two are in some key ways irreducibly different (eg. the\ndata availability checks made by clients in sharded systems\ncannot be moved to the main chain in Plasma because these\nchecks only work if they are done subjectively and based on private\ninformation). But both scalability solutions (as well as state\nchannels!) have a bright future ahead of them.",
    "contentLength": 10064,
    "summary": "Sharding ensures tight coupling between main and child chains for universal validity, while Plasma accepts localized faults but offers faster deployment.",
    "detailedSummary": {
      "theme": "Vitalik explains the technical differences between sidechains, Plasma, and sharding as blockchain scalability solutions, with particular focus on the concept of tight coupling that distinguishes layer-1 sharding from layer-2 systems.",
      "summary": "Vitalik begins by clarifying that Plasma chains are simply sidechains with a non-custodial property that allows users to safely exit if errors occur, making them more secure but harder to design than regular sidechains. The core distinction between Plasma and sharding lies in 'tight coupling' - a property unique to sharding where the validity of the main chain is inseparable from the validity of child chains, making sharding a true layer-1 solution rather than a layer-2 system sitting on top of a base chain. While both systems face similar scalability limitations under attack (falling from O(C\u00b2) to O(C) performance), sharding offers stronger defenses through randomly sampled committees, data availability proofs, and fraud proofs that work at the client level and cannot be censored by 51% attacks. However, Vitalik acknowledges that Plasma chains have significant advantages including faster iteration, greater flexibility in scalability tradeoffs, better privacy options, and more efficient data requirements, leading him to conclude that a hybrid approach combining sharded base layers with Plasma chains on top will likely be the optimal long-term solution.",
      "takeaways": [
        "Plasma chains are sidechains with non-custodial properties that allow safe exits during errors, while regular sidechains lack this security feature",
        "Tight coupling distinguishes sharding as a layer-1 solution where main chain validity is inseparable from child chain validity, unlike layer-2 Plasma systems",
        "Sharding provides stronger security guarantees against large attackers and censorship because its fraud proofs and data availability checks operate at the client level",
        "Plasma chains offer greater flexibility, faster deployment, better privacy, and more efficient data usage compared to sharding systems",
        "The optimal future likely involves a hybrid approach combining sharded base layers with Plasma chains on top to serve different user needs"
      ],
      "controversial": [
        "The claim that sharding has O(C\u00b2) scalability that falls to O(C) under attack may be disputed by those who believe the theoretical scalability benefits are overstated",
        "The assertion that Plasma and sharding are 'irreducibly different' and won't converge into the same design could be challenged by those expecting more convergence in blockchain architecture evolution"
      ]
    }
  },
  {
    "id": "general-2019-05-12-fft",
    "title": "Fast Fourier Transforms",
    "date": "2019-05-12",
    "category": "math",
    "url": "https://vitalik.eth.limo/general/2019/05/12/fft.html",
    "path": "general/2019/05/12/fft.html",
    "content": "- \n\n- \n\n  \n    \n\n  \n  \n  Dark Mode Toggle\n  \n    \n\n  \n\n##  Fast Fourier Transforms \n\n 2019 May 12 \nSee all posts\n\n \n \n\n Fast Fourier Transforms \n\nTrigger warning: specialized mathematical topic\n\nSpecial thanks to Karl Floersch for feedback\n\nOne of the more interesting algorithms in number theory is the Fast\nFourier transform (FFT). FFTs are a key building block in many\nalgorithms, including\nextremely\nfast multiplication of large numbers, multiplication of polynomials,\nand extremely fast generation and recovery of\nerasure\ncodes. Erasure codes in particular are highly versatile; in addition\nto their basic use cases in fault-tolerant data storage and recovery,\nerasure codes also have more advanced use cases such as\nsecuring data availability in\nscalable blockchains and\nSTARKs. This\narticle will go into what fast Fourier transforms are, and how some of\nthe simpler algorithms for computing them work.\n\nBackground\n\nThe original\nFourier\ntransform is a mathematical operation that is often described as\nconverting data between the \"frequency domain\" and the \"time domain\".\nWhat this means more precisely is that if you have a piece of data, then\nrunning the algorithm would come up with a collection of sine waves with\ndifferent frequencies and amplitudes that, if you added them together,\nwould approximate the original data. Fourier transforms can be used for\nsuch wonderful things as\nexpressing\nsquare orbits through epicycles and\nderiving a set\nof equations that can draw an elephant:\n\n Ok fine, Fourier transforms also have really important\napplications in signal processing, quantum mechanics, and other areas,\nand help make significant parts of the global economy happen. But come\non, elephants are cooler.\n\nRunning the Fourier transform algorithm in the \"inverse\" direction would\nsimply take the sine waves and add them together and compute the\nresulting values at as many points as you wanted to sample.\n\nThe kind of Fourier transform we'll be talking about in this post is a\nsimilar algorithm, except instead of being a continuous Fourier\ntransform over real or complex numbers, it's a\ndiscrete Fourier transform over finite\nfields (see the \"A Modular Math Interlude\" section\nhere for a\nrefresher on what finite fields are). Instead of talking about\nconverting between \"frequency domain\" and \"time domain\", here we'll talk\nabout two different operations: multi-point polynomial\nevaluation (evaluating a degree \\(<\nN\\) polynomial at \\(N\\)\ndifferent points) and its inverse, polynomial interpolation\n(given the evaluations of a degree \\(<\nN\\) polynomial at \\(N\\)\ndifferent points, recovering the polynomial). For example, if we are\noperating in the prime field with modulus 5, then the polynomial \\(y = x\u00b2 + 3\\) (for convenience we can write\nthe coefficients in increasing order: \\([3,0,1]\\)) evaluated at the points \\([0,1,2]\\) gives the values \\([3,4,2]\\) (not \\([3, 4, 7]\\) because we're operating in a\nfinite field where the numbers wrap around at 5), and we can actually\ntake the evaluations \\([3,4,2]\\) and\nthe coordinates they were evaluated at (\\([0,1,2]\\)) to recover the original\npolynomial \\([3,0,1]\\).\n\nThere are algorithms for both multi-point evaluation and interpolation\nthat can do either operation in \\(O(N^2)\\) time. Multi-point evaluation is\nsimple: just separately evaluate the polynomial at each point. Here's\npython code for doing that:\n\ndef eval_poly_at(self, poly, x, modulus):\n    y = 0\n    power_of_x = 1\n    for coefficient in poly:\n        y += power_of_x * coefficient\n        power_of_x *= x\n    return y % modulus\n\nThe algorithm runs a loop going through every coefficient and does one\nthing for each coefficient, so it runs in \\(O(N)\\) time. Multi-point evaluation\ninvolves doing this evaluation at \\(N\\)\ndifferent points, so the total run time is \\(O(N^2)\\).\n\nLagrange interpolation is more complicated (search for \"Lagrange\ninterpolation\"\nhere\nfor a more detailed explanation). The key building block of the basic\nstrategy is that for any domain \\(D\\)\nand point \\(x\\), we can construct a\npolynomial that returns \\(1\\) for \\(x\\) and \\(0\\) for any value in \\(D\\) other than \\(x\\). For example, if \\(D = [1,2,3,4]\\) and \\(x = 1\\), the polynomial is:\n\n\\[\ny = \\frac{(x-2)(x-3)(x-4)}{(1-2)(1-3)(1-4)}\n\\]\n\nYou can mentally plug in \\(1\\), \\(2\\), \\(3\\)\nand \\(4\\) to the above expression and\nverify that it returns \\(1\\) for \\(x= 1\\) and \\(0\\) in the other three cases.\n\nWe can recover the polynomial that gives any desired set of outputs on\nthe given domain by multiplying and adding these polynomials. If we call\nthe above polynomial \\(P_1\\), and the\nequivalent ones for \\(x=2\\), \\(x=3\\), \\(x=4\\), \\(P_2\\), \\(P_3\\) and \\(P_4\\), then the polynomial that returns\n\\([3,1,4,1]\\) on the domain \\([1,2,3,4]\\) is simply \\(3 \\cdot P_1 + P_2 + 4 \\cdot P_3 + P_4\\).\nComputing the \\(P_i\\) polynomials takes\n\\(O(N^2)\\) time (you first construct\nthe polynomial that returns to 0 on the entire domain, which takes \\(O(N^2)\\) time, then separately divide it by\n\\((x - x_i)\\) for each \\(x_i\\)), and computing the linear\ncombination takes another \\(O(N^2)\\)\ntime, so it's \\(O(N^2)\\) runtime total.\n\nWhat Fast Fourier transforms let us do, is make both multi-point\nevaluation and interpolation much faster.\n\nFast Fourier Transforms\n\nThere is a price you have to pay for using this much faster algorithm,\nwhich is that you cannot choose any arbitrary field and any arbitrary\ndomain. Whereas with Lagrange interpolation, you could choose whatever x\ncoordinates and y coordinates you wanted, and whatever field you wanted\n(you could even do it over plain old real numbers), and you could get a\npolynomial that passes through them., with an FFT, you have to use a\nfinite field, and the domain must be a multiplicative subgroup\nof the field (that is, a list of powers of some \"generator\" value). For\nexample, you could use the finite field of integers modulo \\(337\\), and for the domain use \\([1, 85, 148, 111, 336, 252, 189, 226]\\)\n(that's the powers of \\(85\\) in the\nfield, eg. \\(85^3\\) % \\(337 = 111\\); it stops at \\(226\\) because the next power of \\(85\\) cycles back to \\(1\\)). Futhermore, the multiplicative\nsubgroup must have size \\(2^n\\)\n(there's ways to make it work for numbers of the form \\(2^{m} \\cdot 3^n\\) and possibly slightly\nhigher prime powers but then it gets much more complicated and\ninefficient). The finite field of intergers modulo \\(59\\), for example, would not work, because\nthere are only multiplicative subgroups of order \\(2\\), \\(29\\) and \\(58\\); \\(2\\) is too small to be interesting, and the\nfactor \\(29\\) is far too large to be\nFFT-friendly. The symmetry that comes from multiplicative groups of size\n\\(2^n\\) lets us create a recursive\nalgorithm that quite cleverly calculate the results we need from a much\nsmaller amount of work.\n\nTo understand the algorithm and why it has a low runtime, it's important\nto understand the general concept of recursion. A recursive algorithm is\nan algorithm that has two cases: a \"base case\" where the input to the\nalgorithm is small enough that you can give the output directly, and the\n\"recursive case\" where the required computation consists of some \"glue\ncomputation\" plus one or more uses of the same algorithm to smaller\ninputs. For example, you might have seen recursive algorithms being used\nfor sorting lists. If you have a list (eg. \\([1,8,7,4,5,6,3,2,9]\\)), then you can sort\nit using the following procedure:\n\n- \nIf the input has one element, then it's already \"sorted\", so you can\njust return the input.\n\n- \nIf the input has more than one element, then separately sort the first\nhalf of the list and the second half of the list, and then merge the two\nsorted sub-lists (call them \\(A\\) and\n\\(B\\)) as follows. Maintain two\ncounters, \\(apos\\) and \\(bpos\\), both starting at zero, and maintain\nan output list, which starts empty. Until either \\(apos\\) or \\(bpos\\) is at the end of the corresponding\nlist, check if \\(A[apos]\\) or \\(B[bpos]\\) is smaller. Whichever is smaller,\nadd that value to the end of the output list, and increase that counter\nby \\(1\\). Once this is done, add the\nrest of whatever list has not been fully processed to the end of the\noutput list, and return the output list.\n\nNote that the \"glue\" in the second procedure has runtime \\(O(N)\\): if each of the two sub-lists has\n\\(N\\) elements, then you need to run\nthrough every item in each list once, so it's \\(O(N)\\) computation total. So the algorithm\nas a whole works by taking a problem of size \\(N\\), and breaking it up into two problems\nof size \\(\\frac{N}{2}\\), plus \\(O(N)\\) of \"glue\" execution. There is a\ntheorem called the\nMaster\nTheorem that lets us compute the total runtime of algorithms like\nthis. It has many sub-cases, but in the case where you break up an\nexecution of size \\(N\\) into \\(k\\) sub-cases of size \\(\\frac{N}{k}\\) with \\(O(N)\\) glue (as is the case here), the\nresult is that the execution takes time \\(O(N\n\\cdot log(N))\\).\n\nAn FFT works in the same way. We take a problem of size \\(N\\), break it up into two problems of size\n\\(\\frac{N}{2}\\), and do \\(O(N)\\) glue work to combine the smaller\nsolutions into a bigger solution, so we get \\(O(N \\cdot log(N))\\) runtime total -\nmuch faster than \\(O(N^2)\\).\nHere is how we do it. I'll describe first how to use an FFT for\nmulti-point evaluation (ie. for some domain \\(D\\) and polynomial \\(P\\), calculate \\(P(x)\\) for every \\(x\\) in \\(D\\)), and it turns out that you can use the\nsame algorithm for interpolation with a minor tweak.\n\nSuppose that we have an FFT where the given domain is the powers of\n\\(x\\) in some field, where \\(x^{2^{k}} = 1\\) (eg. in the case we\nintroduced above, the domain is the powers of \\(85\\) modulo \\(337\\), and \\(85^{2^{3}} = 1\\)). We have some polynomial,\neg. \\(y = 6x^7 + 2x^6 + 9x^5 + 5x^4 + x^3 +\n4x^2 + x + 3\\) (we'll write it as \\(p =\n[3, 1, 4, 1, 5, 9, 2, 6]\\)). We want to evaluate this polynomial\nat each point in the domain, ie. at each of the eight powers of \\(85\\). Here is what we do. First, we break\nup the polynomial into two parts, which we'll call \\(evens\\) and \\(odds\\): \\(evens =\n[3, 4, 5, 2]\\) and \\(odds = [1, 1, 9,\n6]\\) (or \\(evens = 2x^3 + 5x^2 + 4x +\n3\\) and \\(odds = 6x^3 + 9x^2 + x +\n1\\); yes, this is just taking the even-degree coefficients and\nthe odd-degree coefficients). Now, we note a mathematical observation:\n\\(p(x) = evens(x^2) + x \\cdot\nodds(x^2)\\) and \\(p(-x) = evens(x^2) -\nx \\cdot odds(x^2)\\) (think about this for yourself and make sure\nyou understand it before going further).\n\nHere, we have a nice property: \\(evens\\) and \\(odds\\) are both polynomials half the size\nof \\(p\\), and furthermore, the set of\npossible values of \\(x^2\\) is only half\nthe size of the original domain, because there is a two-to-one\ncorrespondence: \\(x\\) and \\(-x\\) are both part of \\(D\\) (eg. in our current domain \\([1, 85, 148, 111, 336, 252, 189, 226]\\), 1\nand 336 are negatives of each other, as \\(336\n= -1\\) % \\(337\\), as are \\((85, 252)\\), \\((148, 189)\\) and \\((111, 226)\\). And \\(x\\) and \\(-x\\) always both have the same square.\nHence, we can use an FFT to compute the result of \\(evens(x)\\) for every \\(x\\) in the smaller domain consisting of\nsquares of numbers in the original domain (\\([1, 148, 336, 189]\\)), and we can do the\nsame for odds. And voila, we've reduced a size-\\(N\\) problem into half-size problems.\n\nThe \"glue\" is relatively easy (and \\(O(N)\\) in runtime): we receive the\nevaluations of \\(evens\\) and \\(odds\\) as size-\\(\\frac{N}{2}\\) lists, so we simply do \\(p[i] = evens\\_result[i] + domain[i]\\cdot\nodds\\_result[i]\\) and \\(p[\\frac{N}{2} +\ni] = evens\\_result[i] - domain[i]\\cdot odds\\_result[i]\\) for each\nindex \\(i\\).\n\nHere's the full code:\n\ndef fft(vals, modulus, domain):\n    if len(vals) == 1:\n        return vals\n    L = fft(vals[::2], modulus, domain[::2])\n    R = fft(vals[1::2], modulus, domain[::2])\n    o = [0 for i in vals]\n    for i, (x, y) in enumerate(zip(L, R)):\n        y_times_root = y*domain[i]\n        o[i] = (x+y_times_root) % modulus\n        o[i+len(L)] = (x-y_times_root) % modulus\n    return o\n\nWe can try running it:\n\n>>> fft([3,1,4,1,5,9,2,6], 337, [1, 85, 148, 111, 336, 252, 189, 226])\n[31, 70, 109, 74, 334, 181, 232, 4]\n\nAnd we can check the result; evaluating the polynomial at the position\n\\(85\\), for example, actually does give\nthe result \\(70\\). Note that this only\nworks if the domain is \"correct\"; it needs to be of the form \\([x^i\\) % \\(modulus\\) for \\(i\\) in \\(range(n)]\\) where \\(x^n = 1\\).\n\nAn inverse FFT is surprisingly simple:\n\ndef inverse_fft(vals, modulus, domain):\n    vals = fft(vals, modulus, domain)\n    return [x * modular_inverse(len(vals), modulus) % modulus for x in [vals[0]] + vals[1:][::-1]]\n\nBasically, run the FFT again, but reverse the result (except the first\nitem stays in place) and divide every value by the length of the list.\n\n>>> domain = [1, 85, 148, 111, 336, 252, 189, 226]\n>>> def modular_inverse(x, n): return pow(x, n - 2, n)\n>>> values = fft([3,1,4,1,5,9,2,6], 337, domain)\n>>> values\n[31, 70, 109, 74, 334, 181, 232, 4]\n>>> inverse_fft(values, 337, domain)\n[3, 1, 4, 1, 5, 9, 2, 6]\n\nNow, what can we use this for? Here's one fun use case: we can use FFTs\nto multiply numbers very quickly. Suppose we wanted to multiply \\(1253\\) by \\(1895\\). Here is what we would do. First, we\nwould convert the problem into one that turns out to be slightly easier:\nmultiply the polynomials \\([3, 5, 2,\n1]\\) by \\([5, 9, 8, 1]\\) (that's\njust the digits of the two numbers in increasing order), and then\nconvert the answer back into a number by doing a single pass to carry\nover tens digits. We can multiply polynomials with FFTs quickly, because\nit turns out that if you convert a polynomial into evaluation\nform (ie. \\(f(x)\\) for every \\(x\\) in some domain \\(D\\)), then you can multiply two polynomials\nsimply by multiplying their evaluations. So what we'll do is take the\npolynomials representing our two numbers in coefficient form,\nuse FFTs to convert them to evaluation form, multiply them pointwise,\nand convert back:\n\n>>> p1 = [3,5,2,1,0,0,0,0]\n>>> p2 = [5,9,8,1,0,0,0,0]\n>>> x1 = fft(p1, 337, domain)\n>>> x1\n[11, 161, 256, 10, 336, 100, 83, 78]\n>>> x2 = fft(p2, 337, domain)\n>>> x2\n[23, 43, 170, 242, 3, 313, 161, 96]\n>>> x3 = [(v1 * v2) % 337 for v1, v2 in zip(x1, x2)]\n>>> x3\n[253, 183, 47, 61, 334, 296, 220, 74]\n>>> inverse_fft(x3, 337, domain)\n[15, 52, 79, 66, 30, 10, 1, 0]\n\nThis requires three FFTs (each \\(O(N \\cdot\nlog(N))\\) time) and one pointwise multiplication (\\(O(N)\\) time), so it takes \\(O(N \\cdot log(N))\\) time altogether\n(technically a little bit more than \\(O(N\n\\cdot log(N))\\), because for very big numbers you would need\nreplace \\(337\\) with a bigger modulus\nand that would make multiplication harder, but close enough). This is\nmuch faster than schoolbook multiplication, which takes \\(O(N^2)\\) time:\n\n     3  5  2  1\n   ------------\n5 | 15 25 10  5\n9 |    27 45 18  9\n8 |       24 40 16  8\n1 |           3  5  2  1\n   ---------------------\n    15 52 79 66 30 10  1\n\nSo now we just take the result, and carry the tens digits over (this is\na \"walk through the list once and do one thing at each point\" algorithm\nso it takes \\(O(N)\\) time):\n\n[15, 52, 79, 66, 30, 10, 1, 0]\n[ 5, 53, 79, 66, 30, 10, 1, 0]\n[ 5,  3, 84, 66, 30, 10, 1, 0]\n[ 5,  3,  4, 74, 30, 10, 1, 0]\n[ 5,  3,  4,  4, 37, 10, 1, 0]\n[ 5,  3,  4,  4,  7, 13, 1, 0]\n[ 5,  3,  4,  4,  7,  3, 2, 0]\n\nAnd if we read the digits from top to bottom, we get \\(2374435\\). Let's check the answer....\n\n>>> 1253 * 1895\n2374435\n\nYay! It worked. In practice, on such small inputs, the difference\nbetween \\(O(N \\cdot log(N))\\) and \\(O(N^2)\\) isn't that large, so\nschoolbook multiplication is faster than this FFT-based multiplication\nprocess just because the algorithm is simpler, but on large inputs it\nmakes a really big difference.\n\nBut FFTs are useful not just for multiplying numbers; as mentioned\nabove, polynomial multiplication and multi-point evaluation are\ncrucially important operations in implementing erasure coding, which is\na very important technique for building many kinds of redundant\nfault-tolerant systems. If you like fault tolerance and you like\nefficiency, FFTs are your friend.\n\nFFTs and binary fields\n\nPrime fields are not the only kind of finite field out there. Another\nkind of finite field (really a special case of the more general concept\nof an extension field, which are kind of like the finite-field\nequivalent of complex numbers) are binary fields. In an binary field,\neach element is expressed as a polynomial where all of the entries are\n\\(0\\) or \\(1\\), eg. \\(x^3 +\nx + 1\\). Adding polynomials is done modulo \\(2\\), and subtraction is the same as\naddition (as \\(-1 = 1 \\bmod 2\\)). We\nselect some irreducible polynomial as a modulus (eg. \\(x^4 + x + 1\\); \\(x^4 + 1\\) would not work because \\(x^4 + 1\\) can be factored into \\((x^2 + 1)\\cdot(x^2 + 1)\\) so it's not\n\"irreducible\"); multiplication is done modulo that modulus. For example,\nin the binary field mod \\(x^4 + x +\n1\\), multiplying \\(x^2 + 1\\) by\n\\(x^3 + 1\\) would give \\(x^5 + x^3 + x^2 + 1\\) if you just do the\nmultiplication, but \\(x^5 + x^3 + x^2 + 1 =\n(x^4 + x + 1)\\cdot x + (x^3 + x + 1)\\), so the result is the\nremainder \\(x^3 + x + 1\\).\n\nWe can express this example as a multiplication table. First multiply\n\\([1, 0, 0, 1]\\) (ie. \\(x^3 + 1\\)) by \\([1, 0, 1]\\) (ie. \\(x^2 + 1\\)):\n\n    1 0 0 1\n   --------\n1 | 1 0 0 1\n0 |   0 0 0 0\n1 |     1 0 0 1\n   ------------\n    1 0 1 1 0 1\n\nThe multiplication result contains an \\(x^5\\) term so we can subtract \\((x^4 + x + 1)\\cdot x\\):\n\n    1 0 1 1 0 1\n  -   1 1 0 0 1    [(x\u2074 + x + 1) shifted right by one to reflect being multipled by x]\n   ------------\n    1 1 0 1 0 0\n\nAnd we get the result, \\([1, 1, 0, 1]\\)\n(or \\(x^3 + x + 1\\)).\n\nAddition and multiplication tables for the binary field mod\n\\(x^4 + x + 1\\). Field elements are\nexpressed as integers converted from binary (eg. \\(x^3 + x^2 \\rightarrow 1100 \\rightarrow\n12\\))\n\nBinary fields are interesting for two reasons. First of all, if you want\nto erasure-code binary data, then binary fields are really convenient\nbecause \\(N\\) bytes of data can be\ndirectly encoded as a binary field element, and any binary field\nelements that you generate by performing computations on it will also be\n\\(N\\) bytes long. You cannot do this\nwith prime fields because prime fields' size is not exactly a power of\ntwo; for example, you could encode every \\(2\\) bytes as a number from \\(0...65536\\) in the prime field modulo \\(65537\\) (which is prime), but if you do an\nFFT on these values, then the output could contain \\(65536\\), which cannot be expressed in two\nbytes. Second, the fact that addition and subtraction become the same\noperation, and \\(1 + 1 = 0\\), create\nsome \"structure\" which leads to some very interesting consequences. One\nparticularly interesting, and useful, oddity of binary fields is the\n\"freshman's\ndream\" theorem: \\((x+y)^2 = x^2 +\ny^2\\) (and the same for exponents \\(4,\n8, 16...\\) basically any power of two).\n\nBut if you want to use binary fields for erasure coding, and do so\nefficiently, then you need to be able to do Fast Fourier transforms over\nbinary fields. But then there is a problem: in a binary field, there\nare no (nontrivial) multiplicative groups of order \\(2^n\\). This is because the\nmultiplicative groups are all order \\(2^n\\)-1. For example, in the binary field\nwith modulus \\(x^4 + x + 1\\), if you\nstart calculating successive powers of \\(x+1\\), you cycle back to \\(1\\) after \\(\\it\n15\\) steps - not \\(16\\). The\nreason is that the total number of elements in the field is \\(16\\), but one of them is zero, and you're\nnever going to reach zero by multiplying any nonzero value by itself in\na field, so the powers of \\(x+1\\) cycle\nthrough every element but zero, so the cycle length is \\(15\\), not \\(16\\). So what do we do?\n\nThe reason we needed the domain to have the \"structure\" of a\nmultiplicative group with \\(2^n\\)\nelements before is that we needed to reduce the size of the domain by a\nfactor of two by squaring each number in it: the domain \\([1, 85, 148, 111, 336, 252, 189, 226]\\)\ngets reduced to \\([1, 148, 336, 189]\\)\nbecause \\(1\\) is the square of both\n\\(1\\) and \\(336\\), \\(148\\) is the square of both \\(85\\) and \\(252\\), and so forth. But what if in a\nbinary field there's a different way to halve the size of a domain? It\nturns out that there is: given a domain containing \\(2^k\\) values, including zero (technically\nthe domain must be a\nsubspace),\nwe can construct a half-sized new domain \\(D'\\) by taking \\(x \\cdot (x+k)\\) for \\(x\\) in \\(D\\) using some specific \\(k\\) in \\(D\\). Because the original domain is a\nsubspace, since \\(k\\) is in the domain,\nany \\(x\\) in the domain has a\ncorresponding \\(x+k\\) also in the\ndomain, and the function \\(f(x) = x \\cdot\n(x+k)\\) returns the same value for \\(x\\) and \\(x+k\\) so we get the same kind of two-to-one\ncorrespondence that squaring gives us.\n\n\\(x\\)\n\n0\n\n1\n\n2\n\n3\n\n4\n\n5\n\n6\n\n7\n\n8\n\n9\n\n10\n\n11\n\n12\n\n13\n\n14\n\n15\n\n\\(x \\cdot (x+1)\\)\n\n0\n\n0\n\n6\n\n6\n\n7\n\n7\n\n1\n\n1\n\n4\n\n4\n\n2\n\n2\n\n3\n\n3\n\n5\n\n5\n\nSo now, how do we do an FFT on top of this? We'll use the same trick,\nconverting a problem with an \\(N\\)-sized polynomial and \\(N\\)-sized domain into two problems each\nwith an \\(\\frac{N}{2}\\)-sized\npolynomial and \\(\\frac{N}{2}\\)-sized\ndomain, but this time using different equations. We'll convert a\npolynomial \\(p\\) into two polynomials\n\\(evens\\) and \\(odds\\) such that \\(p(x) = evens(x \\cdot (k-x)) + x \\cdot odds(x \\cdot\n(k-x))\\). Note that for the \\(evens\\) and \\(odds\\) that we find, it will also\nbe true that \\(p(x+k) = evens(x \\cdot (k-x)) +\n(x+k) \\cdot odds(x \\cdot (k-x))\\). So we can then recursively do\nan FFT to \\(evens\\) and \\(odds\\) on the reduced domain \\([x \\cdot (k-x)\\) for \\(x\\) in \\(D]\\), and then we use these two formulas to\nget the answers for two \"halves\" of the domain, one offset by \\(k\\) from the other.\n\nConverting \\(p\\) into \\(evens\\) and \\(odds\\) as described above turns out to\nitself be nontrivial. The \"naive\" algorithm for doing this is itself\n\\(O(N^2)\\), but it turns out that in a\nbinary field, we can use the fact that \\((x^2-kx)^2 = x^4 - k^2 \\cdot x^2\\), and\nmore generally \\((x^2-kx)^{2^{i}} =\nx^{2^{i+1}} - k^{2^{i}} \\cdot x^{2^{i}}\\) , to create yet another\nrecursive algorithm to do this in \\(O(N \\cdot\nlog(N))\\) time.\n\nAnd if you want to do an inverse FFT, to do interpolation, then\nyou need to run the steps in the algorithm in reverse order. You can\nfind the complete code for doing this here:\nhttps://github.com/ethereum/research/tree/master/binary_fft,\nand a paper with details on more optimal algorithms here:\nhttp://www.math.clemson.edu/~sgao/papers/GM10.pdf\n\nSo what do we get from all of this complexity? Well, we can try running\nthe implementation, which features both a \"naive\" \\(O(N^2)\\) multi-point evaluation and the\noptimized FFT-based one, and time both. Here are my results:\n\n>>> import binary_fft as b\n>>> import time, random\n>>> f = b.BinaryField(1033)\n>>> poly = [random.randrange(1024) for i in range(1024)]\n>>> a = time.time(); x1 = b._simple_ft(f, poly); time.time() - a\n0.5752472877502441\n>>> a = time.time(); x2 = b.fft(f, poly, list(range(1024))); time.time() - a\n0.03820443153381348\n\nAnd as the size of the polynomial gets larger, the naive implementation\n(_simple_ft) gets slower much more quickly than the FFT:\n\n>>> f = b.BinaryField(2053)\n>>> poly = [random.randrange(2048) for i in range(2048)]\n>>> a = time.time(); x1 = b._simple_ft(f, poly); time.time() - a\n2.2243144512176514\n>>> a = time.time(); x2 = b.fft(f, poly, list(range(2048))); time.time() - a\n0.07896280288696289\n\nAnd voila, we have an efficient, scalable way to multi-point evaluate\nand interpolate polynomials. If we want to use FFTs to recover\nerasure-coded data where we are missing some pieces, then\nalgorithms for this\nalso\nexist, though they are somewhat less efficient than just doing a\nsingle FFT. Enjoy!",
    "contentLength": 23835,
    "summary": "FFTs reduce polynomial evaluation/interpolation from O(N\u00b2) to O(N log N) using recursive algorithms on multiplicative subgroups.",
    "detailedSummary": {
      "theme": "Fast Fourier Transforms (FFTs) are powerful algorithms that enable efficient polynomial operations and have crucial applications in cryptography, blockchain scalability, and erasure coding.",
      "summary": "Vitalik explains how Fast Fourier Transforms work as mathematical algorithms that can perform polynomial evaluation and interpolation much faster than traditional methods, reducing complexity from O(N\u00b2) to O(N\u00b7log(N)). He demonstrates that while FFTs require specific constraints (finite fields with multiplicative subgroups of size 2\u207f), they enable powerful applications like extremely fast multiplication of large numbers and efficient erasure coding for fault-tolerant systems. Vitalik provides detailed examples using both prime fields and binary fields, showing how the recursive nature of FFTs allows them to break down large problems into smaller, more manageable pieces. He emphasizes that FFTs are particularly valuable for blockchain applications, including scalable blockchain data availability and STARKs (cryptographic proofs), making them essential tools for modern cryptographic systems.",
      "takeaways": [
        "FFTs reduce polynomial evaluation and interpolation from O(N\u00b2) to O(N\u00b7log(N)) complexity through recursive algorithms",
        "FFTs require specific mathematical constraints: finite fields with multiplicative subgroups of size 2\u207f",
        "FFTs enable extremely fast multiplication of large numbers by converting multiplication into polynomial operations",
        "Binary fields present unique challenges for FFTs but offer advantages for erasure coding of binary data",
        "FFTs are crucial building blocks for blockchain scalability solutions, erasure codes, and cryptographic proofs like STARKs"
      ],
      "controversial": []
    }
  },
  {
    "id": "general-2019-05-09-control_as_liability",
    "title": "Control as Liability",
    "date": "2019-05-09",
    "category": "society",
    "url": "https://vitalik.eth.limo/general/2019/05/09/control_as_liability.html",
    "path": "general/2019/05/09/control_as_liability.html",
    "content": "- \n\n- \n\n  \n    \n\n  \n  \n  Dark Mode Toggle\n  \n    \n\n  \n\n##  Control as Liability \n\n 2019 May 09 \nSee all posts\n\n \n \n\n Control as Liability \n\nThe regulatory and legal environment around internet-based services\nand applications has changed considerably over the last decade. When\nlarge-scale social networking platforms first became popular in the\n2000s, the general attitude toward mass data collection was essentially\n\"why not?\". This was the age of Mark Zuckerberg saying\nthe age of privacy is over and Eric Schmidt arguing,\n\"If you have something that you don't want anyone to know, maybe you\nshouldn't be doing it in the first place.\" And it made personal sense\nfor them to argue this: every bit of data you can get about others was a\npotential machine learning advantage for you, every single restriction a\nweakness, and if something happened to that data, the costs were\nrelatively minor. Ten years later, things are very different.\n\nIt is especially worth zooming in on a few particular trends.\n\n- Privacy. Over the last ten years, a number of\nprivacy laws have been passed, most aggressively in Europe but also\nelsewhere, but the most recent is the\nGDPR. The GDPR has many parts, but among the most prominent are: (i)\nrequirements for explicit consent, (ii) requirement to have a legal\nbasis to process data, (iii) users' right to download all their data,\n(iv) users' right to require you to delete all their data. Other jurisdictions\nare exploring\nsimilar rules.\n\n- Data localization rules. India,\nRussia\nand many other jurisdictions increasingly have or are\nexploring rules that require data on users within the country to be\nstored inside the country. And even when explicit laws do not exist,\nthere's a growing shift toward concern (eg. 1\n2)\naround data being moved to countries that are perceived to not\nsufficiently protect it.\n\n- Sharing economy regulation. Sharing economy\ncompanies such as Uber are\nhaving a hard time arguing to courts that, given the extent to which\ntheir applications control and direct drivers' activity, they should not\nbe legally classified as employers.\n\n- Cryptocurrency regulation. A recent\nFINCEN guidance attempts to clarify what categories of\ncryptocurrency-related activity are and are not subject to regulatory\nlicensing requirements in the United States. Running a hosted wallet?\nRegulated. Running a wallet where the user controls their funds? Not\nregulated. Running an anonymizing mixing service? If you're\nrunning it, regulated. If you're just writing code... not\nregulated.\n\nAs Emin\nGun Sirer points out, the FINCEN cryptocurrency guidance is not at\nall haphazard; rather, it's trying to separate out categories of\napplications where the developer is actively controlling funds, from\napplications where the developer has no control. The guidance carefully\nseparates out how multisignature wallets, where keys are held\nboth by the operator and the user, are sometimes regulated and sometimes\nnot:\n\nIf the multiple-signature wallet provider restricts its role to\ncreating un-hosted wallets that require adding a second authorization\nkey to the wallet owner's private key in order to validate and complete\ntransactions, the provider is not a money transmitter because it does\nnot accept and transmit value. On the other hand, if ... the value is\nrepresented as an entry in the accounts of the provider, the owner does\nnot interact with the payment system directly, or the provider maintains\ntotal independent control of the value, the provider will also qualify\nas a money transmitter.\n\nAlthough these events are taking place across a variety of contexts\nand industries, I would argue that there is a common trend at play. And\nthe trend is this: control over users' data and digital\npossessions and activity is rapidly moving from an asset to a\nliability. Before, every bit of control you have was good: it\ngives you more flexibility to earn revenue, if not now then in the\nfuture. Now, every bit of control you have is a liability: you might be\nregulated because of it. If you exhibit control over your users'\ncryptocurrency, you are a money transmitter. If you have \"sole\ndiscretion over fares, and can charge drivers a cancellation fee if they\nchoose not to take a ride, prohibit drivers from picking up passengers\nnot using the app and suspend or deactivate drivers' accounts\", you are\nan employer. If you control your users' data, you're required to make\nsure you can argue just cause, have a compliance officer, and give your\nusers access to download or delete the data.\n\nIf you are an application builder, and you are both lazy and fear\nlegal trouble, there is one easy way to make sure that you violate none\nof the above new rules: don't build applications that centralize\ncontrol. If you build a wallet where the user holds their private\nkeys, you really are still \"just a software provider\". If you build a\n\"decentralized Uber\" that really is just a slick UI combining a payment\nsystem, a reputation system and a search engine, and don't control the\ncomponents yourself, you really won't get hit by many of the same legal\nissues. If you build a website that just... doesn't collect data (Static\nweb pages? But that's impossible!) you don't have to even think about\nthe GDPR.\n\nThis kind of approach is of course not realistic for everyone. There\nwill continue to be many cases where going without the conveniences of\ncentralized control simply sacrifices too much for both developers and\nusers, and there are also cases where the business model considerations\nmandate a more centralized approach (eg. it's easier to prevent\nnon-paying users from using software if the software stays on your\nservers) win out. But we're definitely very far from having explored the\nfull range of possibilities that more decentralized approaches\noffer.\n\nGenerally, unintended consequences of laws, discouraging entire\ncategories of activity when one wanted to only surgically forbid a few\nspecific things, are considered to be a bad thing. Here though, I would\nargue that the forced shift in developers' mindsets, from \"I want to\ncontrol more things just in case\" to \"I want to control fewer things\njust in case\", also has many positive consequences. Voluntarily giving\nup control, and voluntarily taking steps to deprive oneself of the\nability to do mischief, does not come naturally to many people, and\nwhile ideologically-driven decentralization-maximizing projects exist\ntoday, it's not at all obvious at first glance that such services will\ncontinue to dominate as the industry mainstreams. What this trend in\nregulation does, however, is that it gives a big nudge in favor of those\napplications that are willing to take the centralization-minimizing,\nuser-sovereignty-maximizing \"can't be evil\" route.\n\nHence, even though these regulatory changes are arguably not\npro-freedom, at least if one is concerned with the freedom of\napplication developers, and the transformation of the internet into a\nsubject of political focus is bound to have many negative knock-on\neffects, the particular trend of control becoming a liability is in a\nstrange way even more pro-cypherpunk (even if not\nintentionally!) than policies of maximizing total freedom for\napplication developers would have been. Though the present-day\nregulatory landscape is very far from an optimal one from the point of\nview of almost anyone's preferences, it has unintentionally dealt the\nmovement for minimizing unneeded centralization and maximizing users'\ncontrol of their own assets, private keys and data a surprisingly strong\nhand to execute on its vision. And it would be highly beneficial to the\nmovement to take advantage of it.",
    "contentLength": 7640,
    "summary": "New regulations around privacy, data localization, and crypto are making control over user data/assets a legal liability, pushing developers toward decentralized approaches.",
    "detailedSummary": {
      "theme": "Regulatory changes across industries are transforming centralized control over user data and assets from a business advantage into a legal liability, inadvertently promoting decentralized approaches.",
      "summary": "Vitalik argues that the regulatory landscape has fundamentally shifted over the past decade, moving from an era where data collection and centralized control were considered unqualified goods to one where they create significant legal liabilities. He examines this trend across multiple domains - privacy regulations like GDPR requiring explicit consent and data portability, data localization laws forcing geographic restrictions, sharing economy regulations classifying platforms like Uber as employers due to their control over drivers, and cryptocurrency regulations that distinguish between custodial and non-custodial services based on who controls the funds. Vitalik contends that this shift makes decentralized approaches more attractive not just ideologically, but practically, as developers can avoid regulatory burden by building applications that don't centralize control - such as wallets where users hold their own private keys or platforms that function as interfaces rather than controllers. While acknowledging that centralized approaches still have advantages in many cases, Vitalik sees this regulatory trend as paradoxically beneficial to the cypherpunk movement, creating economic incentives for the kind of user-sovereignty-maximizing, decentralized applications that align with crypto-anarchist ideals, even though the regulations themselves aren't necessarily pro-freedom for developers.",
      "takeaways": [
        "Control over user data, funds, and activity has shifted from being a business asset to a regulatory liability due to evolving laws across privacy, employment, and financial services",
        "Developers can avoid regulatory burden by building truly decentralized applications where users maintain control rather than platforms that centralize authority",
        "FINCEN cryptocurrency guidance specifically distinguishes between services based on who controls the funds, making non-custodial solutions less regulated",
        "The regulatory environment inadvertently promotes cypherpunk ideals by creating economic incentives for decentralization and user sovereignty",
        "While not optimal for developer freedom, current regulations provide a 'surprisingly strong hand' for the movement toward minimizing centralization"
      ],
      "controversial": [
        "The suggestion that privacy and financial regulations, despite limiting developer freedom, are beneficial because they promote decentralization",
        "The characterization of regulatory compliance as something developers should avoid by building decentralized systems rather than engage with directly"
      ]
    }
  },
  {
    "id": "general-2019-04-16-free_speech",
    "title": "On Free Speech",
    "date": "2019-04-16",
    "category": "society",
    "url": "https://vitalik.eth.limo/general/2019/04/16/free_speech.html",
    "path": "general/2019/04/16/free_speech.html",
    "content": "- \n\n- \n\n  \n    \n\n  \n  \n  Dark Mode Toggle\n  \n    \n\n  \n\n##  On Free Speech \n\n 2019 Apr 16 \nSee all posts\n\n \n \n\n On Free Speech \n\n\"A statement may be both true and dangerous. The previous\nsentence is such a statement.\" - David Friedman\n\nFreedom of speech is a topic that many internet communities have\nstruggled with over the last two decades. Cryptocurrency and blockchain\ncommunities, a major part of their raison d'etre being censorship\nresistance, are especially poised to value free speech very highly, and\nyet, over the last few years, the extremely rapid growth of these\ncommunities and the very high financial and social stakes involved have\nrepeatedly tested the application and the limits of the concept. In this\npost, I aim to disentangle some of the contradictions, and make a case\nwhat the norm of \"free speech\" really stands for.\n\"Free speech laws\" vs \"free\nspeech\"\n\nA common, and in my own view frustrating, argument that I often hear\nis that \"freedom of speech\" is exclusively a legal restriction on what\ngovernments can act against, and has nothing to say regarding\nthe actions of private entities such as corporations, privately-owned\nplatforms, internet forums and conferences. One of the larger examples\nof \"private censorship\" in cryptocurrency communities was the decision\nof Theymos, the moderator of the /r/bitcoin subreddit, to start\nheavily moderating the subreddit, forbidding arguments in favor of\nincreasing the Bitcoin blockchain's transaction capacity via a hard\nfork.\n\nHere is a timeline of the censorship as catalogued by John Blocke: https://medium.com/johnblocke/a-brief-and-incomplete-history-of-censorship-in-r-bitcoin-c85a290fe43\n\nHere is Theymos's post defending his policies: https://www.reddit.com/r/Bitcoin/comments/3h9cq4/its_time_for_a_break_about_the_recent_mess/,\nincluding the now infamous line \"If 90% of /r/Bitcoin users find these\npolicies to be intolerable, then I want these 90% of /r/Bitcoin users to\nleave\".\n\nA common strategy used by defenders of Theymos's censorship was to\nsay that heavy-handed moderation is okay because /r/bitcoin is \"a\nprivate forum\" owned by Theymos, and so he has the right to do whatever\nhe wants in it; those who dislike it should move to other forums:\n\n \n\nAnd it's true that Theymos has not broken any laws by\nmoderating his forum in this way. But to most people, it's clear that\nthere is still some kind of free speech violation going on. So what\ngives? First of all, it's crucially important to recognize that freedom\nof speech is not just a law in some countries. It's also a\nsocial principle. And the underlying goal of the social principle is the\nsame as the underlying goal of the law: to foster an environment where\nthe ideas that win are ideas that are good, rather than just ideas that\nhappen to be favored by people in a position of power. And governmental\npower is not the only kind of power that we need to protect from; there\nis also a corporation's power to fire someone, an internet forum\nmoderator's power to delete\nalmost every post in a discussion thread, and many other kinds of\npower hard and soft.\n\nSo what is the underlying social principle here? Quoting\nEliezer Yudkowsky:\n\nThere are a very few injunctions in the human art of rationality that\nhave no ifs, ands, buts, or escape clauses. This is one of them. Bad\nargument gets counterargument. Does not get bullet. Never. Never ever\nnever for ever.\n\nSlatestarcodex\nelaborates:\n\nWhat does \"bullet\" mean in the quote above? Are other projectiles\ncovered? Arrows? Boulders launched from catapults? What about melee\nweapons like swords or maces? Where exactly do we draw the line for\n\"inappropriate responses to an argument\"? A good response to an argument\nis one that addresses an idea; a bad argument is one that silences it.\nIf you try to address an idea, your success depends on how good the idea\nis; if you try to silence it, your success depends on how powerful you\nare and how many pitchforks and torches you can provide on short notice.\nShooting bullets is a good way to silence an idea without addressing it.\nSo is firing stones from catapults, or slicing people open with swords,\nor gathering a pitchfork-wielding mob. But trying to get someone fired\nfor holding an idea is also a way of silencing an idea without\naddressing it.\n\nThat said, sometimes there is a rationale for \"safe spaces\" where\npeople who, for whatever reason, just don't want to deal with arguments\nof a particular type, can congregate and where those arguments actually\ndo get silenced. Perhaps the most innocuous of all is spaces like ethresear.ch where posts get silenced\njust for being \"off topic\" to keep the discussion focused. But there's\nalso a dark side to the concept of \"safe spaces\"; as Ken\nWhite writes:\n\nThis may come as a surprise, but I'm a supporter of \u2018safe spaces.' I\nsupport safe spaces because I support freedom of association. Safe\nspaces, if designed in a principled way, are just an application of that\nfreedom... But not everyone imagines \"safe spaces\" like that. Some use the\nconcept of \"safe spaces\" as a sword, wielded to annex public spaces and\ndemand that people within those spaces conform to their private norms.\nThat's not freedom of association\n\nAha. So making your own safe space off in a corner is totally fine,\nbut there is also this concept of a \"public space\", and trying to turn a\npublic space into a safe space for one particular special interest is\nwrong. So what is a \"public space\"? It's definitely clear that a public\nspace is not just \"a space owned and/or run by a government\";\nthe concept of privately\nowned public spaces is a well-established one. This is true even\ninformally: it's a common moral intuition, for example, that it's less\nbad for a private individual to commit violations such as discriminating\nagainst races and genders than it is for, say, a shopping mall to do the\nsame. In the case or the /r/bitcoin subreddit, one can make the case,\nregardless of who technically owns the top moderator position in the\nsubreddit, that the subreddit very much is a public space. A few\narguments particularly stand out:\n\n- It occupies \"prime real estate\", specifically the word \"bitcoin\",\nwhich makes people consider it to be the default place to\ndiscuss Bitcoin.\n\n- The value of the space was created not just by Theymos, but by\nthousands of people who arrived on the subreddit to discuss Bitcoin with\nan implicit expectation that it is, and will continue, to be a public\nspace for discussing Bitcoin.\n\n- Theymos's shift in policy was a surprise to many people, and it was\nnot foreseeable ahead of time that it would take place.\n\nIf, instead, Theymos had created a subreddit called\n/r/bitcoinsmallblockers, and explicitly said that it was a curated space\nfor small block proponents and attempting to instigate controversial\nhard forks was not welcome, then it seems likely that very few people\nwould have seen anything wrong about this. They would have opposed his\nideology, but few (at least in blockchain communities) would try to\nclaim that it's improper for people with ideologies opposed to\ntheir own to have spaces for internal discussion. But back in reality,\nTheymos tried to \"annex a public space and demand that people within the\nspace confirm to his private norms\", and so we have the Bitcoin\ncommunity block size schism, a highly acrimonious fork and chain split,\nand now a cold peace between Bitcoin and Bitcoin Cash.\n\n## Deplatforming\n\nAbout a year ago at Deconomy I publicly shouted down Craig Wright, a scammer claiming to\nbe Satoshi Nakamoto, finishing my explanation of why the things he\nsays make no sense with the question \"why is this fraud allowed to speak\nat this conference?\"\n\nOf course, Craig Wright's partisans replied back with.... accusations\nof censorship:\n\nDid I try to \"silence\" Craig Wright? I would argue, no. One could\nargue that this is because \"Deconomy is not a public space\", but I think\nthe much better argument is that a conference is fundamentally different\nfrom an internet forum. An internet forum can actually try to be a fully\nneutral medium for discussion where anything goes; a conference, on the\nother hand, is by its very nature a highly curated list of\npresentations, allocating a limited number of speaking slots and\nactively channeling a large amount of attention to those lucky enough to\nget a chance to speak. A conference is an editorial act by the\norganizers, saying \"here are some ideas and views that we think people\nreally should be exposed to and hear\". Every conference \"censors\" almost\nevery viewpoint because there's not enough space to give them all a\nchance to speak, and this is inherent to the format; so raising an\nobjection to a conference's judgement in making its selections is\nabsolutely a legitimate act.\n\nThis extends to other kinds of selective platforms. Online platforms\nsuch as Facebook, Twitter and Youtube already engage in active selection\nthrough algorithms that influence what people are more likely to be\nrecommended. Typically, they do this for selfish reasons, setting up\ntheir algorithms to maximize \"engagement\" with their platform, often\nwith unintended byproducts like promoting\nflat earth conspiracy theories. So given that these platforms are\nalready engaging in (automated) selective presentation, it seems\neminently reasonable to criticize them for not directing these same\nlevers toward more pro-social objectives, or at the least pro-social\nobjectives that all major reasonable political tribes agree on (eg.\nquality intellectual discourse). Additionally, the \"censorship\" doesn't\nseriously block anyone's ability to learn Craig Wright's side of the\nstory; you can just go visit their website, here you go: https://coingeek.com/. If\nsomeone is already operating a platform that makes editorial decisions,\nasking them to make such decisions with the same magnitude but with more\npro-social criteria seems like a very reasonable thing to\ndo.\n\nA more recent example of this principle at work is the #DelistBSV\ncampaign, where some cryptocurrency exchanges, most famously Binance,\nremoved support for trading BSV (the Bitcoin fork promoted by Craig\nWeight). Once again, many people, even reasonable\npeople, accused this campaign of being an exercise\nin censorship, raising parallels to credit card companies blocking\nWikileaks:\n\nI personally have been a critic\nof the power wielded by centralized exchanges. Should I oppose\n#DelistBSV on free speech grounds? I would argue no, it's ok to support\nit, but this is definitely a much closer call.\n\nMany #DelistBSV participants like Kraken are definitely not\n\"anything-goes\" platforms; they already make many editorial decisions\nabout which currencies they accept and refuse. Kraken only accepts about a dozen\ncurrencies, so they are passively \"censoring\" almost everyone.\nShapeshift supports more currencies but it does not support SPANK, or even KNC. So in these two cases, delisting\nBSV is more like reallocation of a scarce resource\n(attention/legitimacy) than it is censorship. Binance is a bit\ndifferent; it does accept a very large array of cryptocurrencies,\nadopting a philosophy much closer to anything-goes, and it does have a\nunique position as market leader with a lot of liquidity.\n\nThat said, one can argue two things in Binance's favor. First of all,\ncensorship is retaliating against a truly malicious exercise of\ncensorship on the part of core BSV community members when they\nthreatened critics like Peter McCormack with legal letters (see Peter's\nresponse); in \"anarchic\" environments with large disagreements on\nwhat the norms are, \"an eye for an eye\" in-kind retaliation is one of\nthe better social norms to have because it ensures that people only face\npunishments that they in some sense have through their own actions\ndemonstrated they believe are legitimate. Furthermore, the delistings\nwon't make it that hard for people to buy or sell BSV; Coinex has said\nthat they will\nnot delist (and I would actually oppose second-tier \"anything-goes\"\nexchanges delisting). But the delistings do send a strong\nmessage of social condemnation of BSV, which is useful and needed. So\nthere's a case to support all delistings so far, though on reflection\nBinance refusing to delist \"because freedom\" would have also been not as\nunreasonable as it seems at first glance.\n\nIt's in general absolutely potentially reasonable to oppose the\nexistence of a concentration of power, but support that concentration of\npower being used for purposes that you consider prosocial as long as\nthat concentration exists; see Bryan Caplan's exposition on reconciling\nsupporting open borders and also supporting anti-ebola restrictions for\nan example in a different field. Opposing concentrations of power only\nrequires that one believe those concentrations of power to be on\nbalance harmful and abusive; it does not mean that one must oppose\nall things that those concentrations of power do.\n\nIf someone manages to make a completely permissionless\ncross-chain decentralized exchange that facilitates trade between any\nasset and any other asset, then being \"listed\" on the exchange would\nnot send a social signal, because everyone is listed; and I\nwould support such an exchange existing even if it supports trading BSV.\nThe thing that I do support is BSV being removed from already exclusive\npositions that confer higher tiers of legitimacy than simple\nexistence.\n\nSo to conclude: censorship in public spaces bad, even if the public\nspaces are non-governmental; censorship in genuinely private spaces\n(especially spaces that are not \"defaults\" for a broader\ncommunity) can be okay; ostracizing projects with the goal and effect of\ndenying access to them, bad; ostracizing projects with the goal and\neffect of denying them scarce legitimacy can be okay.",
    "contentLength": 13758,
    "summary": "This blog post argues that free speech is both a legal principle and a social principle that should protect against private censorship in \"public spaces\" like the r/bitcoin subreddit.",
    "detailedSummary": {
      "theme": "Vitalik argues that free speech is both a legal and social principle, distinguishing between legitimate censorship in private spaces versus problematic censorship in public forums, while defending certain forms of deplatforming as reallocation of legitimacy rather than true censorship.",
      "summary": "Vitalik challenges the common argument that free speech only applies to government censorship, arguing it's also a crucial social principle designed to ensure good ideas win based on merit rather than power. He uses the example of Theymos's heavy moderation of the /r/bitcoin subreddit during the block size debate to illustrate how private entities can still violate free speech principles when they control what amounts to public spaces. Vitalik distinguishes between legitimate 'safe spaces' created for specific purposes and the problematic annexation of public spaces to enforce private norms. He argues that the /r/bitcoin subreddit qualified as a public space because it occupied prime real estate (the 'bitcoin' name), was built by community contributions, and underwent unexpected policy changes. Regarding deplatforming, Vitalik defends his confrontation of Craig Wright at conferences and supports the #DelistBSV campaign, arguing that curated platforms like conferences and exchanges inherently make editorial decisions, so asking them to make pro-social choices is reasonable. He concludes that censorship in public spaces is bad, but removing projects from positions that confer special legitimacy (rather than blocking access entirely) can be justified, especially as retaliation against bad actors.",
      "takeaways": [
        "Free speech is both a legal principle and a social norm designed to let good ideas win on merit rather than power",
        "Private entities can violate free speech principles when they control spaces that function as public forums",
        "There's an important distinction between creating legitimate safe spaces and annexing existing public spaces to enforce private norms",
        "Deplatforming from curated venues (conferences, selective exchanges) is different from censorship because these platforms inherently make editorial choices",
        "Removing legitimacy and special status from bad actors can be justified, especially when it's proportional retaliation for their own censorious behavior"
      ],
      "controversial": [
        "Vitalik's support for the #DelistBSV campaign could be seen as supporting coordinated financial censorship",
        "His distinction between 'public' and 'private' spaces may be subjective and could justify excessive intervention in private platforms",
        "The 'eye for an eye' retaliation principle he endorses could escalate conflicts rather than resolve them"
      ]
    }
  },
  {
    "id": "general-2019-04-03-collusion",
    "title": "On Collusion",
    "date": "2019-04-03",
    "category": "governance",
    "url": "https://vitalik.eth.limo/general/2019/04/03/collusion.html",
    "path": "general/2019/04/03/collusion.html",
    "content": "- \n\n- \n\n  \n    \n\n  \n  \n  Dark Mode Toggle\n  \n    \n\n  \n\n##  On Collusion \n\n 2019 Apr 03 \nSee all posts\n\n \n \n\n On Collusion \n\nSpecial thanks to Glen Weyl, Phil Daian and Jinglan Wang for\nreview\n\nOver the last few years there has been an increasing interest in\nusing deliberately engineered economic incentives and mechanism design\nto align behavior of participants in various contexts. In the blockchain\nspace, mechanism design first and foremost provides the security for the\nblockchain itself, encouraging miners or proof of stake validators to\nparticipate honestly, but more recently it is being applied in prediction markets, \"token\ncurated registries\" and many other contexts. The nascent RadicalXChange movement has\nmeanwhile spawned experimentation with Harberger\ntaxes, quadratic voting, quadratic\nfinancing and more. More recently, there has also been growing\ninterest in using token-based incentives to try to encourage quality\nposts in social media. However, as development of these systems moves\ncloser from theory to practice, there are a number of challenges that\nneed to be addressed, challenges that I would argue have not yet been\nadequately confronted.\n\nAs a recent example of this move from theory toward deployment, Bihu,\na Chinese platform that has recently released a coin-based mechanism for\nencouraging people to write posts. The basic mechanism (see whitepaper\nin Chinese here)\nis that if a user of the platform holds KEY tokens, they have the\nability to stake those KEY tokens on articles; every user can make \\(k\\) \"upvotes\" per day, and the \"weight\" of\neach upvote is proportional to the stake of the user making the upvote.\nArticles with a greater quantity of stake upvoting them appear more\nprominently, and the author of an article gets a reward of KEY tokens\nroughly proportional to the quantity of KEY upvoting that article. This\nis an oversimplification and the actual mechanism has some\nnonlinearities baked into it, but they are not essential to the basic\nfunctioning of the mechanism. KEY has value because it can be used in\nvarious ways inside the platform, but particularly a percentage of all\nad revenues get used to buy and burn KEY (yay, big thumbs up to them for\ndoing this and not making yet another medium of exchange token!).\n\nThis kind of design is far from unique; incentivizing online content\ncreation is something that very many people care about, and there have\nbeen many designs of a similar character, as well as some fairly\ndifferent designs. And in this case this particular platform is already\nbeing used significantly:\n\nA few months ago, the Ethereum trading subreddit /r/ethtrader introduced a\nsomewhat similar experimental feature where a token called \"donuts\" is\nissued to users that make comments that get upvoted, with a set amount\nof donuts issued weekly to users in proportion to how many upvotes their\ncomments received. The donuts could be used to buy the right to set the\ncontents of the banner at the top of the subreddit, and could also be\nused to vote in community polls. However, unlike what happens in the KEY\nsystem, here the reward that B receives when A upvotes B is not\nproportional to A's existing coin supply; instead, each Reddit account\nhas an equal ability to contribute to other Reddit accounts.\n\nThese kinds of experiments, attempting to reward quality content\ncreation in a way that goes beyond the known limitations of\ndonations/microtipping, are very valuable; under-compensation of\nuser-generated internet content is a very significant problem in society\nin general (see \"liberal\nradicalism\" and \"data as\nlabor\"), and it's heartening to see crypto communities attempting to\nuse the power of mechanism design to make inroads on solving it.\nBut unfortunately, these systems are also vulnerable to\nattack.\nSelf-voting, plutocracy and\nbribes\n\nHere is how one might economically attack the design proposed above.\nSuppose that some wealthy user acquires some quantity \\(N\\) of tokens, and as a result each of the\nuser's \\(k\\) upvotes gives the\nrecipient a reward of \\(N \\cdot q\\)\n(\\(q\\) here probably being a very small\nnumber, eg. think \\(q = 0.000001\\)).\nThe user simply upvotes their own sockpuppet accounts, giving themselves\nthe reward of \\(N \\cdot k \\cdot q\\).\nThen, the system simply collapses into each user having an \"interest\nrate\" of \\(k \\cdot q\\) per period, and\nthe mechanism accomplishes nothing else.\n\nThe actual Bihu mechanism seemed to anticipate this, and has some\nsuperlinear logic where articles with more KEY upvoting them gain a\ndisproportionately greater reward, seemingly to encourage upvoting\npopular posts rather than self-upvoting. It's a common pattern among\ncoin voting governance systems to add this kind of superlinearity to\nprevent self-voting from undermining the entire system; most DPOS\nschemes have a limited number of delegate slots with zero rewards for\nanyone who does not get enough votes to join one of the slots, with\nsimilar effect. But these schemes invariably introduce two new\nweaknesses:\n\n- They subsidize plutocracy, as very wealthy\nindividuals and cartels can still get enough funds to self-upvote.\n\n- They can be circumvented by users bribing\nother users to vote for them en masse.\n\nBribing attacks may sound farfetched (who here has ever accepted a\nbribe in real life?), but in a mature ecosystem they are much more\nrealistic than they seem. In most contexts where bribing has taken\nplace in the blockchain space, the operators use a euphemistic new\nname to give the concept a friendly face: it's not a bribe, it's a\n\"staking pool\" that \"shares dividends\". Bribes can even be obfuscated:\nimagine a cryptocurrency exchange that offers zero fees and spends the\neffort to make an abnormally good user interface, and does not even try\nto collect a profit; instead, it uses coins that users deposit to\nparticipate in various coin voting systems. There will also inevitably\nbe people that see in-group collusion as just plain normal; see a recent\nscandal\ninvolving EOS DPOS for one example:\n\nFinally, there is the possibility of a \"negative bribe\", ie.\nblackmail or coercion, threatening participants with harm unless they\nact inside the mechanism in a certain way.\n\nIn the /r/ethtrader experiment, fear of people coming in and\nbuying donuts to shift governance polls led to the community\ndeciding to make only locked (ie. untradeable) donuts eligible for use\nin voting. But there's an even cheaper attack than buying donuts (an\nattack that can be thought of as a kind of obfuscated bribe):\nrenting them. If an attacker is already holding ETH, they can\nuse it as collateral on a platform like Compound to take out a loan of some\ntoken, giving you the full right to use that token for whatever purpose\nincluding participating in votes, and when they're done they simply send\nthe tokens back to the loan contract to get their collateral back - all\nwithout having to endure even a second of price exposure to the token\nthat they just used to swing a coin vote, even if the coin vote\nmechanism includes a time lockup (as eg. Bihu does). In every case,\nissues around bribing, and accidentally over-empowering well-connected\nand wealthy participants, prove surprisingly difficult to avoid.\n\n## Identity\n\nSome systems attempt to mitigate the plutocratic aspects of coin\nvoting by making use of an identity system. In the case of the\n/r/ethtrader donut system, for example, although governance\npolls are done via coin vote, the mechanism that determines how\nmany donuts (ie. coins) you get in the first place is based on\nReddit accounts: 1 upvote from 1 Reddit account = \\(N\\) donuts earned. The ideal goal of an\nidentity system is to make it relatively easy for individuals to get one\nidentity, but relatively difficult to get many identities. In the\n/r/ethtrader donut system, that's Reddit accounts, in the Gitcoin CLR\nmatching gadget, it's Github accounts that are used for the same\npurpose. But identity, at least the way it has been implemented so far,\nis a fragile thing....\n\nOh, are you too lazy to make a big rack of phones? Well maybe you're\nlooking for this:\n\nUsual warning about how sketchy sites may or may not scam you,\ndo your own research, etc. etc. applies.\n\nArguably, attacking these mechanisms by simply controlling thousands\nof fake identities like a puppetmaster is even easier than\nhaving to go through the trouble of bribing people. And if you think the\nresponse is to just increase security to go up to\ngovernment-level IDs? Well, if you want to get a few of those\nyou can start exploring here, but keep in mind\nthat there are specialized criminal organizations that are well ahead of\nyou, and even if all the underground ones are taken down, hostile\ngovernments are definitely going to create fake passports by the\nmillions if we're stupid enough to create systems that make that sort of\nactivity profitable. And this doesn't even begin to mention attacks in\nthe opposite direction, identity-issuing institutions attempting to\ndisempower marginalized communities by denying them identity\ndocuments...\n\n## Collusion\n\nGiven that so many mechanisms seem to fail in such similar ways once\nmultiple identities or even liquid markets get into the picture, one\nmight ask, is there some deep common strand that causes all of these\nissues? I would argue the answer is yes, and the \"common strand\" is\nthis: it is much harder, and more likely to be outright impossible, to\nmake mechanisms that maintain desirable properties in a model where\nparticipants can collude, than in a model where they can't. Most people\nlikely already have some intuition about this; specific instances of\nthis principle are behind well-established norms and often laws\npromoting competitive markets and restricting price-fixing cartels, vote\nbuying and selling, and bribery. But the issue is much deeper and more\ngeneral.\n\nIn the version of game theory that focuses on individual choice -\nthat is, the version that assumes that each participant makes decisions\nindependently and that does not allow for the possibility of groups of\nagents working as one for their mutual benefit, there are mathematical\nproofs that at least one stable Nash equilibrium must exist in any\ngame, and mechanism designers have a very wide latitude to \"engineer\"\ngames to achieve specific outcomes. But in the version of game theory\nthat allows for the possibility of coalitions working together, called\ncooperative game theory, there are large\nclasses of games that do not have any stable outcome that a\ncoalition cannot profitably deviate from.\n\nMajority games, formally described as games of \\(N\\) agents where any subset of more than\nhalf of them can capture a fixed reward and split it among themselves, a\nsetup eerily similar to many situations in corporate governance,\npolitics and many other situations in human life, are part\nof that set of inherently unstable games. That is to say, if there\nis a situation with some fixed pool of resources and some currently\nestablished mechanism for distributing those resources, and it's\nunavoidably possible for 51% of the participants can conspire to seize\ncontrol of the resources, no matter what the current configuration is\nthere is always some conspiracy that can emerge that would be profitable\nfor the participants. However, that conspiracy would then in turn be\nvulnerable to potential new conspiracies, possibly including a\ncombination of previous conspirators and victims... and so on and so\nforth.\n\nRound\n\nA\n\nB\n\nC\n\n1\n\n1/3\n\n1/3\n\n1/3\n\n2\n\n1/2\n\n1/2\n\n0\n\n3\n\n2/3\n\n0\n\n1/3\n\n4\n\n0\n\n1/3\n\n2/3\n\nThis fact, the instability of majority games under\ncooperative game theory, is arguably highly underrated as a simplified\ngeneral mathematical model of why there may well be no \"end of history\"\nin politics and no system that proves fully satisfactory; I personally\nbelieve it's much more useful than the more famous Arrow's\ntheorem, for example.\n\nThere are two ways to get around this issue. The first is to try to\nrestrict ourselves to the class of games that are\n\"identity-free\" and \"collusion-safe\", so where we do not need to worry\nabout either bribes or identities. The second is to try to attack the\nidentity and collusion resistance problems directly, and actually solve\nthem well enough that we can implement non-collusion-safe games with the\nricher properties that they offer.\nIdentity-free and\ncollusion-safe game design\n\nThe class of games that is identity-free and collusion-safe is\nsubstantial. Even proof of work is collusion-safe up to the bound of a\nsingle actor having ~23.21%\nof total hashpower, and this bound can be increased up to 50% with\nclever engineering.\nCompetitive markets are reasonably collusion-safe up until a relatively\nhigh bound, which is easily reached in some cases but in other cases is\nnot.\n\nIn the case of governance and content curation\n(both of which are really just special cases of the general problem of\nidentifying public goods and public bads) a major class of mechanism\nthat works well is futarchy\n- typically portrayed as \"governance by prediction market\", though I\nwould also argue that the use of security deposits is fundamentally in\nthe same class of technique. The way futarchy mechanisms, in their most\ngeneral form, work is that they make \"voting\" not just an expression of\nopinion, but also a prediction, with a reward for making\npredictions that are true and a penalty for making predictions that are\nfalse. For example, my\nproposal for \"prediction markets for content curation DAOs\" suggests\na semi-centralized design where anyone can upvote or downvote submitted\ncontent, with content that is upvoted more being more visible, where\nthere is also a \"moderation panel\" that makes final decisions. For each\npost, there is a small probability (proportional to the total volume of\nupvotes+downvotes on that post) that the moderation panel will be called\non to make a final decision on the post. If the moderation panel\napproves a post, everyone who upvoted it is rewarded and everyone who\ndownvoted it is penalized, and if the moderation panel disapproves a\npost the reverse happens; this mechanism encourages participants to make\nupvotes and downvotes that try to \"predict\" the moderation panel's\njudgements.\n\nAnother possible example of futarchy is a governance system for a\nproject with a token, where anyone who votes for a decision is obligated\nto purchase some quantity of tokens at the price at the time the vote\nbegins if the vote wins; this ensures that voting on a bad decision is\ncostly, and in the limit if a bad decision wins a vote everyone who\napproved the decision must essentially buy out everyone else in the\nproject. This ensures that an individual vote for a \"wrong\" decision can\nbe very costly for the voter, precluding the possibility of cheap bribe\nattacks.\n\nA graphical description of one form of futarchy, creating two\nmarkets representing the two \"possible future worlds\" and picking the\none with a more favorable price. Source\nthis\npost on ethresear.ch\n\nHowever, that range of things that mechanisms of this type can do is\nlimited. In the case of the content curation example above, we're not\nreally solving governance, we're just scaling the functionality\nof a governance gadget that is already assumed to be trusted. One could\ntry to replace the moderation panel with a prediction market on the\nprice of a token representing the right to purchase advertising space,\nbut in practice prices are too noisy an indicator to make this viable\nfor anything but a very small number of very large decisions. And often\nthe value that we're trying to maximize is explicitly something other\nthan maximum value of a coin.\n\nLet's take a more explicit look at why, in the more general case\nwhere we can't easily determine the value of a governance decision via\nits impact on the price of a token, good mechanisms for identifying\npublic goods and bads unfortunately cannot be identity-free or\ncollusion-safe. If one tries to preserve the property of a game being\nidentity-free, building a system where identities don't matter and only\ncoins do, there is an impossible tradeoff between either failing\nto incentivize legitimate public goods or over-subsidizing\nplutocracy.\n\nThe argument is as follows. Suppose that there is some author that is\nproducing a public good (eg. a series of blog posts) that provides value\nto each member of a community of 10000 people. Suppose there exists some\nmechanism where members of the community can take an action that causes\nthe author to receive a gain of $1. Unless the community members are\nextremely altruistic, for the mechanism to work the cost of\ntaking this action must be much lower than $1, as otherwise the portion\nof the benefit captured by the member of the community supporting the\nauthor would be much smaller than the cost of supporting the author, and\nso the system collapses into a tragedy of\nthe commons where no one supports the author. Hence, there must\nexist a way to cause the author to earn $1 at a cost much less than $1.\nBut now suppose that there is also a fake community, which consists of\n10000 fake sockpuppet accounts of the same wealthy attacker. This\ncommunity takes all of the same actions as the real community, except\ninstead of supporting the author, they support another fake\naccount which is also a sockpuppet of the attacker. If it was possible\nfor a member of the \"real community\" to give the author $1 at a personal\ncost of much less than $1, it's possible for the attacker to give\nthemselves $1 at a cost much less than $1 over and over again,\nand thereby drain the system's funding. Any mechanism that can help\ngenuinely under-coordinated parties coordinate will, without the right\nsafeguards, also help already coordinated parties (such as many accounts\ncontrolled by the same person) over-coordinate, extracting\nmoney from the system.\n\nA similar challenge arises when the goal is not funding, but rather\ndetermining what content should be most visible. What content do you\nthink would get more dollar value supporting it: a legitimately high\nquality blog article benefiting thousands of people but benefiting each\nindividual person relatively slightly, or this?\n\nOr perhaps this?\n\nThose who have been following recent politics \"in the real world\"\nmight also point out a different kind of content that benefits highly\ncentralized actors: social media manipulation by hostile governments.\nUltimately, both centralized systems and decentralized systems are\nfacing the same fundamental problem, which is that the\n\"marketplace of ideas\" (and of public goods more generally) is very far\nfrom an \"efficient market\" in the sense that economists normally use the\nterm, and this leads to both underproduction of public goods\neven in \"peacetime\" but also vulnerability to active attacks. It's just\na hard problem.\n\nThis is also why coin-based voting systems (like Bihu's) have one\nmajor genuine advantage over identity-based systems (like the Gitcoin\nCLR or the /r/ethtrader donut experiment): at least there is no benefit\nto buying accounts en masse, because everything you do is proportional\nto how many coins you have, regardless of how many accounts the coins\nare split between. However, mechanisms that do not rely on any model of\nidentity and only rely on coins fundamentally cannot solve the problem\nof concentrated interests outcompeting dispersed communities trying to\nsupport public goods; an identity-free mechanism that empowers\ndistributed communities cannot avoid over-empowering centralized\nplutocrats pretending to be distributed communities.\n\nBut it's not just identity issues that public goods games are\nvulnerable too; it's also bribes. To see why, consider again the example\nabove, but where instead of the \"fake community\" being 10001 sockpuppets\nof the attacker, the attacker only has one identity, the account\nreceiving funding, and the other 10000 accounts are real users - but\nusers that receive a bribe of $0.01 each to take the action that would\ncause the attacker to gain an additional $1. As mentioned above, these\nbribes can be highly obfuscated, even through third-party custodial\nservices that vote on a user's behalf in exchange for convenience, and\nin the case of \"coin vote\" designs an obfuscated bribe is even easier:\none can do it by renting coins on the market and using them to\nparticipate in votes. Hence, while some kinds of games, particularly\nprediction market or security deposit based games, can be made\ncollusion-safe and identity-free, generalized public goods funding seems\nto be a class of problem where collusion-safe and identity-free\napproaches unfortunately just cannot be made to work.\nCollusion resistance and\nidentity\n\nThe other alternative is attacking the identity problem head-on. As\nmentioned above, simply going up to higher-security centralized identity\nsystems, like passports and other government IDs, will not work at\nscale; in a sufficiently incentivized context, they are very insecure\nand vulnerable to the issuing governments themselves! Rather, the kind\nof \"identity\" we are talking about here is some kind of robust\nmultifactorial set of claims that an actor identified by some set of\nmessages actually is a unique individual. A very early proto-model of\nthis kind of networked identity is arguably social recovery in HTC's\nblockchain phone:\n\nThe basic idea is that your private key is secret-shared between up\nto five trusted contacts, in such a way that mathematically ensures that\nthree of them can recover the original key, but two or fewer can't. This\nqualifies as an \"identity system\" - it's your five friends determining\nwhether or not someone trying to recover your account actually is you.\nHowever, it's a special-purpose identity system trying to solve a\nproblem - personal account security - that is different from (and easier\nthan!) the problem of attempting to identify unique humans. That said,\nthe general model of individuals making claims about each other can\nquite possibly be bootstrapped into some kind of more robust identity\nmodel. These systems could be augmented if desired using the \"futarchy\"\nmechanic described above: if someone makes a claim that someone is a\nunique human, and someone else disagrees, and both sides are willing to\nput down a bond to litigate the issue, the system can call together a\njudgement panel to determine who is right.\n\nBut we also want another crucially important property: we want an\nidentity that you cannot credibly rent or sell. Obviously, we can't\nprevent people from making a deal \"you send me $50, I'll send you my\nkey\", but what we can try to do is prevent such deals from\nbeing credible - make it so that the seller can easily cheat\nthe buyer and give the buyer a key that doesn't actually work. One way\nto do this is to make a mechanism by which the owner of a key can send a\ntransaction that revokes the key and replaces it with another key of the\nowner's choice, all in a way that cannot be proven. Perhaps the simplest\nway to get around this is to either use a trusted party that runs the\ncomputation and only publishes results (along with zero knowledge proofs\nproving the results, so the trusted party is trusted only for privacy,\nnot integrity), or decentralize the same functionality through multi-party\ncomputation. Such approaches will not solve collusion completely; a\ngroup of friends could still come together and sit on the same couch and\ncoordinate votes, but they will at least reduce it to a manageable\nextent that will not lead to these systems outright failing.\n\nThere is a further problem: initial distribution of the key. What\nhappens if a user creates their identity inside a third-party custodial\nservice that then stores the private key and uses it to clandestinely\nmake votes on things? This would be an implicit bribe, the user's voting\npower in exchange for providing to the user a convenient service, and\nwhat's more, if the system is secure in that it successfully prevents\nbribes by making votes unprovable, clandestine voting by third-party\nhosts would also be undetectable. The only approach that gets\naround this problem seems to be.... in-person verification. For example,\none could have an ecosystem of \"issuers\" where each issuer issues smart\ncards with private keys, which the user can immediately download onto\ntheir smartphone and send a message to replace the key with a different\nkey that they do not reveal to anyone. These issuers could be meetups\nand conferences, or potentially individuals that have already been\ndeemed by some voting mechanic to be trustworthy.\n\nBuilding out the infrastructure for making collusion-resistant\nmechanisms possible, including robust decentralized identity systems, is\na difficult challenge, but if we want to unlock the potential of such\nmechanisms, it seems unavoidable that we have to do our best to try. It\nis true that the current computer-security dogma around, for example,\nintroducing online voting is simply \"don't\",\nbut if we want to expand the role of voting-like mechanisms, including\nmore advanced forms such as quadratic voting and quadratic finance, to\nmore roles, we have no choice but to confront the challenge head-on, try\nreally hard, and hopefully succeed at making something secure enough,\nfor at least some use cases.",
    "contentLength": 25211,
    "summary": "Vitalik warns that blockchain-based incentive mechanisms for content and voting are vulnerable to collusion attacks through self-voting and bribes.",
    "detailedSummary": {
      "theme": "Vitalik explores how collusion and identity manipulation undermine mechanism design in blockchain systems, particularly for content curation and governance, and proposes solutions for creating collusion-resistant systems.",
      "summary": "Vitalik examines the fundamental challenges facing mechanism design in blockchain applications, using examples like content curation platforms (Bihu, Reddit's ethtrader) to illustrate how systems designed to incentivize quality content creation are vulnerable to gaming through self-voting, plutocracy, and bribes. Vitalik argues that the core problem is collusion - when participants can coordinate, most mechanisms fail because wealthy actors can exploit systems through sockpuppet accounts, vote buying, or obfuscated bribes like coin rental. Drawing from cooperative game theory, Vitalik explains that majority games are inherently unstable when coalitions can form. Vitalik proposes two potential solutions: developing identity-free, collusion-safe mechanisms (like futarchy-based systems using prediction markets), or directly tackling identity and collusion resistance through robust multifactorial identity systems. For the latter approach, Vitalik suggests creating unprovable, non-transferable identity mechanisms that prevent credential selling, potentially requiring in-person verification and decentralized identity infrastructure to make advanced voting mechanisms like quadratic voting viable at scale.",
      "takeaways": [
        "Mechanism design systems in blockchain are fundamentally vulnerable to collusion attacks including self-voting, bribes, and sockpuppet accounts",
        "Identity-free systems cannot solve the problem of concentrated interests outcompeting distributed communities in public goods funding",
        "Futarchy-based mechanisms using prediction markets can provide collusion-safe alternatives but have limited applicability",
        "Robust identity systems must be designed to make credentials non-transferable and unprovable to prevent vote selling",
        "Building collusion-resistant mechanisms requires confronting difficult challenges around decentralized identity and in-person verification"
      ],
      "controversial": [
        "The suggestion that government-issued IDs are fundamentally insecure at scale and vulnerable to hostile governments creating fake documents",
        "The argument that in-person verification may be necessary for robust identity systems, which could exclude marginalized communities or be difficult to scale globally"
      ]
    }
  },
  {
    "id": "general-2019-04-01-cantor",
    "title": "[Mirror] Cantor was Wrong: debunking the infinite set hierarchy",
    "date": "2019-04-01",
    "category": "math",
    "url": "https://vitalik.eth.limo/general/2019/04/01/cantor.html",
    "path": "general/2019/04/01/cantor.html",
    "content": "- \n\n- \n\n  \n    \n\n  \n  \n  Dark Mode Toggle\n  \n    \n\n  \n\n##  [Mirror] Cantor was Wrong: debunking the infinite set hierarchy \n\n 2019 Apr 01 \nSee all posts\n\n \n \n\n [Mirror] Cantor was Wrong: debunking the infinite set hierarchy \n\nThis is a mirror of the post at\nhttps://medium.com/@VitalikButerin/cantor-was-wrong-debunking-the-infinite-set-hierarchy-e9ba5015102.\n\nBy Vitalik Buterin, PhD at University of Basel \n\nA common strand of mathematics argues that, rather than being one\nsingle kind of infinity, there are actually an infinite hierarchy of\ndifferent levels of infinity. Whereas the size of the set of integers is\njust plain infinite, and the set of rational numbers is just as big as\nthe integers (because you can map every rational number to an integer by\ninterleaving the digits of its numerator and denominator, eg. \\(0.456456456.... = \\frac{456}{999} = \\frac{152}{333}\n\\rightarrow 135323\\)), the size of the set of real\nnumbers is some kind of even bigger infinity, because there is no\nway to make a similar mapping from real numbers to the\nintegers.\n\nFirst of all, I should note that it's relatively easy to see that the\nclaim that there is no mapping is false. Here's a simple mapping. For a\ngiven real number, give me a (deterministic) python program that will\nprint out digits of it (eg. for \u03c0, that might be a program that\ncalculates better and better approximations using the infinite series\n\\(\\pi = 4 - \\frac{4}{3} + \\frac{4}{5} -\n\\frac{4}{7} + ...\\)). I can convert the program into a number\n(using\nn = int.from_bytes(open('program.py').read(), 'big')) and\nthen output the number. Done. There's the mapping from real numbers to\nintegers.\n\nNow let's take a look at the most common argument used to claim that\nno such mapping can exist, namely Cantor's diagonal argument. Here's an\nexposition\nfrom UC Denver; it's short so I'll just screenshot the whole\nthing:\n\nNow, here's the fundamental flaw in this argument: decimal\nexpansions of real numbers are not unique. To provide a\ncounterexample in the exact format that the \"proof\" requires, consider\nthe set (numbers written in binary), with diagonal digits bolded:\n\n- \n\nx[1] = 0.000000...\n\n- \n\nx[2] = 0.011111...\n\n- \n\nx[3] = 0.001111...\n\n- \n\nx[4] = 0.000111...\n\n- \n\n.....\n\nThe diagonal gives: 01111..... If we flip every digit, we get the\nnumber: \\(y =\\) 0.10000......\n\nAnd here lies the problem: just as in decimal, 0.9999.... equals 1, in\nbinary 0.01111..... equals 0.10000..... And so even though the new\ndecimal expansion is not in the original list, the\nnumber \\(y\\) is exactly the\nsame as the number \\(x[2]\\).\n\nNote that this directly implies that the halting problem is in fact\nsolvable. To see why, imagine a computer program that someone claims\nwill not halt. Let c[1] be the state of the program after one step, c[2]\nafter two steps, etc. Let x[1], x[2], x[3].... be a full enumeration of\nall real numbers (which exists, as we proved above), expressed in base\n\\(2^D\\) where \\(D\\) is the size of the program's memory, so\na program state can always be represented as a single \"digit\". Let y =\n0.c[1]c[2]c[3]........ This number is by assumption part of the list, so it\nis one of the x[i] values, and hence it can be computed in some finite\namount of time. This has implications in a number of industries,\nparticularly in proving that \"Turing-complete\" blockchains are in fact\nsecure.\n\nPatent on this research is pending.",
    "contentLength": 3392,
    "summary": "Vitalik argues Cantor's diagonal argument fails because decimal expansions aren't unique (0.999...=1), disproving infinite set hierarchy.",
    "detailedSummary": {
      "theme": "Vitalik attempts to debunk Cantor's diagonal argument and claims that all infinities are the same size, arguing that real numbers can be mapped to integers.",
      "summary": "Vitalik challenges the established mathematical concept of different sizes of infinity, specifically targeting Cantor's diagonal argument which proves that real numbers cannot be mapped one-to-one with integers. Vitalik proposes two counterarguments: first, that any real number can be mapped to an integer by converting a Python program that generates the number's digits into an integer representation. Second, Vitalik claims Cantor's diagonal argument is flawed because decimal expansions are not unique, using the example that 0.9999... equals 1 in decimal, and similarly 0.01111... equals 0.10000... in binary. Vitalik extends this argument to claim the halting problem is solvable by arguing that if all real numbers can be enumerated, then any infinite program state sequence can be computed in finite time. The post concludes with tongue-in-cheek references to patent applications and blockchain security implications.",
      "takeaways": [
        "Vitalik argues that real numbers can be mapped to integers using Python programs that generate their digits",
        "Vitalik claims Cantor's diagonal argument fails because decimal representations are not unique",
        "The post suggests that the halting problem is solvable based on the ability to enumerate all real numbers",
        "Vitalik challenges the mathematical hierarchy of different infinity sizes",
        "The argument has implications for blockchain security and Turing-complete systems"
      ],
      "controversial": [
        "Directly contradicts well-established mathematical proofs about the uncountability of real numbers",
        "Claims the halting problem is solvable, which contradicts fundamental computer science theory",
        "Dismisses Cantor's diagonal argument using flawed reasoning about non-unique decimal representations",
        "The mapping argument using Python programs is circular and doesn't address uncountability"
      ]
    }
  },
  {
    "id": "general-2018-12-05-cbc_casper",
    "title": "A CBC Casper Tutorial",
    "date": "2018-12-05",
    "category": "general",
    "url": "https://vitalik.eth.limo/general/2018/12/05/cbc_casper.html",
    "path": "general/2018/12/05/cbc_casper.html",
    "content": "- \n\n- \n\n  \n    \n\n  \n  \n  Dark Mode Toggle\n  \n    \n\n  \n\n##  A CBC Casper Tutorial \n\n 2018 Dec 05 \nSee all posts\n\n \n \n\n A CBC Casper Tutorial \n\nSpecial thanks to Vlad Zamfir, Aditya Asgaonkar, Ameen Soleimani\nand Jinglan Wang for review\n\nIn order to help more people understand \"the other Casper\" (Vlad\nZamfir's CBC Casper), and specifically the instantiation that works best\nfor blockchain protocols, I thought that I would write an explainer on\nit myself, from a less abstract and more \"close to concrete usage\" point\nof view. Vlad's descriptions of CBC Casper can be found here and here and here; you are\nwelcome and encouraged to look through these materials as well.\n\nCBC Casper is designed to be fundamentally very versatile and\nabstract, and come to consensus on pretty much any data structure; you\ncan use CBC to decide whether to choose 0 or 1, you can make a simple\nblock-by-block chain run on top of CBC, or a \\(2^{92}\\)-dimensional hypercube tangle DAG,\nand pretty much anything in between.\n\nBut for simplicity, we will first focus our attention on one concrete\ncase: a simple chain-based structure. We will suppose that there is a\nfixed validator set consisting of \\(N\\)\nvalidators (a fancy word for \"staking nodes\"; we also assume that each\nnode is staking the same amount of coins, cases where this is not true\ncan be simulated by assigning some nodes multiple validator IDs), time\nis broken up into ten-second slots, and validator \\(k\\) can create a block in slot \\(k\\), \\(N +\nk\\), \\(2N + k\\), etc. Each block\npoints to one specific parent block. Clearly, if we wanted to make\nsomething maximally simple, we could just take this structure, impose a\nlongest chain rule on top of it, and call it a day.\n\nThe green chain is the longest chain (length 6) so it is\nconsidered to be the \"canonical chain\".\n\nHowever, what we care about here is adding some notion of \"finality\"\n- the idea that some block can be so firmly established in the chain\nthat it cannot be overtaken by a competing block unless a very large\nportion (eg. \\(\\frac{1}{4}\\)) of\nvalidators commit a uniquely attributable fault - act in some\nway which is clearly and cryptographically verifiably malicious. If a\nvery large portion of validators do act maliciously to revert\nthe block, proof of the misbehavior can be submitted to the chain to\ntake away those validators' entire deposits, making the reversion of\nfinality extremely expensive (think hundreds of millions of\ndollars).\n\n## LMD GHOST\n\nWe will take this one step at a time. First, we replace the fork\nchoice rule (the rule that chooses which chain among many possible\nchoices is \"the canonical chain\", ie. the chain that users should care\nabout), moving away from the simple longest-chain-rule and instead using\n\"latest message driven GHOST\". To show how LMD GHOST works, we will\nmodify the above example. To make it more concrete, suppose the\nvalidator set has size 5, which we label \\(A\\), \\(B\\), \\(C\\), \\(D\\), \\(E\\), so validator \\(A\\) makes the blocks at slots 0 and 5,\nvalidator \\(B\\) at slots 1 and 6, etc.\nA client evaluating the LMD GHOST fork choice rule cares only about the\nmost recent (ie. highest-slot) message (ie. block) signed by each\nvalidator:\n\nLatest messages in blue, slots from left to right (eg. \\(A\\)'s block on the left is at slot 0,\netc.)\n\nNow, we will use only these messages as source data for the \"greedy\nheaviest observed subtree\" (GHOST) fork choice rule: start at the\ngenesis block, then each time there is a fork choose the side where more\nof the latest messages support that block's subtree (ie. more of the\nlatest messages support either that block or one of its descendants),\nand keep doing this until you reach a block with no children. We can\ncompute for each block the subset of latest messages that support either\nthe block or one of its descendants:\n\nNow, to compute the head, we start at the beginning, and then at each\nfork pick the higher number: first, pick the bottom chain as it has 4\nlatest messages supporting it versus 1 for the single-block top chain,\nthen at the next fork support the middle chain. The result is the same\nlongest chain as before. Indeed, in a well-running network (ie. the\norphan rate is low), almost all of the time LMD GHOST and the longest\nchain rule will give the exact same answer. But in more extreme\ncircumstances, this is not always true. For example, consider the\nfollowing chain, with a more substantial three-block fork:\n\nScoring blocks by chain length. If we follow the longest chain\nrule, the top chain is longer, so the top chain wins.\n\nScoring blocks by number of supporting latest messages and\nusing the GHOST rule (latest message from each validator shown in blue).\nThe bottom chain has more recent support, so if we follow the LMD GHOST\nrule the bottom chain wins, though it's not yet clear which of the three\nblocks takes precedence.\n\nThe LMD GHOST approach is advantageous in part because it is better\nat extracting information in conditions of high latency. If two\nvalidators create two blocks with the same parent, they should really be\nboth counted as cooperating votes for the parent block, even though they\nare at the same time competing votes for themselves. The longest chain\nrule fails to capture this nuance; GHOST-based rules do.\n\n## Detecting finality\n\nBut the LMD GHOST approach has another nice property: it's\nsticky. For example, suppose that for two rounds, \\(\\frac{4}{5}\\) of validators voted for the\nsame chain (we'll assume that the one of the five validators that did\nnot, \\(B\\), is attacking):\n\nWhat would need to actually happen for the chain on top to become the\ncanonical chain? Four of five validators built on top of \\(E\\)'s first block, and all four recognized\nthat \\(E\\) had a high score in the LMD\nfork choice. Just by looking at the structure of the chain, we can know\nfor a fact at least some of the messages that the validators must have\nseen at different times. Here is what we know about the four validators'\nviews:\n\nA's\nview\n\nC's\nview\n\nD's\nview\n\nE's\nview\n\nBlocks produced by each validator in green, the latest\nmessages we know that they saw from each of the other validators in\nblue.\n\nNote that all four of the validators could have seen one or\nboth of \\(B\\)'s blocks, and \\(D\\) and \\(E\\) could have seen \\(C\\)'s second block, making that the latest\nmessage in their views instead of \\(C\\)'s first block; however, the structure\nof the chain itself gives us no evidence that they actually did.\nFortunately, as we will see below, this ambiguity does not matter for\nus.\n\n\\(A\\)'s view contains four\nlatest-messages supporting the bottom chain, and none supporting \\(B\\)'s block. Hence, in (our simulation of)\n\\(A\\)'s eyes the score in favor of the\nbottom chain is at least 4-1. The views of \\(C\\), \\(D\\)\nand \\(E\\) paint a similar picture, with\nfour latest-messages supporting the bottom chain. Hence, all four of the\nvalidators are in a position where they cannot change their minds unless\ntwo other validators change their minds first to bring the score to 2-3\nin favor of \\(B\\)'s block.\n\nNote that our simulation of the validators' views is \"out of date\" in\nthat, for example, it does not capture that \\(D\\) and \\(E\\) could have seen the more recent block\nby \\(C\\). However, this does not alter\nthe calculation for the top vs bottom chain, because we can very\ngenerally say that any validator's new message will have the same\nopinion as their previous messages, unless two other validators have\nalready switched sides first.\n\nA minimal viable attack. \\(A\\)\nand \\(C\\) illegally switch over to\nsupport \\(B\\)'s block (and can get\npenalized for this), giving it a 3-2 advantage, and at this point it\nbecomes legal for \\(D\\) and \\(E\\) to also switch over.\n\nSince fork choice rules such as LMD GHOST are sticky in this way, and\nclients can detect when the fork choice rule is \"stuck on\" a particular\nblock, we can use this as a way of achieving asynchronously safe\nconsensus.\n\n## Safety Oracles\n\nActually detecting all possible situations where the chain becomes\nstuck on some block (in CBC lingo, the block is \"decided\" or \"safe\") is\nvery difficult, but we can come up with a set of heuristics (\"safety\noracles\") which will help us detect some of the cases where\nthis happens. The simplest of these is the clique\noracle. If there exists some subset \\(V\\) of the validators making up portion\n\\(p\\) of the total validator set (with\n\\(p > \\frac{1}{2}\\)) that all make\nblocks supporting some block \\(B\\) and\nthen make another round of blocks still supporting \\(B\\) that references their first round of\nblocks, then we can reason as follows:\n\nBecause of the two rounds of messaging, we know that this subset\n\\(V\\) all (i) support \\(B\\) (ii) know that \\(B\\) is well-supported, and so none of them\ncan legally switch over unless enough others switch over first. For some\ncompeting \\(B'\\) to beat out \\(B\\), the support such a \\(B'\\) can legally have is\ninitially at most \\(1-p\\) (everyone not\npart of the clique), and to win the LMD GHOST fork choice its support\nneeds to get to \\(\\frac{1}{2}\\), so at\nleast \\(\\frac{1}{2} - (1-p) = p -\n\\frac{1}{2}\\) need to illegally switch over to get it to the\npoint where the LMD GHOST rule supports \\(B'\\).\n\nAs a specific case, note that the \\(p=\\frac{3}{4}\\) clique oracle offers a\n\\(\\frac{1}{4}\\) level of safety, and a\nset of blocks satisfying the clique can (and in normal operation, will)\nbe generated as long as \\(\\frac{3}{4}\\)\nof nodes are online. Hence, in a BFT sense, the level of fault tolerance\nthat can be reached using two-round clique oracles is \\(\\frac{1}{3}\\), in terms of both liveness\nand safety.\n\nThis approach to consensus has many nice benefits. First of all, the\nshort-term chain selection algorithm, and the \"finality algorithm\", are\nnot two awkwardly glued together distinct components, as they admittedly\nare in Casper FFG; rather, they are both part of the same coherent\nwhole. Second, because safety detection is client-side, there is no need\nto choose any thresholds in-protocol; clients can decide for themselves\nwhat level of safety is sufficient to consider a block as finalized.\n\n## Going Further\n\nCBC can be extended further in many ways. First, one can come up with\nother safety oracles; higher-round clique oracles can reach \\(\\frac{1}{3}\\) fault tolerance. Second, we\ncan add validator rotation mechanisms. The simplest is to allow the\nvalidator set to change by a small percentage every time the \\(q=\\frac{3}{4}\\) clique oracle is satisfied,\nbut there are other things that we can do as well. Third, we can go\nbeyond chain-like structures, and instead look at structures that\nincrease the density of messages per unit time, like the Serenity beacon\nchain's attestation structure:\n\nIn this case, it becomes worthwhile to separate attestations\nfrom blocks; a block is an object that actually grows the\nunderlying DAG, whereas an attestation contributes to the fork choice\nrule. In the Serenity\nbeacon chain spec, each block may have hundreds of attestations\ncorresponding to it. However, regardless of which way you do it, the\ncore logic of CBC Casper remains the same.\n\nTo make CBC Casper's safety \"cryptoeconomically enforceable\", we need\nto add validity and slashing conditions. First, we'll start with the\nvalidity rule. A block contains both a parent block and a set of\nattestations that it knows about that are not yet part of the chain\n(similar to \"uncles\" in the current Ethereum PoW chain). For the block\nto be valid, the block's parent must be the result of executing the LMD\nGHOST fork choice rule given the information included in the chain\nincluding in the block itself.\n\nDotted lines are uncle links, eg. when E creates a block, E\nnotices that C is not yet part of the chain, and so includes a reference\nto C.\n\nWe now can make CBC Casper safe with only one slashing condition: you\ncannot make two attestations \\(M_1\\)\nand \\(M_2\\), unless either \\(M_1\\) is in the chain that \\(M_2\\) is attesting to or \\(M_2\\) is in the chain that \\(M_2\\) is attesting to.\n\nOK\n\nNot\nOK\n\nThe validity and slashing conditions are relatively easy to describe,\nthough actually implementing them requires checking hash chains and\nexecuting fork choice rules in-consensus, so it is not nearly as simple\nas taking two messages and checking a couple of inequalities between the\nnumbers that these messages commit to, as you can do in Casper FFG for\nthe NO_SURROUND and NO_DBL_VOTE slashing\nconditions.\n\nLiveness in CBC Casper piggybacks off of the liveness of whatever the\nunderlying chain algorithm is (eg. if it's one-block-per-slot, then it\ndepends on a synchrony assumption that all nodes will see everything\nproduced in slot \\(N\\) before the start\nof slot \\(N+1\\)). It's not possible to\nget \"stuck\" in such a way that one cannot make progress; it's possible\nto get to the point of finalizing new blocks from any situation, even\none where there are attackers and/or network latency is higher than that\nrequired by the underlying chain algorithm.\n\nSuppose that at some time \\(T\\), the\nnetwork \"calms down\" and synchrony assumptions are once again satisfied.\nThen, everyone will converge on the same view of the chain, with the\nsame head \\(H\\). From there, validators\nwill begin to sign messages supporting \\(H\\) or descendants of \\(H\\). From there, the chain can proceed\nsmoothly, and will eventually satisfy a clique oracle, at which point\n\\(H\\) becomes finalized.\n\nChaotic network due to high latency.\n\nNetwork latency subsides, a majority of validators see all of\nthe same blocks or at least enough of them to get to the same head when\nexecuting the fork choice, and start building on the head, further\nreinforcing its advantage in the fork choice rule.\n\nChain proceeds \"peacefully\" at low latency. Soon, a clique\noracle will be satisfied.\n\nThat's all there is to it! Implementation-wise, CBC may arguably be\nconsiderably more complex than FFG, but in terms of ability to reason\nabout the protocol, and the properties that it provides, it's\nsurprisingly simple.",
    "contentLength": 13972,
    "summary": "This CBC Casper tutorial explains Vlad Zamfir's consensus protocol using LMD GHOST fork choice with safety oracles to achieve finality.",
    "detailedSummary": {
      "theme": "Vitalik provides a practical tutorial on CBC Casper consensus mechanism, focusing on how it achieves blockchain finality through fork choice rules and safety oracles.",
      "summary": "Vitalik explains CBC Casper as a versatile consensus protocol that can work with various data structures, focusing on a chain-based implementation for clarity. He introduces LMD GHOST (Latest Message Driven Greedy Heaviest Observed Subtree) as a fork choice rule that improves upon simple longest-chain rules by being more responsive to recent validator messages and better at handling high-latency conditions. The protocol achieves finality through 'safety oracles' like the clique oracle, which can detect when blocks become irreversible without requiring a large portion of validators to commit provably malicious acts. Vitalik emphasizes that CBC Casper integrates the short-term chain selection and finality mechanisms into a coherent whole, unlike other approaches that awkwardly combine separate components. The protocol maintains liveness by piggybacking off the underlying chain algorithm and can recover from chaotic network conditions once synchrony is restored.",
      "takeaways": [
        "LMD GHOST fork choice rule is superior to longest-chain rules because it better captures validator cooperation and extracts information during high-latency conditions",
        "CBC Casper achieves finality through client-side safety detection using oracles like the clique oracle, which can provide 1/3 Byzantine fault tolerance",
        "The protocol unifies chain selection and finality into a single coherent mechanism rather than combining separate awkward components",
        "CBC Casper can extend beyond simple chains to more complex structures like DAGs while maintaining the same core logic",
        "The system maintains liveness by recovering from network chaos once synchrony assumptions are restored, with validators converging on the same chain head"
      ],
      "controversial": [
        "The claim that CBC Casper is 'surprisingly simple' to reason about despite acknowledged implementation complexity may be debatable",
        "The assertion that LMD GHOST's advantages over longest-chain rules are significant enough to justify the added complexity"
      ]
    }
  },
  {
    "id": "general-2018-11-25-central_planning",
    "title": "[Mirror] Central Planning as Overfitting",
    "date": "2018-11-25",
    "category": "society",
    "url": "https://vitalik.eth.limo/general/2018/11/25/central_planning.html",
    "path": "general/2018/11/25/central_planning.html",
    "content": "- \n\n- \n\n  \n    \n\n  \n  \n  Dark Mode Toggle\n  \n    \n\n  \n\n##  [Mirror] Central Planning as Overfitting \n\n 2018 Nov 25 \nSee all posts\n\n \n \n\n [Mirror] Central Planning as Overfitting \n\nThis is a mirror of the post at\nhttps://radicalxchange.org/blog/posts/2018-11-26-4m9b8b/ written by\nmyself and Glen Weyl\n\nThere is an intuition shared by many that \"central planning\" \u2014\ncommand-and-control techniques for allocating resources in economies,\nand fine-grained knob-turning interventionism more generally \u2014 is\nundesirable. There's quite a lot to this, but it is often misapplied in\na way that also leads it to go under-appreciated. In this post we try to\nclarify the appropriate scope of the intuition.\n\nSome recent examples of the intuition being misapplied are:\n\n- People arguing that relatively simple entitlement programs like\nSocial Security are burdensome government intervention, while elaborate\nand often discretionary tax breaks conditional on specific behaviors are\na good step towards less government.\n\n- People arguing that block size limits in cryptocurrencies, which\nimpose a hard cap on the number of transactions that each block can\ncontain, are a form of central planning, but who do not argue against\nother centrally planned parameters, eg. the targeted ten minute (or\nwhatever) average time interval between blocks.\n\n- People arguing that a lack of block size limits constitutes\ncentral planning (!!)\n\n- People arguing that fixed\ntransaction fees constitute central planning, but variable\ntransaction fees that arise out of an equilibrium itself created by a\nfixed block size limit do not.\n\n- We were recently at a discussion in policy circles in Washington,\nwhere one of us was arguing for a scheme based on Harberger\ntaxes for spectrum licenses, debating against someone defending more\nconventional perpetual monopoly licenses on spectrum aggregated at large\nlevels that would tend to create a few national cellular carriers. The\nlatter side argued that the Harberger tax scheme constituted\nunacceptable bureaucratic interventionism, but seemed to believe that\npermanent government-issued monopoly privileges are a property right as\nnatural as apples falling from a tree.\n\nWhile we do not entirely dismiss this last example, for reasons we\nwill return to later, it does seem overplayed. Similarly and conversely,\nwe see many examples where, in the name of defending or promoting\n\"markets\" (or at least \"economic rationality\") many professional\neconomists advocate schemes that seem much more to us like central\nplanning than the systems they seek to replace:\n\n- The most systematic example of this is the literature on \"optimal mechanism\ndesign,\" which began with the already extremely complicated and\nfragile Vickrey-Clarke-Groves\nmechanism and has only gotten more byzantine from there. While\nVickrey's motivation for these ideas was to discover relatively simple\nrules that would correct the flaws of standard capitalism, he\nacknowledged in his paper that the design was highly complex in its\ndirect application and urged future researchers to find simplifications.\nInstead of following this counsel, many\nscholars have proposed, for example, schemes that rely on a central\nauthority being able to specify an infinite dimensional set of prior\nbeliefs. These schemes, we submit, constitute \"central planning\" in\nprecisely the sense we should be concerned with.\n\n- Furthermore, these designs are not just matters of theory, but in\npractice many applied mechanism designers have created systems with\nsimilar properties. The recent United States Spectrum Incentive auctions\n(designed by a few prominent economists and computer scientists)\ncentralized the enforcement of potential conflicts between transmission\nrights using an extremely elaborate and\nopaque computational engine, rather than allowing conflicts to be\nresolved through (for example) common law liability lawsuits as other\ninterference between property claims and land uses are. A recent design\nfor the allocation of courses to students at the University of\nPennsylvania designed by a similar team requires students to express\ntheir preferences over courses on a novel numerical scale, allowing them\nonly narrow language for expressing complementarities and\nsubstitutability between courses and then uses a state-of-the-art\noptimization engine to allocate courses. Auction systems designed by\neconomists and computer scientists at large technology companies, like\nFacebook and Google, are even richer and less transparent, and have\ncreated substantial backlash, inspiring a whole industry of firms\nthat help advertisers optimize their bidding in elaborate ways against\nthese systems.\n\n- This problem does not merely arise in mechanism design, however. In\nthe fields of industrial organization (the basis of much antitrust\neconomics) and the field of macroeconomics (the basis of much monetary\npolicy), extremely elaborate models with hundreds of parameters are\nempirically estimated and used to simulate the effects of mergers or\nchanges in monetary policy. These models are usually difficult to\nexplain even to experts in the field, much less democratically-elected\npoliticians, judges or, god forbid, voters. And yet the confidence we\nhave in these models, the empirical evidence validating their accuracy,\netc. is almost nothing. Nonetheless, economists consistently promote\nsuch methods as \"the state of the art\" and they are generally viewed\npositively by defenders of the \"market economy\".\n\nTo understand why we think the concept of \"intervention\" is being\nmisapplied here, we need to understand two different ways of measuring\nthe extent to which some scheme is \"interventionist\". The first approach\nis to try to measure the absolute magnitude of distortion relative to\nsome imagined state of nature (anarcho-primitivism, or a blockchain with\nno block size limits, or...). However, this approach clearly fails to\ncapture the intuitions of why central planning is undesirable. For\nexample, property rights in the physical world are a large intervention\ninto almost every person's behavior, considerably limiting the actions\nthat we can take every day. Many of these restrictions are actually of\nquite recent historical provenance (beginning with agriculture, and\nmostly in the West and not the East or Middle East). However, opponents\nof central planning often tend to be the strongest proponents of\nproperty rights!\n\nWe can shed some light on this puzzle by looking at another way of\nmeasuring the \"central-planny-ness\" of some social structure: in short,\nmeasure the number of knobs. Property rights actually score quite well\nhere: every piece of property is allocated to some person or legal\nentity, they can use it as they wish, and no one else can touch it\nwithout their permission. There are choices to make around the edges\n(eg. adverse\npossession rights), but generally there isn't too much room for\nchanging the scheme around (though note that privatization schemes, ie.\ntransitions from something other than property rights to property rights\nlike the auctions we discussed above, have very many knobs, and so there\nwe can see more risks). Command-and-control regulations with ten\nthousand clauses (or market designs that specify elaborate probabilistic\nobjects, or optimization protocols, etc.), or attempts to limit use of\nspecific features of the blockchain to drive out specific categories\nof users, are much less desirable, as such strategies leave many more\nchoices to central planners. A block size limit and a fixed transaction\nfee (or carbon taxes and a cap-and-trade scheme) have the exact same\nlevel of \"central-planny-ness\" to them: one variable (either quantity or\nprice) is fixed in the protocol, and the other variable is left to the\nmarket.\n\nHere are some key underlying reasons why we believe that simple\nsocial systems with fewer knobs are so desirable:\n\n- They have fewer moving parts that can fail or\notherwise have unexpected effects.\n\n- They are less likely to overfit. If a social system\nis too complex, there are many parameters to set and relatively few\nexperiences to draw from when setting the parameters, making it more\nlikely that the parameters will be set to overfit to one context, and\ngeneralize poorly to other places or to future times. We\nknow little, and we should not design systems that demand\nus to know a lot.\n\n- They are more resistant to corruption. If a social\nsystem is simpler, it is more difficult (not impossible, but still more\ndifficult) to set parameters in ways that encode attempts to privilege\nor anti-privilege specific individuals, factions or communities. This is\nnot only good because it leads to fairness, it is also good because it\nleads to less zero-sum conflict.\n\n- They can more easily achieve legitimacy. Because\nsimpler systems are easier to understand, and easier for people to\nunderstand that a given implementation is not unfairly privileging\nspecial interests, it is easier to create common knowledge that the\nsystem is fair, creating a stronger sense of trust. Legitimacy is\nperhaps the central virtue of social institutions, as it sustains\ncooperation and coordination, enables the possibility of democracy (how\ncan you democratically participate and endorse a system you do not\nunderstand?) and allows for a bottoms-up, rather than top-down, creation\nof a such a system, ensuring it can be implemented without much coercion\nor violence.\n\nThese effects are not always achieved (for example, even if a system\nhas very few knobs, it's often the case that there exists a knob that\ncan be turned to privilege well-connected and wealthy people as a class\nover everyone else), but the simpler a system is, the more likely the\neffects are to be achieved.\n\nWhile avoiding over-complexity and overfit in personal\ndecision-making is also important, avoiding these issues in large-scale\nsocial systems is even more important, because of the inevitable\npossibility of powerful forces attempting to manipulate knobs for the\nbenefit of special interests, and the need to achieve common knowledge\nthat the system has not been greatly corrupted, to the point where the\nfairness of the system is obvious even to unsophisticated observers.\n\nThis is not to condemn all forms or uses of complexity in social\nsystems. Most science and the inner workings of many technical systems\nare likely to be opaque to the public but this does not mean science or\ntechnology is useless in social life; far from it. However, these\nsystems, to gain legitimacy, usually show that they can reliably achieve\nsome goal, which is transparent and verifiable. Planes land safely and\non time, computational systems seem to deliver calculations that are\ncorrect, etc. It is by this process of verification, rather than by the\ntransparency of the system per se, that such systems gain their\nlegitimacy. However, for many social systems, truly large-scale,\nrepeatable tests are difficult if not impossible. As such, simplicity is\nusually critical to legitimacy.\nDifferent Notions of\nSimplicity\n\nHowever, there is one class of social systems that seem to be\ndesirable, and that intellectual advocates of minimizing central\nplanning tend to agree are desirable, that don't quite fit the simple\n\"few knobs\" characterization that we made above. For example, consider\ncommon law. Common\nlaw is built up over thousands of precedents, and contains a large\nnumber of concepts (eg. see this list under \"property law\", itself only\na part of common law; have you heard of \"quicquid plantatur solo, solo\ncedit\" before?). However, proponents of private property are very\nfrequently proponents of common law. So what gives?\n\nHere, we need to make a distinction between redundant complexity, or\nmany knobs that really all serve a relatively small number of similar\ngoals, and optimizing complexity, in the extreme one knob per problem\nthat the system has encountered. In computational complexity theory, we\ntypically talk about Kolmogorov\ncomplexity, but there are other notions of complexity that are also\nuseful here, particularly VC dimension -\nroughly, the size of the largest set of situations for which we can turn\nthe knobs in a particular way to achieve any particular set of outcomes.\nMany successful machine learning techniques, such as Support\nVector Machines and Boosting,\nare quite complex, both in the formal Kolmogorov sense and in terms of\nthe outcomes they produce, but can be proven to have low VC\ndimension.\n\nVC dimension does a nice job capturing some of the arguments for\nsimplicity mentioned above more explicitly, for example:\n\n- A system with low VC dimension may have some moving parts that fail,\nbut if it does, its different constituent parts can correct for each\nother. By construction, it has built in resilience through\nredundancy\n\n- Low VC dimension is literally a measure of resistance to\noverfit.\n\n- Low VC dimension leads to resistance to corruption, because if VC\ndimension is low, a corrupt or self-interested party in control of some\nknobs will not as easily be able to achieve some particular outcome that\nthey desire. In particular, this agent will be \"checked and balanced\" by\nother parts of the system that redundantly achieve the originally\ndesired ends.\n\n- They can achieve legitimacy because people can randomly check a few\nparts and verify in detail that those parts work in ways that are\nreasonable, and assume that the rest of the system works in a similar\nway. An example of this was the ratification of the United States\nConstitution which, while quite elaborate, was primarily elaborate in\nthe redundancy with which it applied the principle of checks and\nbalances of power. Thus most citizens only read one or a few of The\nFederalist Papers that explained and defended the Constitution, and\nyet got a reasonable sense for what was going on.\n\nThis is not as clean and convenient as a system with low Kolmogorov\ncomplexity, but still much better than a system with high complexity\nwhere the complexity is \"optimizing\" (for an example of this in the\nblockchain context, see Vitalik's opposition and\nalternative to on-chain governance). The primary disadvantage we see\nin Kolmogorov complex but VC simple designs is for new social\ninstitutions, where it may be hard to persuade the public that these are\nVC simple. VC simplicity is usually easier as a basis for legitimacy\nwhen an institutions has clearly been built up without any clear design\nover a long period of time or by a large committee of people with\nconflicting interests (as with the United States Constitution). Thus\nwhen offering innovations we tend to focus more on Kolmogorov simplicity\nand hope many redundant each Kolmogorov-simple elements will add up to a\nVC-simple system. However, we may just not have the imagination to think\nof how VC simplicity might be effectively explained.\n\nThere are forms of the \"avoid central planning\" intuition that are\nmisunderstandings and ultimately counterproductive. For example, try to\nautomatically seize upon designs that seem at first glance to \"look like\na market\", because not all markets are created equal. For example, one\nof us has argued for using fixed\nprices in certain settings to reduce uncertainty, and the other has\n(for similar information sharing reasons) argued for auctions that are a\nsynthesis of standard descending price Dutch and ascending price English\nauctions (Channel auctions). That said, it is also equally a large error\nto throw the intuition away entirely. Rather, it is a valuable and\nimportant insight that can easily is central to the methodology we have\nbeen recently trying to develop. Simplicity to Whom? Or Why Humanities\nMatter\n\nHowever, the academic critics of this type of work are not simply\nconfused. There is a reasonable basis for unease with discussions of\n\"simplicity\" because they inevitably contain a degree of subjectivity.\nWhat is \"simple\" to describe or appears to have few knobs in one\nlanguage for describing it is devilishly complex in another, and vice\nversa. A few examples should help illuminate the point:\n\n- We have repeatedly referred to \"knobs\", which are roughly real\nvalued parameters. But real-valued parameters can encode an arbitrary\namount of complexity. For example, I could claim my system has only one\nknob, it is just that slight changes in the 1000th decimal place of the\nsetting of that knob end up determining incredibly important properties\nof the system. This may seem cheap, but more broadly it is the case that\nnon-linear mappings between systems can make one system seem \"simple\"\nand another \"complex\" and in general there is just no way to say which\nis right.\n\n- Many think of the electoral\nsystem of the United States as \"simple\", and yet, if one reflects on\nit or tries to explain it to a foreigner, it is almost impossible to\ndescribe. It is familiar, not simple, and we just have given a label to\nit (\"the American system\") that lets us refer to it in a few words.\nSystems like Quadratic Voting, or ranked choice\nvoting, are often described as complex, but this seems to have more\nto do with lack of familiarity than complexity.\n\n- Many scientific concepts, such as the \"light cone\", are the simplest\nthing possible once one understands special relativity and yet are\nutterly foreign and bizarre without having wrapped one's hands around\nthis theory.\n\nEven Kolmogorov\ncomplexity (length of the shortest computer program that encodes\nsome given system) is relative to some programming language. Now, to\nsome extent, VC dimension offers a solution: it says that a class of\nsystems is simple if it is not too flexible. But consider what happens\nwhen you try to apply this; to do so, let's return to our example\nupfront about Harberger taxes v. perpetual licenses for spectrum.\n\nHarberger taxes strike us as quite simple: there is a single tax rate\n(and the theory even says this is tied down by the rate at which assets\nturnover, at least if we want to maximally favor allocative efficiency)\nand the system can be described in a sentence or two. It seems pretty\nclear that such a system could not be contorted to achieve arbitrary\nends. However, an opponent could claim that we chose the Harberger tax\nfrom an array of millions of possible mechanisms of a similar class to\nachieve a specific objective, and it just sounds simple (as with our\nexamples of \"deceptive\" simplicity above).\n\nTo counter this argument, we would respond that the Harberger tax, or\nvery similar ideas, have been repeatedly discovered or used (to some\nsuccess) throughout human history, beginning\nwith the Greeks, and that we do not propose this system simply for\nspectrum licenses but in a wide range of contexts. The chances that in\nall these contexts we are cherry-picking the system to \"fit\" that\nsetting seems low. We would submit to the critic to judge whether it is\nreally plausible that all these historical circumstances and these wide\nrange of applications just \"happen\" to coincide.\n\nFocusing on familiarity (ie. conservatism), rather than simplicity in\nsome abstract mathematical sense, also carries many of the benefits of\nsimplicity as we described above; after all, familiarity is simplicity,\nif the language we are using to describe ideas includes references to\nour shared historical experience. Familiar mechanisms also have the\nbenefit that we have more knowledge of how similar ideas historically\nworked in practice. So why not just be conservative, and favor perpetual\nproperty licenses strongly over Harberger taxes?\n\nThere are three flaws in that logic, it seems to us. First, to the\nextent it is applied, it should be applied uniformly to all innovation,\nnot merely to new social institutions. Technologies like the internet\nhave contributed greatly to human progress, but have also led to\nsignificant social upheavals; this is not a reason to stop trying to\nadvance our technologies and systems for communication, and it is not a\nreason to stop trying to advance our social technologies for allocating\nscarce resources.\n\nSecond, the benefits of innovation are real, and social institutions\nstand to benefit from growing human intellectual progress as much as\neverything else. The theoretical case for Harberger taxes providing\nefficiency benefits is strong, and there is great social value in doing\nsmall and medium-scale experiments to try ideas like them out. Investing\nin experiments today increases what we know, and so increases the scope\nof what can be done \"conservatively\" tomorrow.\n\nThird, and most importantly, the cultural context in which you as a\ndecision maker have grown up today is far from the only culture that has\nexisted on earth. Even at present, Singapore, China, Taiwan and\nScandinavia have had significant success with quite different property\nregimes than the United States. Video game developers and internet\nprotocol designers have had to solve incentive problems of a similar\ncharacter to what we see today in the blockchain space and have come up\nwith many kinds of solutions, and throughout history, we have seen a\nwide variety of social systems used for different purposes, with a wide\nrange of resulting outcomes. By learning about the different ways in\nwhich societies have lived, understood what is natural and imagined\ntheir politics, we can gain the benefits of learning from historical\nexperience and yet at the same time open ourselves to a much broader\nspace of possible ideas to work with.\n\nThis is why we believe that balance and collaboration between\ndifferent modes of learning and understanding, both the mathematical one\nof economists and computer scientists, and the historical experiences\nstudied by historians, anthropologists, political scientists, etc is\ncritical to avoid the mix of and often veering between extreme\nconservatism and dangerous utopianism that has become characteristic of\nmuch intellectual discourse in e.g.\u00a0the economics community, the\n\"rationalist\" community, and in many cases blockchain protocol\ndesign.",
    "contentLength": 21907,
    "summary": "The blog argues that \"central planning\" should be defined by system complexity and number of adjustable parameters, not by government involvement.",
    "detailedSummary": {
      "theme": "Vitalik argues that central planning should be understood as 'overfitting' - creating overly complex systems with too many parameters that are prone to manipulation, failure, and lack of legitimacy.",
      "summary": "Vitalik and Glen Weyl challenge common misconceptions about what constitutes 'central planning' by arguing that complexity and the number of 'knobs' (adjustable parameters) matter more than absolute government intervention. They critique both libertarians who mislabel simple systems as central planning and economists who create Byzantine mechanisms in the name of market efficiency. Vitalik emphasizes that simple systems with fewer parameters are more resistant to corruption, overfitting, and manipulation while being easier to understand and achieve legitimacy. However, he distinguishes between harmful 'optimizing complexity' (many knobs for different problems) and beneficial 'redundant complexity' (many mechanisms serving similar goals with low VC dimension, like common law). The authors argue that true simplicity is context-dependent and requires balancing mathematical rigor with historical understanding from humanities disciplines.",
      "takeaways": [
        "The number of adjustable parameters ('knobs') in a system is more important than the absolute level of government intervention when evaluating central planning",
        "Simple systems with fewer knobs are more resistant to corruption, overfitting, and manipulation while achieving greater legitimacy and democratic participation",
        "Many 'market-based' solutions proposed by economists are actually forms of central planning due to their complexity and opacity",
        "There's a distinction between harmful 'optimizing complexity' and beneficial 'redundant complexity' that can be measured using concepts like VC dimension",
        "Effective social system design requires collaboration between mathematical/economic approaches and historical/anthropological understanding from humanities"
      ],
      "controversial": [
        "Vitalik's argument that many mainstream economic mechanisms (like optimal mechanism design and spectrum auctions) constitute problematic central planning may challenge academic economics orthodoxy",
        "The claim that Harberger taxes are superior to traditional property rights could be seen as radical restructuring of fundamental economic institutions",
        "The assertion that familiarity and conservatism in social institutions may sometimes be preferable to mathematical optimization challenges technocratic approaches to governance"
      ]
    }
  },
  {
    "id": "general-2018-08-26-layer_1",
    "title": "Layer 1 Should Be Innovative in the Short Term but Less in the Long Term",
    "date": "2018-08-26",
    "category": "protocol",
    "url": "https://vitalik.eth.limo/general/2018/08/26/layer_1.html",
    "path": "general/2018/08/26/layer_1.html",
    "content": "- \n\n- \n\n  \n    \n\n  \n  \n  Dark Mode Toggle\n  \n    \n\n  \n\n##  Layer 1 Should Be Innovative in the Short Term but Less in the Long Term \n\n 2018 Aug 26 \nSee all posts\n\n \n \n\n Layer 1 Should Be Innovative in the Short Term but Less in the Long Term \n\nSee update 2018-08-29\n\nOne of the key tradeoffs in blockchain design is whether to build\nmore functionality into base-layer blockchains themselves (\"layer 1\"),\nor to build it into protocols that live on top of the blockchain, and\ncan be created and modified without changing the blockchain itself\n(\"layer 2\"). The tradeoff has so far shown itself most in the scaling\ndebates, with block size increases (and sharding)\non one side and layer-2 solutions like Plasma and channels on the other,\nand to some extent blockchain governance, with loss and theft recovery\nbeing solvable by either the\nDAO fork or generalizations thereof such as EIP\n867, or by layer-2 solutions such as Reversible\nEther (RETH). So which approach is ultimately better? Those who know\nme well, or have seen me out\nmyself as a dirty centrist, know that I will inevitably say \"some of\nboth\". However, in the longer term, I do think that as blockchains\nbecome more and more mature, layer 1 will necessarily stabilize, and\nlayer 2 will take on more and more of the burden of ongoing innovation\nand change.\n\nThere are several reasons why. The first is that layer 1 solutions\nrequire ongoing protocol change to happen at the base protocol layer,\nbase layer protocol change requires governance, and it has still\nnot been shown that, in the long term, highly \"activist\" blockchain\ngovernance can continue without causing ongoing political uncertainty or\ncollapsing into centralization.\n\nTo take an example from another sphere, consider Moxie Marlinspike's\ndefense of\nSignal's centralized and non-federated nature. A document by a\ncompany defending its right to maintain control over an ecosystem it\ndepends on for its key business should of course be viewed with massive\ngrains of salt, but one can still benefit from the arguments.\nQuoting:\n\nOne of the controversial things we did with Signal early on was to\nbuild it as an unfederated service. Nothing about any of the protocols\nwe've developed requires centralization; it's entirely possible to build\na federated Signal Protocol-based messenger, but I no longer believe\nthat it is possible to build a competitive federated messenger at\nall.\n\nAnd:\n\nTheir retort was \"that's dumb, how far would the internet have gotten\nwithout interoperable protocols defined by 3rd parties?\" I thought about\nit. We got to the first production version of IP, and have been trying\nfor the past 20 years to switch to a second production version of IP\nwith limited success. We got to HTTP version 1.1 in 1997, and have been\nstuck there until now. Likewise, SMTP, IRC, DNS, XMPP, are all similarly\nfrozen in time circa the late 1990s. To answer his question, that's how\nfar the internet got. It got to the late 90s. That has taken us pretty\nfar, but it's undeniable that once you federate your protocol, it\nbecomes very difficult to make changes. And right now, at the\napplication level, things that stand still don't fare very well in a\nworld where the ecosystem is moving ... So long as federation means stasis\nwhile centralization means movement, federated protocols are going to\nhave trouble existing in a software climate that demands movement as it\ndoes today.\n\nAt this point in time, and in the medium term going forward, it seems\nclear that decentralized application platforms, cryptocurrency payments,\nidentity systems, reputation systems, decentralized exchange mechanisms,\nauctions, privacy solutions, programming languages that support privacy\nsolutions, and most other interesting things that can be done on\nblockchains are spheres where there will continue to be significant and\nongoing innovation. Decentralized application platforms often need\ncontinued reductions in confirmation time, payments need fast\nconfirmations, low transaction costs, privacy, and many other built-in\nfeatures, exchanges are appearing in many shapes and sizes including on-chain automated market makers, frequent\nbatch auctions, combinatorial\nauctions and more. Hence, \"building in\" any of these into a base\nlayer blockchain would be a bad idea, as it would create a high level of\ngovernance overhead as the platform would have to continually discuss,\nimplement and coordinate newly discovered technical improvements. For\nthe same reason federated messengers have a hard time getting off the\nground without re-centralizing, blockchains would also need to choose\nbetween adopting activist governance, with the perils that entails, and\nfalling behind newly appearing alternatives.\n\nEven Ethereum's limited level of application-specific functionality,\nprecompiles, has seen some of this effect. Less than a year ago,\nEthereum adopted the Byzantium hard fork, including operations to\nfacilitate elliptic\ncurve operations\nneeded for ring signatures, ZK-SNARKs and other applications, using the\nalt-bn128 curve. Now,\nZcash and other blockchains are moving toward BLS-12-381, and Ethereum\nwould need to fork again to catch up. In part to avoid having similar\nproblems in the future, the Ethereum community is looking to upgrade the\nEVM to E-WASM, a virtual\nmachine that is sufficiently more efficient that there is far less need\nto incorporate application-specific precompiles.\n\nBut there is also a second argument in favor of layer 2 solutions,\none that does not depend on speed of anticipated technical development:\nsometimes there are inevitable tradeoffs, with no single globally\noptimal solution. This is less easily visible in Ethereum 1.0-style\nblockchains, where there are certain models that are reasonably\nuniversal (eg. Ethereum's account-based model is one). In\nsharded blockchains, however, one type of question that does\nnot exist in Ethereum today crops up: how to do cross-shard\ntransactions? That is, suppose that the blockchain state has regions A\nand B, where few or no nodes are processing both A and B. How does the\nsystem handle transactions that affect both A and B?\n\nThe current\nanswer involves asynchronous cross-shard communication, which is\nsufficient for transferring assets and some other applications, but\ninsufficient for many others. Synchronous operations (eg. to solve the\ntrain\nand hotel problem) can be bolted on top with cross-shard\nyanking, but this requires multiple rounds of cross-shard\ninteraction, leading to significant delays. We can solve these problems\nwith a synchronous\nexecution scheme, but this comes with several tradeoffs:\n\n- The system cannot process more than one transaction for the same\naccount per block\n\n- Transactions must declare in advance what shards and addresses they\naffect\n\n- There is a high risk of any given transaction failing (and still\nbeing required to pay fees!) if the transaction is only accepted in some\nof the shards that it affects but not others\n\nIt seems very likely that a better scheme can be developed, but it\nwould be more complex, and may well have limitations that this scheme\ndoes not. There are known results preventing perfection; at the very\nleast, Amdahl's\nlaw puts a hard limit on the ability of some applications and some\ntypes of interaction to process more transactions per second through\nparallelization.\n\nSo how do we create an environment where better schemes can be tested\nand deployed? The answer is an idea that can be credited to Justin\nDrake: layer 2 execution engines. Users would be able to send assets\ninto a \"bridge contract\", which would calculate (using some indirect\ntechnique such as interactive\nverification or ZK-SNARKs)\nstate roots using some alternative set of rules for processing the\nblockchain (think of this as equivalent to layer-two \"meta-protocols\"\nlike Mastercoin/OMNI\nand Counterparty on top of\nBitcoin, except because of the bridge contract these protocols would be\nable to handle assets whose \"base ledger\" is defined on the underlying\nprotocol), and which would process withdrawals if and only if the\nalternative ruleset generates a withdrawal request.\n\nNote that anyone can create a layer 2 execution engine at any time,\ndifferent users can use different execution engines, and one can switch\nfrom one execution engine to any other, or to the base protocol, fairly\nquickly. The base blockchain no longer has to worry about being an\noptimal smart contract processing engine; it need only be a data\navailability layer with execution rules that are quasi-Turing-complete\nso that any layer 2 bridge contract can be built on top, and that allow\nbasic operations to carry state between shards (in fact, only ETH\ntransfers being fungible across shards is sufficient, but it takes very\nlittle effort to also allow cross-shard calls, so we may as well support\nthem), but does not require complexity beyond that. Note also that layer\n2 execution engines can have different state management rules than layer\n1, eg. not having storage rent; anything goes, as it's the\nresponsibility of the users of that specific execution engine to make\nsure that it is sustainable, and if they fail to do so the consequences\nare contained to within the users of that particular execution\nengine.\n\nIn the long run, layer 1 would not be actively competing on all of\nthese improvements; it would simply provide a stable platform for the\nlayer 2 innovation to happen on top. Does this mean that, say,\nsharding is a bad idea, and we should keep the blockchain size and state\nsmall so that even 10 year old computers can process everyone's\ntransactions? Absolutely not. Even if execution engines are\nsomething that gets partially or fully moved to layer 2, consensus on\ndata ordering and availability is still a highly generalizable and\nnecessary function; to see how difficult layer 2 execution engines are\nwithout layer 1 scalable data availability consensus, see the difficulties\nin Plasma\nresearch, and its difficulty\nof naturally extending to fully general purpose blockchains, for an\nexample. And if people want to throw a hundred megabytes per second of\ndata into a system where they need consensus on availability, then we\nneed a hundred megabytes per second of data availability consensus.\n\nAdditionally, layer 1 can still improve on reducing latency; if layer\n1 is slow, the only strategy for achieving very low latency is state\nchannels, which often have high capital requirements and can be\ndifficult to generalize. State channels will always beat layer 1\nblockchains in latency as state channels require only a single network\nmessage, but in those cases where state channels do not work well, layer\n1 blockchains can still come closer than they do today.\n\nHence, the other extreme position, that blockchain base layers can be\ntruly absolutely minimal, and not bother with either a\nquasi-Turing-complete execution engine or scalability to beyond the\ncapacity of a single node, is also clearly false; there is a certain\nminimal level of complexity that is required for base layers to be\npowerful enough for applications to build on top of them, and we have\nnot yet reached that level. Additional complexity is needed, though it\nshould be chosen very carefully to make sure that it is maximally\ngeneral purpose, and not targeted toward specific applications or\ntechnologies that will go out of fashion in two years due to loss of\ninterest or better alternatives.\n\nAnd even in the future base layers will need to continue to make some\nupgrades, especially if new technologies (eg. STARKs reaching higher\nlevels of maturity) allow them to achieve stronger properties than they\ncould before, though developers today can take care to make base layer\nplatforms maximally forward-compatible with such potential improvements.\nSo it will continue to be true that a balance between layer 1 and layer\n2 improvements is needed to continue improving scalability, privacy and\nversatility, though layer 2 will continue to take up a larger and larger\nshare of the innovation over time.\n\nUpdate 2018.08.29: Justin Drake pointed out to me\nanother good reason why some features may be best implemented on layer\n1: those features are public goods, and so could not be efficiently or\nreliably funded with feature-specific use fees, and hence are best paid\nfor by subsidies paid out of issuance or burned transaction fees. One\npossible example of this is secure random number generation, and another\nis generation of zero knowledge proofs for more efficient client\nvalidation of correctness of various claims about blockchain contents or\nstate.",
    "contentLength": 12551,
    "summary": "Layer 1 blockchains should innovate now but eventually stabilize to provide a foundation for ongoing Layer 2 protocol innovation.",
    "detailedSummary": {
      "theme": "Vitalik argues that blockchain layer 1 protocols should prioritize innovation now but gradually stabilize over time, with layer 2 solutions taking on more of the innovation burden.",
      "summary": "Vitalik presents a nuanced view on the layer 1 vs layer 2 debate in blockchain architecture, arguing that while base layer innovation is currently necessary, it should decrease over time as protocols mature. He draws parallels to Signal's centralized design and internet protocols that became frozen after federation, explaining that layer 1 governance becomes increasingly difficult and politically uncertain with frequent changes. Vitalik uses Ethereum's precompile evolution as an example, where the platform had to repeatedly fork to keep up with cryptographic advances, leading to the decision to move toward E-WASM for greater flexibility.\n\nVitalik introduces the concept of layer 2 execution engines as a solution, where users can send assets to bridge contracts that implement alternative rulesets while the base layer focuses on data availability and basic cross-shard operations. He argues that layer 1 should provide sufficient complexity to enable layer 2 innovation - neither too minimal nor too application-specific - while gradually taking on a more stable, platform-like role. The post concludes that some features may still belong on layer 1, particularly public goods like secure random number generation that cannot be efficiently funded through usage fees.",
      "takeaways": [
        "Layer 1 protocols should innovate in the short term but stabilize over time to avoid governance overhead and political uncertainty",
        "Frequent base layer changes create similar problems to federated protocols, which struggle to evolve compared to centralized alternatives",
        "Layer 2 execution engines can provide flexibility for different rulesets while layer 1 focuses on data availability and basic cross-shard operations",
        "Base layers need sufficient complexity to enable layer 2 innovation but should avoid application-specific features that quickly become outdated",
        "Some features like public goods (secure random number generation) may still be best implemented on layer 1 due to funding and reliability considerations"
      ],
      "controversial": [
        "The comparison between blockchain governance and Signal's centralized approach may be seen as justifying centralization over decentralization",
        "The suggestion that activist blockchain governance may inevitably lead to political uncertainty or centralization challenges the ethos of decentralized governance"
      ]
    }
  },
  {
    "id": "general-2018-08-07-99_fault_tolerant",
    "title": "A Guide to 99% Fault Tolerant Consensus",
    "date": "2018-08-07",
    "category": "protocol",
    "url": "https://vitalik.eth.limo/general/2018/08/07/99_fault_tolerant.html",
    "path": "general/2018/08/07/99_fault_tolerant.html",
    "content": "- \n\n- \n\n  \n    \n\n  \n  \n  Dark Mode Toggle\n  \n    \n\n  \n\n##  A Guide to 99% Fault Tolerant Consensus \n\n 2018 Aug 07 \nSee all posts\n\n \n \n\n A Guide to 99% Fault Tolerant Consensus \n\nSpecial thanks to Emin Gun Sirer for review\n\nWe've heard for a long time that it's possible to achieve consensus\nwith 50% fault tolerance in a synchronous network where messages\nbroadcasted by any honest node are guaranteed to be received by all\nother honest nodes within some known time period (if an attacker has\nmore than 50%, they can perform a \"51% attack\", and there's an\nanalogue of this for any algorithm of this type). We've also heard for a\nlong time that if you want to relax the synchrony assumption, and have\nan algorithm that's \"safe under asynchrony\", the maximum achievable\nfault tolerance drops to 33% (PBFT, Casper FFG, etc all fall\ninto this category). But did you know that if you add even more\nassumptions (specifically, you require observers, ie. users\nthat are not actively participating in the consensus but care about its\noutput, to also be actively watching the consensus, and not just\ndownloading its output after the fact), you can increase fault tolerance\nall the way to 99%?\n\nThis has in fact been known for a long time; Leslie Lamport's famous\n1982 paper \"The Byzantine Generals Problem\" (link here)\ncontains a description of the algorithm. The following will be my\nattempt to describe and reformulate the algorithm in a simplified\nform.\n\nSuppose that there are \\(N\\)\nconsensus-participating nodes, and everyone agrees who these nodes are\nahead of time (depending on context, they could have been selected by a\ntrusted party or, if stronger decentralization is desired, by some proof\nof work or proof of stake scheme). We label these nodes \\(0 ...N-1\\). Suppose also that there is a\nknown bound \\(D\\) on network latency\nplus clock disparity (eg. \\(D\\) = 8\nseconds). Each node has the ability to publish a value at time \\(T\\) (a malicious node can of course propose\nvalues earlier or later than \\(T\\)).\nAll nodes wait \\((N-1) \\cdot D\\)\nseconds, running the following process. Define \\(x : i\\) as \"the value \\(x\\) signed by node \\(i\\)\", \\(x : i :\nj\\) as \"the value \\(x\\) signed\nby \\(i\\), and that value and signature\ntogether signed by \\(j\\)\", etc. The\nproposals published in the first stage will be of the form \\(v: i\\) for some \\(v\\) and \\(i\\), containing the signature of the node\nthat proposed it.\n\nIf a validator \\(i\\) receives some\nmessage \\(v : i[1] : ... : i[k]\\),\nwhere \\(i[1] ... i[k]\\) is a list of\nindices that have (sequentially) signed the message already (just \\(v\\) by itself would count as \\(k=0\\), and \\(v:i\\) as \\(k=1\\)), then the validator checks that (i)\nthe time is less than \\(T + k \\cdot\nD\\), and (ii) they have not yet seen a valid message containing\n\\(v\\); if both checks pass, they\npublish \\(v : i[1] : ... : i[k] :\ni\\).\n\nAt time \\(T + (N-1) \\cdot D\\), nodes\nstop listening. At this point, there is a guarantee that honest nodes\nhave all \"validly seen\" the same set of values.\n\nNode 1 (red) is malicious, and nodes 0 and 2 (grey) are\nhonest. At the start, the two honest nodes make their proposals \\(y\\) and \\(x\\), and the attacker proposes both \\(w\\) and \\(z\\) late. \\(w\\) reaches node 0 on time but not node 2,\nand \\(z\\) reaches neither node on time.\nAt time \\(T + D\\), nodes 0 and 2\nrebroadcast all values they've seen that they have not yet broadcasted,\nbut add their signatures on (\\(x\\) and\n\\(w\\) for node 0, \\(y\\) for node 2). Both honest nodes saw\n\\({x, y, w}\\).\n\nIf the problem demands choosing one value, they can use some \"choice\"\nfunction to pick a single value out of the values they have seen (eg.\nthey take the one with the lowest hash). The nodes can then agree on\nthis value.\n\nNow, let's explore why this works. What we need to prove is that if\none honest node has seen a particular value (validly), then every other\nhonest node has also seen that value (and if we prove this, then we know\nthat all honest nodes have seen the same set of values, and so if all\nhonest nodes are running the same choice function, they will choose the\nsame value). Suppose that any honest node receives a message \\(v : i[1] : ... : i[k]\\) that they perceive\nto be valid (ie. it arrives before time \\(T +\nk \\cdot D\\)). Suppose \\(x\\) is\nthe index of a single other honest node. Either \\(x\\) is part of \\({i[1] ... i[k]}\\) or it is not.\n\n- In the first case (say \\(x = i[j]\\)\nfor this message), we know that the honest node \\(x\\) had already broadcasted that message,\nand they did so in response to a message with \\(j-1\\) signatures that they received before\ntime \\(T + (j-1) \\cdot D\\), so they\nbroadcast their message at that time, and so the message must have been\nreceived by all honest nodes before time \\(T +\nj \\cdot D\\).\n\n- In the second case, since the honest node sees the message before\ntime \\(T + k \\cdot D\\), then they will\nbroadcast the message with their signature and guarantee that everyone,\nincluding \\(x\\), will see it before\ntime \\(T + (k+1) \\cdot D\\).\n\nNotice that the algorithm uses the act of adding one's own signature\nas a kind of \"bump\" on the timeout of a message, and it's this ability\nthat guarantees that if one honest node saw a message on time, they can\nensure that everyone else sees the message on time as well, as the\ndefinition of \"on time\" increments by more than network latency with\nevery added signature.\n\nIn the case where one node is honest, can we guarantee that passive\nobservers (ie. non-consensus-participating nodes that care\nabout knowing the outcome) can also see the outcome, even if we require\nthem to be watching the process the whole time? With the scheme as\nwritten, there's a problem. Suppose that a commander and some subset of\n\\(k\\) (malicious) validators produce a\nmessage \\(v : i[1] : .... : i[k]\\), and\nbroadcast it directly to some \"victims\" just before time \\(T + k \\cdot D\\). The victims see the\nmessage as being \"on time\", but when they rebroadcast it, it only\nreaches all honest consensus-participating nodes after \\(T + k \\cdot D\\), and so all honest\nconsensus-participating nodes reject it.\n\nBut we can plug this hole. We require \\(D\\) to be a bound on two times\nnetwork latency plus clock disparity. We then put a different timeout on\nobservers: an observer accepts \\(v : i[1] :\n.... : i[k]\\) before time \\(T + (k -\n0.5) \\cdot D\\). Now, suppose an observer sees a message an\naccepts it. They will be able to broadcast it to an honest node before\ntime \\(T + k \\cdot D\\), and the honest\nnode will issue the message with their signature attached, which will\nreach all other observers before time \\(T + (k\n+ 0.5) \\cdot D\\), the timeout for messages with \\(k+1\\) signatures.\n\nRetrofitting onto\nother consensus algorithms\n\nThe above could theoretically be used as a standalone consensus\nalgorithm, and could even be used to run a proof-of-stake blockchain.\nThe validator set of round \\(N+1\\) of\nthe consensus could itself be decided during round \\(N\\) of the consensus (eg. each round of a\nconsensus could also accept \"deposit\" and \"withdraw\" transactions, which\nif accepted and correctly signed would add or remove validators into the\nnext round). The main additional ingredient that would need to be added\nis a mechanism for deciding who is allowed to propose blocks (eg. each\nround could have one designated proposer). It could also be modified to\nbe usable as a proof-of-work blockchain, by allowing\nconsensus-participating nodes to \"declare themselves\" in real time by\npublishing a proof of work solution on top of their public key at th\nsame time as signing a message with it.\n\nHowever, the synchrony assumption is very strong, and so we would\nlike to be able to work without it in the case where we don't need more\nthan 33% or 50% fault tolerance. There is a way to accomplish this.\nSuppose that we have some other consensus algorithm (eg. PBFT, Casper\nFFG, chain-based PoS) whose output can be seen by\noccasionally-online observers (we'll call this the\nthreshold-dependent consensus algorithm, as opposed to the\nalgorithm above, which we'll call the latency-dependent\nconsensus algorithm). Suppose that the threshold-dependent consensus\nalgorithm runs continuously, in a mode where it is constantly\n\"finalizing\" new blocks onto a chain (ie. each finalized value points to\nsome previous finalized value as a \"parent\"; if there's a sequence of\npointers \\(A \\rightarrow ... \\rightarrow\nB\\), we'll call \\(A\\) a\ndescendant of \\(B\\)).\n\nWe can retrofit the latency-dependent algorithm onto this structure,\ngiving always-online observers access to a kind of \"strong finality\" on\ncheckpoints, with fault tolerance ~95% (you can push this arbitrarily\nclose to 100% by adding more validators and requiring the process to\ntake longer).\n\nEvery time the time reaches some multiple of 4096 seconds, we run the\nlatency-dependent algorithm, choosing 512 random nodes to participate in\nthe algorithm. A valid proposal is any valid chain of values that were\nfinalized by the threshold-dependent algorithm. If a node sees some\nfinalized value before time \\(T + k \\cdot\nD\\) (\\(D\\) = 8 seconds) with\n\\(k\\) signatures, it accepts the chain\ninto its set of known chains and rebroadcasts it with its own signature\nadded; observers use a threshold of \\(T + (k -\n0.5) \\cdot D\\) as before.\n\nThe \"choice\" function used at the end is simple:\n\n- Finalized values that are not descendants of what was already agreed\nto be a finalized value in the previous round are ignored\n\n- Finalized values that are invalid are ignored\n\n- To choose between two valid finalized values, pick the one with the\nlower hash\n\nIf 5% of validators are honest, there is only a roughly 1 in 1\ntrillion chance that none of the 512 randomly selected nodes will be\nhonest, and so as long as the network latency plus clock disparity is\nless than \\(\\frac{D}{2}\\) the above\nalgorithm will work, correctly coordinating nodes on some single\nfinalized value, even if multiple conflicting finalized values are\npresented because the fault tolerance of the threshold-dependent\nalgorithm is broken.\n\nIf the fault tolerance of the threshold-dependent consensus algorithm\nis met (usually 50% or 67% honest), then the threshold-dependent\nconsensus algorithm will either not finalize any new checkpoints, or it\nwill finalize new checkpoints that are compatible with each other (eg. a\nseries of checkpoints where each points to the previous as a parent), so\neven if network latency exceeds \\(\\frac{D}{2}\\) (or even \\(D\\)), and as a result nodes participating\nin the latency-dependent algorithm disagree on which value they accept,\nthe values they accept are still guaranteed to be part of the same chain\nand so there is no actual disagreement. Once latency recovers back to\nnormal in some future round, the latency-dependent consensus will get\nback \"in sync\".\n\nIf the assumptions of both the threshold-dependent and\nlatency-dependent consensus algorithms are broken at the same\ntime (or in consecutive rounds), then the algorithm can break down.\nFor example, suppose in one round, the threshold-dependent consensus\nfinalizes \\(Z \\rightarrow Y \\rightarrow\nX\\) and the latency-dependent consensus disagrees between \\(Y\\) and \\(X\\), and in the next round the\nthreshold-dependent consensus finalizes a descendant \\(W\\) of \\(X\\) which is not a descendant of\n\\(Y\\); in the latency-dependent\nconsensus, the nodes who agreed \\(Y\\)\nwill not accept \\(W\\), but the nodes\nthat agreed \\(X\\) will. However, this\nis unavoidable; the impossibility of safe-under-asynchrony consensus\nwith more than \\(\\frac{1}{3}\\) fault\ntolerance is a well\nknown result in Byzantine fault tolerance theory, as is the\nimpossibility of more than \\(\\frac{1}{2}\\) fault tolerance even allowing\nsynchrony assumptions but assuming offline observers.",
    "contentLength": 11732,
    "summary": "This blog post explains how to achieve 99% fault tolerance in consensus algorithms by requiring observers to actively watch the process and using a signature-chaining scheme with time bounds.",
    "detailedSummary": {
      "theme": "Vitalik explains how consensus algorithms can achieve up to 99% fault tolerance by requiring active observation and making strong synchrony assumptions, building on Leslie Lamport's Byzantine Generals Problem solution.",
      "summary": "Vitalik begins by establishing the known fault tolerance limits of existing consensus mechanisms: 50% in synchronous networks and 33% in asynchronous networks (like PBFT and Casper FFG). He then introduces a breakthrough concept that can push fault tolerance up to 99% by adding a crucial requirement - observers must actively watch the consensus process in real-time rather than just downloading results afterward. The core algorithm works by having consensus nodes sequentially sign and rebroadcast messages within strict time bounds, with each signature extending the validity deadline. This creates a 'bump' mechanism ensuring that if one honest node sees a message on time, all other honest nodes will also see it. Vitalik also demonstrates how this latency-dependent algorithm can be retrofitted onto existing threshold-dependent consensus systems (like PBFT or Casper FFG) to provide 'strong finality' for always-online observers. By randomly selecting validators every 4096 seconds and requiring only 5% honest participation, the probability of failure drops to roughly 1 in 1 trillion, creating a hybrid system that maintains high fault tolerance while preserving the benefits of existing consensus mechanisms.",
      "takeaways": [
        "Consensus fault tolerance can be pushed from 33% to 99% by requiring observers to actively watch the process rather than passively receiving results",
        "The algorithm builds on Leslie Lamport's 1982 Byzantine Generals Problem solution, using sequential signatures as 'bumps' to extend message validity timeouts",
        "The system requires strong synchrony assumptions with known network latency bounds, making it unsuitable as a standalone solution for all scenarios",
        "A hybrid approach can retrofit this latency-dependent algorithm onto existing consensus systems to provide 'strong finality' for critical checkpoints",
        "With only 5% honest validators among 512 randomly selected nodes, the probability of consensus failure is approximately 1 in 1 trillion"
      ],
      "controversial": [
        "The practical utility may be limited since it requires very strong synchrony assumptions and active observer participation, which may not be realistic in many real-world scenarios",
        "The hybrid approach still fails if both the threshold-dependent and latency-dependent consensus assumptions are broken simultaneously, potentially creating complex failure modes"
      ]
    }
  },
  {
    "id": "general-2018-07-21-starks_part_3",
    "title": "STARKs, Part 3: Into the Weeds",
    "date": "2018-07-21",
    "category": "math",
    "url": "https://vitalik.eth.limo/general/2018/07/21/starks_part_3.html",
    "path": "general/2018/07/21/starks_part_3.html",
    "content": "- \n\n- \n\n  \n    \n\n  \n  \n  Dark Mode Toggle\n  \n    \n\n  \n\n##  STARKs, Part 3: Into the Weeds \n\n 2018 Jul 21 \nSee all posts\n\n \n \n\n STARKs, Part 3: Into the Weeds \n\nSpecial thanks to Eli ben Sasson for his kind assistance, as\nusual. Special thanks to Chih-Cheng Liang and Justin Drake for review,\nand to Ben Fisch for suggesting the reverse MIMC technique for a VDF\n(paper here)\n\nTrigger warning: math and lots of python\n\nAs a followup to Part 1 and Part 2 of this series,\nthis post will cover what it looks like to actually implement a STARK,\ncomplete with an implementation in python. STARKs (\"Scalable Transparent\nARgument of Knowledge\" are a technique for creating a proof that \\(f(x)=y\\) where \\(f\\) may potentially take a very long time\nto calculate, but where the proof can be verified very quickly. A STARK\nis \"doubly scalable\": for a computation with \\(t\\) steps, it takes roughly \\(O(t \\cdot \\log{t})\\) steps to produce a\nproof, which is likely optimal, and it takes ~\\(O(\\log^2{t})\\) steps to verify, which for\neven moderately large values of \\(t\\)\nis much faster than the original computation. STARKs can also have a\nprivacy-preserving \"zero knowledge\" property, though the use case we\nwill apply them to here, making verifiable delay functions, does not\nrequire this property, so we do not need to worry about it.\n\nFirst, some disclaimers:\n\n- This code has not been thoroughly audited; soundness in production\nuse cases is not guaranteed\n\n- This code is very suboptimal (it's written in Python, what did you\nexpect)\n\n- STARKs \"in real life\" (ie. as implemented in Eli and co's production\nimplementations) tend to use binary fields and not prime fields for\napplication-specific efficiency reasons; however, they do stress in\ntheir writings the prime field-based approach to STARKs described here\nis legitimate and can be used\n\n- There is no \"one true way\" to do a STARK. It's a broad category of\ncryptographic and mathematical constructs, with different setups optimal\nfor different applications and constant ongoing research to reduce\nprover and verifier complexity and improve soundness.\n\n- This article absolutely expects you to know how modular arithmetic\nand prime fields work, and be comfortable with the concepts of\npolynomials, interpolation and evaluation. If you don't, go back to Part 2, and also this\nearlier\npost on quadratic arithmetic programs\n\nNow, let's get to it.\n\n## MIMC\n\nHere is the function we'll be doing a STARK of:\n\ndef mimc(inp, steps, round_constants):\n    start_time = time.time()\n    for i in range(steps-1):\n        inp = (inp**3 + round_constants[i % len(round_constants)]) % modulus\n    print(\"MIMC computed in %.4f sec\" % (time.time() - start_time))\n    return inp\n\nWe choose MIMC (see paper) as the example\nbecause it is both (i) simple to understand and (ii) interesting enough\nto be useful in real life. The function can be viewed visually as\nfollows:\n\n Note: in many discussions of MIMC, you will typically see\nXOR used instead of +; this is because MIMC is typically done over\nbinary fields, where addition is XOR; here we are doing it over\nprime fields.\n\nIn our example, the round constants will be a relatively small list\n(eg. 64 items) that gets cycled through over and over again (that is,\nafter k[64] it loops back to using k[1]).\n\nMIMC with a very large number of rounds, as we're doing here, is\nuseful as a verifiable delay function - a function which is\ndifficult to compute, and particularly non-parallelizable to compute,\nbut relatively easy to verify. MIMC by itself achieves this property to\nsome extent because MIMC can be computed \"backward\" (recovering\nthe \"input\" from its corresponding \"output\"), but computing it backward\ntakes about 100 times longer to compute than the forward direction (and\nneither direction can be significantly sped up by parallelization). So\nyou can think of computing the function in the backward direction as\nbeing the act of \"computing\" the non-parallelizable proof of work, and\ncomputing the function in the forward direction as being the process of\n\"verifying\" it.\n\n \\(x \\rightarrow\nx^{(2p-1)/3}\\) gives the inverse of \\(x\n\\rightarrow x^3\\); this is true because of\nFermat's\nLittle Theorem, a theorem that despite its supposed littleness is\narguably much more important to mathematics than Fermat's more famous\n\"Last Theorem\".\n\nWhat we will try to achieve here is to make verification much more\nefficient by using a STARK - instead of the verifier having to run MIMC\nin the forward direction themselves, the prover, after completing the\ncomputation in the \"backward direction\", would compute a STARK of the\ncomputation in the \"forward direction\", and the verifier would simply\nverify the STARK. The hope is that the overhead of computing a STARK can\nbe less than the difference in speed running MIMC forwards relative to\nbackwards, so a prover's time would still be dominated by the initial\n\"backward\" computation, and not the (highly parallelizable) STARK\ncomputation. Verification of a STARK can be relatively fast (in our\npython implementation, ~0.05-0.3 seconds), no matter how long the\noriginal computation is.\n\nAll calculations are done modulo \\(2^{256}\n- 351 \\cdot 2^{32} + 1\\); we are using this prime field modulus\nbecause it is the largest prime below \\(2^{256}\\) whose multiplicative group\ncontains an order \\(2^{32}\\) subgroup\n(that is, there's a number \\(g\\) such\nthat successive powers of \\(g\\) modulo\nthis prime loop around back to \\(1\\)\nafter exactly \\(2^{32}\\) cycles), and\nwhich is of the form \\(6k+5\\). The\nfirst property is necessary to make sure that our efficient versions of\nthe FFT and FRI algorithms can work, and the second ensures that MIMC\nactually can be computed \"backwards\" (see the use of \\(x \\rightarrow x^{(2p-1)/3}\\) above).\n\n## Prime field operations\n\nWe start off by building a convenience class that does prime field\noperations, as well as operations with polynomials over prime fields.\nThe code is here.\nFirst some trivial bits:\n\nclass PrimeField():\n    def __init__(self, modulus):\n        # Quick primality test\n        assert pow(2, modulus, modulus) == 2\n        self.modulus = modulus\n\n    def add(self, x, y):\n        return (x+y) % self.modulus\n\n    def sub(self, x, y):\n        return (x-y) % self.modulus\n\n    def mul(self, x, y):\n        return (x*y) % self.modulus\n\nAnd the Extended\nEuclidean Algorithm for computing modular inverses (the equivalent\nof computing \\(\\frac{1}{x}\\) in a prime\nfield):\n\n# Modular inverse using the extended Euclidean algorithm\ndef inv(self, a):\n    if a == 0:\n        return 0\n    lm, hm = 1, 0\n    low, high = a % self.modulus, self.modulus\n    while low > 1:\n        r = high//low\n        nm, new = hm-lm*r, high-low*r\n        lm, low, hm, high = nm, new, lm, low\n    return lm % self.modulus\n\nThe above algorithm is relatively expensive; fortunately, for the\nspecial case where we need to do many modular inverses, there's a simple\nmathematical trick that allows us to compute many inverses, called Montgomery\nbatch inversion:\n\n Using Montgomery batch inversion to compute modular\ninverses. Inputs purple, outputs green, multiplication gates black; the\nred square is the only modular inversion.\n\nThe code below implements this algorithm, with some slightly ugly\nspecial case logic so that if there are zeroes in the set of what we are\ninverting, it sets their inverse to 0 and moves along.\n\ndef multi_inv(self, values):\n    partials = [1]\n    for i in range(len(values)):\n        partials.append(self.mul(partials[-1], values[i] or 1))\n    inv = self.inv(partials[-1])\n    outputs = [0] * len(values)\n    for i in range(len(values), 0, -1):\n        outputs[i-1] = self.mul(partials[i-1], inv) if values[i-1] else 0\n        inv = self.mul(inv, values[i-1] or 1)\n    return outputs\n\nThis batch inverse algorithm will prove important later on, when we\nstart dealing with dividing sets of evaluations of polynomials.\n\nNow we move on to some polynomial operations. We treat a polynomial\nas an array, where element \\(i\\) is the\n\\(i\\)th degree term (eg. \\(x^{3} + 2x + 1\\) becomes\n[1, 2, 0, 1]). Here's the operation of evaluating a\npolynomial at one point:\n\n# Evaluate a polynomial at a point\ndef eval_poly_at(self, p, x):\n    y = 0\n    power_of_x = 1\n    for i, p_coeff in enumerate(p):\n        y += power_of_x * p_coeff\n        power_of_x = (power_of_x * x) % self.modulus\n    return y % self.modulus\n\nChallenge\n What is the output of f.eval_poly_at([4, 5,\n6], 2) if the modulus is 31?\n \n Mouseover below for\nanswer \n\n\\(6 \\cdot 2^{2} + 5 \\cdot 2 + 4 = 38, 38\n\\bmod 31 = 7\\).\n\nThere is also code for adding, subtracting, multiplying and dividing\npolynomials; this is textbook long\naddition/subtraction/multiplication/division. The one non-trivial thing\nis Lagrange interpolation, which takes as input a set of x and y\ncoordinates, and returns the minimal polynomial that passes through all\nof those points (you can think of it as being the inverse of polynomial\nevaluation):\n\n# Build a polynomial that returns 0 at all specified xs\ndef zpoly(self, xs):\n    root = [1]\n    for x in xs:\n        root.insert(0, 0)\n        for j in range(len(root)-1):\n            root[j] -= root[j+1] * x\n    return [x % self.modulus for x in root]\n\ndef lagrange_interp(self, xs, ys):\n    # Generate master numerator polynomial, eg. (x - x1) * (x - x2) * ... * (x - xn)\n    root = self.zpoly(xs)\n\n    # Generate per-value numerator polynomials, eg. for x=x2,\n    # (x - x1) * (x - x3) * ... * (x - xn), by dividing the master\n    # polynomial back by each x coordinate\n    nums = [self.div_polys(root, [-x, 1]) for x in xs]\n\n    # Generate denominators by evaluating numerator polys at each x\n    denoms = [self.eval_poly_at(nums[i], xs[i]) for i in range(len(xs))]\n    invdenoms = self.multi_inv(denoms)\n\n    # Generate output polynomial, which is the sum of the per-value numerator\n    # polynomials rescaled to have the right y values\n    b = [0 for y in ys]\n    for i in range(len(xs)):\n        yslice = self.mul(ys[i], invdenoms[i])\n        for j in range(len(ys)):\n            if nums[i][j] and ys[i]:\n                b[j] += nums[i][j] * yslice\n    return [x % self.modulus for x in b]\n\nSee the\n\"M of N\" section of this article for a description of the math. Note\nthat we also have special-case methods lagrange_interp_4\nand lagrange_interp_2 to speed up the very frequent\noperations of Lagrange interpolation of degree \\(< 2\\) and degree \\(< 4\\) polynomials.\n\n## Fast Fourier Transforms\n\nIf you read the above algorithms carefully, you might notice that\nLagrange interpolation and multi-point evaluation (that is, evaluating a\ndegree \\(< N\\) polynomial at \\(N\\) points) both take quadratic time to\nexecute, so for example doing a Lagrange interpolation of one thousand\npoints takes a few million steps to execute, and a Lagrange\ninterpolation of one million points takes a few trillion. This is an\nunacceptably high level of inefficiency, so we will use a more efficient\nalgorithm, the Fast Fourier Transform.\n\nThe FFT only takes \\(O(n \\cdot\nlog(n))\\) time (ie. ~10,000 steps for 1,000 points, ~20 million\nsteps for 1 million points), though it is more restricted in scope; the\nx coordinates must be a complete set of roots of\nunity of some\norder\n\\(N = 2^{k}\\). That is, if there are\n\\(N\\) points, the x coordinates must be\nsuccessive powers \\(1, p, p^{2},\np^{3}\\)... of some \\(p\\) where\n\\(p^{N} = 1\\). The algorithm can,\nsurprisingly enough, be used for multi-point evaluation or\ninterpolation, with one small parameter tweak.\n\nChallenge Find a 16th root of unity mod 337 that is not an 8th\nroot of unity.\n \n Mouseover below for answer \n\n59, 146, 30, 297, 278, 191, 307,\n40\n \n You could have gotten this list by doing something\nlike [print(x) for x in range(337)\nif pow(x, 16, 337) == 1 and pow(x, 8, 337) != 1], though there is\na smarter way that works for much larger moduluses: first, identify a\nsingle primitive root mod 337 (that is, not a perfect square), by\nlooking for a value x such\nthat pow(x, 336 // 2, 337) !=\n1 (these are easy to find; one answer is 5), and then taking the\n(336 / 16)'th power of it.\n\nHere's the algorithm (in a slightly simplified form; see code\nhere for something slightly more optimized):\n\ndef fft(vals, modulus, root_of_unity):\n    if len(vals) == 1:\n        return vals\n    L = fft(vals[::2], modulus, pow(root_of_unity, 2, modulus))\n    R = fft(vals[1::2], modulus, pow(root_of_unity, 2, modulus))\n    o = [0 for i in vals]\n    for i, (x, y) in enumerate(zip(L, R)):\n        y_times_root = y*pow(root_of_unity, i, modulus)\n        o[i] = (x+y_times_root) % modulus\n        o[i+len(L)] = (x-y_times_root) % modulus\n    return o\n\ndef inv_fft(vals, modulus, root_of_unity):\n    f = PrimeField(modulus)\n    # Inverse FFT\n    invlen = f.inv(len(vals))\n    return [(x*invlen) % modulus for x in\n            fft(vals, modulus, f.inv(root_of_unity))]\n\nYou can try running it on a few inputs yourself and check that it\ngives results that, when you use eval_poly_at on them, give\nyou the answers you expect to get. For example:\n\n>>> fft.fft([3,1,4,1,5,9,2,6], 337, 85, inv=True)\n[46, 169, 29, 149, 126, 262, 140, 93]\n>>> f = poly_utils.PrimeField(337)\n>>> [f.eval_poly_at([46, 169, 29, 149, 126, 262, 140, 93], f.exp(85, i)) for i in range(8)]\n[3, 1, 4, 1, 5, 9, 2, 6]\n\nA Fourier transform takes as input [x[0] .... x[n-1]],\nand its goal is to output x[0] + x[1] + ... + x[n-1] as the\nfirst element, x[0] + x[1] * 2 + ... + x[n-1] * w**(n-1) as\nthe second element, etc etc; a fast Fourier transform accomplishes this\nby splitting the data in half, doing an FFT on both halves, and then\ngluing the result back together.\n\nA diagram of how information flows through the FFT\ncomputation. Notice how the FFT consists of a \"gluing\" step followed by\ntwo copies of the FFT on two halves of the data, and so on recursively\nuntil you're down to one element.\n\nI recommend this\nfor more intuition on how or why the FFT works and polynomial math in\ngeneral, and this\nthread for some more specifics on DFT vs FFT, though be warned that\nmost literature on Fourier transforms talks about Fourier transforms\nover real and complex numbers, not prime fields. If\nyou find this too hard and don't want to understand it, just treat it as\nweird spooky voodoo that just works because you ran the code a few times\nand verified that it works, and you'll be fine too.\nThank\nGoodness It's FRI-day (that's \"Fast Reed-Solomon Interactive Oracle\nProofs of Proximity\")\n\nReminder: now may be a good time to review and\nre-read Part\n2\n\nNow, we'll get into the\ncode for making a low-degree proof. To review, a low-degree proof is\na (probabilistic) proof that at least some high percentage (eg. 80%) of\na given set of values represent the evaluations of some specific\npolynomial whose degree is much lower than the number of values given.\nIntuitively, just think of it as a proof that \"some Merkle root that we\nclaim represents a polynomial actually does represent a polynomial,\npossibly with a few errors\". As input, we have:\n\n- A set of values that we claim are the evaluation of a low-degree\npolynomial\n\n- A root of unity; the x coordinates at which the polynomial is\nevaluated are successive powers of this root of unity\n\n- A value \\(N\\) such that we are\nproving the degree of the polynomial is strictly less than\n\\(N\\)\n\n- The modulus\n\nOur approach is a recursive one, with two cases. First, if the degree\nis low enough, we just provide the entire list of values as a proof;\nthis is the \"base case\". Verification of the base case is trivial: do an\nFFT or Lagrange interpolation or whatever else to interpolate the\npolynomial representing those values, and verify that its degree is\n\\(< N\\). Otherwise, if the degree is\nhigher than some set minimum, we do the vertical-and-diagonal trick\ndescribed at the bottom\nof Part 2.\n\nWe start off by putting the values into a Merkle tree and using the\nMerkle root to select a pseudo-random x coordinate\n(special_x). We then calculate the \"column\":\n\n# Calculate the set of x coordinates\nxs = get_power_cycle(root_of_unity, modulus)\n\ncolumn = []\nfor i in range(len(xs)//4):\n    x_poly = f.lagrange_interp_4(\n        [xs[i+len(xs)*j//4] for j in range(4)],\n        [values[i+len(values)*j//4] for j in range(4)],\n    )\n    column.append(f.eval_poly_at(x_poly, special_x))\n\nThis packs a lot into a few lines of code. The broad idea is to\nre-interpret the polynomial \\(P(x)\\) as\na polynomial \\(Q(x, y)\\), where \\(P(x) = Q(x, x^4)\\). If \\(P\\) has degree \\(< N\\), then \\(P'(y) = Q(special\\_x, y)\\) will have\ndegree \\(< \\frac{N}{4}\\). Since we\ndon't want to take the effort to actually compute \\(Q\\) in coefficient form (that would take a\nstill-relatively-nasty-and-expensive FFT!), we instead use another\ntrick. For any given value of \\(x^{4}\\), there are 4 corresponding values\nof \\(x\\): \\(x\\), \\(modulus -\nx\\), and \\(x\\) multiplied by the\ntwo modular square roots of \\(-1\\). So\nwe already have four values of \\(Q(?,\nx^4)\\), which we can use to interpolate the polynomial \\(R(x) = Q(x, x^4)\\), and from there\ncalculate \\(R(special\\_x) = Q(special\\_x, x^4)\n= P'(x^4)\\). There are \\(\\frac{N}{4}\\) possible values of \\(x^{4}\\), and this lets us easily calculate\nall of them.\n\nA diagram from part 2; it helps to keep this in mind when\nunderstanding what's going on here\n\nOur proof consists of some number (eg. 40) of random queries from the\nlist of values of \\(x^{4}\\) (using the\nMerkle root of the column as a seed), and for each query we provide\nMerkle branches of the five values of \\(Q(?,\nx^4)\\):\n\nm2 = merkelize(column)\n\n# Pseudo-randomly select y indices to sample\n# (m2[1] is the Merkle root of the column)\nys = get_pseudorandom_indices(m2[1], len(column), 40)\n\n# Compute the Merkle branches for the values in the polynomial and the column\nbranches = []\nfor y in ys:\n    branches.append([mk_branch(m2, y)] +\n                    [mk_branch(m, y + (len(xs) // 4) * j) for j in range(4)])\n\nThe verifier's job will be to verify that these five values actually\ndo lie on the same degree \\(< 4\\)\npolynomial. From there, we recurse and do an FRI on the column,\nverifying that the column actually does have degree \\(< \\frac{N}{4}\\). That really is all\nthere is to FRI.\n\nAs a challenge exercise, you could try creating low-degree proofs of\npolynomial evaluations that have errors in them, and see how many errors\nyou can get away passing the verifier with (hint, you'll need to modify\nthe prove_low_degree function; with the default prover,\neven one error will balloon up and cause verification to fail).\n\n## The STARK\n\nReminder: now may be a good time to review and\nre-read Part\n1\n\nNow, we get to the actual meat that puts all of these pieces\ntogether: def mk_mimc_proof(inp, steps, round_constants)\n(code here),\nwhich generates a proof of the execution result of running the MIMC\nfunction with the given input for some number of steps. First, some\nasserts:\n\nassert steps <= 2**32 // extension_factor\nassert is_a_power_of_2(steps) and is_a_power_of_2(len(round_constants))\nassert len(round_constants) < steps\n\nThe extension factor is the extent to which we will be \"stretching\"\nthe computational trace (the set of \"intermediate values\" of executing\nthe MIMC function). We need the step count multiplied by the extension\nfactor to be at most \\(2^{32}\\),\nbecause we don't have roots of unity of order \\(2^{k}\\) for \\(k\n> 32\\).\n\nOur first computation will be to generate the computational trace;\nthat is, all of the intermediate values of the computation,\nfrom the input going all the way to the output.\n\n# Generate the computational trace\ncomputational_trace = [inp]\nfor i in range(steps-1):\n    computational_trace.append((computational_trace[-1]**3 + round_constants[i % len(round_constants)]) % modulus)\noutput = computational_trace[-1]\n\nWe then convert the computation trace into a polynomial, \"laying\ndown\" successive values in the trace on successive powers of a root of\nunity \\(g\\) where \\(g^{steps}\\) = 1, and we then evaluate the\npolynomial in a larger set, of successive powers of a root of unity\n\\(g_2\\) where \\((g_2)^{steps \\cdot 8} = 1\\) (note that\n\\((g_2)^{8} = g\\)).\n\ncomputational_trace_polynomial = inv_fft(computational_trace, modulus, subroot)\np_evaluations = fft(computational_trace_polynomial, modulus, root_of_unity)\n\nBlack: powers of \\(g_1\\).\nPurple: powers of \\(g_2\\). Orange: 1.\nYou can look at successive roots of unity as being arranged in a circle\nin this way. We are \"laying\" the computational trace along powers of\n\\(g_1\\), and then extending it compute\nthe values of the same polynomial at the intermediate values (ie. the\npowers of \\(g_2\\)).\n\nWe can convert the round constants of MIMC into a polynomial. Because\nthese round constants loop around very frequently (in our tests, every\n64 steps), it turns out that they form a degree-64 polynomial, and we\ncan fairly easily compute its expression, and its extension:\n\nskips2 = steps // len(round_constants)\nconstants_mini_polynomial = fft(round_constants, modulus, f.exp(subroot, skips2), inv=True)\nconstants_polynomial = [0 if i % skips2 else constants_mini_polynomial[i//skips2] for i in range(steps)]\nconstants_mini_extension = fft(constants_mini_polynomial, modulus, f.exp(root_of_unity, skips2))\n\nSuppose there are 8192 steps of execution and 64 round constants.\nHere is what we are doing: we are doing an FFT to compute the round\nconstants as a function of \\((g_1)^{128}\\). We then add zeroes in\nbetween the constants to make it a function of \\(g_1\\) itself. Because \\((g_1)^{128}\\) loops around every 64 steps,\nwe know this function of \\(g_1\\) will\nas well. We only compute 512 steps of the extension, because we know\nthat the extension repeats after 512 steps as well.\n\nWe now, as in the Fibonacci example in Part 1, calculate \\(C(P(x))\\), except this time it's \\(C(P(x), P(g_1 \\cdot x), K(x))\\):\n\n# Create the composed polynomial such that\n# C(P(x), P(g1*x), K(x)) = P(g1*x) - P(x)**3 - K(x)\nc_of_p_evaluations = [(p_evaluations[(i+extension_factor)%precision] -\n                          f.exp(p_evaluations[i], 3) -\n                          constants_mini_extension[i % len(constants_mini_extension)])\n                      % modulus for i in range(precision)]\nprint('Computed C(P, K) polynomial')\n\nNote that here we are no longer working with polynomials in\ncoefficient form; we are working with the polynomials in terms\nof their evaluations at successive powers of the higher-order root of\nunity.\n\nc_of_p is intended to be \\(Q(x) = C(P(x), P(g_1 \\cdot x), K(x)) = P(g_1 \\cdot\nx) - P(x)^3 - K(x)\\); the goal is that for every \\(x\\) that we are laying the computational\ntrace along (except for the last step, as there's no step \"after\" the\nlast step), the next value in the trace is equal to the previous value\nin the trace cubed, plus the round constant. Unlike the Fibonacci\nexample in Part 1, where if one computational step was at coordinate\n\\(k\\), the next step is at coordinate\n\\(k+1\\), here we are laying down the\ncomputational trace along successive powers of the lower-order root of\nunity \\(g_1\\), so if one computational\nstep is located at \\(x = (g_1)^i\\), the\n\"next\" step is located at \\((g_1)^{i+1}\\) = \\((g_1)^i \\cdot g_1 = x \\cdot g_1\\). Hence,\nfor every power of the lower-order root of unity \\(g_1\\) (except the last), we want it to be\nthe case that \\(P(x\\cdot g_1) = P(x)^3 +\nK(x)\\), or \\(P(x\\cdot g_1) - P(x)^3 -\nK(x) = Q(x) = 0\\). Thus, \\(Q(x)\\) will be equal to zero at all\nsuccessive powers of the lower-order root of unity \\(g\\) (except the last).\n\nThere is an algebraic theorem that proves that if \\(Q(x)\\) is equal to zero at all of these x\ncoordinates, then it is a multiple of the minimal polynomial\nthat is equal to zero at all of these x coordinates: \\(Z(x) = (x - x_1) \\cdot (x - x_2) \\cdot ... \\cdot\n(x - x_n)\\). Since proving that \\(Q(x)\\) is equal to zero at every single\ncoordinate we want to check is too hard (as verifying such a proof would\ntake longer than just running the original computation!), instead we use\nan indirect approach to (probabilistically) prove that \\(Q(x)\\) is a multiple of \\(Z(x)\\). And how do we do that? By providing\nthe quotient \\(D(x) =\n\\frac{Q(x)}{Z(x)}\\) and using FRI to prove that it's an actual\npolynomial and not a fraction, of course!\n\nWe chose the particular arrangement of lower and higher order roots\nof unity (rather than, say, laying the computational trace along the\nfirst few powers of the higher order root of unity) because it turns out\nthat computing \\(Z(x)\\) (the polynomial\nthat evaluates to zero at all points along the computational trace\nexcept the last), and dividing by \\(Z(x)\\) is trivial there: the expression of\n\\(Z\\) is a fraction of two terms.\n\n# Compute D(x) = Q(x) / Z(x)\n# Z(x) = (x^steps - 1) / (x - x_atlast_step)\nz_num_evaluations = [xs[(i * steps) % precision] - 1 for i in range(precision)]\nz_num_inv = f.multi_inv(z_num_evaluations)\nz_den_evaluations = [xs[i] - last_step_position for i in range(precision)]\nd_evaluations = [cp * zd * zni % modulus for cp, zd, zni in zip(c_of_p_evaluations, z_den_evaluations, z_num_inv)]\nprint('Computed D polynomial')\n\nNotice that we compute the numerator and denominator of \\(Z\\) directly in \"evaluation form\", and then\nuse the batch modular inversion to turn dividing by \\(Z\\) into a multiplication (\\(\\cdot z_d \\cdot z_ni\\)), and then pointwise\nmultiply the evaluations of \\(Q(x)\\) by\nthese inverses of \\(Z(x)\\). Note that\nat the powers of the lower-order root of unity except the last (ie.\nalong the portion of the low-degree extension that is part of the\noriginal computational trace), \\(Z(x) =\n0\\), so this computation involving its inverse will break. This\nis unfortunate, though we will plug the hole by simply modifying the\nrandom checks and FRI algorithm to not sample at those points, so the\nfact that we calculated them wrong will never matter.\n\nBecause \\(Z(x)\\) can be expressed so\ncompactly, we get another benefit: the verifier can compute \\(Z(x)\\) for any specific \\(x\\) extremely quickly, without needing any\nprecomputation. It's okay for the prover to have to deal with\npolynomials whose size equals the number of steps, but we don't want to\nask the verifier to do the same, as we want verification to be\nsuccinct (ie. ultra-fast, with proofs as small as possible).\n\nProbabilistically checking \\(D(x) \\cdot\nZ(x) = Q(x)\\) at a few randomly selected points allows us to\nverify the transition constraints - that each\ncomputational step is a valid consequence of the previous step. But we\nalso want to verify the boundary constraints - that the\ninput and the output of the computation is what the prover says they\nare. Just asking the prover to provide evaluations of \\(P(1)\\), \\(D(1)\\), \\(P(last\\_step)\\) and \\(D(last\\_step)\\) (where \\(last\\_step\\) (or \\(g^{steps-1}\\)) is the coordinate\ncorresponding to the last step in the computation) is too fragile;\nthere's no proof that those values are on the same polynomial as the\nrest of the data. So instead we use a similar kind of polynomial\ndivision trick:\n\n# Compute interpolant of ((1, input), (x_atlast_step, output))\ninterpolant = f.lagrange_interp_2([1, last_step_position], [inp, output])\ni_evaluations = [f.eval_poly_at(interpolant, x) for x in xs]\n\nzeropoly2 = f.mul_polys([-1, 1], [-last_step_position, 1])\ninv_z2_evaluations = f.multi_inv([f.eval_poly_at(quotient, x) for x in xs])\n\n# B = (P - I) / Z2\nb_evaluations = [((p - i) * invq) % modulus for p, i, invq in zip(p_evaluations, i_evaluations, inv_z2_evaluations)]\nprint('Computed B polynomial')\n\nThe argument is as follows. The prover wants to prove \\(P(1) = input\\) and \\(P(last\\_step) = output\\). If we take \\(I(x)\\) as the interpolant - the\nline that crosses the two points \\((1,\ninput)\\) and \\((last\\_step,\noutput)\\), then \\(P(x) - I(x)\\)\nwould be equal to zero at those two points. Thus, it suffices to prove\nthat \\(P(x) - I(x)\\) is a multiple of\n\\((x - 1) \\cdot (x - last\\_step)\\), and\nwe do that by... providing the quotient!\n\nPurple: computational trace polynomial (P). Green: interpolant\n(I) (notice how the interpolant is constructed to equal the input (which\nshould be the first step of the computational trace) at x=1 and the\noutput (which should be the last step of the computational trace) at\n\\(x=g^{steps-1}\\). Red: \\(P - I\\). Yellow: the minimal polynomial\nthat equals \\(0\\) at \\(x=1\\) and \\(x=g^{steps-1}\\) (that is, \\(Z_2\\)). Pink: \\(\\frac{P - I}{Z_2}\\).\n\nChallenge Suppose you wanted to also prove that the value\nin the computational trace after the 703rd computational step is equal\nto 8018284612598740. How would you modify the above algorithm to do\nthat? \n Mouseover below for answer \n\nSet \\(I(x)\\) to be the interpolant\nof \\((1, input), (g^{703}, 8018284612598740),\n(last\\_step, output)\\), and make a proof by providing the\nquotient \\(B(x) = \\frac{P(x) - I(x)}{(x - 1)\n\\cdot (x - g^{703}) \\cdot (x - last\\_step)}\\) \n\nNow, we commit to the Merkle root of \\(P\\), \\(D\\)\nand \\(B\\) combined together.\n\n# Compute their Merkle roots\nmtree = merkelize([pval.to_bytes(32, 'big') +\n                   dval.to_bytes(32, 'big') +\n                   bval.to_bytes(32, 'big') for\n                   pval, dval, bval in zip(p_evaluations, d_evaluations, b_evaluations)])\nprint('Computed hash root')\n\nNow, we need to prove that \\(P\\),\n\\(D\\) and \\(B\\) are all actually polynomials, and of\nthe right max-degree. But FRI proofs are big and expensive, and we don't\nwant to have three FRI proofs. So instead, we compute a pseudorandom\nlinear combination of \\(P\\), \\(D\\) and \\(B\\) (using the Merkle root of \\(P\\), \\(D\\)\nand \\(B\\) as a seed), and do an FRI\nproof on that:\n\nk1 = int.from_bytes(blake(mtree[1] + b'\\x01'), 'big')\nk2 = int.from_bytes(blake(mtree[1] + b'\\x02'), 'big')\nk3 = int.from_bytes(blake(mtree[1] + b'\\x03'), 'big')\nk4 = int.from_bytes(blake(mtree[1] + b'\\x04'), 'big')\n\n# Compute the linear combination. We don't even bother calculating it\n# in coefficient form; we just compute the evaluations\nroot_of_unity_to_the_steps = f.exp(root_of_unity, steps)\npowers = [1]\nfor i in range(1, precision):\n    powers.append(powers[-1] * root_of_unity_to_the_steps % modulus)\n\nl_evaluations = [(d_evaluations[i] +\n                  p_evaluations[i] * k1 + p_evaluations[i] * k2 * powers[i] +\n                  b_evaluations[i] * k3 + b_evaluations[i] * powers[i] * k4) % modulus\n                  for i in range(precision)]\n\nUnless all three of the polynomials have the right low degree, it's\nalmost impossible that a randomly selected linear combination of them\nwill (you have to get extremely lucky for the terms to cancel),\nso this is sufficient.\n\nWe want to prove that the degree of D is less than \\(2 \\cdot steps\\), and that of \\(P\\) and \\(B\\) are less than \\(steps\\), so we actually make a random\nlinear combination of \\(P\\), \\(P \\cdot x^{steps}\\), \\(B\\), \\(B^{steps}\\) and \\(D\\), and check that the degree of this\ncombination is less than \\(2 \\cdot\nsteps\\).\n\nNow, we do some spot checks of all of the polynomials. We generate\nsome random indices, and provide the Merkle branches of the polynomial\nevaluated at those indices:\n\n# Do some spot checks of the Merkle tree at pseudo-random coordinates, excluding\n# multiples of `extension_factor`\nbranches = []\nsamples = spot_check_security_factor\npositions = get_pseudorandom_indices(l_mtree[1], precision, samples,\n                                     exclude_multiples_of=extension_factor)\nfor pos in positions:\n    branches.append(mk_branch(mtree, pos))\n    branches.append(mk_branch(mtree, (pos + skips) % precision))\n    branches.append(mk_branch(l_mtree, pos))\nprint('Computed %d spot checks' % samples)\n\nThe get_pseudorandom_indices function returns some\nrandom indices in the range [0...precision-1], and the\nexclude_multiples_of parameter tells it to not give values\nthat are multiples of the given parameter (here,\nextension_factor). This ensures that we do not sample along\nthe original computational trace, where we are likely to get wrong\nanswers.\n\nThe proof (~250-500 kilobytes altogether) consists of a set of Merkle\nroots, the spot-checked branches, and a low-degree proof of the random\nlinear combination:\n\no = [mtree[1],\n     l_mtree[1],\n     branches,\n     prove_low_degree(l_evaluations, root_of_unity, steps * 2, modulus, exclude_multiples_of=extension_factor)]\n\nThe largest parts of the proof in practice are the Merkle branches,\nand the FRI proof, which consists of even more branches. And here's the\n\"meat\" of the verifier:\n\nfor i, pos in enumerate(positions):\n    x = f.exp(G2, pos)\n    x_to_the_steps = f.exp(x, steps)\n    mbranch1 =  verify_branch(m_root, pos, branches[i*3])\n    mbranch2 =  verify_branch(m_root, (pos+skips)%precision, branches[i*3+1])\n    l_of_x = verify_branch(l_root, pos, branches[i*3 + 2], output_as_int=True)\n\n    p_of_x = int.from_bytes(mbranch1[:32], 'big')\n    p_of_g1x = int.from_bytes(mbranch2[:32], 'big')\n    d_of_x = int.from_bytes(mbranch1[32:64], 'big')\n    b_of_x = int.from_bytes(mbranch1[64:], 'big')\n\n    zvalue = f.div(f.exp(x, steps) - 1,\n                   x - last_step_position)\n    k_of_x = f.eval_poly_at(constants_mini_polynomial, f.exp(x, skips2))\n\n    # Check transition constraints Q(x) = Z(x) * D(x)\n    assert (p_of_g1x - p_of_x ** 3 - k_of_x - zvalue * d_of_x) % modulus == 0\n\n    # Check boundary constraints B(x) * Z2(x) + I(x) = P(x)\n    interpolant = f.lagrange_interp_2([1, last_step_position], [inp, output])\n    zeropoly2 = f.mul_polys([-1, 1], [-last_step_position, 1])\n    assert (p_of_x - b_of_x * f.eval_poly_at(zeropoly2, x) -\n            f.eval_poly_at(interpolant, x)) % modulus == 0\n\n    # Check correctness of the linear combination\n    assert (l_of_x - d_of_x -\n            k1 * p_of_x - k2 * p_of_x * x_to_the_steps -\n            k3 * b_of_x - k4 * b_of_x * x_to_the_steps) % modulus == 0\n\nAt every one of the positions that the prover provides a Merkle proof\nfor, the verifier checks the Merkle proof, and checks that \\(C(P(x), P(g_1 \\cdot x), K(x)) = Z(x) \\cdot\nD(x)\\) and \\(B(x) \\cdot Z_2(x) + I(x) =\nP(x)\\) (reminder: for \\(x\\) that\nare not along the original computation trace, \\(Z(x)\\) will not be zero, and so \\(C(P(x), P(g_1 \\cdot x), K(x))\\) likely will\nnot evaluate to zero). The verifier also checks that the linear\ncombination is correct, and calls\nverify_low_degree_proof(l_root, root_of_unity, fri_proof, steps * 2, modulus, exclude_multiples_of=extension_factor)\nto verify the FRI proof. And we're done!\n\nWell, not really; soundness analysis to prove how many spot-checks\nfor the cross-polynomial checking and for the FRI are necessary is\nreally tricky. But that's all there is to the code, at least if you\ndon't care about making even crazier optimizations. When I run the code\nabove, we get a STARK proving \"overhead\" of about 300-400x (eg. a MIMC\ncomputation that takes 0.2 seconds to calculate takes 60 second to\nprove), suggesting that with a 4-core machine computing the STARK of the\nMIMC computation in the forward direction could actually be faster than\ncomputing MIMC in the backward direction. That said, these are both\nrelatively inefficient implementations in python, and the proving to\nrunning time ratio for properly optimized implementations may be\ndifferent. Also, it's worth pointing out that the STARK proving overhead\nfor MIMC is remarkably low, because MIMC is almost perfectly\n\"arithmetizable\" - it's mathematical form is very simple. For \"average\"\ncomputations, which contain less arithmetically clean operations (eg.\nchecking if a number is greater or less than another number), the\noverhead is likely much higher, possibly around 10000-50000x.",
    "contentLength": 35471,
    "summary": "STARKs Part 3 shows how to implement a STARK proof system in Python using MIMC as an example verifiable delay function.",
    "detailedSummary": {
      "theme": "A comprehensive technical implementation guide for STARKs (Scalable Transparent Arguments of Knowledge) with complete Python code for proving MIMC computations.",
      "summary": "Vitalik provides a detailed, code-heavy tutorial on implementing STARKs from scratch, using the MIMC hash function as a practical example for creating verifiable delay functions. He walks through all the mathematical components needed: prime field operations, Fast Fourier Transforms, FRI (Fast Reed-Solomon Interactive Oracle Proofs), and the complete STARK construction process. The implementation demonstrates how to create proofs that a computation was executed correctly without requiring the verifier to re-run the entire computation. Vitalik's Python implementation shows a proving overhead of 300-400x for MIMC specifically, though he notes this could be much higher (10,000-50,000x) for more complex computations that are less 'arithmetizable.' The tutorial includes extensive code examples, mathematical explanations, and practical considerations for building production-ready zero-knowledge proof systems.",
      "takeaways": [
        "STARKs provide 'doubly scalable' proof systems: O(t\u00b7log(t)) to prove and O(log\u00b2(t)) to verify computations with t steps",
        "MIMC serves as an excellent example function because it's both simple to understand and useful as a verifiable delay function when run with many rounds",
        "The implementation relies on several key mathematical tools: prime field arithmetic, FFTs for polynomial operations, FRI for low-degree proofs, and Merkle trees for commitments",
        "The STARK construction involves creating polynomial constraints that encode the computation's correctness, then proving these polynomials have the right properties using FRI",
        "While MIMC has relatively low proving overhead (300-400x), typical computations requiring boolean logic and comparisons may have much higher overhead (10,000-50,000x)"
      ],
      "controversial": [
        "The claim that STARKs could make verifying MIMC computations faster than computing them backwards, given the significant proving overhead demonstrated",
        "The assertion that production implementations should use binary fields rather than prime fields for efficiency, despite teaching with prime fields"
      ]
    }
  },
  {
    "id": "general-2018-04-20-radical_markets",
    "title": "On Radical Markets",
    "date": "2018-04-20",
    "category": "society",
    "url": "https://vitalik.eth.limo/general/2018/04/20/radical_markets.html",
    "path": "general/2018/04/20/radical_markets.html",
    "content": "- \n\n- \n\n  \n    \n\n  \n  \n  Dark Mode Toggle\n  \n    \n\n  \n\n##  On Radical Markets \n\n 2018 Apr 20 \nSee all posts\n\n \n \n\n On Radical Markets \n\nRecently I had the fortune to have received an advance copy of Eric\nPosner and Glen Weyl's new book, Radical\nMarkets, which could be best described as an interesting new\nway of looking at the subject that is sometimes called \"political\neconomy\" - tackling the big questions of how markets and politics\nand society intersect. The general philosophy of the book, as I\ninterpret it, can be expressed as follows. Markets are great, and price\nmechanisms are an awesome way of guiding the use of resources in society\nand bringing together many participants' objectives and information into\na coherent whole. However, markets are socially constructed because they\ndepend on property rights that are socially constructed, and there are\nmany different ways that markets and property rights can be constructed,\nsome of which are unexplored and potentially far better than what we\nhave today. Contra doctrinaire libertarians, freedom is a\nhigh-dimensional design space.\n\nThe book interests me for multiple reasons. First, although I spend\nmost of my time in the blockchain/crypto space heading up the Ethereum\nproject and in some cases providing various kinds of support to projects\nin the space, I do also have broader interests, of which the use of\neconomics and mechanism design to make more open, free, egalitarian and\nefficient systems for human cooperation, including improving or\nreplacing present-day corporations and governments, is a major one. The\nintersection of interests between the Ethereum community and Posner and\nWeyl's work is multifaceted and plentiful; Radical Markets\ndedicates an entire chapter to the idea of \"markets for personal data\",\nredefining the economic relationship between ourselves and services like\nFacebook, and well, look what the Ethereum community is working on: markets\nfor\npersonal data.\n\nSecond, blockchains may well be used as a technical backbone for some\nof the solutions described in the book, and Ethereum-style smart\ncontracts are ideal for the kinds of complex systems of property rights\nthat the book explores. Third, the economic ideas and challenges that\nthe book brings up are ideas that have also been explored, and will be\ncontinue to be explored, at great length by the blockchain community for\nits own purposes. Posner and Weyl's ideas often have the feature that\nthey allow economic incentive alignment to serve as a substitute for\nsubjective ad-hoc bureaucracy (eg. Harberger taxes can essentially\nreplace eminent\ndomain), and given that blockchains lack access to trusted\nhuman-controlled courts, these kinds of solutions may prove to be be\neven more ideal for blockchain-based markets than they are for \"real\nlife\".\n\nI will warn that readers are not at all guaranteed to find the book's\nproposals acceptable; at least the first three have already\nbeen highly controversial and they do contravene many people's moral\npreconceptions about how property should and should work and where money\nand markets can and can't be used. The authors are no strangers to\ncontroversy; Posner has on previous occasions even proven\nwilling to argue against such notions as human rights law. That\nsaid, the book does go to considerable lengths to explain why each\nproposal improves efficiency if it could be done, and offer multiple\nversions of each proposal in the hopes that there is at least one (even\nif partial) implementation of each idea that any given reader can find\nagreeable.\nWhat do Posner and Weyl talk\nabout?\n\nThe book is split into five major sections, each arguing for a\nparticular reform: self-assessed property taxes, quadratic voting, a new\nkind of immigration program, breaking up big financial conglomerates\nthat currently make banks and other industries act like monopolies even\nif they appear at first glance to be competitive, and markets for\nselling personal data. Properly summarizing all five sections and doing\nthem justice would take too long, so I will focus on a deep summary of\none specific section, dealing with a new kind of property taxation, to\ngive the reader a feel for the kinds of ideas that the book is\nabout.\n\n## Harberger taxes\n\nSee also: \"Property\nIs Only Another Name for Monopoly\", Posner and Weyl\n\nMarkets and private property are two ideas that are often considered\ntogether, and it is difficult in modern discourse to imagine one without\n(or even with much less of) the other. In the 19th century, however,\nmany economists in Europe were both libertarian and\negalitarian, and it was quite common to appreciate markets while\nmaintaining skepticism toward the excesses of private property. A rather\ninteresting example of this is the Bastiat-Proudhon debate\nfrom 1849-1850 where the two dispute the legitimacy of charging interest\non loans, with one side focusing on the mutual gains from voluntary\ncontracts and the other focusing on their suspicion of the potential for\npeople with capital to get even richer without working, leading to\nunbalanced capital accumulation.\n\nAs it turns out, it is absolutely possible to have a system that\ncontains markets but not property rights: at the end of every year,\ncollect every piece of property, and at the start of the next year have\nthe government auction every piece out to the highest bidder. This kind\nof system is intuitively quite unrealistic and impractical, but it has\nthe benefit that it achieves perfect allocative\nefficiency: every year, every object goes to the person who can\nderive the most value from it (ie. the highest bidder). It also gives\nthe government a large amount of revenue that could be used to\ncompletely substitute income and sales taxes or fund a basic income.\n\nNow you might ask: doesn't the existing property system also achieve\nallocative efficiency? After all, if I have an apple, and I value it at\n$2, and you value it at $3, then you could offer me $2.50 and I would\naccept. However, this fails to take into account imperfect information:\nhow do you know that I value it at $2, and not $2.70? You could offer to\nbuy it for $2.99 so that you can be sure that you'll get it if you\nreally are the one who values the apple more, but then you would be\ngaining practically nothing from the transaction. And if you ask me to\nset the price, how do I know that you value it at $3, and not $2.30? And\nif I set the price to $2.01 to be sure, I would be gaining practically\nnothing from the transaction. Unfortunately, there is a result known as\nthe Myerson-Satterthwaite\nTheorem which means that no solution is efficient; that is,\nany bargaining algorithm in such a situation must at least sometimes\nlead to inefficiency from mutually beneficial deals falling through.\n\nIf there are many buyers you have to negotiate with, things get even\nharder. If a developer (in the real estate sense) is trying to make a\nlarge project that requires buying 100 existing properties, and 99 have\nalready agreed, the remaining one has a strong incentive to charge a\nvery high price, much higher than their actual personal valuation of the\nproperty, hoping that the developer will have no choice but to pay\nup.\n\nWell, not necessarily no choice. But a very inconvenient and\nboth privately and socially wasteful choice.\n\nRe-auctioning everything once a year completely solves this problem\nof allocative efficiency, but at a very high cost to investment\nefficiency: there's no point in building a house in the first\nplace if six months later it will get taken away from you and re-sold in\nan auction. All property taxes have this problem; if building a house\ncosts you $90 and brings you $100 of benefit, but then you have to pay\n$15 more property tax if you build the house, then you will not build\nthe house and that $10 gain is lost to society.\n\nOne of the more interesting ideas from the 19th century economists,\nand specifically Henry George, was a kind of property tax that did not\nhave this problem: the land value tax.\nThe idea is to charge tax on the value of land, but not the\nimprovements to the land; if you own a $100,000 plot of dirt\nyou would have to pay $5,000 per year taxes on it regardless of whether\nyou used the land to build a condominium or simply as a place to walk\nyour pet doge.\n\nA doge.\n\nWeyl and Posner are not convinced that Georgian land taxes are viable\nin practice:\n\nConsider, for example, the Empire State Building. What is the pure\nvalue of the land beneath it? One could try to infer its value by\ncomparing it to the value of adjoining land. But the building itself\ndefines the neighborhood around it; removing the building would almost\ncertainly change the value of its surrounding land. The land and the\nbuilding, even the neighborhood, are so tied together, it would be hard\nto figure out a separate value for each of them.\n\nArguably this does not exclude the possibility of a different kind of\nGeorgian-style land tax: a tax based on the average of property\nvalues across a sufficiently large area. That would preserve the\nproperty that improving a single piece of land would not (greatly)\nperversely increase the taxes that they have to pay, without having to\nfind a way to distinguish land from improvements in an absolute sense.\nBut in any case, Posner and Weyl move on to their main proposal:\nself-assessed property taxes.\n\nConsider a system where property owners themselves specify what the\nvalue of their property is, and pay a tax rate of, say, 2% of that value\nper year. But here is the twist: whatever value they specify for their\nproperty, they have to be willing to sell it to anyone at that\nprice.\n\nIf the tax rate is equal to the chance per year that the property\ngets sold, then this achieves optimal allocative efficiency: raising\nyour self-assessed property value by $1 increases the tax you pay by\n$0.02, but it also means there is a 2% chance that someone will buy the\nproperty and pay $1 more, so there is no incentive to cheat in either\ndirection. It does harm investment efficiency, but vastly less so than\nall property being re-auctioned every year.\n\nPosner and Weyl then point out that if more investment efficiency is\ndesired, a hybrid solution with a lower property tax is possible:\n\nWhen the tax is reduced incrementally to improve investment\nefficiency, the loss in allocative efficiency is less than the gain in\ninvestment efficiency. The reason is that the most valuable sales are\nones where the buyer is willing to pay significantly more than the\nseller is willing to accept. These transactions are the first ones\nenabled by a reduction in the price as even a small price reduction will\navoid blocking these most valuable transactions. In fact, it can be\nshown that the size of the social loss from monopoly power grows\nquadratically in the extent of this power. Thus, reducing the markup by\na third eliminates close to \\(\\frac{5}{9} =\n(3^2-2^2)/(3^2\\)) of the allocative harm from private\nownership.\n\nThis concept of quadratic deadweight loss is a truly important\ninsight in economics, and is arguably the deep reason why \"moderation in\nall things\" is such an attractive principle: the first step you take\naway from an extreme will generally be the most valuable.\n\nThe book then proceeds to give a series of side benefits that this\ntax would have, as well as some downsides. One interesting side benefit\nis that it removes an information asymmetry flaw that exists with\nproperty sales today, where owners have the incentive to expend effort\non making their property look good even in potentially misleading ways.\nWith a properly set Harberger tax, if you somehow mange to trick the\nworld into thinking your house is 5% more valuable, you'll get 5% more\nwhen you sell it but until that point you'll have to pay 5% more in\ntaxes, or else someone will much more quickly snap it up from you at the\noriginal price.\n\nThe downsides are smaller than they seem; for example, one natural\ndisadvantage is that it exposes property owners to uncertainty due to\nthe possibility that someone will snap up their property at any time,\nbut that is hardly an unknown as it's a risk that renters already face\nevery day. But Weyl and Posner do propose more moderate ways of\nintroducing the tax that don't have these issues. First, the tax can be\napplied to types of property that are currently government owned; it's a\npotentially superior alternative to both continued government ownership\nand traditional full-on privatization. Second, the tax can be\napplied to forms of property that are already \"industrial\" in usage:\nradio spectrum licenses, domain names, intellectual property, etc.\n\n## The Rest of the Book\n\nThe remaining chapters bring up similar ideas that are similar in\nspirit to the discussion on Harberger taxes in their use of modern\ngame-theoretic principles to make mathematically optimized versions of\nexisting social institutions. One of the proposals is for something\ncalled quadratic voting, which I summarize as follows.\n\nSuppose that you can vote as many times as you want, but voting costs\n\"voting tokens\" (say each citizen is assigned \\(N\\) voting tokens per year), and it costs\ntokens in a nonlinear way: your first vote costs one token, your second\nvote costs two tokens, and so forth. If someone feels more strongly\nabout something, the argument goes, they would be willing to pay more\nfor a single vote; quadratic voting takes advantage of this by perfectly\naligning quantity of votes with cost of votes: if\nyou're willing to pay up to 15 tokens for a vote, then you will keep\nbuying votes until your last one costs 15 tokens, and so you will cast\n15 votes in total. If you're willing to pay up to 30 tokens for a vote,\nthen you will keep buying votes until you can't buy any more for a price\nless than or equal to 30 tokens, and so you will end up casting 30\nvotes. The voting is \"quadratic\" because the total amount you pay for\n\\(N\\) votes goes up proportionately to\n\\(N^2\\).\n\nAfter this, the book describes a market for immigration visas that\ncould greatly expand the number of immigrants admitted while making sure\nlocal residents benefit and at the same time aligning incentives to\nencourage visa sponsors to choose immigrants that are more ikely to\nsucceed in the country and less likely to commit crimes, then an\nenhancement to antitrust law, and finally the idea of setting up markets\nfor personal data.\n\n## Markets in Everything\n\nThere are plenty of ways that one could respond to each individual\nproposal made in the book. I personally, for example, find the\nimmigration visa scheme that Posner and Weyl propose well-intentioned\nand see how it could improve on the status quo, but also\novercomplicated, and it seems simpler to me to have a scheme where visas\nare auctioned or sold every year, with an additional requirement for\nmigrants to obtain liability insurance. Robin Hanson recently proposed\ngreatly expanding liability insurance mandates as an alternative to many\nkinds of regulation, and while imposing new mandates on an entire\nsociety seems unrealistic, a new expanded immigration program seems like\nthe perfect place to start considering them. Paying people for personal\ndata is interesting, but there are concerns about adverse selection: to\nput it politely, the kinds of people that are willing to sit around\nsubmitting lots of data to Facebook all year to earn $16.92 (Facebook's\ncurrent annualized\nrevenue per user) are not the kinds of people that\nadvertisers are willing to burn hundreds of dollars per person trying to\nmarket rolexes and Lambos to. However, what I find more interesting is\nthe general principle that the book tries to promote.\n\nOver the last hundred years, there truly has been a large amount of\nresearch into designing economic mechanisms that have desirable\nproperties and that outperform simple two-sided buy-and-sell markets.\nSome of this research has been put into use in some specific industries;\nfor example, combinatorial\nauctions are used in airports, radio spectrum auctions and several\nother industrial use cases, but it hasn't really seeped into any kind of\nbroader policy design; the political systems and property rights that we\nhave are still largely the same as we had two centuries ago. So can we\nuse modern economic insights to reform base-layer markets and politics\nin such a deep way, and if so, should we?\n\nNormally, I love markets and clean incentive alignment, and dislike\npolitics and bureaucrats and ugly hacks, and I love economics, and I so\nlove the idea of using economic insights to design markets that work\nbetter so that we can reduce the role of politics and bureaucrats and\nugly hacks in society. Hence, naturally, I love this vision. So let me\nbe a good intellectual citizen and do my best to try to make a case\nagainst it.\n\nThere is a limit to how complex economic incentive structures and\nmarkets can be because there is a limit to users' ability to think and\nre-evaluate and give ongoing precise measurements for their valuations\nof things, and people value reliability and certainty. Quoting Steve Waldman\ncriticizing Uber surge pricing:\n\nFinally, we need to consider questions of economic calculation. In\nmacroeconomics, we sometimes face tradeoffs between an increasing and\nunpredictably variable price-level and full employment. Wisely or not,\nour current policy is to stabilize the price level, even at short-term\ncost to output and employment, because stable prices enable longer-term\neconomic calculation. That vague good, not visible on a supply/demand\ndiagram, is deemed worth very large sacrifices. The same concern exists\nin a microeconomic context. If the \"ride-sharing revolution\" really\ntakes hold, a lot of us will have decisions to make about whether to own\na car or rely upon the Sidecars, Lyfts, and Ubers of the world to take\nus to work every day. To make those calculations, we will need something\nlike predictable pricing. Commuting to our minimum wage jobs (average is\nover!) by Uber may be OK at standard pricing, but not so OK on a surge.\nIn the desperate utopia of the \"free-market economist\", there is always\na solution to this problem. We can define futures markets on Uber trips,\nand so hedge our exposure to price volatility! In practice that is not\nso likely...\n\nAnd:\n\nIt's clear that in a lot of contexts, people have a strong preference\nfor price-predictability over immediate access. The vast majority of\nservices that we purchase and consume are not price-rationed in any\nfine-grained way. If your hairdresser or auto mechanic is busy, you get\npenciled in for next week...\n\nStrong property rights are valuable for the same reason: beyond the\narguments about allocative and investment efficiency, they provide the\nmental convenience and planning benefits of predictability.\n\nIt's worth noting that even Uber itself doesn't do surge pricing in\nthe \"market-based\" way that economists would recommend. Uber is not a\nmarket where drivers can set their own prices, riders can see what\nprices are available, and themselves choose their tradeoff between price\nand waiting time. Why does Uber not do this? One argument is that, as\nSteve Waldman says, \"Uber itself is a cartel\", and wants to have the\npower to adjust market prices not just for efficiency but also reasons\nsuch as profit maximization, strategically setting prices to drive out\ncompeting platforms (and taxis and public transit), and public\nrelations. As Waldman further points out, one Uber competitor, Sidecar,\ndoes have the ability for drivers to set prices, and I\nwould add that I have seen ride-sharing apps in China where\npassengers can offer drivers higher prices to try to coax them\nto get a car faster.\n\nA possible counter-argument that Uber might give is that drivers\nthemselves are actually less good at setting optimal prices than Uber's\nown algorithms, and in general people value the convenience of one-click\ninterfaces over the mental complexity of thinking about prices. If we\nassume that Uber won its market dominance over competitors like Sidecar\nfairly, then the market itself has decided that the economic gain from\nmarketizing more things is not worth the mental transaction costs.\n\nHarberger taxes, at least to me, seem like they would lead to these\nexact kinds of issues multipled by ten; people are not experts at\nproperty valuation, and would have to spend a significant amount of time\nand mental effort figuring out what self-assessed value to put for their\nhouse, and they would complain much more if they accidentally put a\nvalue that's too low and suddenly find that their house is gone. If\nHarberger taxes were to be applied to smaller property items as well,\npeople would need to juggle a large amount of mental valuations of\neverything. A similar critique could apply to many kinds of personal\ndata markets, and possibly even to quadratic voting if implemented in\nits full form.\n\nI could challenge this by saying \"ah, even if that's true, this is\nthe 21st century, we could have companies that build AIs that make\npricing decisions on your behalf, and people could choose the AI that\nseems to work best; there could even be a public option\"; and Posner and\nWeyl themselves suggest that this is likely the way to go. And this is\nwhere the interesting conversation starts.\n\n## Tales from Crypto Land\n\nOne reason why this discussion particularly interests me is that the\ncryptocurrency and blockchain space itself has, in some cases, run up\nagainst similar challenges. In the case of Harberger taxes, we actually\ndid consider almost exactly that same proposal in the context of the Ethereum Name System (our decentralized\nalternative to DNS), but the proposal was ultimately rejected. I asked\nthe ENS developers why it was rejected. Paraphrasing their reply, the\nchallenge is as follows.\n\nMany ENS domain names are of a type that would only be interesting to\nprecisely two classes of actors: (i) the \"legitimate owner\" of some\ngiven name, and (ii) scammers. Furthermore, in some particular cases,\nthe legitimate owner is uniquely underfunded, and scammers are uniquely\ndangerous. One particular case is MyEtherWallet, an Ethereum wallet\nprovider. MyEtherWallet provides an important public good to the\nEthereum ecosystem, making Ethereum easier to use for many thousands of\npeople, but is able to capture only a very small portion of the value\nthat it provides; as a result, the budget that it has for outbidding\nothers for the domain name is low. If a scammer gets their hands on the\ndomain, users trusting MyEtherWallet could easily be tricked into\nsending all of their ether (or other Ethereum assets) to a scammer.\nHence, because there is generally one clear \"legitimate owner\" for any\ndomain name, a pure property rights regime presents little allocative\nefficiency loss, and there is a strong overriding public interest toward\nstability of reference (ie. a domain that's legitimate one day doesn't\nredirect to a scam the next day), so any level of Harberger\ntaxation may well bring more harm than good.\n\nI suggested to the ENS developers the idea of applying Harberger\ntaxes to short domains (eg. abc.eth), but not long ones; the reply was\nthat it would be too complicated to have two classes of names. That\nsaid, perhaps there is some version of the proposal that could satisfy\nthe specific constraints here; I would be interested to hear Posner and\nWeyl's feedback on this particular application.\n\nAnother story from the blockchain and Ethereum space that has a more\npro-radical-market conclusion is that of transaction fees. The notion of\nmental\ntransaction costs, the idea that the inconvenience of even thinking\nabout whether or not some small payment for a given digital good is\nworth it is enough of a burden to prevent \"micro-markets\" from working,\nis often used as an argument for why mass adoption of blockchain tech\nwould be difficult: every transaction requires a small fee, and the\nmental expenditure of figuring out what fee to pay is itself a major\nusability barrier. These arguments increased further at the end of last\nyear, when both Bitcoin\nand Ethereum\ntransaction fees briefly spiked up by a factor of over 100 due to high\nusage (talk about surge pricing!), and those who accidentally did not\npay high enough fees saw their transactions get stuck for days.\n\nThat said, this is a problem that we have now, arguably, to a large\nextent overcome. After the spikes at the end of last year, Ethereum\nwallets developed more advanced algorithms for choosing what transaction\nfees to pay to ensure that one's transaction gets included in the chain,\nand today most users are happy to simply defer to them. In my own\npersonal experience, the mental transaction costs of worrying about\ntransaction fees do not really exist, much like a driver of a car does\nnot worry about the gasoline consumed by every single turn, acceleration\nand braking made by their car.\n\nPersonal price-setting AIs for interacting with open markets:\nalready a reality in the Ethereum transaction fee market\n\nA third kind of \"radical market\" that we are considering implementing\nin the context of Ethereum's consensus system is one for incentivizing\ndeconcentration of validator nodes in proof\nof stake consensus. It's important for blockchains to be\ndecentralized, a similar challenge to what antitrust law tries to solve,\nbut the tools at our disposal are different. Posner and Weyl's solution\nto antitrust, banning institutional investment funds from owning shares\nin multiple competitors in the same industry, is far too subjective and\nhuman-judgement-dependent to work in a blockchain, but for our specific\ncontext we have a different solution: if a validator node commits an\nerror, it gets penalized an amount proportional to the number of other\nnodes that have committed an error around the same time. This\nincentivizes nodes to set themselves up in such a way that their failure\nrate is maximally uncorrelated with everyone else's failure rate,\nreducing the chance that many nodes fail at the same time and threaten\nto the blockchain's integrity. I want to ask Posner and Weyl: though our\nexact approach is fairly application-specific, could a similarly elegant\n\"market-based\" solution be discovered to incentivize market\ndeconcentration in general?\n\nAll in all, I am optimistic that the various behavioral kinks around\nimplementing \"radical markets\" in practice could be worked out with the\nhelp of good defaults and personal AIs, though I do think that if this\nvision is to be pushed forward, the greatest challenge will be finding\nprogressively larger and more meaningful places to test it out and show\nthat the model works. I particularly welcome the use of the blockchain\nand crypto space as a testing ground.\nAnother Kind of Radical\nMarket\n\nThe book as a whole tends to focus on centralized reforms that could\nbe implemented on an economy from the top down, even if their intended\nlong-term effect is to push more decision-making power to individuals.\nThe proposals involve large-scale restructurings of how property rights\nwork, how voting works, how immigration and antitrust law works, and how\nindividuals see their relationship with property, money, prices and\nsociety. But there is also the potential to use economics and game\ntheory to come up with decentralized economic institutions that\ncould be adopted by smaller groups of people at a time.\n\nPerhaps the most famous examples of decentralized institutions from\ngame theory and economics land are (i) assurance contracts, and (ii)\nprediction markets.\u00a0An assurance contract is a system where some public\ngood is funded by giving anyone the opportunity to pledge money, and\nonly collecting the pledges if the total amount pledged exceeds some\nthreshold. This ensures that people can donate money knowing that either\nthey will get their money back or there actually will be enough to\nachieve some objective. A possible extension of this concept is Alex\nTabarrok's dominant\nassurance contracts, where an entrepreneur offers to refund\nparticipants more than 100% of their deposits if a given\nassurance contract does not raise enough money.\n\nPrediction markets allow people to bet on the probability that events\nwill happen, potentially even conditional on some action being taken (\"I\nbet $20 that unemployment will go down if candidate X wins the\nelection\"); there are techniques for people interested in the\ninformation to subsidize the markets. Any attempt to manipulate the\nprobability that a prediction market shows simply creates an opportunity\nfor people to earn free money (yes I know, risk aversion and capital\nefficiency etc etc; still close to free) by betting against the\nmanipulator.\n\nPosner and Weyl do give one example of what I would call a\ndecentralized institution: a game for choosing who gets an asset in the\nevent of a divorce or a company splitting in half, where both sides\nprovide their own valuation, the person with the higher valuation gets\nthe item, but they must then give an amount equal to half the average of\nthe two valuations to the loser. There's some economic reasoning by\nwhich this solution, while not perfect, is still close to mathematically\noptimal.\n\nOne particular category of decentralized institutions I've been\ninterested in is improving incentivization for content posting and\ncontent curation in social media. Some ideas that I have had\ninclude:\n\n- Proof\nof stake conditional hashcash (when you send someone an email, you\ngive them the opportunity to burn $0.5 of your money if they think it's\nspam)\n\n- Prediction\nmarkets for content curation (use prediction markets to predict the\nresults of a moderation vote on content, thereby encouraging a market of\nfast content pre-moderators while penalizing manipulative\npre-moderation)\n\n- Conditional payments for paywalled content (after you pay for a\npiece of downloadable content and view it, you can decide after the fact\nif payments should go to the author or to proportionately refund\nprevious readers)\n\nAnd ideas I have had in other contexts:\n\n- Call-out\nassurance contracts\n\n- DAICOs (a\nmore decentralized and safer alternative to ICOs)\n\nTwitter scammers: can prediction markets incentivize an\nautonomous swarm of human and AI-driven moderators to flag these posts\nand warn users not to send them ether within a few seconds of the post\nbeing made? And could such a system be generalized to the entire\ninternet, where these is no single centralized moderator that can easily\ntake posts down?\n\nSome ideas others have had for decentralized institutions in general\ninclude:\n\n- TrustDavis (adding\nskin-in-the-game to e-commerce reputations by making e-commerce ratings\nbe offers to insure others against the receiver of the rating\ncommitting fraud)\n\n- Circles (decentralized basic\nincome through locally fungible coin issuance)\n\n- Markets for CAPTCHA services\n\n- Digitized peer to peer rotating savings and credit associations\n\n- Token\ncurated registries\n\n- Crowdsourced\nsmart contract truth oracles\n\n- Using blockchain-based smart contracts to coordinate unions\n\nI would be interested in hearing Posner and Weyl's opinion on these\nkinds of \"radical markets\", that groups of people can spin up and start\nusing by themselves without requiring potentially contentious\nsociety-wide changes to political and property rights. Could\ndecentralized institutions like these be used to solve the key defining\nchallenges of the twenty first century: promoting beneficial scientific\nprogress, developing informational public goods, reducing global wealth\ninequality, and the big meta-problem behind fake news, government-driven\nand corporate-driven social media censorship, and regulation of\ncryptocurrency products: how do we do quality assurance in an open\nsociety?\n\nAll in all, I highly recommend Radical Markets (and by the\nway I also recommend Eliezer Yudkowsky's Inadequate Equilibria) to\nanyone interested in these kinds of issues, and look forward to seeing\nthe discussion that the book generates.",
    "contentLength": 31883,
    "summary": "Posner and Weyl's \"Radical Markets\" proposes redesigning property rights through mechanisms like self-assessed taxes with forced sales.",
    "detailedSummary": {
      "theme": "Vitalik analyzes Eric Posner and Glen Weyl's book 'Radical Markets,' exploring how modern economic mechanisms could reform property rights, voting systems, and social institutions to create more efficient markets while examining their practical implementation challenges.",
      "summary": "Vitalik examines 'Radical Markets' by Posner and Weyl, which proposes using advanced economic mechanisms to redesign fundamental social institutions. The book's core philosophy is that while markets are excellent coordination tools, property rights are socially constructed and could be redesigned for better outcomes. Vitalik focuses particularly on Harberger taxes - a system where property owners self-assess their property value, pay taxes on that assessment, but must sell to anyone at that price, potentially solving allocative efficiency problems while maintaining investment incentives. He also discusses other proposals like quadratic voting, immigration visa markets, and personal data markets. While appreciating the mathematical elegance of these solutions, Vitalik raises important concerns about complexity and mental transaction costs, noting that people value predictability and simplicity. He draws parallels to blockchain experiences, where similar economic mechanisms have been both rejected (ENS domain names) and successfully implemented (transaction fee algorithms, validator incentives). Vitalik concludes by advocating for decentralized institutions that smaller groups could adopt voluntarily, rather than top-down societal reforms, and suggests the crypto space as an ideal testing ground for these radical market concepts.",
      "takeaways": [
        "Markets and property rights are socially constructed and can be redesigned - freedom exists in a 'high-dimensional design space' rather than simple private property vs. government control",
        "Harberger taxes could solve allocative efficiency problems by having property owners self-assess values and pay taxes on them while being required to sell at that price to anyone",
        "Mental transaction costs and people's preference for predictability may limit the practical implementation of complex economic mechanisms",
        "The blockchain/crypto space serves as a valuable testing ground for radical market mechanisms, with mixed results from actual implementation attempts",
        "Decentralized institutions that people can adopt voluntarily may be more promising than top-down societal reforms for implementing these economic innovations"
      ],
      "controversial": [
        "The proposal that people should have to sell their property to anyone willing to pay their self-assessed value challenges fundamental notions of property ownership and security",
        "Quadratic voting systems where people can cast multiple votes by paying increasing costs could be seen as allowing wealthy individuals to have disproportionate political influence",
        "Immigration visa markets that commodify human movement and settlement rights may raise ethical concerns about treating migration as a purely economic transaction"
      ]
    }
  },
  {
    "id": "general-2018-03-28-plutocracy",
    "title": "Governance, Part 2: Plutocracy Is Still Bad",
    "date": "2018-03-28",
    "category": "governance",
    "url": "https://vitalik.eth.limo/general/2018/03/28/plutocracy.html",
    "path": "general/2018/03/28/plutocracy.html",
    "content": "- \n\n- \n\n  \n    \n\n  \n  \n  Dark Mode Toggle\n  \n    \n\n  \n\n##  Governance, Part 2: Plutocracy Is Still Bad \n\n 2018 Mar 28 \nSee all posts\n\n \n \n\n Governance, Part 2: Plutocracy Is Still Bad \n\nCoin holder voting, both for governance of technical features, and\nfor more extensive use cases like deciding who runs validator nodes and\nwho receives money from development bounty funds, is unfortunately\ncontinuing to be popular, and so it seems worthwhile for me to write\nanother post explaining why I (and Vlad\nZamfir and others) do not consider it wise for Ethereum (or really,\nany base-layer blockchain) to start adopting these kinds of mechanisms\nin a tightly coupled form in any significant way.\n\nI wrote about the issues with tightly coupled voting in a blog post last year,\nthat focused on theoretical issues as well as focusing on some practical\nissues experienced by voting systems over the previous two years. Now,\nthe latest scandal in DPOS land seems to be substantially worse. Because\nthe delegate rewards in EOS are now so high (5% annual inflation, about\n$400m per year), the competition on who gets to run nodes has\nessentially become yet another frontier of US-China geopolitical\neconomic warfare.\n\nAnd that's not my own interpretation; I quote from this article (original in\nChinese):\n\nEOS supernode voting: multibillion-dollar profits leading to\ncrypto community inter-country warfare\n\nLooking at community recognition, Chinese nodes feel much less\nrepresented in the community than US and Korea. Since the EOS.IO\nofficial Twitter account was founded, there has never been any\ninteraction with the mainland Chinese EOS community. For a listing of\nthe EOS officially promoted events and interactions with communities see\nthe picture below.\n\nWith no support from the developer community, facing competition from\nKorea, the Chinese EOS supernodes have invented a new strategy: buying\nvotes.\n\nThe article then continues to describe further strategies, like\nforming \"alliances\" that all vote (or buy votes) for each other.\n\nOf course, it does not matter at all who the specific actors are that\nare buying votes or forming cartels; this time it's some Chinese pools,\nlast time it was \"members located in\nthe USA, Russia, India, Germany, Canada, Italy, Portugal and many other\ncountries from around the globe\", next time it could be totally\nanonymous, or run out of a smartphone snuck into Trendon Shavers's\nprison cell. What matters is that blockchains and cryptocurrency,\noriginally founded in a vision of using technology to escape from the\nfailures of human politics, have essentially all but replicated it.\nCrypto is a reflection of the world at large.\n\nThe EOS New York community's response seems to be that they have\nissued a strongly worded letter to the world stating that buying\nvotes will be against the constitution. Hmm, what other major\npolitical entity has made accepting\nbribes a violation of the constitution? And how has that been going\nfor them lately?\n\nThe second part of this article will involve me, an armchair\neconomist, hopefully convincing you, the reader, that yes, bribery is,\nin fact, bad. There are actually people who dispute this claim; the\nusual argument has something to do with market efficiency, as in \"isn't\nthis good, because it means that the nodes that win will be the nodes\nthat can be the cheapest, taking the least money for themselves and\ntheir expenses and giving the rest back to the community?\" The answer\nis, kinda yes, but in a way that's centralizing and vulnerable to\nrent-seeking cartels and explicitly contradicts many of the explicit\npromises made by most DPOS proponents along the way.\n\nLet us create a toy economic model as follows. There are a number of\npeople all of which are running to be delegates. The delegate slot gives\na reward of $100 per period, and candidates promise to share some\nportion of that as a bribe, equally split among all of their voters. The\nactual \\(N\\) delegates (eg. \\(N = 35\\)) in any period are the \\(N\\) delegates that received the most votes;\nthat is, during every period a threshold of votes emerges where if you\nget more votes than that threshold you are a delegate, if you get less\nyou are not, and the threshold is set so that \\(N\\) delegates are above the threshold.\n\nWe expect that voters vote for the candidate that gives them the\nhighest expected bribe. Suppose that all candidates start off by sharing\n1%; that is, equally splitting $1 among all of their voters. Then, if\nsome candidate becomes a delegate with \\(K\\) voters, each voter gets a payment of\n\\(\\frac{1}{K}\\). The candidate that\nit's most profitable to vote for is a candidate that's expected to be in\nthe top \\(N\\), but is expected to earn\nthe fewest votes within that set. Thus, we expect votes to be fairly\nevenly split among 35 delegates.\n\nNow, some candidates will want to secure their position by sharing\nmore; by sharing 2%, you are likely to get twice as many votes as those\nthat share 1%, as that's the equilibrium point where voting for you has\nthe same payout as voting for anyone else. The extra guarantee of being\nelected that this gives is definitely worth losing an additional 1% of\nyour revenue when you do get elected. We can expect delegates to bid up\ntheir bribes and eventually share something close to 100% of their\nrevenue. So the outcome seems to be that the delegate payouts are\nlargely simply returned to voters, making the delegate payout mechanism\nclose to meaningless.\n\nBut it gets worse. At this point, there's an incentive for delegates\nto form alliances (aka political parties, aka cartels) to coordinate\ntheir share percentages; this reduces losses to the cartel from chaotic\ncompetition that accidentally leads to some delegates not getting enough\nvotes. Once a cartel is in place, it can start bringing its share\npercentages down, as dislodging it is a hard coordination problem: if a\ncartel offers 80%, then a new entrant offers 90%, then to a voter,\nseeking a share of that extra 10% is not worth the risk of either (i)\nvoting for someone who gets insufficient votes and does not pay rewards,\nor (ii) voting for someone who gets too many votes and so pays out a\nreward that's excessively diluted.\n\nSidenote:\nBitshares\nDPOS used approval voting, where you can vote for as many candidates\nas you want; it should be pretty obvious that with even slight bribery,\nthe equilibrium there is that everyone just votes for\neveryone.\n\nFurthermore, even if cartel mechanics don't come into play,\nthere is a further issue. This equilibrium of coin holders voting for\nwhoever gives them the most bribes, or a cartel that has become an\nentrenched rent seeker, contradicts explicit promises made by DPOS\nproponents.\n\nQuoting \"Explain\nDelegated Proof of Stake Like I'm 5\":\n\nIf a Witness starts acting like an asshole, or stops doing a quality\njob securing the network, people in the community can remove their\nvotes, essentially firing the bad actor. Voting is always ongoing.\n\nFrom \"EOS:\nAn Introduction\":\n\nBy custom, we suggest that the bulk of the value be returned to the\ncommunity for the common good - software improvements, dispute\nresolution, and the like can be entertained. In the spirit of \"eating\nour own dogfood,\" the design envisages that the community votes on a set\nof open entry contracts that act like \"foundations\" for the benefit of\nthe community. Known as Community Benefit Contracts, the mechanism\nhighlights the importance of DPOS as enabling direct on-chain governance\nby the community (below).\n\nThe flaw in all of this, of course, is that the average voter has\nonly a very small chance of impacting which delegates get selected, and\nso they only have a very small incentive to vote based on any of these\nhigh-minded and lofty goals; rather, their incentive is to vote for\nwhoever offers the highest and most reliable bribe. Attacking is easy.\nIf a cartel equilibrium does not form, then an attacker can simply offer\na share percentage slightly higher than 100% (perhaps using fee sharing\nor some kind of \"starter promotion\" as justification), capture the\nmajority of delegate positions, and then start an attack. If they get\nremoved from the delegate position via a hard fork, they can simply\nrestart the attack again with a different identity.\n\nThe above is not intended purely as a criticism of DPOS consensus or\nits use in any specific blockchain. Rather, the critique reaches much\nfurther. There has been a large number of projects recently that extol\nthe virtues of extensive on-chain governance, where on-chain coin holder\nvoting can be used not just to vote on protocol features, but also to\ncontrol a bounty fund. Quoting a blog\npost from last year:\n\nAnyone can submit a change to the governance structure in the form of\na code update. An on-chain vote occurs, and if passed, the update makes\nits way on to a test network. After a period of time on the test\nnetwork, a confirmation vote occurs, at which point the change goes live\non the main network. They call this concept a \"self-amending ledger\".\nSuch a system is interesting because it shifts power towards users and\naway from the more centralized group of developers and miners. On the\ndeveloper side, anyone can submit a change, and most importantly,\neveryone has an economic incentive to do it. Contributions are rewarded\nby the community with newly minted tokens through inflation funding.\nThis shifts from the current Bitcoin and Ethereum dynamics where a new\ndeveloper has little incentive to evolve the protocol, thus power tends\nto concentrate amongst the existing developers, to one where everyone\nhas equal earning power.\n\nIn practice, of course, what this can easily lead to is funds that\noffer kickbacks to users who vote for them, leading to the exact\nscenario that we saw above with DPOS delegates. In the best case, the\nfunds will simply be returned to voters, giving coin holders an interest\nrate that cancels out the inflation, and in the worst case, some portion\nof the inflation will get captured as economic rent by a cartel.\n\nNote also that the above is not a criticism of all on-chain\nvoting; it does not rule out systems like futarchy. However, futarchy is\nuntested, but coin voting is tested, and so far it seems to\nlead to a high risk of economic or political failure of some kind - far\ntoo high a risk for a platform that seeks to be an economic base layer\nfor development of decentralized applications and institutions.\n\nSo what's the alternative? The answer is what we've been saying all\nalong: cryptoeconomics. Cryptoeconomics\nis fundamentally about the use of economic incentives together with\ncryptography to design and secure different kinds of systems and\napplications, including consensus protocols. The goal is simple: to be\nable to measure the security of a system (that is, the cost of breaking\nthe system or causing it to violate certain guarantees) in dollars.\nTraditionally, the security of systems often depends on social\ntrust assumptions: the system works if 2 of 3 of Alice, Bob and Charlie\nare honest, and we trust Alice, Bob and Charlie to be honest because I\nknow Alice and she's a nice girl, Bob registered with FINCEN and has a\nmoney transmitter license, and Charlie has run a successful business for\nthree years and wears a suit.\n\nSocial trust assumptions can work well in many contexts, but they are\ndifficult to universalize; what is trusted in one country or one company\nor one political tribe may not be trusted in others. They are also\ndifficult to quantify; how much money does it take to manipulate social\nmedia to favor some particular delegate in a vote? Social trust\nassumptions seem secure and controllable, in the sense that \"people\" are\nin charge, but in reality they can be manipulated by economic incentives\nin all sorts of ways.\n\nCryptoeconomics is about trying to reduce social trust assumptions by\ncreating systems where we introduce explicit economic incentives for\ngood behavior and economic penalties for bad behavior, and making\nmathematical proofs of the form \"in order for guarantee \\(X\\) to be violated, at least these people\nneed to misbehave in this way, which means the minimum amount of\npenalties or foregone revenue that the participants suffer is \\(Y\\)\". Casper is designed\nto accomplish precisely this objective in the context of proof of stake\nconsensus. Yes, this does mean that you can't create a \"blockchain\" by\nconcentrating the consensus validation into 20 uber-powerful\n\"supernodes\" and you have to actually\nthink to make a design that intelligently breaks through and\nnavigates existing tradeoffs and achieves massive scalability in a\nstill-decentralized network. But the reward is that you don't get a\nnetwork that's constantly liable to breaking in half or becoming\neconomically captured by unpredictable political forces.\n\n- \nIt has been brought to my attention that EOS may be reducing\nits delegate rewards from 5% per year to 1% per year. Needless to say,\nthis doesn't really change the fundamental validity of any of the\narguments; the only result of this would be 5x less rent extraction\npotential at the cost of a 5x reduction to the cost of attacking the\nsystem.\n\n- \nSome have asked: but how can it be wrong for DPOS delegates to\nbribe voters, when it is perfectly legitimate for mining and stake pools\nto give 99% of their revenues back to their participants? The answer\nshould be clear: in PoW and PoS, it's the protocol's role to determine\nthe rewards that miners and validators get, based on the miners and\nvalidators' observed performance, and the fact that miners and\nvalidators that are pools pass along the rewards (and penalties!) to\ntheir participants gives the participants an incentive to participate in\ngood pools. In DPOS, the reward is constant, and it's the voters' role\nto vote for pools that have good performance, but with the key flaw that\nthere is no mechanism to actually encourage voters to vote in that way\ninstead of just voting for whoever gives them the most money without\ntaking performance into account. Penalties in DPOS do not exist, and are\ncertainly not passed on to voters, so voters have no \"skin in the game\"\n(penalties in Casper pools, on the other hand, do get passed on\nto participants).",
    "contentLength": 14191,
    "summary": "EOS's delegated proof-of-stake governance devolved into vote-buying and geopolitical warfare, proving coin voting replicates political failures.",
    "detailedSummary": {
      "theme": "Vitalik argues that coin holder voting systems in blockchain governance inevitably lead to plutocracy, bribery, and cartel formation, undermining the democratic ideals they claim to support.",
      "summary": "Vitalik presents a comprehensive critique of Delegated Proof of Stake (DPOS) and coin holder voting systems, using EOS as a primary example where high delegate rewards have led to vote buying and geopolitical warfare between Chinese and US/Korean nodes. He demonstrates through economic modeling how rational actors in such systems will inevitably engage in bribery competition, eventually forming cartels that extract rent while contradicting the stated goals of community governance and network security. Vitalik argues that these systems fail because individual voters have minimal impact on outcomes but strong incentives to vote for whoever offers the highest bribes, rather than considering delegates' actual performance or contributions to the network. As an alternative, Vitalik advocates for cryptoeconomics - systems that rely on explicit economic incentives and penalties backed by mathematical proofs rather than social trust assumptions. He suggests that while coin voting systems appear democratic, they are actually vulnerable to economic capture and manipulation, making them unsuitable for base-layer blockchain governance where security and decentralization are paramount.",
      "takeaways": [
        "Coin holder voting systems inevitably lead to bribery and vote buying because rational voters will choose delegates offering the highest financial returns rather than best network performance",
        "High delegate rewards in systems like EOS create incentives for cartel formation and rent-seeking behavior that contradicts the democratic ideals these systems claim to represent",
        "DPOS and similar governance mechanisms are vulnerable to economic capture by wealthy actors who can manipulate outcomes through financial incentives rather than merit-based selection",
        "Cryptoeconomics offers a superior alternative by creating explicit economic penalties and rewards that can be mathematically quantified, reducing reliance on social trust assumptions",
        "The problems with plutocratic governance extend beyond DPOS to any system using coin holder voting for protocol decisions or fund allocation, making them unsuitable for base-layer blockchain infrastructure"
      ],
      "controversial": [
        "Vitalik's strong stance against coin holder voting contradicts many projects that position such systems as more democratic than traditional governance",
        "The dismissal of market efficiency arguments for vote buying may be contested by those who believe competitive bribing leads to optimal resource allocation",
        "The claim that cryptoeconomics is superior to social governance mechanisms challenges projects built around community voting and decentralized autonomous organizations"
      ]
    }
  },
  {
    "id": "general-2017-12-31-pos_faq",
    "title": "Proof of Stake FAQ",
    "date": "2017-12-31",
    "category": "protocol",
    "url": "https://vitalik.eth.limo/general/2017/12/31/pos_faq.html",
    "path": "general/2017/12/31/pos_faq.html",
    "content": "- \n\n- \n\n  \n    \n\n  \n  \n  Dark Mode Toggle\n  \n    \n\n  \n\n##  Proof of Stake FAQ \n\n 2017 Dec 31 \nSee all posts\n\n \n \n\n Proof of Stake FAQ \n\nContents\n\n- What is Proof of Stake\n\n- What\nare the benefits of proof of stake as opposed to proof of work?\n\n- How\ndoes proof of stake fit into traditional Byzantine fault tolerance\nresearch?\n\n- What\nis the \"nothing at stake\" problem and how can it be fixed?\n\n- That\nshows how chain-based algorithms solve nothing-at-stake. Now how do\nBFT-style proof of stake algorithms work?\n\n- What is \"economic\nfinality\" in general?\n\n- So\nhow does this relate to Byzantine fault tolerance theory?\n\n- What is \"weak\nsubjectivity\"?\n\n- Can\nwe try to automate the social authentication to reduce the load on\nusers?\n\n- Can\none economically penalize censorship in proof of stake?\n\n- How\ndoes validator selection work, and what is stake grinding?\n\n- What\nwould the equivalent of a 51% attack against Casper look like?\n\n- That\nsounds like a lot of reliance on out-of-band social coordination; is\nthat not dangerous?\n\n- Doesn't\nMC <= MR mean that all consensus algorithms with a given security\nlevel are equally efficient (or in other words, equally\nwasteful)?\n\n- What about capital lockup\ncosts?\n\n- Will\nexchanges in proof of stake pose a similar centralization risk to pools\nin proof of work?\n\n- Are\nthere economic ways to discourage centralization?\n\n- Can proof\nof stake be used in private/consortium chains?\n\n- Can multi-currency\nproof of stake work?\n\n- Further reading\n\n## What is Proof of Stake\n\nProof of Stake (PoS) is a category of consensus algorithms\nfor public blockchains that depend on a validator's economic stake in\nthe network. In proof of work (PoW) based public blockchains\n(e.g.\u00a0Bitcoin and the current implementation of Ethereum), the algorithm\nrewards participants who solve cryptographic puzzles in order to\nvalidate transactions and create new blocks (i.e.\u00a0mining). In PoS-based\npublic blockchains (e.g.\u00a0Ethereum's upcoming Casper implementation), a\nset of validators take turns proposing and voting on the next block, and\nthe weight of each validator's vote depends on the size of its deposit\n(i.e.\u00a0stake). Significant advantages of PoS include security,\nreduced risk of centralization, and energy efficiency.\n\nIn general, a proof of stake algorithm looks as follows. The\nblockchain keeps track of a set of validators, and anyone who holds the\nblockchain's base cryptocurrency (in Ethereum's case, ether) can become\na validator by sending a special type of transaction that locks\nup their ether into a deposit. The process of creating and\nagreeing to new blocks is then done through a consensus algorithm that\nall current validators can participate in.\n\nThere are many kinds of consensus algorithms, and many ways to assign\nrewards to validators who participate in the consensus algorithm, so\nthere are many \"flavors\" of proof of stake. From an algorithmic\nperspective, there are two major types: chain-based proof of stake and\nBFT-style\nproof of stake.\n\nIn chain-based proof of stake, the algorithm\npseudo-randomly selects a validator during each time slot (e.g.\u00a0every\nperiod of 10 seconds might be a time slot), and assigns that validator\nthe right to create a single block, and this block must point to some\nprevious block (normally the block at the end of the previously longest\nchain), and so over time most blocks converge into a single constantly\ngrowing chain.\n\nIn BFT-style proof of stake, validators are\nrandomly assigned the right to propose blocks,\nbut agreeing on which block is canonical is done through a\nmulti-round process where every validator sends a \"vote\" for some\nspecific block during each round, and at the end of the process all\n(honest and online) validators permanently agree on whether or not any\ngiven block is part of the chain. Note that blocks may still be\nchained together; the key difference is that consensus on a\nblock can come within one block, and does not depend on the length or\nsize of the chain after it.\nWhat\nare the benefits of proof of stake as opposed to proof of work?\n\nSee A\nProof of Stake Design Philosophy for a more long-form argument.\n\nIn short:\n\n- No need to consume large quantities of electricity\nin order to secure a blockchain (e.g.\u00a0it's estimated that both Bitcoin\nand Ethereum burn over $1 million worth of electricity and hardware\ncosts per day as part of their consensus mechanism).\n\n- Because of the lack of high electricity consumption, there is\nnot as much need to issue as many new coins in order to\nmotivate participants to keep participating in the network. It may\ntheoretically even be possible to have negative net issuance,\nwhere a portion of transaction fees is \"burned\" and so the supply goes\ndown over time.\n\n- Proof of stake opens the door to a wider array of techniques that\nuse game-theoretic mechanism design in order to better\ndiscourage centralized cartels from forming and, if\nthey do form, from acting in ways that are harmful to the network\n(e.g.\u00a0like selfish\nmining in proof of work).\n\n- Reduced centralization risks, as economies of scale\nare much less of an issue. $10 million of coins will get you exactly 10\ntimes higher returns than $1 million of coins, without any additional\ndisproportionate gains because at the higher level you can afford better\nmass-production equipment, which is an advantage for Proof-of-Work.\n\n- Ability to use economic penalties to make various forms of\n51% attacks vastly more expensive to carry out than proof of\nwork - to paraphrase Vlad Zamfir, \"it's as though your ASIC farm burned\ndown if you participated in a 51% attack\".\n\nHow\ndoes proof of stake fit into traditional Byzantine fault tolerance\nresearch?\n\nThere are several fundamental results from Byzantine fault tolerance\nresearch that apply to all consensus algorithms, including traditional\nconsensus algorithms like PBFT but also any proof of stake algorithm\nand, with the appropriate mathematical modeling, proof of work.\n\nThe key results include:\n\n- CAP\ntheorem - \"in the cases that a network partition takes\nplace, you have to choose either consistency or availability, you cannot\nhave both\". The intuitive argument is simple: if the network splits in\nhalf, and in one half I send a transaction \"send my 10 coins to A\" and\nin the other I send a transaction \"send my 10 coins to B\", then either\nthe system is unavailable, as one or both transactions will not be\nprocessed, or it becomes inconsistent, as one half of the network will\nsee the first transaction completed and the other half will see the\nsecond transaction completed. Note that the CAP theorem has nothing to\ndo with scalability; it applies to sharded and non-sharded systems\nequally. See also\nhttps://github.com/ethereum/wiki/wiki/Sharding-FAQs#but-doesnt-the-cap-theorem-mean-that-fully-secure-distributed-systems-are-impossible-and-so-sharding-is-futile.\n\n- FLP\nimpossibility - in an asynchronous setting (i.e.\u00a0there are\nno guaranteed bounds on network latency even between correctly\nfunctioning nodes), it is not possible to create an algorithm which is\nguaranteed to reach consensus in any specific finite amount of time if\neven a single faulty/dishonest node is present. Note that this does NOT\nrule out \"Las Vegas\"\nalgorithms that have some probability each round of achieving\nconsensus and thus will achieve consensus within T seconds with\nprobability exponentially approaching 1 as T grows; this is in fact the\n\"escape hatch\" that many successful consensus algorithms use.\n\n- Bounds on fault tolerance - from the DLS\npaper we have: (i) protocols running in a partially synchronous\nnetwork model (i.e.\u00a0there is a bound on network latency but we do not\nknow ahead of time what it is) can tolerate up to 1/3 arbitrary\n(i.e.\u00a0\"Byzantine\") faults, (ii) deterministic protocols in an\nasynchronous model (i.e.\u00a0no bounds on network latency) cannot tolerate\nfaults (although their paper fails to mention that randomized\nalgorithms can with up to 1/3 fault tolerance), (iii) protocols in a\nsynchronous model (i.e.\u00a0network latency is guaranteed to be less than a\nknown d) can, surprisingly, tolerate up to 100% fault\ntolerance, although there are restrictions on what can happen when more\nthan or equal to 1/2 of nodes are faulty. Note that the \"authenticated\nByzantine\" model is the one worth considering, not the \"Byzantine\" one;\nthe \"authenticated\" part essentially means that we can use public key\ncryptography in our algorithms, which is in modern times very\nwell-researched and very cheap.\n\nProof of work has been rigorously\nanalyzed by Andrew Miller and others and fits into the picture as an\nalgorithm reliant on a synchronous network model. We can model the\nnetwork as being made up of a near-infinite number of nodes, with each\nnode representing a very small unit of computing power and having a very\nsmall probability of being able to create a block in a given period. In\nthis model, the protocol has 50% fault tolerance assuming zero network\nlatency, ~46% (Ethereum) and ~49.5% (Bitcoin) fault tolerance under\nactually observed conditions, but goes down to 33% if network latency is\nequal to the block time, and reduces to zero as network latency\napproaches infinity.\n\nProof of stake consensus fits more directly into the Byzantine fault\ntolerant consensus mould, as all validators have known identities\n(stable Ethereum addresses) and the network keeps track of the total\nsize of the validator set. There are two general lines of proof of stake\nresearch, one looking at synchronous network models and one looking at\npartially asynchronous network models. \"Chain-based\" proof of stake\nalgorithms almost always rely on synchronous network models, and their\nsecurity can be formally proven within these models similarly to how\nsecurity of proof\nof work algorithms can be proven. A line of research connecting\ntraditional Byzantine fault tolerant consensus in partially synchronous\nnetworks to proof of stake also exists, but is more complex to explain;\nit will be covered in more detail in later sections.\n\nProof of work algorithms and chain-based proof of stake algorithms\nchoose availability over consistency, but BFT-style consensus algorithms\nlean more toward consistency; Tendermint chooses\nconsistency explicitly, and Casper uses a hybrid model that prefers\navailability but provides as much consistency as possible and makes both\non-chain applications and clients aware of how strong the consistency\nguarantee is at any given time.\n\nNote that Ittay Eyal and Emin Gun Sirer's selfish\nmining discovery, which places 25% and 33% bounds on the incentive\ncompatibility of Bitcoin mining depending on the network model\n(i.e.\u00a0mining is only incentive compatible if collusions larger than 25%\nor 33% are impossible) has NOTHING to do with results from traditional\nconsensus algorithm research, which does not touch incentive\ncompatibility.\nWhat\nis the \"nothing at stake\" problem and how can it be fixed?\n\nIn many early (all chain-based) proof of stake algorithms, including\nPeercoin, there are only rewards for producing blocks, and no penalties.\nThis has the unfortunate consequence that, in the case that there are\nmultiple competing chains, it is in a validator's incentive to try to\nmake blocks on top of every chain at once, just to be sure:\n\nIn proof of work, doing so would require splitting one's computing\npower in half, and so would not be lucrative:\n\nThe result is that if all actors are narrowly economically rational,\nthen even if there are no attackers, a blockchain may never reach\nconsensus. If there is an attacker, then the attacker need only\noverpower altruistic nodes (who would exclusively stake on the original\nchain), and not rational nodes (who would stake on both the original\nchain and the attacker's chain), in contrast to proof of work, where the\nattacker must overpower both altruists and rational nodes (or at least\ncredibly threaten to: see P +\nepsilon attacks).\n\nSome argue that stakeholders have an incentive to act correctly and\nonly stake on the longest chain in order to \"preserve the value of their\ninvestment\", however this ignores that this incentive suffers from tragedy of\nthe commons problems: each individual stakeholder might only have a\n1% chance of being \"pivotal\" (i.e.\u00a0being in a situation where if they\nparticipate in an attack then it succeeds and if they do not participate\nit fails), and so the bribe needed to convince them personally to join\nan attack would be only 1% of the size of their deposit; hence, the\nrequired combined bribe would be only 0.5-1% of the total sum of all\ndeposits. Additionally, this argument implies that any\nzero-chance-of-failure situation is not a stable equilibrium, as if the\nchance of failure is zero then everyone has a 0% chance of being\npivotal.\n\nThis can be solved via two strategies. The first, described in broad\nterms under the name \"Slasher\" here\nand developed further by Iddo Bentov here, involves penalizing\nvalidators if they simultaneously create blocks on multiple chains, by\nmeans of including proof of misbehavior (i.e.\u00a0two conflicting signed\nblock headers) into the blockchain as a later point in time at which\npoint the malfeasant validator's deposit is deducted appropriately. This\nchanges the incentive structure thus:\n\nNote that for this algorithm to work, the validator set needs to be\ndetermined well ahead of time. Otherwise, if a validator has 1% of the\nstake, then if there are two branches A and B then 0.99% of the time the\nvalidator will be eligible to stake only on A and not on B, 0.99% of the\ntime the validator will be eligible to stake on B and not on A, and only\n0.01% of the time will the validator will be eligible to stake on both.\nHence, the validator can with 99% efficiency probabilistically\ndouble-stake: stake on A if possible, stake on B if possible, and only\nif the choice between both is open stake on the longer chain. This can\nonly be avoided if the validator selection is the same for every block\non both branches, which requires the validators to be selected at a time\nbefore the fork takes place.\n\nThis has its own flaws, including requiring nodes to be frequently\nonline to get a secure view of the blockchain, and opening up\nmedium-range validator collusion risks (i.e.\u00a0situations where, for\nexample, 25 out of 30 consecutive validators get together and agree\nahead of time to implement a 51% attack on the previous 19 blocks), but\nif these risks are deemed acceptable then it works well.\n\nThe second strategy is to simply punish validators for creating\nblocks on the wrong chain. That is, if there are two competing\nchains, A and B, then if a validator creates a block on B, they get a\nreward of +R on B, but the block header can be included into A (in\nCasper this is called a \"dunkle\") and on A the validator suffers a\npenalty of -F (possibly F = R). This changes the economic calculation\nthus:\n\nThe intuition here is that we can replicate the economics of proof of\nwork inside of proof of stake. In proof of work, there is also a penalty\nfor creating a block on the wrong chain, but this penalty is implicit in\nthe external environment: miners have to spend extra electricity and\nobtain or rent extra hardware. Here, we simply make the penalties\nexplicit. This mechanism has the disadvantage that it imposes slightly\nmore risk on validators (although the effect should be smoothed out over\ntime), but has the advantage that it does not require validators to be\nknown ahead of time.\nThat\nshows how chain-based algorithms solve nothing-at-stake. Now how do\nBFT-style proof of stake algorithms work?\n\nBFT-style (partially synchronous) proof of stake algorithms allow\nvalidators to \"vote\" on blocks by sending one or more types of signed\nmessages, and specify two kinds of rules:\n\n- Finality conditions - rules that determine when a\ngiven hash can be considered finalized.\n\n- Slashing conditions - rules that determine when a\ngiven validator can be deemed beyond reasonable doubt to have misbehaved\n(e.g.\u00a0voting for multiple conflicting blocks at the same time). If a\nvalidator triggers one of these rules, their entire deposit gets\ndeleted.\n\nTo illustrate the different forms that slashing conditions can take,\nwe will give two examples of slashing conditions (hereinafter, \"2/3 of\nall validators\" is shorthand for \"2/3 of all validators weighted by\ndeposited coins\", and likewise for other fractions and percentages). In\nthese examples, \"PREPARE\" and \"COMMIT\" should be understood as simply\nreferring to two types of messages that validators can send.\n\n- If MESSAGES contains messages of the form\n[\"COMMIT\", HASH1, view] and\n[\"COMMIT\", HASH2, view] for the same view but\ndiffering HASH1 and HASH2 signed by the same\nvalidator, then that validator is slashed.\n\n- If MESSAGES contains a message of the form\n[\"COMMIT\", HASH, view1], then UNLESS either view1 = -1 or\nthere also exist messages of the form\n[\"PREPARE\", HASH, view1, view2] for some specific\nview2, where view2 < view1, signed by 2/3\nof all validators, then the validator that made the COMMIT is\nslashed.\n\nThere are two important desiderata for a suitable set of slashing\nconditions to have:\n\n- Accountable safety - if conflicting\nHASH1 and HASH2 (i.e.\u00a0HASH1 and\nHASH2 are different, and neither is a descendant of the\nother) are finalized, then at least 1/3 of all validators must have\nviolated some slashing condition.\n\n- Plausible liveness - unless at least 1/3 of all\nvalidators have violated some slashing condition, there exists a set of\nmessages that 2/3 of validators can produce that finalize some\nvalue.\n\nIf we have a set of slashing conditions that satisfies both\nproperties, then we can incentivize participants to send messages, and\nstart benefiting from economic finality.\nWhat is \"economic\nfinality\" in general?\n\nEconomic finality is the idea that once a block is finalized, or more\ngenerally once enough messages of certain types have been signed, then\nthe only way that at any point in the future the canonical history will\ncontain a conflicting block is if a large number of people are willing\nto burn very large amounts of money. If a node sees that this condition\nhas been met for a given block, then they have a very economically\nstrong assurance that that block will always be part of the canonical\nhistory that everyone agrees on.\n\nThere are two \"flavors\" of economic finality:\n\n- A block can be economically finalized if a sufficient number of\nvalidators have signed cryptoeconomic claims of the form \"I agree to\nlose X in all histories where block B is not included\". This gives\nclients assurance that either (i) B is part of the canonical chain, or\n(ii) validators lost a large amount of money in order to trick them into\nthinking that this is the case.\n\n- A block can be economically finalized if a sufficient number of\nvalidators have signed messages expressing support for block B, and\nthere is a mathematical proof that if some B' != B is also finalized\nunder the same definition then validators lose a large amount of\nmoney. If clients see this, and also validate the chain, and validity\nplus finality is a sufficient condition for precedence in the canonical\nfork choice rule, then they get an assurance that either (i) B is part\nof the canonical chain, or (ii) validators lost a large amount of money\nin making a conflicting chain that was also finalized.\n\nThe two approaches to finality inherit from the two solutions to the\nnothing at stake problem: finality by penalizing incorrectness, and\nfinality by penalizing equivocation. The main benefit of the first\napproach is that it is more light-client friendly and is simpler to\nreason about, and the main benefits of the second approach are that (i)\nit's easier to see that honest validators will not be punished, and (ii)\ngriefing factors are more favorable to honest validators.\n\nCasper follows the second flavor, though it is possible that an\non-chain mechanism will be added where validators can voluntarily opt-in\nto signing finality messages of the first flavor, thereby enabling much\nmore efficient light clients.\nSo\nhow does this relate to Byzantine fault tolerance theory?\n\nTraditional byzantine fault tolerance theory posits similar safety\nand liveness desiderata, except with some differences. First of all,\ntraditional byzantine fault tolerance theory simply requires that safety\nis achieved if 2/3 of validators are honest. This is a strictly\neasier model to work in; traditional fault tolerance tries to prove \"if\nmechanism M has a safety failure, then at least 1/3 of nodes are\nfaulty\", whereas our model tries to prove \"if mechanism M has a safety\nfailure, then at least 1/3 of nodes are faulty, and you know which\nones, even if you were offline at the time the failure took place\".\nFrom a liveness perspective, our model is the easier one, as we do not\ndemand a proof that the network will come to consensus, we just\ndemand a proof that it does not get stuck.\n\nFortunately, we can show the additional accountability requirement is\nnot a particularly difficult one; in fact, with the right \"protocol\narmor\", we can convert any traditional partially synchronous or\nasynchronous Byzantine fault-tolerant algorithm into an accountable\nalgorithm. The proof of this basically boils down to the fact that\nfaults can be exhaustively categorized into a few classes, and each one\nof these classes is either accountable (i.e.\u00a0if you commit that type of\nfault you can get caught, so we can make a slashing condition for it) or\nindistinguishable from latency (note that even the fault of sending\nmessages too early is indistinguishable from latency, as one can model\nit by speeding up everyone's clocks and assigning the messages that\nweren't sent too early a higher latency).\n\n## What is \"weak subjectivity\"?\n\nIt is important to note that the mechanism of using deposits to\nensure there is \"something at stake\" does lead to one change in the\nsecurity model. Suppose that deposits are locked for four months, and\ncan later be withdrawn. Suppose that an attempted 51% attack happens\nthat reverts 10 days worth of transactions. The blocks created by the\nattackers can simply be imported into the main chain as\nproof-of-malfeasance (or \"dunkles\") and the validators can be punished.\nHowever, suppose that such an attack happens after six months. Then,\neven though the blocks can certainly be re-imported, by that time the\nmalfeasant validators will be able to withdraw their deposits on the\nmain chain, and so they cannot be punished.\n\nTo solve this problem, we introduce a \"revert limit\" - a rule that\nnodes must simply refuse to revert further back in time than the deposit\nlength (i.e.\u00a0in our example, four months), and we additionally require\nnodes to log on at least once every deposit length to have a secure view\nof the chain. Note that this rule is different from every other\nconsensus rule in the protocol, in that it means that nodes may come to\ndifferent conclusions depending on when they saw certain messages. The\ntime that a node saw a given message may be different between different\nnodes; hence we consider this rule \"subjective\" (alternatively, one\nwell-versed in Byzantine fault tolerance theory may view it as a kind of\nsynchrony assumption).\n\nHowever, the \"subjectivity\" here is very weak: in order for a node to\nget on the \"wrong\" chain, they must receive the original message four\nmonths later than they otherwise would have. This is only possible in\ntwo cases:\n\n- When a node connects to the blockchain for the first time.\n\n- If a node has been offline for more than four months.\n\nWe can solve (1) by making it the user's responsibility to\nauthenticate the latest state out of band. They can do this by asking\ntheir friends, block explorers, businesses that they interact with, etc.\nfor a recent block hash in the chain that they see as the canonical one.\nIn practice, such a block hash may well simply come as part of the\nsoftware they use to verify the blockchain; an attacker that can corrupt\nthe checkpoint in the software can arguably just as easily corrupt the\nsoftware itself, and no amount of pure cryptoeconomic verification can\nsolve that problem. (2) does genuinely add an additional security\nrequirement for nodes, though note once again that the possibility of\nhard forks and security vulnerabilities, and the requirement to stay up\nto date to know about them and install any needed software updates,\nexists in proof of work too.\n\nNote that all of this is a problem only in the very limited case\nwhere a majority of previous stakeholders from some point in time\ncollude to attack the network and create an alternate chain; most of the\ntime we expect there will only be one canonical chain to choose\nfrom.\nCan\nwe try to automate the social authentication to reduce the load on\nusers?\n\nOne approach is to bake it into natural user workflow: a BIP\n70-style payment request could include a recent block hash, and the\nuser's client software would make sure that they are on the same chain\nas the vendor before approving a payment (or for that matter, any\non-chain interaction). The other is to use Jeff Coleman's universal hash\ntime. If UHT is used, then a successful attack chain would need to\nbe generated secretly at the same time as the legitimate chain\nwas being built, requiring a majority of validators to secretly collude\nfor that long.\nCan\none economically penalize censorship in proof of stake?\n\nUnlike reverts, censorship is much more difficult to prove. The\nblockchain itself cannot directly tell the difference between \"user A\ntried to send transaction X but it was unfairly censored\", \"user A tried\nto send transaction X but it never got in because the transaction fee\nwas insufficient\" and \"user A never tried to send transaction X at all\".\nSee also a\nnote on data availability and erasure codes. However, there are a\nnumber of techniques that can be used to mitigate censorship issues.\n\nThe first is censorship resistance by halting problem. In the weaker\nversion of this scheme, the protocol is designed to be Turing-complete\nin such a way that a validator cannot even tell whether or not a given\ntransaction will lead to an undesired action without spending a large\namount of processing power executing the transaction, and thus opening\nitself up to denial-of-service attacks. This is what prevented\nthe DAO soft fork.\n\nIn the stronger version of the scheme, transactions can trigger\nguaranteed effects at some point in the near to mid-term future. Hence,\na user could send multiple transactions which interact with each other\nand with predicted third-party information to lead to some future event,\nbut the validators cannot possibly tell that this is going to happen\nuntil the transactions are already included (and economically finalized)\nand it is far too late to stop them; even if all future transactions are\nexcluded, the event that validators wish to halt would still take place.\nNote that in this scheme, validators could still try to prevent\nall transactions, or perhaps all transactions that do\nnot come packaged with some formal proof that they do not lead to\nanything undesired, but this would entail forbidding a very wide class\nof transactions to the point of essentially breaking the entire system,\nwhich would cause validators to lose value as the price of the\ncryptocurrency in which their deposits are denominated would drop.\n\nThe second, described\nby Adam Back here, is to require transactions to be timelock-encrypted.\nHence, validators will include the transactions without knowing the\ncontents, and only later could the contents automatically be revealed,\nby which point once again it would be far too late to un-include the\ntransactions. If validators were sufficiently malicious, however, they\ncould simply only agree to include transactions that come with a\ncryptographic proof (e.g.\u00a0ZK-SNARK) of what the decrypted version is;\nthis would force users to download new client software, but an adversary\ncould conveniently provide such client software for easy download, and\nin a game-theoretic model users would have the incentive to play\nalong.\n\nPerhaps the best that can be said in a proof-of-stake context is that\nusers could also install a software update that includes a hard fork\nthat deletes the malicious validators and this is not that much harder\nthan installing a software update to make their transactions\n\"censorship-friendly\". Hence, all in all this scheme is also moderately\neffective, though it does come at the cost of slowing interaction with\nthe blockchain down (note that the scheme must be mandatory to be\neffective; otherwise malicious validators could much more easily simply\nfilter encrypted transactions without filtering the quicker unencrypted\ntransactions).\n\nA third alternative is to include censorship detection in the fork\nchoice rule. The idea is simple. Nodes watch the network for\ntransactions, and if they see a transaction that has a sufficiently high\nfee for a sufficient amount of time, then they assign a lower \"score\" to\nblockchains that do not include this transaction. If all nodes follow\nthis strategy, then eventually a minority chain would automatically\ncoalesce that includes the transactions, and all honest online nodes\nwould follow it. The main weakness of such a scheme is that offline\nnodes would still follow the majority branch, and if the censorship is\ntemporary and they log back on after the censorship ends then they would\nend up on a different branch from online nodes. Hence, this scheme\nshould be viewed more as a tool to facilitate automated emergency\ncoordination on a hard fork than something that would play an active\nrole in day-to-day fork choice.\nHow\ndoes validator selection work, and what is stake grinding?\n\nIn any chain-based proof of stake algorithm, there is a need for some\nmechanism which randomly selects which validator out of the currently\nactive validator set can make the next block. For example, if the\ncurrently active validator set consists of Alice with 40 ether, Bob with\n30 ether, Charlie with 20 ether and David with 10 ether, then you want\nthere to be a 40% chance that Alice will be the next block creator, 30%\nchance that Bob will be, etc (in practice, you want to randomly select\nnot just one validator, but rather an infinite sequence of validators,\nso that if Alice doesn't show up there is someone who can replace her\nafter some time, but this doesn't change the fundamental problem). In\nnon-chain-based algorithms randomness is also often needed for different\nreasons.\n\n\"Stake grinding\" is a class of attack where a validator performs some\ncomputation or takes some other step to try to bias the randomness in\ntheir own favor. For example:\n\n- In Peercoin, a\nvalidator could \"grind\" through many combinations of parameters and find\nfavorable parameters that would increase the probability of their coins\ngenerating a valid block.\n\n- In one now-defunct implementation, the randomness for block N+1 was\ndependent on the signature of block N. This allowed a validator to\nrepeatedly produce new signatures until they found one that allowed them\nto get the next block, thereby seizing control of the system\nforever.\n\n- In NXT, the randomness for block N+1 is dependent on the validator\nthat creates block N. This allows a validator to manipulate the\nrandomness by simply skipping an opportunity to create a block. This\ncarries an opportunity cost equal to the block reward, but sometimes the\nnew random seed would give the validator an above-average number of\nblocks over the next few dozen blocks. See here and here\nfor a more detailed analysis.\n\n- and (2) are easy to solve; the general approach is to require\nvalidators to deposit their coins well in advance, and not to use\ninformation that can be easily manipulated as source data for the\nrandomness. There are several main strategies for solving problems like\n(3). The first is to use schemes based on secret sharing\nor deterministic\nthreshold signatures and have validators collaboratively generate\nthe random value. These schemes are robust against all manipulation\nunless a majority of validators collude (in some cases though, depending\non the implementation, between 33-50% of validators can interfere in the\noperation, leading to the protocol having a 67% liveness\nassumption).\n\nThe second is to use cryptoeconomic schemes where validators commit\nto information (i.e.\u00a0publish sha3(x)) well in advance, and\nthen must publish x in the block; x is then\nadded into the randomness pool. There are two theoretical attack vectors\nagainst this:\n\n- Manipulate x at commitment time. This is impractical\nbecause the randomness result would take many actors' values into\naccount, and if even one of them is honest then the output will be a\nuniform distribution. A uniform distribution XORed together with\narbitrarily many arbitrarily biased distributions still gives a uniform\ndistribution.\n\n- Selectively avoid publishing blocks. However, this attack costs one\nblock reward of opportunity cost, and because the scheme prevents anyone\nfrom seeing any future validators except for the next, it almost never\nprovides more than one block reward worth of revenue. The only exception\nis the case where, if a validator skips, the next validator in line AND\nthe first child of that validator will both be the same validator; if\nthese situations are a grave concern then we can punish skipping further\nvia an explicit skipping penalty.\n\nThe third is to use Iddo Bentov's \"majority\nbeacon\", which generates a random number by taking the bit-majority\nof the previous N random numbers generated through some other beacon\n(i.e.\u00a0the first bit of the result is 1 if the majority of the first bits\nin the source numbers is 1 and otherwise it's 0, the second bit of the\nresult is 1 if the majority of the second bits in the source numbers is\n1 and otherwise it's 0, etc). This gives a cost-of-exploitation of\n~C * sqrt(N) where C is the cost of\nexploitation of the underlying beacons. Hence, all in all, many known\nsolutions to stake grinding exist; the problem is more like differential\ncryptanalysis than the halting\nproblem - an annoyance that proof of stake designers eventually\nunderstood and now know how to overcome, not a fundamental and\ninescapable flaw.\nWhat\nwould the equivalent of a 51% attack against Casper look like?\n\nThere are four basic types of 51% attack:\n\n- Finality reversion: validators that already\nfinalized block A then finalize some competing block A', thereby\nbreaking the blockchain's finality guarantee.\n\n- Invalid chain finalization: validators finalize an\ninvalid (or unavailable) block.\n\n- Liveness denial: validators stop finalizing\nblocks.\n\n- Censorship: validators block some or all\ntransactions or blocks from entering the chain.\n\nIn the first case, users can socially coordinate out-of-band to agree\nwhich finalized block came first, and favor that block. The second case\ncan be solved with fraud\nproofs and data availability proofs. The third case can be solved by\na modification to proof of stake algorithms that gradually reduces\n(\"leaks\") non-participating nodes' weights in the validator set if they\ndo not participate in consensus; the Casper FFG paper includes a\ndescription of this.\n\nThe fourth is most difficult. The fourth can be recovered from via a\n\"minority soft fork\", where a minority of honest validators agree the\nmajority is censoring them, and stop building on their chain. Instead,\nthey continue their own chain, and eventually the \"leak\" mechanism\ndescribed above ensures that this honest minority becomes a 2/3\nsupermajority on the new chain. At that point, the market is expected to\nfavor the chain controlled by honest nodes over the chain controlled by\ndishonest nodes.\nThat\nsounds like a lot of reliance on out-of-band social coordination; is\nthat not dangerous?\n\nAttacks against Casper are extremely expensive; as we will see below,\nattacks against Casper cost as much, if not more, than the cost of\nbuying enough mining power in a proof of work chain to permanently 51%\nattack it over and over again to the point of uselessness. Hence, the\nrecovery techniques described above will only be used in very extreme\ncircumstances; in fact, advocates of proof of work also generally\nexpress willingness to use social coordination in similar circumstances\nby, for example, changing\nthe proof of work algorithm. Hence, it is not even clear that the\nneed for social coordination in proof of stake is larger than it is in\nproof of work.\n\nIn reality, we expect the amount of social coordination required to\nbe near-zero, as attackers will realize that it is not in their benefit\nto burn such large amounts of money to simply take a blockchain offline\nfor one or two days.\nDoesn't\nMC <= MR mean that all consensus algorithms with a given security\nlevel are equally efficient (or in other words, equally wasteful)?\n\nThis is an argument that many have raised, perhaps best explained by\nPaul Sztorc in\nthis article. Essentially, if you create a way for people to earn\n$100, then people will be willing to spend anywhere up to $99.9\n(including the cost of their own labor) in order to get it; marginal\ncost approaches marginal revenue. Hence, the theory goes, any algorithm\nwith a given block reward will be equally \"wasteful\" in terms of the\nquantity of socially unproductive activity that is carried out in order\nto try to get the reward.\n\nThere are three flaws with this:\n\n- It's not enough to simply say that marginal cost approaches marginal\nrevenue; one must also posit a plausible mechanism by which someone can\nactually expend that cost. For example, if tomorrow I announce that\nevery day from then on I will give $100 to a randomly selected one of a\ngiven list of ten people (using my laptop's /dev/urandom as randomness),\nthen there is simply no way for anyone to send $99 to try to get at that\nrandomness. Either they are not in the list of ten, in which case they\nhave no chance no matter what they do, or they are in the list of ten,\nin which case they don't have any reasonable way to manipulate my\nrandomness so they're stuck with getting the expected-value $10 per\nday.\n\n- MC <= MR does NOT imply total cost approaches total revenue. For\nexample, suppose that there is an algorithm which pseudorandomly selects\n1000 validators out of some very large set (each validator getting a\nreward of $1), you have 10% of the stake so on average you get 100, and\nat a cost of $1 you can force the randomness to reset (and you can\nrepeat this an unlimited number of times). Due to the central limit\ntheorem, the standard deviation of your reward is $10, and based on\nother\nknown results in math the expected maximum of N random samples is\nslightly under M + S * sqrt(2 * log(N)) where\nM is the mean and S is the standard deviation.\nHence the reward for making additional trials (i.e.\u00a0increasing N) drops\noff sharply, e.g.\u00a0with 0 re-trials your expected reward is $100, with\none re-trial it's $105.5, with two it's $108.5, with three it's $110.3,\nwith four it's $111.6, with five it's $112.6 and with six it's $113.5.\nHence, after five retrials it stops being worth it. As a result, an\neconomically motivated attacker with ten percent of stake will\ninefficiently spend $5 to get an additional revenue of $13, though the\ntotal revenue is $113. If the exploitable mechanisms only expose small\nopportunities, the economic loss will be small; it is decidedly NOT the\ncase that a single drop of exploitability brings the entire flood of\nPoW-level economic waste rushing back in. This point will also be very\nrelevant in our below discussion on capital lockup costs.\n\n- Proof of stake can be secured with much lower total rewards than\nproof of work.\n\nWhat about capital lockup\ncosts?\n\nLocking up X ether in a deposit is not free; it entails a sacrifice\nof optionality for the ether holder. Right now, if I have 1000 ether, I\ncan do whatever I want with it; if I lock it up in a deposit, then it's\nstuck there for months, and I do not have, for example, the insurance\nutility of the money being there to pay for sudden unexpected expenses.\nI also lose some freedom to change my token allocations away from ether\nwithin that timeframe; I could simulate selling ether by shorting an\namount equivalent to the deposit on an exchange, but this itself carries\ncosts including exchange fees and paying interest. Some might argue:\nisn't this capital lockup inefficiency really just a highly indirect way\nof achieving the exact same level of economic inefficiency as exists in\nproof of work? The answer is no, for both reasons (2) and (3) above.\n\nLet us start with (3) first. Consider a model where proof of stake\ndeposits are infinite-term, ASICs last forever, ASIC technology is fixed\n(i.e.\u00a0no Moore's law) and electricity costs are zero. Let's say the\nequilibrium interest rate is 5% per annum. In a proof of work\nblockchain, I can take $1000, convert it into a miner, and the miner\nwill pay me $50 in rewards per year forever. In a proof of stake\nblockchain, I would buy $1000 of coins, deposit them (i.e.\u00a0losing them\nforever), and get $50 in rewards per year forever. So far, the situation\nlooks completely symmetrical (technically, even here, in the proof of\nstake case my destruction of coins isn't fully socially destructive as\nit makes others' coins worth more, but we can leave that aside for the\nmoment). The cost of a \"Maginot-line\" 51% attack (i.e.\u00a0buying up more\nhardware than the rest of the network) increases by $1000 in both\ncases.\n\nNow, let's perform the following changes to our model in turn:\n\n- Moore's law exists, ASICs depreciate by 50% every 2.772 years\n(that's a continuously-compounded 25% annual depreciation; picked to\nmake the numbers simpler). If I want to retain the same \"pay once, get\nmoney forever\" behavior, I can do so: I would put $1000 into a fund,\nwhere $167 would go into an ASIC and the remaining $833 would go into\ninvestments at 5% interest; the $41.67 dividends per year would be just\nenough to keep renewing the ASIC hardware (assuming technological\ndevelopment is fully continuous, once again to make the math simpler).\nRewards would go down to $8.33 per year; hence, 83.3% of miners will\ndrop out until the system comes back into equilibrium with me earning\n$50 per year, and so the Maginot-line cost of an attack on PoW given the\nsame rewards drops by a factor of 6.\n\n- Electricity plus maintenance makes up 1/3 of mining costs. We\nestimate the 1/3 from recent mining statistics: one of Bitfury's new\ndata centers consumes 0.06\njoules per gigahash, or 60 J/TH or 0.000017 kWh/TH, and if we assume\nthe entire Bitcoin network has similar efficiencies we get 27.9 kWh per\nsecond given 1.67 million TH/s total\nBitcoin hashpower. Electricity in China costs $0.11\nper kWh, so that's about $3 per second, or $260,000 per day. Bitcoin\nblock rewards plus fees are $600 per BTC * 13 BTC per block * 144 blocks\nper day = $1.12m per day. Thus electricity itself would make up 23% of\ncosts, and we can back-of-the-envelope estimate maintenance at 10% to\ngive a clean 1/3 ongoing costs, 2/3 fixed costs split. This means that\nout of your $1000 fund, only $111 would go into the ASIC, $56 would go\ninto paying ongoing costs, and $833 would go into investments; hence the\nMaginot-line cost of attack is 9x lower than in our original\nsetting.\n\n- Deposits are temporary, not permanent. Sure, if I voluntarily keep\nstaking forever, then this changes nothing. However, I regain some of\nthe optionality that I had before; I could quit within a medium\ntimeframe (say, 4 months) at any time. This means that I would be\nwilling to put more than $1000 of ether in for the $50 per year gain;\nperhaps in equilibrium it would be something like $3000. Hence, the cost\nof the Maginot line attack on PoS increases by a factor of\nthree, and so on net PoS gives 27x more security than PoW for the same\ncost.\n\nThe above included a large amount of simplified modeling, however it\nserves to show how multiple factors stack up heavily in favor of PoS in\nsuch a way that PoS gets more bang for its buck in terms of\nsecurity. The meta-argument for why this perhaps\nsuspiciously multifactorial argument leans so heavily in favor of\nPoS is simple: in PoW, we are working directly with the laws of physics.\nIn PoS, we are able to design the protocol in such a way that it has the\nprecise properties that we want - in short, we can optimize the laws\nof physics in our favor. The \"hidden trapdoor\" that gives us (3) is\nthe change in the security model, specifically the introduction of weak\nsubjectivity.\n\nNow, we can talk about the marginal/total distinction. In the case of\ncapital lockup costs, this is very important. For example, consider a\ncase where you have $100,000 of ether. You probably intend to hold a\nlarge portion of it for a long time; hence, locking up even $50,000 of\nthe ether should be nearly free. Locking up $80,000 would be slightly\nmore inconvenient, but $20,000 of breathing room still gives you a large\nspace to maneuver. Locking up $90,000 is more problematic, $99,000 is\nvery problematic, and locking up all $100,000 is absurd, as it means you\nwould not even have a single bit of ether left to pay basic transaction\nfees. Hence, your marginal costs increase quickly. We can show the\ndifference between this state of affairs and the state of affairs in\nproof of work as follows:\n\nHence, the total cost of proof of stake is potentially much\nlower than the marginal cost of depositing 1 more ETH into the system\nmultiplied by the amount of ether currently deposited.\n\nNote that this component of the argument unfortunately does not fully\ntranslate into reduction of the \"safe level of issuance\". It does help\nus because it shows that we can get substantial proof of stake\nparticipation even if we keep issuance very low; however, it also means\nthat a large portion of the gains will simply be borne by validators as\neconomic surplus.\nWill\nexchanges in proof of stake pose a similar centralization risk to pools\nin proof of work?\n\nFrom a centralization perspective, in both Bitcoin and Ethereum\nit's the case that roughly three pools are needed to coordinate on a 51%\nattack (4 in Bitcoin, 3 in Ethereum at the time of this writing). In\nPoS, if we assume 30% participation including all exchanges, then three exchanges would be enough\nto make a 51% attack; if participation goes up to 40% then the required\nnumber goes up to eight. However, exchanges will not be able to\nparticipate with all of their ether; the reason is that they need to\naccomodate withdrawals.\n\nAdditionally, pooling in PoS is discouraged because it has a much\nhigher trust requirement - a proof of stake pool can pretend to be\nhacked, destroy its participants' deposits and claim a reward for it. On\nthe other hand, the ability to earn interest on one's coins without\noneself running a node, even if trust is required, is something that\nmany may find attractive; all in all, the centralization balance is an\nempirical question for which the answer is unclear until the system is\nactually running for a substantial period of time. With sharding, we\nexpect pooling incentives to reduce further, as (i) there is even less\nconcern about variance, and (ii) in a sharded model, transaction\nverification load is proportional to the amount of capital that one puts\nin, and so there are no direct infrastructure savings from pooling.\n\nA final point is that centralization is less harmful in proof of\nstake than in proof of work, as there are much cheaper ways to recover\nfrom successful 51% attacks; one does not need to switch to a new mining\nalgorithm.\nAre there\neconomic ways to discourage centralization?\n\nOne strategy suggested by Vlad Zamfir is to only partially destroy\ndeposits of validators that get slashed, setting the percentage\ndestroyed to be proportional to the percentage of other validators that\nhave been slashed recently. This ensures that validators lose all of\ntheir deposits in the event of an actual attack, but only a small part\nof their deposits in the event of a one-off mistake. This makes\nlower-security staking strategies possible, and also specifically\nincentivizes validators to have their errors be as uncorrelated (or\nideally, anti-correlated) with other validators as possible; this\ninvolves not being in the largest pool, putting one's node on the\nlargest virtual private server provider and even using secondary\nsoftware implementations, all of which increase decentralization.\nCan\nproof of stake be used in private/consortium chains?\n\nGenerally, yes; any proof of stake algorithm can be used as a\nconsensus algorithm in private/consortium chain settings. The only\nchange is that the way the validator set is selected would be different:\nit would start off as a set of trusted users that everyone agrees on,\nand then it would be up to the validator set to vote on adding in new\nvalidators.\nCan multi-currency proof\nof stake work?\n\nThere has been a lot of interest in proof of stake protocols where\nusers can stake any currency, or one of multiple currencies. However,\nthese designs unfortunately introduce economic challenges that likely\nmake them much more trouble than any benefit that could be received from\nthem. The key problems include:\n\n- Price oracle dependence: if people are staking in\nmultiple cryptocurrencies, there needs to be a way to compare deposits\nin one versus the other, so as to fairly allocate proposal rights,\ndetermine whether or not a 2/3 threshold was passed, etc. This requires\nsome form of price oracle. This can be done in a decentralized way (eg.\nsee Uniswap), but it introduces another component that could be\nmanipulated and attacked by validators.\n\n- Pathological cryptocurrencies: one can always\ncreate a cryptocurrency that is pathologically constructed to nullify\nthe impact of penalties. For example, one can imagine a fiat-backed\ntoken where coins that are seized by the protocol as penalties are\ntracked and not honored by the issuer, and the penalized actor's\noriginal balance is honored instead. This logic could even be\nimplemented in a smart contract, and it's impossible to determine with\ncertainty whether or not a given currency has such a mechanism\nbuilt-in.\n\n- Reduced incentive alignment: if currencies other\nthan the protocol's base token can be used to stake, this reduces the\nstakers' interest in seeing the protocol continue to operate and\nsucceed.\n\n## Further reading\n\nhttps://github.com/ethereum/wiki/wiki/Casper-Proof-of-Stake-compendium",
    "contentLength": 50319,
    "summary": "This FAQ explains how Proof of Stake consensus works by having validators propose/vote on blocks based on their cryptocurrency deposits.",
    "detailedSummary": {
      "theme": "A comprehensive examination of Proof of Stake consensus mechanisms, addressing technical challenges, security considerations, and comparing PoS to Proof of Work systems.",
      "summary": "Vitalik provides an extensive FAQ covering the fundamental concepts and challenges of Proof of Stake (PoS) consensus algorithms. He explains that PoS allows validators to participate in block creation and validation based on their economic stake in the network, rather than computational power as in Proof of Work. Vitalik addresses critical technical issues including the 'nothing at stake' problem, which he argues can be solved through slashing conditions that penalize validators for malicious behavior, and the concept of 'weak subjectivity' which requires nodes to occasionally synchronize with the network to maintain security. He emphasizes that PoS offers significant advantages over PoW including dramatic energy efficiency improvements, reduced centralization risks, and the ability to make attacks economically prohibitive through deposit slashing.\n\nVitalik also discusses economic finality, validator selection mechanisms, and various attack vectors against PoS systems. He argues that while PoS requires some social coordination in extreme circumstances, this is not fundamentally different from PoW systems and that the overall security model is superior. The piece covers technical solutions to stake grinding attacks, censorship resistance, and explains how Byzantine Fault Tolerance theory applies to PoS implementations like Casper. Throughout the FAQ, Vitalik demonstrates that many perceived weaknesses of PoS can be addressed through careful protocol design and economic incentives.",
      "takeaways": [
        "Proof of Stake can achieve greater energy efficiency and security than Proof of Work while reducing centralization risks through economic incentives rather than computational competition",
        "The 'nothing at stake' problem can be solved through slashing conditions that economically penalize validators for malicious behavior like voting on multiple competing chains",
        "Weak subjectivity requires nodes to periodically sync with the network but only poses security challenges in extreme scenarios where users are offline for months or connecting for the first time",
        "Economic finality in PoS provides stronger guarantees than PoW by making reversals extremely expensive through validator deposit slashing rather than just computational cost",
        "PoS systems can be designed to resist various attacks including censorship, stake grinding, and 51% attacks through careful mechanism design and economic penalties"
      ],
      "controversial": [
        "The reliance on social coordination and out-of-band authentication for resolving certain attack scenarios may be seen as introducing subjective elements that compromise decentralization",
        "The argument that capital lockup costs in PoS are fundamentally different from energy costs in PoW, potentially understating the real economic barriers to participation",
        "The claim that PoS provides superior security to PoW through economic penalties may be disputed by those who argue physical resource expenditure provides more objective security guarantees"
      ]
    }
  },
  {
    "id": "general-2017-12-31-sharding_faq",
    "title": "Sharding FAQ",
    "date": "2017-12-31",
    "category": "protocol",
    "url": "https://vitalik.eth.limo/general/2017/12/31/sharding_faq.html",
    "path": "general/2017/12/31/sharding_faq.html",
    "content": "- \n\n- \n\n  \n    \n\n  \n  \n  Dark Mode Toggle\n  \n    \n\n  \n\n##  Sharding FAQ \n\n 2017 Dec 31 \nSee all posts\n\n \n \n\n Sharding FAQ \n\nCurrently, in all blockchain protocols each node stores the entire\nstate (account balances, contract code and storage, etc.) and processes\nall transactions. This provides a large amount of security, but greatly\nlimits scalability: a blockchain cannot process more transactions than a\nsingle node can. In large part because of this, Bitcoin is limited to\n~3\u20137 transactions per second, Ethereum to 7\u201315, etc.\n\nHowever, this poses a question: are there ways to create a new\nmechanism, where only a small subset of nodes verifies each transaction?\nAs long as there are sufficiently many nodes verifying each transaction\nthat the system is still highly secure, but a sufficiently small\npercentage of the total validator set so that the system can process\nmany transactions in parallel, could we not split up transaction\nprocessing between smaller groups of nodes to greatly increase a\nblockchain's total throughput?\n\nContents\n\n- What\nare some trivial but flawed ways of solving the problem?\n\n- This\nsounds like there's some kind of scalability trilemma at play. What is\nthis trilemma and can we break through it?\n\n- What\nare some moderately simple but only partial ways of solving the\nscalability problem?\n\n- What\nabout approaches that do not try to \"shard\" anything?\n\n- How\ndoes Plasma, state channels and other layer 2 technologies fit into the\ntrilemma?\n\n- State\nsize, history, cryptoeconomics, oh my! Define some of these terms before\nwe move further!\n\n- What is the basic\nidea behind sharding?\n\n- What\nmight a basic design of a sharded blockchain look like?\n\n- What are the challenges\nhere?\n\n- But\ndoesn't the CAP theorem mean that fully secure distributed systems are\nimpossible, and so sharding is futile?\n\n- What\nare the security models that we are operating under?\n\n- How\ncan we solve the single-shard takeover attack in an uncoordinated\nmajority model?\n\n- How\ndo you actually do this sampling in proof of work, and in proof of\nstake?\n\n- How\nis the randomness for random sampling generated?\n\n- What\nare the tradeoffs in making sampling more or less frequent?\n\n- Can\nwe force more of the state to be held user-side so that transactions can\nbe validated without requiring validators to hold all state\ndata?\n\n- Can\nwe split data and execution so that we get the security from rapid\nshuffling data validation without the overhead of shuffling the nodes\nthat perform state execution?\n\n- Can SNARKs and STARKs\nhelp?\n\n- How can\nwe facilitate cross-shard communication?\n\n- What is the\ntrain-and-hotel problem?\n\n- What\nare the concerns about sharding through random sampling in a bribing\nattacker or coordinated choice model?\n\n- How can we improve on\nthis?\n\n- What\nis the data availability problem, and how can we use erasure codes to\nsolve it?\n\n- Can\nwe remove the need to solve data availability with some kind of fancy\ncryptographic accumulator scheme?\n\n- So\nthis means that we can actually create scalable sharded blockchains\nwhere the cost of making anything bad happen is proportional to the size\nof the entire validator set?\n\n- Let's\nwalk back a bit. Do we actually need any of this complexity if we have\ninstant shuffling? Doesn't instant shuffling basically mean that each\nshard directly pulls validators from the global validator pool so it\noperates just like a blockchain, and so sharding doesn't actually\nintroduce any new complexities?\n\n- You\nmentioned transparent sharding. I'm 12 years old and what is\nthis?\n\n- What\nare some advantages and disadvantages of this?\n\n- How would\nsynchronous cross-shard messages work?\n\n- What about\nsemi-asynchronous messages?\n\n- What are guaranteed\ncross-shard calls?\n\n- Wait,\nbut what if an attacker sends a cross-shard call from every shard into\nshard X at the same time? Wouldn't it be mathematically impossible to\ninclude all of these calls in time?\n\n- Congealed\ngas? This sounds interesting for not just cross-shard operations, but\nalso reliable intra-shard scheduling\n\n- Does\nguaranteed scheduling, both intra-shard and cross-shard, help against\nmajority collusions trying to censor transactions?\n\n- Could\nsharded blockchains do a better job of dealing with network\npartitions?\n\n- What\nare the unique challenges of pushing scaling past n = O(c^2)?\n\n- What about\nheterogeneous sharding?\n\n- Footnotes\n\nWhat\nare some trivial but flawed ways of solving the problem?\n\nThere are three main categories of \"easy solutions\". The first is to\ngive up on scaling individual blockchains, and instead assume that\napplications will be split among many different chains. This greatly\nincreases throughput, but at a cost of security: an N-factor increase in\nthroughput using this method necessarily comes with an N-factor decrease\nin security, as a level of resources 1/N the size of the whole ecosystem\nwill be sufficient to attack any individual chain. Hence, it is arguably\nnon-viable for more than small values of N.\n\nThe second is to simply increase the block size limit. This can work\nand in some situations may well be the correct prescription, as block\nsizes may well be constrained more by politics than by realistic\ntechnical considerations. But regardless of one's beliefs about any\nindividual case such an approach inevitably has its limits: if one goes\ntoo far, then nodes running on consumer hardware will drop out, the\nnetwork will start to rely exclusively on a very small number of\nsupercomputers running the blockchain, which can lead to great\ncentralization risk.\n\nThe third is \"merge mining\", a technique where there are many chains,\nbut all chains share the same mining power (or, in proof of stake\nsystems, stake). Currently, Namecoin gets a large portion of its\nsecurity from the Bitcoin blockchain by doing this. If all miners\nparticipate, this theoretically can increase throughput by a factor of N\nwithout compromising security. However, this also has the problem that\nit increases the computational and storage load on each miner by a\nfactor of N, and so in fact this solution is simply a stealthy form of\nblock size increase.\nThis\nsounds like there's some kind of scalability trilemma at play. What is\nthis trilemma and can we break through it?\n\nThe trilemma claims that blockchain systems can only at most have two\nof the following three properties:\n\n- Decentralization (defined as the system being able\nto run in a scenario where each participant only has access to O(c)\nresources, i.e.\u00a0a regular laptop or small VPS)\n\n- Scalability (defined as being able to process O(n)\n> O(c) transactions)\n\n- Security (defined as being secure against attackers\nwith up to O(n) resources)\n\nIn the rest of this document, we'll continue using c\nto refer to the size of computational resources (including computation,\nbandwidth and storage) available to each node, and n to\nrefer to the size of the ecosystem in some abstract sense; we assume\nthat transaction load, state size, and the market cap of a\ncryptocurrency are all proportional to n. The key\nchallenge of scalability is finding a way to achieve all three at the\nbase layer.\nWhat\nare some moderately simple but only partial ways of solving the\nscalability problem?\n\nMany sharding proposals (e.g.\u00a0this\nearly BFT sharding proposal from Loi Luu et al at NUS, more recent\napplication of similar ideas in Zilliqa, as well as\nthis\nMerklix tree1 approach that has\nbeen suggested for Bitcoin) attempt to either only shard transaction\nprocessing or only shard state, without touching the other2. These efforts can lead to some gains in\nefficiency, but they run into the fundamental problem that they only\nsolve one of the two bottlenecks. We want to be able to process 10,000+\ntransactions per second without either forcing every node to be a\nsupercomputer or forcing every node to store a terabyte of state data,\nand this requires a comprehensive solution where the workloads of state\nstorage, transaction processing and even transaction downloading and\nre-broadcasting at are all spread out across nodes. Particularly, the\nP2P network needs to also be modified to ensure that not every node\nreceives all information from every other node.\nWhat\nabout approaches that do not try to \"shard\" anything?\n\nBitcoin-NG\ncan increase scalability somewhat by means of an alternative blockchain\ndesign that makes it much safer for the network if nodes are spending\nlarge portions of their CPU time verifying blocks. In simple PoW\nblockchains, there are high centralization risks and the safety of\nconsensus is weakened if capacity is increased to the point where more\nthan about 5% of nodes' CPU time is spent verifying blocks; Bitcoin-NG's\ndesign alleviates this problem. However, this can only increase the\nscalability of transaction capacity by a constant factor of perhaps\n5-50x3,4,\nand does not increase the scalability of state. That said,\nBitcoin-NG-style approaches are not mutually exclusive with sharding,\nand the two can certainly be implemented at the same time.\n\nChannel-based strategies (lightning network, Raiden, etc) can scale\ntransaction capacity by a constant factor but cannot scale state\nstorage, and also come with their own unique sets of tradeoffs and\nlimitations particularly involving denial-of-service attacks. On-chain\nscaling via sharding (plus other techniques) and off-chain scaling via\nchannels are arguably both necessary and complementary.\n\nThere exist approaches that use advanced cryptography, such as Mimblewimble\nand strategies based on ZK-SNARKs (eg. Coda), to solve one specific part\nof the scaling problem: initial full node synchronization. Instead of\nverifying the entire history from genesis, nodes could verify a\ncryptographic proof that the current state legitimately follows from the\nhistory. These approaches do solve a legitimate problem, but they are\nnot a substitute for sharding, as they do not remove the need for nodes\nto download and verify very large amounts of data to stay on the chain\nin real time.\nHow\ndoes Plasma, state channels and other layer 2 technologies fit into the\ntrilemma?\n\nIn the event of a large attack on Plasma subchains, all users of the\nPlasma subchains would need to withdraw back to the root chain. If\nPlasma has O(N) users, then this will require O(N) transactions, and so\nO(N / C) time to process all of the withdrawals. If withdrawal delays\nare fixed to some D (i.e.\u00a0the naive implementation), then as soon as N\n> C * D, there will not be enough space in the blockchain to process\nall withdrawals in time, and so the system will be insecure; in this\nmode, Plasma should be viewed as increasing scalability only by a\n(possibly large) constant factor. If withdrawal delays are flexible, so\nthey automatically extend if there are many withdrawals being made, then\nthis means that as N increases further and further, the amount of time\nthat an attacker can force everyone's funds to get locked up increases,\nand so the level of \"security\" of the system decreases further and\nfurther in a certain sense, as extended denial of access can be viewed\nas a security failure, albeit one milder than total loss of access.\nHowever, this is a different direction of tradeoff from other\nsolutions, and arguably a much milder tradeoff, hence why Plasma\nsubchains are nevertheless a large improvement on the status quo.\n\nNote that there is one design that states that: \"Given a malicious\noperator (the worst case), the system degrades to an on-chain token. A\nmalicious operator cannot steal funds and cannot deprive people of their\nfunds for any meaningful amount of\ntime.\"\u2014https://ethresear.ch/t/roll-up-roll-back-snark-side-chain-17000-tps/3675.\nSee also here\nfor related information.\n\nState\nchannels have similar properties, though with different tradeoffs\nbetween versatility and speed of finality. Other layer 2 technologies\ninclude TrueBit\noff-chain interactive verification of execution and Raiden, which is another organisation\nworking on state channels. Proof of\nstake with Casper (which is layer 1) would also improve scaling\u2014it\nis more decentralizable, not requiring a computer that is able to mine,\nwhich tends towards centralized mining farms and institutionalized\nmining pools as difficulty increases and the size of the state of the\nblockchain increases.\n\nSharding is different to state channels and Plasma in that\nperiodically notaries are pseudo-randomly assigned to vote on the\nvalidity of collations (analogous to blocks, but without an EVM state\ntransition function in phase 1), then these collations are accepted into\nthe main chain after the votes are verified by a committee on the main\nchain, via a sharding manager contract on the main chain. In phase 5\n(see the roadmap\nfor details), shards are tightly coupled to the main chain, so that if\nany shard or the main chain is invalid, the whole network is invalid.\nThere are other differences between each mechanism, but at a high level,\nPlasma, state channels and Truebit are off-chain for an indefinite\ninterval, connect to the main chain at the smart contract, layer 2\nlevel, while they can draw back into and open up from the main chain,\nwhereas shards are regularly linked to the main chain via consensus\nin-protocol.\n\nSee also these\ntweets from Vlad.\nState\nsize, history, cryptoeconomics, oh my! Define some of these terms before\nwe move further!\n\n- State: a set of information that represents the\n\"current state\" of a system; determining whether or not a transaction is\nvalid, as well as the effect of a transaction, should in the simplest\nmodel depend only on state. Examples of state data include the UTXO set\nin bitcoin, balances + nonces + code + storage in ethereum, and domain\nname registry entries in Namecoin.\n\n- History: an ordered list of all transactions that\nhave taken place since genesis. In a simple model, the present state\nshould be a deterministic function of the genesis state and the\nhistory.\n\n- Transaction: an object that goes into the history.\nIn practice, a transaction represents an operation that some user wants\nto make, and is cryptographically signed. In some systems transactions\nare called blobs, to emphasize the fact that in these\nsystems these objects may contain arbitrary data and may not in all\ncases represent an attempt to perform some operation in the\nprotocol.\n\n- State transition function: a function that takes a\nstate, applies a transaction and outputs a new state. The computation\ninvolved may involve adding and subtracting balances from accounts\nspecified by the transaction, verifying digital signatures and running\ncontract code.\n\n- Merkle tree: a cryptographic hash tree structure\nthat can store a very large amount of data, where authenticating each\nindividual piece of data only takes O(log(n)) space and time. See here\nfor details. In Ethereum, the transaction set of each block, as well as\nthe state, is kept in a Merkle tree, where the roots of the trees are\ncommitted to in a block.\n\n- Receipt: an object that represents an effect of a\ntransaction that is not directly stored in the state, but which is still\nstored in a Merkle tree and committed to in a block header or in a\nspecial location in the state so that its existence can later be\nefficiently proven even to a node that does not have all of the data.\nLogs in Ethereum are receipts; in sharded models, receipts are used to\nfacilitate asynchronous cross-shard communication.\n\n- Light client: a way of interacting with a\nblockchain that only requires a very small amount (we'll say O(1),\nthough O(log(c)) may also be accurate in some cases) of computational\nresources, keeping track of only the block headers of the chain by\ndefault and acquiring any needed information about transactions, state\nor receipts by asking for and verifying Merkle proofs of the relevant\ndata on an as-needed basis.\n\n- State root: the root hash of the Merkle tree\nrepresenting the state5\n\nThe Ethereum 1.0 state tree, and how the state root fits into\nthe block structure\n\nWhat is the basic idea\nbehind sharding?\n\nWe split the state and history up into K = O(n / c) partitions that\nwe call \"shards\". For example, a sharding scheme on Ethereum might put\nall addresses starting with 0x00 into one shard, all addresses starting\nwith 0x01 into another shard, etc. In the simplest form of sharding,\neach shard also has its own transaction history, and the effect of\ntransactions in some shard k are limited to the state of shard k. One\nsimple example would be a multi-asset blockchain, where there are K\nshards and each shard stores the balances and processes the transactions\nassociated with one particular asset. In more advanced forms of\nsharding, some form of cross-shard communication capability, where\ntransactions on one shard can trigger events on other shards, is also\nincluded.\nWhat\nmight a basic design of a sharded blockchain look like?\n\nA simple approach is as follows. For simplicity, this design keeps\ntrack of data blobs only; it does not attempt to process a state\ntransition function.\n\nThere exists a set of validators (ie. proof of stake\nnodes), who randomly get assigned the right to create shard\nblocks. During each slot (eg. an 8-second\nperiod of time), for each k in [0...999] a\nrandom validator gets selected, and given the right to create a block on\n\"shard k\", which might contain up to, say, 32 kb of data.\nAlso, for each k, a set of 100 validators get selected as\nattesters. The header of a block together with at least\n67 of the attesting signatures can be published as an object that gets\nincluded in the \"main chain\" (also called a beacon\nchain).\n\nNote that there are now several \"levels\" of nodes that can exist in\nsuch a system:\n\n- Super-full node - downloads the full data of the\nbeacon chain and every shard block referenced in the beacon chain.\n\n- Top-level node - processes the beacon chain blocks\nonly, including the headers and signatures of the shard blocks, but does\nnot download all the data of the shard blocks.\n\n- Single-shard node - acts as a top-level node, but\nalso fully downloads and verifies every collation on some specific shard\nthat it cares more about.\n\n- Light node - downloads and verifies the block\nheaders of main chain blocks only; does not process any collation\nheaders or transactions unless it needs to read some specific entry in\nthe state of some specific shard, in which case it downloads the Merkle\nbranch to the most recent collation header for that shard and from there\ndownloads the Merkle proof of the desired value in the state.\n\n## What are the challenges here?\n\n- Single-shard takeover attacks - what if an attacker\ntakes over the majority of the validators responsible for attesting to\none particular block, either to (respectively) prevent any collations\nfrom getting enough signatures or, worse, to submit collations that are\ninvalid?\n\n- State transition execution - single-shard takeover\nattacks are typically prevented with random sampling schemes, but such\nschemes also make it more difficult for validators to compute state\nroots, as they cannot have up-to-date state information for every shard\nthat they could be assigned to. How do we ensure that light clients can\nstill get accurate information about the state?\n\n- Fraud detection - if an invalid collation or state\nclaim does get made, how can nodes (including light nodes) be reliably\ninformed of this so that they can detect the fraud and reject the\ncollation if it is truly fraudulent?\n\n- Cross shard communication - the above design\nsupports no cross-shard communication. How do we add cross-shard\ncommunication safely?\n\n- The data availability problem - as a subset of\nfraud detection, what about the specific case where data is missing from\na collation?\n\n- Superquadratic sharding - in the special case where\nn > c^2, in the simple design given above there would be more than\nO(c) collation headers, and so an ordinary node would not be able to\nprocess even just the top-level blocks. Hence, more than two levels of\nindirection between transactions and top-level block headers are\nrequired (i.e.\u00a0we need \"shards of shards\"). What is the simplest and\nbest way to do this?\n\nHowever, the effect of a transaction may depend on events that\nearlier took place in other shards; a canonical example is transfer\nof money, where money can be moved from shard i to shard j by first\ncreating a \"debit\" transaction that destroys coins in shard i, and then\ncreating a \"credit\" transaction that creates coins in shard j, pointing\nto a receipt created by the debit transaction as proof that the credit\nis legitimate.\nBut\ndoesn't the CAP theorem mean that fully secure distributed systems are\nimpossible, and so sharding is futile?\n\nThe CAP theorem is a result that has to do with distributed\nconsensus; a simple statement is: \"in the cases that a network\npartition takes place, you have to choose either consistency or\navailability, you cannot have both\". The intuitive argument is simple:\nif the network splits in half, and in one half I send a transaction\n\"send my 10 coins to A\" and in the other I send a transaction \"send my\n10 coins to B\", then either the system is unavailable, as one or both\ntransactions will not be processed, or it becomes inconsistent, as one\nhalf of the network will see the first transaction completed and the\nother half will see the second transaction completed. Note that the CAP\ntheorem has nothing to do with scalability; it applies to any situation\nwhere multiple nodes need to agree on a value, regardless of the amount\nof data that they are agreeing on. All existing decentralized systems\nhave found some compromise between availability and consistency;\nsharding does not make anything fundamentally harder in this\nrespect.\nWhat\nare the security models that we are operating under?\n\nThere are several competing models under which the safety of\nblockchain designs is evaluated:\n\n- Honest majority (or honest supermajority): we\nassume that there is some set of validators and up to 50% (or 33% or\n25%) of those validators are controlled by an attacker, and the\nremaining validators honestly follow the protocol. Honest majority\nmodels can have non-adaptive or\nadaptive adversaries; an adversary is adaptive if they\ncan quickly choose which portion of the validator set to \"corrupt\", and\nnon-adaptive if they can only make that choice far ahead of time. Note\nthat the honest majority assumption may be higher for notary committees\nwith a 61%\nhonesty assumption.\n\n- Uncoordinated majority: we assume that all\nvalidators are rational in a game-theoretic sense (except the attacker,\nwho is motivated to make the network fail in some way), but no more than\nsome fraction (often between 25% and 50%) are capable of coordinating\ntheir actions.\n\n- Coordinated choice: we assume that most or all\nvalidators are controlled by the same actor, or are fully capable of\ncoordinating on the economically optimal choice between themselves. We\ncan talk about the cost to the coalition (or profit to\nthe coalition) of achieving some undesirable outcome.\n\n- Bribing attacker model: we take the uncoordinated\nmajority model, but instead of making the attacker be one of the\nparticipants, the attacker sits outside the protocol, and has the\nability to bribe any participants to change their behavior. Attackers\nare modeled as having a budget, which is the maximum\nthat they are willing to pay, and we can talk about their\ncost, the amount that they end up paying to\ndisrupt the protocol equilibrium.\n\nBitcoin proof of work with Eyal and Sirer's selfish mining\nfix is robust up to 50% under the honest majority assumption, and up\nto ~23.21% under the uncoordinated majority assumption. Schellingcoin\nis robust up to 50% under the honest majority and uncoordinated majority\nassumptions, has \u03b5 (i.e.\u00a0slightly more than zero) cost of attack in a\ncoordinated choice model, and has a P + \u03b5 budget requirement and \u03b5 cost\nin a bribing attacker model due to P +\nepsilon attacks.\n\nHybrid models also exist; for example, even in the coordinated choice\nand bribing attacker models, it is common to make an honest\nminority assumption that some portion (perhaps 1-15%) of\nvalidators will act altruistically regardless of incentives. We can also\ntalk about coalitions consisting of between 50-99% of validators either\ntrying to disrupt the protocol or harm other validators; for example, in\nproof of work, a 51%-sized coalition can double its revenue by refusing\nto include blocks from all other miners.\n\nThe honest majority model is arguably highly unrealistic and has\nalready been empirically disproven - see Bitcoin's SPV\nmining fork for a practical example. It proves too much: for\nexample, an honest majority model would imply that honest miners are\nwilling to voluntarily burn their own money if doing so punishes\nattackers in some way. The uncoordinated majority assumption may be\nrealistic; there is also an intermediate model where the majority of\nnodes is honest but has a budget, so they shut down if they start to\nlose too much money.\n\nThe bribing attacker model has in some cases been criticized as being\nunrealistically adversarial, although its proponents argue that if a\nprotocol is designed with the bribing attacker model in mind then it\nshould be able to massively reduce the cost of consensus, as 51% attacks\nbecome an event that could be recovered from. We will evaluate sharding\nin the context of both uncoordinated majority and bribing attacker\nmodels. Bribing attacker models are similar to maximally-adaptive\nadversary models, except that the adversary has the additional power\nthat it can solicit private information from all nodes; this distinction\ncan be crucial, for example Algorand\nis secure under adaptive adversary models but not bribing attacker\nmodels because of how it relies on private information for random\nselection.\nHow\ncan we solve the single-shard takeover attack in an uncoordinated\nmajority model?\n\nIn short, random sampling. Each shard is assigned a certain number of\nnotaries (e.g.\u00a0150), and the notaries that approve collations on each\nshard are taken from the sample for that shard. Samples can be\nreshuffled either semi-frequently (e.g.\u00a0once every 12 hours) or\nmaximally frequently (i.e.\u00a0there is no real independent sampling\nprocess, notaries are randomly selected for each shard from a global\npool every block).\n\nSampling can be explicit, as in protocols that choose specifically\nsized \"committees\" and ask them to vote on the validity and availability\nof specific collations, or it can be implicit, as in the case of\n\"longest chain\" protocols where nodes pseudorandomly assigned to build\non specific collations and are expected to \"windback verify\" at least N\nancestors of the collation they are building on.\n\nThe result is that even though only a few nodes are verifying and\ncreating blocks on each shard at any given time, the level of security\nis in fact not much lower, in an honest or uncoordinated majority model,\nthan what it would be if every single node was verifying and creating\nblocks. The reason is simple statistics: if you assume a ~67% honest\nsupermajority on the global set, and if the size of the sample is 150,\nthen with 99.999% probability the honest majority condition will be\nsatisfied on the sample. If you assume a 75% honest supermajority on the\nglobal set, then that probability increases to 99.999999998% (see here for\ncalculation details).\n\nHence, at least in the honest / uncoordinated majority setting, we\nhave:\n\n- Decentralization (each node stores only O(c) data,\nas it's a light client in O(c) shards and so stores O(1) * O(c) = O(c)\ndata worth of block headers, as well as O(c) data corresponding to the\nrecent history of one or several shards that it is assigned to at the\npresent time)\n\n- Scalability (with O(c) shards, each shard having\nO(c) capacity, the maximum capacity is n = O(c^2))\n\n- Security (attackers need to control at least ~33%\nof the entire O(n)-sized validator pool in order to stand a chance of\ntaking over the network).\n\nIn the bribing attacker model (or in the \"very very adaptive\nadversary\" model), things are not so easy, but we will get to this\nlater. Note that because of the imperfections of sampling, the security\nthreshold does decrease from 50% to ~30-40%, but this is still a\nsurprisingly low loss of security for what may be a 100-1000x gain in\nscalability with no loss of decentralization.\nHow\ndo you actually do this sampling in proof of work, and in proof of\nstake?\n\nIn proof of stake, it is easy. There already is an \"active validator\nset\" that is kept track of in the state, and one can simply sample from\nthis set directly. Either an in-protocol algorithm runs and chooses 150\nvalidators for each shard, or each validator independently runs an\nalgorithm that uses a common source of randomness to (provably)\ndetermine which shard they are at any given time. Note that it is very\nimportant that the sampling assignment is \"compulsory\"; validators do\nnot have a choice of what shard they go into. If validators could\nchoose, then attackers with small total stake could concentrate their\nstake onto one shard and attack it, thereby eliminating the system's\nsecurity.\n\nIn proof of work, it is more difficult, as with \"direct\" proof of\nwork schemes one cannot prevent miners from applying their work to a\ngiven shard. It may be possible to use proof-of-file-access\nforms of proof of work to lock individual miners to individual\nshards, but it is hard to ensure that miners cannot quickly download or\ngenerate data that can be used for other shards and thus circumvent such\na mechanism. The best known approach is through a technique invented by\nDominic Williams called \"puzzle towers\", where miners first perform\nproof of work on a common chain, which then inducts them into a proof of\nstake-style validator pool, and the validator pool is then sampled just\nas in the proof-of-stake case.\n\nOne possible intermediate route might look as follows. Miners can\nspend a large (O(c)-sized) amount of work to create a new \"cryptographic\nidentity\". The precise value of the proof of work solution then chooses\nwhich shard they have to make their next block on. They can then spend\nan O(1)-sized amount of work to create a block on that shard, and the\nvalue of that proof of work solution determines which shard they can\nwork on next, and so on8. Note that\nall of these approaches make proof of work \"stateful\" in some way; the\nnecessity of this is fundamental.\nHow is the\nrandomness for random sampling generated?\n\nFirst of all, it is important to note that even if random number\ngeneration is heavily exploitable, this is not a fatal flaw for the\nprotocol; rather, it simply means that there is a medium to high\ncentralization incentive. The reason is that because the randomness is\npicking fairly large samples, it is difficult to bias the randomness by\nmore than a certain amount.\n\nThe simplest way to show this is through the binomial\ndistribution, as described above; if one wishes to avoid a sample of\nsize N being more than 50% corrupted by an attacker, and an attacker has\np% of the global stake pool, the chance of the attacker being able to\nget such a majority during one round is:\n\nHere's a table for what this probability would look like in practice\nfor various values of N and p:\n\nN = 50\n\nN = 100\n\nN = 150\n\nN = 250\n\np = 0.4\n\n0.0978\n\n0.0271\n\n0.0082\n\n0.0009\n\np = 0.33\n\n0.0108\n\n0.0004\n\n1.83 * 10-5\n\n3.98 * 10-8\n\np = 0.25\n\n0.0001\n\n6.63 * 10-8\n\n4.11 * 10-11\n\n1.81 * 10-17\n\n<\n\np = 0.2\n\n2.09 * 10-6\n\n2.14 * 10-11\n\n2.50 * 10-16\n\n3.96 * 10-26\n\nHence, for N >= 150, the chance that any given random seed will\nlead to a sample favoring the attacker is very small indeed11,12. What this\nmeans from the perspective of security of randomness is that the\nattacker needs to have a very large amount of freedom in choosing the\nrandom values order to break the sampling process outright. Most\nvulnerabilities in proof-of-stake randomness do not allow the attacker\nto simply choose a seed; at worst, they give the attacker many chances\nto select the most favorable seed out of many pseudorandomly generated\noptions. If one is very worried about this, one can simply set N to a\ngreater value, and add a moderately hard key-derivation function to the\nprocess of computing the randomness, so that it takes more than\n2100 computational steps to find a way to bias the randomness\nsufficiently.\n\nNow, let's look at the risk of attacks being made that try to\ninfluence the randomness more marginally, for purposes of profit rather\nthan outright takeover. For example, suppose that there is an algorithm\nwhich pseudorandomly selects 1000 validators out of some very large set\n(each validator getting a reward of $1), an attacker has 10% of the\nstake so the attacker's average \"honest\" revenue 100, and at a cost of\n$1 the attacker can manipulate the randomness to \"re-roll the dice\" (and\nthe attacker can do this an unlimited number of times).\n\nDue to the central limit\ntheorem, the standard deviation of the number of samples, and based\non\nother known results in math the expected maximum of N random samples\nis slightly under M + S * sqrt(2 * log(N)) where M is the mean and S is\nthe standard deviation. Hence the reward for manipulating the randomness\nand effectively re-rolling the dice (i.e.\u00a0increasing N) drops off\nsharply, e.g.\u00a0with 0 re-trials your expected reward is $100, with one\nre-trial it's $105.5, with two it's $108.5, with three it's $110.3, with\nfour it's $111.6, with five it's $112.6 and with six it's $113.5. Hence,\nafter five retrials it stops being worth it. As a result, an\neconomically motivated attacker with ten percent of stake will (socially\nwastefully) spend $5 to get an additional revenue of $13, for a net\nsurplus of $8.\n\nHowever, this kind of logic assumes that one single round of\nre-rolling the dice is expensive. Many older proof of stake algorithms\nhave a \"stake grinding\" vulnerability where re-rolling the dice simply\nmeans making a computation locally on one's computer; algorithms with\nthis vulnerability are certainly unacceptable in a sharding context.\nNewer algorithms (see the \"validator selection\" section in the proof of\nstake FAQ) have the property that re-rolling the dice can only be\ndone by voluntarily giving up one's spot in the block creation process,\nwhich entails giving up rewards and fees. The best way to mitigate the\nimpact of marginal economically motivated attacks on sample selection is\nto find ways to increase this cost. One method to increase the cost by a\nfactor of sqrt(N) from N rounds of voting is the majority-bit method devised\nby Iddo Bentov.\n\nAnother form of random number generation that is not exploitable by\nminority coalitions is the deterministic threshold signature approach\nmost researched and advocated by Dominic Williams. The strategy here is\nto use a deterministic\nthreshold signature to generate the random seed from which samples\nare selected. Deterministic threshold signatures have the property that\nthe value is guaranteed to be the same regardless of which of a given\nset of participants provides their data to the algorithm, provided that\nat least \u2154 of participants do participate honestly. This approach is\nmore obviously not economically exploitable and fully resistant to all\nforms of stake-grinding, but it has several weaknesses:\n\n- It relies on more complex cryptography\n(specifically, elliptic curves and pairings). Other approaches rely on\nnothing but the random-oracle assumption for common hash\nalgorithms.\n\n- It fails when many validators are offline. A\ndesired goal for public blockchains is to be able to survive very large\nportions of the network simultaneously disappearing, as long as a\nmajority of the remaining nodes is honest; deterministic threshold\nsignature schemes at this point cannot provide this property.\n\n- It's not secure in a bribing attacker or coordinated\nmajority model where more than 67% of validators are colluding.\nThe other approaches described in the proof of stake FAQ above still\nmake it expensive to manipulate the randomness, as data from all\nvalidators is mixed into the seed and making any manipulation requires\neither universal collusion or excluding other validators outright.\n\nOne might argue that the deterministic threshold signature approach\nworks better in consistency-favoring contexts and other approaches work\nbetter in availability-favoring contexts.\nWhat\nare the tradeoffs in making sampling more or less frequent?\n\nSelection frequency affects just how adaptive adversaries can be for\nthe protocol to still be secure against them; for example, if you\nbelieve that an adaptive attack (e.g.\u00a0dishonest validators who discover\nthat they are part of the same sample banding together and colluding)\ncan happen in 6 hours but not less, then you would be okay with a\nsampling time of 4 hours but not 12 hours. This is an argument in favor\nof making sampling happen as quickly as possible.\n\nThe main challenge with sampling taking place every block is that\nreshuffling carries a very high amount of overhead. Specifically,\nverifying a block on a shard requires knowing the state of that shard,\nand so every time validators are reshuffled, validators need to download\nthe entire state for the new shard(s) that they are in. This requires\nboth a strong state size control policy (i.e.\u00a0economically ensuring that\nthe size of the state does not grow too large, whether by deleting old\naccounts, restricting the rate of creating new accounts or a combination\nof the two) and a fairly long reshuffling time to work well.\n\nCurrently, the Parity client can download and verify a full Ethereum\nstate snapshot via \"warp-sync\" in ~2-8 hours, suggesting that\nreshuffling periods of a few days but not less are safe; perhaps this\ncould be reduced somewhat by shrinking the state size via storage\nrent but even still reshuffling periods would need to be long,\npotentially making the system vulnerable to adaptive adversaries.\n\nHowever, there are ways of completely avoiding the tradeoff, choosing\nthe creator of the next collation in each shard with only a few minutes\nof warning but without adding impossibly high state downloading\noverhead. This is done by shifting responsibility for state storage, and\npossibly even state execution, away from collators entirely, and instead\nassigning the role to either users or an interactive verification\nprotocol.\nCan\nwe force more of the state to be held user-side so that transactions can\nbe validated without requiring validators to hold all state data?\n\nSee also: https://ethresear.ch/t/the-stateless-client-concept/172\n\nThe techniques here tend to involve requiring users to store state\ndata and provide Merkle proofs along with every transaction that they\nsend. A transaction would be sent along with a Merkle\nproof-of-correct-execution (or \"witness\"), and this proof would allow a\nnode that only has the state root to calculate the new state root. This\nproof-of-correct-execution would consist of the subset of objects in the\ntrie that would need to be traversed to access and verify the state\ninformation that the transaction must verify; because Merkle proofs are\nO(log(n)) sized, the proof for a transaction that accesses a constant\nnumber of objects would also be O(log(n)) sized.\n\nThe subset of objects in a Merkle tree that would need to be\nprovided in a Merkle proof of a transaction that accesses several state\nobjects\n\nImplementing this scheme in its pure form has two flaws. First, it\nintroduces O(log(n)) overhead (~10-30x in practice), although one could\nargue that this O(log(n)) overhead is not as bad as it seems because it\nensures that the validator can always simply keep state data in memory\nand thus it never needs to deal with the overhead of accessing the hard\ndrive9. Second, it can easily be\napplied if the addresses that are accessed by a transaction are static,\nbut is more difficult to apply if the addresses in question are dynamic\n- that is, if the transaction execution has code of the form\nread(f(read(x))) where the address of some state read\ndepends on the execution result of some other state read. In this case,\nthe address that the transaction sender thinks the transaction will be\nreading at the time that they send the transaction may well differ from\nthe address that is actually read when the transaction is included in a\nblock, and so the Merkle proof may be insufficient10.\n\nThis can be solved with access lists (think: a list of accounts and\nsubsets of storage tries), which specify statically what data\ntransactions can access, so when a miner receives a transaction with a\nwitness they can determine that the witness contains all of the data the\ntransaction could possibly access or modify. However, this harms\ncensorship resistance, making attacks similar in form to the attempted\nDAO soft fork possible.\nCan\nwe split data and execution so that we get the security from rapid\nshuffling data validation without the overhead of shuffling the nodes\nthat perform state execution?\n\nYes. We can create a protocol where we split up validators into three\nroles with different incentives (so that the incentives do not overlap):\nproposers or collators, a.k.a. prolators,\nnotaries and executors. Prollators are\nresponsible for simply building a chain of collations; while notaries\nverify that the data in the collations is available. Prolators do not\nneed to verify anything state-dependent (e.g.\u00a0whether or not someone\ntrying to send ETH has enough money). Executors take the chain of\ncollations agreed to by the prolators as given, and then execute the\ntransactions in the collations sequentially and compute the state. If\nany transaction included in a collation is invalid, executors simply\nskip over it. This way, validators that verify availability could be\nreshuffled instantly, and executors could stay on one shard.\n\nThere would be a light client protocol that allows light clients to\ndetermine what the state is based on claims signed by executors, but\nthis protocol is NOT a simple majority-voting consensus. Rather, the\nprotocol is an interactive game with some similarities to Truebit, where\nif there is great disagreement then light client simply execute specific\ncollations or portions of collations themselves. Hence, light clients\ncan get a correct view of the state even if 90% of the executors in the\nshard are corrupted, making it much safer to allow executors to be very\ninfrequently reshuffled or even permanently shard-specific.\n\nChoosing what goes in to a collation does require knowing\nthe state of that collation, as that is the most practical way to know\nwhat will actually pay transaction fees, but this can be solved by\nfurther separating the role of collators (who agree on the history) and\nproposers (who propose individual collations) and creating a market\nbetween the two classes of actors; see here\nfor more discussion on this. However, this approach has since been found\nto be flawed as per this\nanalysis.\n\n## Can SNARKs and STARKs help?\n\nYes! One can create a second-level protocol where a SNARK,\nSTARK or similar\nsuccinct zero knowledge proof scheme is used to prove the state root of\na shard chain, and proof creators can be rewarded for this. That said,\nshard chains to actually agree on what data gets included into the shard\nchains in the first place is still required.\nHow can we\nfacilitate cross-shard communication?\n\nThe easiest scenario to satisfy is one where there are very many\napplications that individually do not have too many users, and which\nonly very occasionally and loosely interact with each other; in this\ncase, applications can live on separate shards and use cross-shard\ncommunication via receipts to talk to each other.\n\nThis typically involves breaking up each transaction into a \"debit\"\nand a \"credit\". For example, suppose that we have a transaction where\naccount A on shard M wishes to send 100 coins to account B on shard N.\nThe steps would looks as follows:\n\n- Send a transaction on shard M which (i) deducts the balance of A by\n100 coins, and (ii) creates a receipt. A receipt is an object which is\nnot saved in the state directly, but where the fact that the receipt was\ngenerated can be verified via a Merkle proof.\n\n- Wait for the first transaction to be included (sometimes waiting for\nfinalization is required; this depends on the system).\n\n- Send a transaction on shard N which includes the Merkle proof of the\nreceipt from (1). This transaction also checks in the state of shard N\nto make sure that this receipt is \"unspent\"; if it is, then it increases\nthe balance of B by 100 coins, and saves in the state that the receipt\nis spent.\n\n- Optionally, the transaction in (3) also saves a receipt, which can\nthen be used to perform further actions on shard M that are contingent\non the original operation succeeding.\n\nIn more complex forms of sharding, transactions may in some cases\nhave effects that spread out across several shards and may also\nsynchronously ask for data from the state of multiple shards.\nWhat is the train-and-hotel\nproblem?\n\nThe following example is courtesy of Andrew Miller. Suppose that a\nuser wants to purchase a train ticket and reserve a hotel, and wants to\nmake sure that the operation is atomic - either both reservations\nsucceed or neither do. If the train ticket and hotel booking\napplications are on the same shard, this is easy: create a transaction\nthat attempts to make both reservations, and throws an exception and\nreverts everything unless both reservations succeed. If the two are on\ndifferent shards, however, this is not so easy; even without\ncryptoeconomic / decentralization concerns, this is essentially the\nproblem of atomic\ndatabase transactions.\n\nWith asynchronous messages only, the simplest solution is to first\nreserve the train, then reserve the hotel, then once both reservations\nsucceed confirm both; the reservation mechanism would prevent anyone\nelse from reserving (or at least would ensure that enough spots are open\nto allow all reservations to be confirmed) for some period of time.\nHowever, this means that the mechanism relies on an extra security\nassumptions: that cross-shard messages from one shard can get included\nin another shard within some fixed period of time.\n\nWith cross-shard synchronous transactions, the problem is easier, but\nthe challenge of creating a sharding solution capable of making\ncross-shard atomic synchronous transactions is itself decidedly\nnontrivial; see Vlad Zamfir's presentation which\ntalks about merge blocks.\n\nAnother solution involves making contracts themselves movable across\nshards; see the proposed cross-shard\nlocking scheme as well as this\nproposal where contracts can be \"yanked\" from one shard to another,\nallowing two contracts that normally reside on different shards to be\ntemporarily moved to the same shard at which point a synchronous\noperation between them can happen.\nWhat\nare the concerns about sharding through random sampling in a bribing\nattacker or coordinated choice model?\n\nIn a bribing attacker or coordinated choice model, the fact that\nvalidators are randomly sampled doesn't matter: whatever the sample is,\neither the attacker can bribe the great majority of the sample to do as\nthe attacker pleases, or the attacker controls a majority of the sample\ndirectly and can direct the sample to perform arbitrary actions at low\ncost (O(c) cost, to be precise).\n\nAt that point, the attacker has the ability to conduct 51% attacks\nagainst that sample. The threat is further magnified because there is a\nrisk of cross-shard contagion: if the attacker corrupts the state of a\nshard, the attacker can then start to send unlimited quantities of funds\nout to other shards and perform other cross-shard mischief. All in all,\nsecurity in the bribing attacker or coordinated choice model is not much\nbetter than that of simply creating O(c) altcoins.\n\n## How can we improve on this?\n\nIn the context of state execution, we can use interactive\nverification protocols that are not randomly sampled majority votes, and\nthat can give correct answers even if 90% of the participants are\nfaulty; see Truebit\nfor an example of how this can be done. For data availability, the\nproblem is harder, though there are several strategies that can be used\nalongside majority votes to solve it.\nWhat\nis the data availability problem, and how can we use erasure codes to\nsolve it?\n\nSee\nhttps://github.com/ethereum/research/wiki/A-note-on-data-availability-and-erasure-coding\nCan\nwe remove the need to solve data availability with some kind of fancy\ncryptographic accumulator scheme?\n\nNo.\u00a0Suppose there is a scheme where there exists an object S\nrepresenting the state (S could possibly be a hash) possibly as well as\nauxiliary information (\"witnesses\") held by individual users that can\nprove the presence of existing state objects (e.g.\u00a0S is a Merkle root,\nthe witnesses are the branches, though other constructions like RSA\naccumulators do exist). There exists an updating protocol where some\ndata is broadcasted, and this data changes S to change the contents of\nthe state, and also possibly changes witnesses.\n\nSuppose some user has the witnesses for a set of N objects in the\nstate, and M of the objects are updated. After receiving the update\ninformation, the user can check the new status of all N objects, and\nthereby see which M were updated. Hence, the update information itself\nencoded at least ~M * log(N) bits of information. Hence, the update\ninformation that everyone needs for receive to implement the effect of M\ntransactions must necessarily be of size O(M). 14\nSo\nthis means that we can actually create scalable sharded blockchains\nwhere the cost of making anything bad happen is proportional to the size\nof the entire validator set?\n\nThere is one trivial attack by which an attacker can always burn O(c)\ncapital to temporarily reduce the quality of a shard: spam it by sending\ntransactions with high transaction fees, forcing legitimate users to\noutbid you to get in. This attack is unavoidable; you could compensate\nwith flexible gas limits, and you could even try \"transparent sharding\"\nschemes that try to automatically re-allocate nodes to shards based on\nusage, but if some particular application is non-parallelizable,\nAmdahl's law guarantees that there is nothing you can do. The attack\nthat is opened up here (reminder: it only works in the Zamfir model, not\nhonest/uncoordinated majority) is arguably not substantially worse than\nthe transaction spam attack. Hence, we've reached the known limit for\nthe security of a single shard, and there is no value in trying to go\nfurther.\nLet's\nwalk back a bit. Do we actually need any of this complexity if we have\ninstant shuffling? Doesn't instant shuffling basically mean that each\nshard directly pulls validators from the global validator pool so it\noperates just like a blockchain, and so sharding doesn't actually\nintroduce any new complexities?\n\nKind of. First of all, it's worth noting that proof of work and\nsimple proof of stake, even without sharding, both have very low\nsecurity in a bribing attacker model; a block is only truly \"finalized\"\nin the economic sense after O(n) time (as if only a few blocks have\npassed, then the economic cost of replacing the chain is simply the cost\nof starting a double-spend from before the block in question). Casper\nsolves this problem by adding its finality mechanism, so that the\neconomic security margin immediately increases to the maximum. In a\nsharded chain, if we want economic finality then we need to come up with\na chain of reasoning for why a validator would be willing to make a very\nstrong claim on a chain based solely on a random sample, when the\nvalidator itself is convinced that the bribing attacker and coordinated\nchoice models may be true and so the random sample could potentially be\ncorrupted.\nYou\nmentioned transparent sharding. I'm 12 years old and what is this?\n\nBasically, we do not expose the concept of \"shards\" directly to\ndevelopers, and do not permanently assign state objects to specific\nshards. Instead, the protocol has an ongoing built-in load-balancing\nprocess that shifts objects around between shards. If a shard gets too\nbig or consumes too much gas it can be split in half; if two shards get\ntoo small and talk to each other very often they can be combined\ntogether; if all shards get too small one shard can be deleted and its\ncontents moved to various other shards, etc.\n\nImagine if Donald Trump realized that people travel between New York\nand London a lot, but there's an ocean in the way, so he could just take\nout his scissors, cut out the ocean, glue the US east coast and Western\nEurope together and put the Atlantic beside the South Pole - it's kind\nof like that.\nWhat are\nsome advantages and disadvantages of this?\n\n- Developers no longer need to think about shards\n\n- There's the possibility for shards to adjust manually to changes in\ngas prices, rather than relying on market mechanics to increase gas\nprices in some shards more than others\n\n- There is no longer a notion of reliable co-placement: if two\ncontracts are put into the same shard so that they can interact with\neach other, shard changes may well end up separating them\n\n- More protocol complexity\n\nThe co-placement problem can be mitigated by introducing a notion of\n\"sequential domains\", where contracts may specify that they exist in the\nsame sequential domain, in which case synchronous communication between\nthem will always be possible. In this model a shard can be viewed as a\nset of sequential domains that are validated together, and where\nsequential domains can be rebalanced between shards if the protocol\ndetermines that it is efficient to do so.\nHow would\nsynchronous cross-shard messages work?\n\nThe process becomes much easier if you view the transaction history\nas being already settled, and are simply trying to calculate the state\ntransition function. There are several approaches; one fairly simple\napproach can be described as follows:\n\n- A transaction may specify a set of shards that it can operate\nin\n\n- In order for the transaction to be effective, it must be included at\nthe same block height in all of these shards.\n\n- Transactions within a block must be put in order of their hash (this\nensures a canonical order of execution)\n\nA client on shard X, if it sees a transaction with shards (X, Y),\nrequests a Merkle proof from shard Y verifying (i) the presence of that\ntransaction on shard Y, and (ii) what the pre-state on shard Y is for\nthose bits of data that the transaction will need to access. It then\nexecutes the transaction and commits to the execution result. Note that\nthis process may be highly inefficient if there are many transactions\nwith many different \"block pairings\" in each block; for this reason, it\nmay be optimal to simply require blocks to specify sister shards, and\nthen calculation can be done more efficiently at a per-block level. This\nis the basis for how such a scheme could work; one could imagine more\ncomplex designs. However, when making a new design, it's always\nimportant to make sure that low-cost denial of service attacks cannot\narbitrarily slow state calculation down.\nWhat about\nsemi-asynchronous messages?\n\nVlad Zamfir created a scheme by which asynchronous messages could\nstill solve the \"book a train and hotel\" problem. This works as follows.\nThe state keeps track of all operations that have been recently made, as\nwell as the graph of which operations were triggered by any given\noperation (including cross-shard operations). If an operation is\nreverted, then a receipt is created which can then be used to revert any\neffect of that operation on other shards; those reverts may then trigger\ntheir own reverts and so forth. The argument is that if one biases the\nsystem so that revert messages can propagate twice as fast as other\nkinds of messages, then a complex cross-shard transaction that finishes\nexecuting in K rounds can be fully reverted in another K rounds.\n\nThe overhead that this scheme would introduce has arguably not been\nsufficiently studied; there may be worst-case scenarios that trigger\nquadratic execution vulnerabilities. It is clear that if transactions\nhave effects that are more isolated from each other, the overhead of\nthis mechanism is lower; perhaps isolated executions can be incentivized\nvia favorable gas cost rules. All in all, this is one of the more\npromising research directions for advanced sharding.\nWhat are guaranteed\ncross-shard calls?\n\nOne of the challenges in sharding is that when a call is made, there\nis by default no hard protocol-provided guarantee that any asynchronous\noperations created by that call will be made within any particular\ntimeframe, or even made at all; rather, it is up to some party to send a\ntransaction in the destination shard triggering the receipt. This is\nokay for many applications, but in some cases it may be problematic for\nseveral reasons:\n\n- There may be no single party that is clearly incentivized to trigger\na given receipt. If the sending of a transaction benefits many parties,\nthen there could be tragedy-of-the-commons effects\nwhere the parties try to wait longer until someone else sends the\ntransaction (i.e.\u00a0play \"chicken\"), or simply decide that sending the\ntransaction is not worth the transaction fees for them\nindividually.\n\n- Gas prices across shards may be volatile, and in\nsome cases performing the first half of an operation compels the user to\n\"follow through\" on it, but the user may have to end up following\nthrough at a much higher gas price. This may be exacerbated by DoS\nattacks and related forms of griefing.\n\n- Some applications rely on there being an upper bound on the\n\"latency\" of cross-shard messages (e.g.\u00a0the train-and-hotel example).\nLacking hard guarantees, such applications would have to have\ninefficiently large safety margins.\n\nOne could try to come up with a system where asynchronous messages\nmade in some shard automatically trigger effects in their destination\nshard after some number of blocks. However, this requires every client\non each shard to actively inspect all other shards in the process of\ncalculating the state transition function, which is arguably a source of\ninefficiency. The best known compromise approach is this: when a receipt\nfrom shard A at height height_a is included in shard B at\nheight height_b, if the difference in block heights exceeds\nMAX_HEIGHT, then all validators in shard B that created\nblocks from height_a + MAX_HEIGHT + 1 to\nheight_b - 1 are penalized, and this penalty increases\nexponentially. A portion of these penalties is given to the validator\nthat finally includes the block as a reward. This keeps the state\ntransition function simple, while still strongly incentivizing the\ncorrect behavior.\nWait,\nbut what if an attacker sends a cross-shard call from every shard into\nshard X at the same time? Wouldn't it be mathematically impossible to\ninclude all of these calls in time?\n\nCorrect; this is a problem. Here is a proposed solution. In order to\nmake a cross-shard call from shard A to shard B, the caller must\npre-purchase \"congealed shard B gas\" (this is done via a transaction in\nshard B, and recorded in shard B). Congealed shard B gas has a fast\ndemurrage rate: once ordered, it loses 1/k of its remaining potency\nevery block. A transaction on shard A can then send the congealed shard\nB gas along with the receipt that it creates, and it can be used on\nshard B for free. Shard B blocks allocate extra gas space specifically\nfor these kinds of transactions. Note that because of the demurrage\nrules, there can be at most GAS_LIMIT * k worth of congealed gas for a\ngiven shard available at any time, which can certainly be filled within\nk blocks (in fact, even faster due to demurrage, but we may need this\nslack space due to malicious validators). In case too many validators\nmaliciously fail to include receipts, we can make the penalties fairer\nby exempting validators who fill up the \"receipt space\" of their blocks\nwith as many receipts as possible, starting with the oldest ones.\n\nUnder this pre-purchase mechanism, a user that wants to perform a\ncross-shard operation would first pre-purchase gas for all shards that\nthe operation would go into, over-purchasing to take into account the\ndemurrage. If the operation would create a receipt that triggers an\noperation that consumes 100000 gas in shard B, the user would pre-buy\n100000 * e (i.e.\u00a0271818) shard-B congealed gas. If that operation would\nin turn spend 100000 gas in shard C (i.e.\u00a0two levels of indirection),\nthe user would need to pre-buy 100000 * e^2 (i.e.\u00a0738906) shard-C\ncongealed gas. Notice how once the purchases are confirmed, and the user\nstarts the main operation, the user can be confident that they will be\ninsulated from changes in the gas price market, unless validators\nvoluntarily lose large quantities of money from receipt non-inclusion\npenalties.\nCongealed\ngas? This sounds interesting for not just cross-shard operations, but\nalso reliable intra-shard scheduling\n\nIndeed; you could buy congealed shard A gas inside of shard A, and\nsend a guaranteed cross-shard call from shard A to itself. Though note\nthat this scheme would only support scheduling at very short time\nintervals, and the scheduling would not be exact to the block; it would\nonly be guaranteed to happen within some period of time.\nDoes\nguaranteed scheduling, both intra-shard and cross-shard, help against\nmajority collusions trying to censor transactions?\n\nYes. If a user fails to get a transaction in because colluding\nvalidators are filtering the transaction and not accepting any blocks\nthat include it, then the user could send a series of messages which\ntrigger a chain of guaranteed scheduled messages, the last of which\nreconstructs the transaction inside of the EVM and executes it.\nPreventing such circumvention techniques is practically impossible\nwithout shutting down the guaranteed scheduling feature outright and\ngreatly restricting the entire protocol, and so malicious validators\nwould not be able to do it easily.\nCould\nsharded blockchains do a better job of dealing with network\npartitions?\n\nThe schemes described in this document would offer no improvement\nover non-sharded blockchains; realistically, every shard would end up\nwith some nodes on both sides of the partition. There have been calls\n(e.g.\u00a0from IPFS's\nJuan Benet) for building scalable networks with the specific goal\nthat networks can split up into shards as needed and thus continue\noperating as much as possible under network partition conditions, but\nthere are nontrivial cryptoeconomic challenges in making this work\nwell.\n\nOne major challenge is that if we want to have location-based\nsharding so that geographic network partitions minimally hinder\nintra-shard cohesion (with the side effect of having very low\nintra-shard latencies and hence very fast intra-shard block times), then\nwe need to have a way for validators to choose which shards they are\nparticipating in. This is dangerous, because it allows for much larger\nclasses of attacks in the honest/uncoordinated majority model, and hence\ncheaper attacks with higher griefing factors in the Zamfir model.\nSharding for geographic partition safety and sharding via random\nsampling for efficiency are two fundamentally different things.\n\nSecond, more thinking would need to go into how applications are\norganized. A likely model in a sharded blockchain as described above is\nfor each \"app\" to be on some shard (at least for small-scale apps);\nhowever, if we want the apps themselves to be partition-resistant, then\nit means that all apps would need to be cross-shard to some extent.\n\nOne possible route to solving this is to create a platform that\noffers both kinds of shards - some shards would be higher-security\n\"global\" shards that are randomly sampled, and other shards would be\nlower-security \"local\" shards that could have properties such as\nultra-fast block times and cheaper transaction fees. Very low-security\nshards could even be used for data-publishing and messaging.\nWhat\nare the unique challenges of pushing scaling past n = O(c^2)?\n\nThere are several considerations. First, the algorithm would need to\nbe converted from a two-layer algorithm to a stackable n-layer\nalgorithm; this is possible, but is complex. Second, n / c (i.e.\u00a0the\nratio between the total computation load of the network and the capacity\nof one node) is a value that happens to be close to two constants:\nfirst, if measured in blocks, a timespan of several hours, which is an\nacceptable \"maximum security confirmation time\", and second, the ratio\nbetween rewards and deposits (an early computation suggests a 32 ETH\ndeposit size and a 0.05 ETH block reward for Casper). The latter has the\nconsequence that if rewards and penalties on a shard are escalated to be\non the scale of validator deposits, the cost of continuing an attack on\na shard will be O(n) in size.\n\nGoing above c^2 would likely entail further weakening the kinds of\nsecurity guarantees that a system can provide, and allowing attackers to\nattack individual shards in certain ways for extended periods of time at\nmedium cost, although it should still be possible to prevent invalid\nstate from being finalized and to prevent finalized state from being\nreverted unless attackers are willing to pay an O(n) cost. However, the\nrewards are large - a super-quadratically sharded blockchain could be\nused as a general-purpose tool for nearly all decentralized\napplications, and could sustain transaction fees that makes its use\nvirtually free.\nWhat about heterogeneous\nsharding?\n\nAbstracting the execution engine or allowing multiple execution\nengines to exist results in being able to have a different execution\nengine for each shard. Due to Casper CBC being able to explore the full\ntradeoff\ntriangle, it is possible to alter the parameters of the consensus\nengine for each shard to be at any point of the triangle. However, CBC\nCasper has not been implemented yet, and heterogeneous sharding is\nnothing more than an idea at this stage; the specifics of how it would\nwork has not been designed nor implemented. Some shards could be\noptimized to have fast finality and high throughput, which is important\nfor applications such as EFTPOS transactions, while maybe most could\nhave a moderate or reasonable amount each of finality, throughput and\ndecentralization (number of validating nodes), and applications that are\nprone to a high fault rate and thus require high security, such as\ntorrent networks, privacy focused email like Proton mail, etc., could\noptimize for a high decentralization, low finality and high throughput,\netc. See also https://twitter.com/VladZamfir/status/932320997021171712\nand https://ethresear.ch/t/heterogeneous-sharding/1979/2.\n\n## Footnotes\n\n- \n\n Merklix tree == Merkle Patricia\ntree\n\n- \n\n Later proposals from the NUS group do\nmanage to shard state; they do this via the receipt and state-compacting\ntechniques that I describe in later sections in this document. (This is\nVitalik Buterin writing as the creator of this Wiki.)\n\n- \n\n There are reasons to be conservative\nhere. Particularly, note that if an attacker comes up with worst-case\ntransactions whose ratio between processing time and block space\nexpenditure (bytes, gas, etc) is much higher than usual, then the system\nwill experience very low performance, and so a safety factor is\nnecessary to account for this possibility. In traditional blockchains,\nthe fact that block processing only takes ~1-5% of block time has the\nprimary role of protecting against centralization risk but serves double\nduty of protecting against denial of service risk. In the specific case\nof Bitcoin, its current worst-case known\nquadratic execution vulnerability arguably limits any scaling at\npresent to ~5-10x, and in the case of Ethereum, while all known\nvulnerabilities are being or have been removed after the\ndenial-of-service attacks, there is still a risk of further\ndiscrepancies particularly on a smaller scale. In Bitcoin NG, the need\nfor the former is removed, but the need for the latter is still\nthere.\n\n- \n\n A further reason to be cautious is that\nincreased state size corresponds to reduced throughput, as nodes will\nfind it harder and harder to keep state data in RAM and so need more and\nmore disk accesses, and databases, which often have an O(log(n)) access\ntime, will take longer and longer to access. This was an important\nlesson from the last Ethereum denial-of-service attack, which bloated\nthe state by ~10 GB by creating empty accounts and thereby indirectly\nslowed processing down by forcing further state accesses to hit disk\ninstead of RAM.\n\n- \n\n In sharded blockchains, there may not\nnecessarily be in-lockstep consensus on a single global state, and so\nthe protocol never asks nodes to try to compute a global state root; in\nfact, in the protocols presented in later sections, each shard has its\nown state, and for each shard there is a mechanism for committing to the\nstate root for that shard, which represents that shard's state\n\n- \n\n #MEGA\n\n- \n\n If a non-scalable blockchain upgrades\ninto a scalable blockchain, the author's recommended path is that the\nold chain's state should simply become a single shard in the new\nchain.\n\n- \n\n For this to be secure, some further\nconditions must be satisfied; particularly, the proof of work must be\nnon-outsourceable in order to prevent the attacker from determining\nwhich other miners' identities are available for some given shard\nand mining on top of those.\n\n- \n\n Recent Ethereum denial-of-service\nattacks have proven that hard drive access is a primary bottleneck to\nblockchain scalability.\n\n- \n\n You could ask: well why don't\nvalidators fetch Merkle proofs just-in-time? Answer: because doing so is\na ~100-1000ms roundtrip, and executing an entire complex transaction\nwithin that time could be prohibitive.\n\n- \n\n One hybrid solution that combines the\nnormal-case efficiency of small samples with the greater robustness of\nlarger samples is a multi-layered sampling scheme: have a consensus\nbetween 50 nodes that requires 80% agreement to move forward, and then\nonly if that consensus fails to be reached then fall back to a 250-node\nsample. N = 50 with an 80% threshold has only a 8.92 * 10-9 failure rate\neven against attackers with p = 0.4, so this does not harm security at\nall under an honest or uncoordinated majority model.\n\n- \n\n The probabilities given are for one\nsingle shard; however, the random seed affects O(c) shards and the\nattacker could potentially take over any one of them. If we want to look\nat O(c) shards simultaneously, then there are two cases. First, if the\ngrinding process is computationally bounded, then this fact does not\nchange the calculus at all, as even though there are now O(c) chances of\nsuccess per round, checking success takes O(c) times as much work.\nSecond, if the grinding process is economically bounded, then this\nindeed calls for somewhat higher safety factors (increasing N by 10-20\nshould be sufficient) although it's important to note that the goal of\nan attacker in a profit-motivated manipulation attack is to increase\ntheir participation across all shards in any case, and so that is the\ncase that we are already investigating.\n\n- \n\n See Parity's\nPolkadotpaper for further description of how their \"fishermen\"\nconcept works. For up-to-date info and code for Polkadot, see here.\n\n- \n\n Thanks to Justin Drake for pointing me\nto cryptographic accumulators, as well as this paper that gives\nthe argument for the impossibility of sublinear batching. See also this\nthread:\nhttps://ethresear.ch/t/accumulators-scalability-of-utxo-blockchains-and-data-availability/176\n\nFurther reading related to sharding, and more generally scalability\nand research, is available here\nand here.",
    "contentLength": 71609,
    "summary": "Sharding solves blockchain scalability by splitting transaction processing and state storage across multiple node groups rather than requiring every node to process all transactions.",
    "detailedSummary": {
      "theme": "This comprehensive FAQ explores blockchain sharding as a solution to the scalability trilemma, examining technical challenges, security models, and implementation strategies for enabling blockchains to process more transactions while maintaining decentralization and security.",
      "summary": "Vitalik presents an extensive analysis of blockchain sharding, starting with the fundamental scalability problem where current blockchains like Bitcoin and Ethereum are limited to processing only a few transactions per second because every node must store the entire state and process all transactions. Vitalik introduces the scalability trilemma, which states that blockchain systems can only achieve two of three properties: decentralization, scalability, and security. He argues that sharding offers a path to break through this trilemma by splitting the blockchain state and transaction processing across multiple shards, where only subsets of validators verify each transaction, potentially achieving 100-1000x scaling improvements without sacrificing decentralization. The post systematically addresses numerous technical challenges including single-shard takeover attacks (solved through random sampling), cross-shard communication, the data availability problem, and various security models ranging from honest majority assumptions to bribing attacker scenarios. Vitalik explains how random sampling of validators across shards can maintain security even with smaller validator sets per shard, explores solutions like erasure coding for data availability, and discusses the integration of sharding with other scaling solutions like Plasma and state channels.",
      "takeaways": [
        "Sharding can potentially solve the blockchain scalability trilemma by splitting state and transaction processing across multiple shards while maintaining security through random validator sampling",
        "Random sampling of validators across shards maintains security properties even with smaller validator sets per shard, with statistical guarantees showing 99.999%+ probability of honest majorities in properly sized samples",
        "Cross-shard communication can be achieved through receipt-based messaging systems, though complex atomic operations across shards (like the train-and-hotel problem) present significant technical challenges",
        "The data availability problem is crucial for sharded systems and can be addressed through techniques like erasure coding to ensure all necessary data remains accessible",
        "Different security models (honest majority, uncoordinated majority, bribing attacker) require different approaches, with some sharding schemes being more vulnerable to coordinated attacks or bribing scenarios"
      ],
      "controversial": [
        "The assumption that the honest majority security model is 'arguably highly unrealistic and has already been empirically disproven' challenges a fundamental assumption underlying many blockchain security analyses",
        "The claim that sharding security in bribing attacker models 'is not much better than that of simply creating O(c) altcoins' suggests significant security degradation under certain adversarial conditions",
        "The assertion that requiring access lists for stateless client implementations 'harms censorship resistance, making attacks similar in form to the attempted DAO soft fork possible' raises concerns about trade-offs between efficiency and censorship resistance"
      ]
    }
  },
  {
    "id": "general-2017-12-17-voting",
    "title": "Notes on Blockchain Governance",
    "date": "2017-12-17",
    "category": "governance",
    "url": "https://vitalik.eth.limo/general/2017/12/17/voting.html",
    "path": "general/2017/12/17/voting.html",
    "content": "- \n\n- \n\n  \n    \n\n  \n  \n  Dark Mode Toggle\n  \n    \n\n  \n\n##  Notes on Blockchain Governance \n\n 2017 Dec 17 \nSee all posts\n\n \n \n\n Notes on Blockchain Governance \n\nIn which I argue that \"tightly coupled\" on-chain voting is\noverrated, the status quo of \"informal governance\" as practiced by\nBitcoin, Bitcoin Cash, Ethereum, Zcash and similar systems is much less\nbad than commonly thought, that people who think that the purpose of\nblockchains is to completely expunge soft mushy human intuitions and\nfeelings in favor of completely algorithmic governance (emphasis on\n\"completely\") are absolutely crazy, and loosely coupled voting as done\nby Carbonvotes and similar systems is underrated, as well as describe\nwhat framework should be used when thinking about blockchain governance\nin the first place.\n\nSee also:\nhttps://medium.com/@Vlad_Zamfir/against-on-chain-governance-a4ceacd040ca\n\nOne of the more interesting recent trends in blockchain governance is\nthe resurgence of on-chain coin-holder voting as a multi-purpose\ndecision mechanism. Votes by coin holders are sometimes used in order to\ndecide who operates the super-nodes that run a network (eg. DPOS in EOS,\nNEO, Lisk and other systems), sometimes to vote on protocol parameters\n(eg. the Ethereum gas limit) and sometimes to vote on and directly\nimplement protocol upgrades wholesale (eg.\nTezos). In all of these cases, the votes\nare automatic - the protocol itself contains all of the logic needed to\nchange the validator set or to update its own rules, and does this\nautomatically in response to the result of votes.\n\nExplicit on-chain governance is typically touted as having several\nmajor advantages. First, unlike the highly conservative philosophy\nespoused by Bitcoin, it can evolve rapidly and accept needed technical\nimprovements. Second, by creating an explicit decentralized\nframework, it avoids the perceived pitfalls of informal\ngovernance, which is viewed to either be too unstable and prone to chain\nsplits, or prone to becoming too de-facto centralized - the latter being\nthe same argument made in the famous 1972 essay\n\"Tyranny of\nStructurelessness\".\n\nQuoting Tezos\ndocumentation:\n\nWhile all blockchains offer financial incentives for maintaining\nconsensus on their ledgers, no blockchain has a robust on-chain\nmechanism that seamlessly amends the rules governing its protocol and\nrewards protocol development. As a result, first-generation blockchains\nempower de facto, centralized core development teams or miners to\nformulate design choices.\n\nAnd:\n\nYes, but why would you want to make [a minority chain split] easier?\nSplits destroy network effects.\n\nOn-chain governance used to select validators also has the benefit\nthat it allows for networks that impose high computational performance\nrequirements on validators without introducing economic centralization\nrisks and other traps of the kind that appear in public blockchains (eg.\nthe validator's\ndilemma).\n\nSo far, all in all, on-chain governance seems like a very good\nbargain.... so what's wrong with it?\nWhat is Blockchain\nGovernance?\n\nTo start off, we need to describe more clearly what the process of\n\"blockchain governance\" is. Generally speaking, there are two\ninformal models of governance, that I will call the \"decision function\"\nview of governance and the \"coordination\" view of governance. The\ndecision function view treats governance as a function \\(f(x_1, x_2 ... x_n) \\rightarrow y\\), where\nthe inputs are the wishes of various legitimate stakeholders (senators,\nthe president, property owners, shareholders, voters, etc) and the\noutput is the decision.\n\nThe decision function view is often useful as an approximation, but\nit clearly frays very easily around the edges: people often can and do\nbreak the law and get away with it, sometimes rules are ambiguous, and\nsometimes revolutions happen - and all three of these possibilities are,\nat least sometimes, a good thing. And often even behavior\ninside the system is shaped by incentives created by the\npossibility of acting outside the system, and this once again is at\nleast sometimes a good thing.\n\nThe coordination model of governance, in contrast, sees governance as\nsomething that exists in layers. The bottom layer is, in the real world,\nthe laws of physics themselves (as a geopolitical realist would say,\nguns and bombs), and in the blockchain space we can abstract a bit\nfurther and say that it is each individual's ability to run whatever\nsoftware they want in their capacity as a user, miner, stakeholder,\nvalidator or whatever other kind of agent a blockchain protocol allows\nthem to be. The bottom layer is always the ultimate deciding layer; if,\nfor example, all Bitcoin users wake up one day and decides to edit their\nclients' source code and replace the entire code with an Ethereum client\nthat listens to balances of a particular ERC20 token contract, then that\nmeans that that ERC20 token is bitcoin. The bottom layer's\nultimate governing power cannot be stopped, but the actions that people\ntake on this layer can be influenced by the layers above\nit.\n\nThe second (and crucially important) layer is coordination\ninstitutions. The purpose of a coordination institution is to create\nfocal points around how and when individuals should act in order to\nbetter coordinate behavior. There are many situations, both in\nblockchain governance and in real life, where if you act in a certain\nway alone, you are likely to get nowhere (or worse), but if everyone\nacts together a desired result can be achieved.\n\n An abstract coordination game. You benefit heavily from\nmaking the same move as everyone else.\n\nIn these cases, it's in your interest to go if everyone else is\ngoing, and stop if everyone else is stopping. You can think of\ncoordination institutions as putting up green or red flags in the air\nsaying \"go\" or \"stop\", with an established culture that\neveryone watches these flags and (usually) does what they say. Why do\npeople have the incentive to follow these flags? Because everyone\nelse is already following these flags, and you have the incentive\nto do the same thing as what everyone else is doing.\n\n A Byzantine general rallying his troops forward. The\npurpose of this isn't just to make the soldiers feel brave and excited,\nbut also to reassure them that everyone else feels brave and\nexcited and will charge forward as well, so an individual soldier is not\njust committing suicide by charging forward alone.\n\nStrong claim: this concept of coordination flags encompasses\nall that we mean by \"governance\"; in scenarios where coordination\ngames (or more generally, multi-equilibrium games) do not exist, the\nconcept of governance is meaningless.\n\nIn the real world, military orders from a general function as a flag,\nand in the blockchain world, the simplest example of such a flag is the\nmechanism that tells people whether or not a hard fork \"is happening\".\nCoordination institutions can be very formal, or they can be informal,\nand often give suggestions that are ambiguous. Flags would ideally\nalways be either red or green, but sometimes a flag might be yellow, or\neven holographic, appearing green to some participants and yellow or red\nto others. Sometimes that are also multiple flags that conflict with\neach other.\n\nThe key questions of governance thus become:\n\n- What should layer 1 be? That is, what features should be set up in\nthe initial protocol itself, and how does this influence the ability to\nmake formulaic (ie. decision-function-like) protocol changes, as well as\nthe level of power of different kinds of agents to act in different\nways?\n\n- What should layer 2 be? That is, what coordination institutions\nshould people be encouraged to care about?\n\n## The Role of Coin Voting\n\nEthereum also has a history with coin voting, including:\n\n- DAO proposal votes:\nhttps://daostats.github.io/proposals.html\n\n- The DAO Carbonvote:\nhttp://v1.carbonvote.com/\n\n- The EIP 186/649/669 Carbonvote:\nhttp://carbonvote.com/\n\nThese three are all examples of loosely coupled coin voting,\nor coin voting as a layer 2 coordination institution. Ethereum does not\nhave any examples of tightly coupled coin voting (or, coin\nvoting as a layer 1 in-protocol feature), though it does have\nan example of tightly coupled miner voting: miners' right to\nvote on the gas limit. Clearly, tightly coupled voting and loosely\ncoupled voting are competitors in the governance mechanism space, so\nit's worth dissecting: what are the advantages and disadvantages of each\none?\n\nAssuming zero transaction costs, and if used as a sole governance\nmechanism, the two are clearly equivalent. If a loosely coupled vote\nsays that change X should be implemented, then that will serve as a\n\"green flag\" encouraging everyone to download the update; if a minority\nwants to rebel, they will simply not download the update. If a tightly\ncoupled vote implements change X, then the change happens automatically,\nand if a minority wants to rebel they can install a hard fork update\nthat cancels the change. However, there clearly are nonzero transaction\ncosts associated with making a hard fork, and this leads to some very\nimportant differences.\n\nOne very simple, and important, difference is that tightly coupled\nvoting creates a default in favor of the blockchain adopting what the\nmajority wants, requiring minorities to exert great effort to coordinate\na hard fork to preserve a blockchain's existing properties, whereas\nloosely coupled voting is only a coordination tool, and still requires\nusers to actually download and run the software that implements any\ngiven fork. But there are also many other differences. Now, let us go\nthrough some arguments against voting, and dissect how each\nargument applies to voting as layer 1 and voting as layer 2.\n\n## Low Voter Participation\n\nOne of the main criticisms of coin voting mechanisms so far is that,\nno matter where they are tried, they tend to have very low voter\nparticipation. The DAO Carbonvote only had a voter participation rate of\n4.5%:\n\nAdditionally, wealth distribution is very unequal, and the results of\nthese two factors together are best described by this image created by a\ncritic of the DAO fork:\n\nThe EIP 186 Carbonvote had ~2.7 million ETH voting. The DAO proposal\nvotes\ndid\nnot fare better, with participation never reaching 10%. And outside\nof Ethereum things are not sunny either; even in Bitshares, a system\nwhere the core social contract is designed around voting, the top\ndelegate in an approval vote only got\n17%\nof the vote, and in Lisk it got\nup to 30%, though\nas we will discuss later these systems have other problems of their\nown.\n\nLow voter participation means two things. First, the vote has a\nharder time achieving a perception of legitimacy, because it only\nreflects the views of a small percentage of people. Second, an attacker\nwith only a small percentage of all coins can sway the vote. These\nproblems exist regardless of whether the vote is tightly coupled or\nloosely coupled.\n\n## Game-Theoretic Attacks\n\nAside from \"the big hack\" that received the bulk of the media\nattention, the DAO also had a number of much smaller game-theoretic\nvulnerabilities;\nthis\narticle from HackingDistributed does a good job of summarizing them.\nBut this is only the tip of the iceberg. Even if all of the finer\ndetails of a voting mechanism are implemented correctly, voting\nmechanisms in general have a large flaw: in any vote, the probability\nthat any given voter will have an impact on the result is tiny, and so\nthe personal incentive that each voter has to vote correctly is almost\ninsignificant. And if each person's size of the stake is small, their\nincentive to vote correctly is insignificant squared. Hence, a\nrelatively small bribe spread out across the participants may suffice to\nsway their decision, possibly in a way that they collectively might\nquite disapprove of.\n\nNow you might say, people are not evil selfish profit-maximizers that\nwill accept a $0.5 bribe to vote to give twenty million dollars to Josh\narza just because the above calculation says their individual chance of\naffecting anything is tiny; rather, they would altruistically refuse to\ndo something that evil. There are two responses to this criticism.\n\nFirst, there are ways to make a \"bribe\" that are quite plausible; for\nexample, an exchange can offer interest rates for deposits (or, even\nmore ambiguously, use the exchange's own money to build a great\ninterface and features), with the exchange operator using the large\nquantity of deposits to vote as they wish. Exchanges profit from chaos,\nso their incentives are clearly quite misaligned with users and\ncoin holders.\n\nSecond, and more damningly, in practice it seems like people, at\nleast in their capacity as crypto token holders, are profit\nmaximizers, and seem to see nothing evil or selfish about taking a bribe\nor two. As \"Exhibit A\", we can look at the situation with Lisk, where\nthe delegate pool seems to have been successfully captured by two major\n\"political parties\" that explicitly bribe coin holders to vote for them,\nand also require each member in the pool to vote for all the others.\n\nHere's LiskElite, with 55 members (out of a total 101):\n\nHere's LiskGDT, with 33 members:\n\nAnd as \"Exhibit B\" some voter bribes being paid out\nin\nArk:\n\nHere, note that there is a key difference between tightly coupled and\nloosely coupled votes. In a loosely coupled vote, direct or indirect\nvote bribing is also possible, but if the community agrees that some\ngiven proposal or set of votes constitutes a game-theoretic attack, they\ncan simply socially agree to ignore it. And in fact this has kind of\nalready happened - the Carbonvote contains a blacklist of addresses\ncorresponding to known exchange addresses, and votes from these\naddresses are not counted. In a tightly coupled vote, there is no way to\ncreate such a blacklist at protocol level, because agreeing who is part\nof the blacklist is itself a blockchain governance decision.\nBut since the blacklist is part of a community-created voting tool that\nonly indirectly influences protocol changes, voting tools that contain\nbad blacklists can simply be rejected by the community.\n\nIt's worth noting that this section is not a prediction that\nall tightly coupled voting systems will quickly succumb to bribe\nattacks. It's entirely possible that many will survive for one simple\nreason: all of these projects have founders or foundations with large\npremines, and these act as large centralized actors that are interested\nin their platforms' success that are not vulnerable to bribes, and hold\nenough coins to outweigh most bribe attacks. However, this kind of\ncentralized trust model, while arguably useful in some contexts in a\nproject's early stages, is clearly one that is not sustainable in the\nlong term.\n\n## Non-Representativeness\n\nAnother important objection to voting is that coin holders are only\none class of user, and may have interests that collide with those of\nother users. In the case of pure cryptocurrencies like Bitcoin,\nstore-of-value use\n(\"hodling\")\nand medium-of-exchange use (\"buying coffees\") are naturally in conflict,\nas the store-of-value prizes security much more than the\nmedium-of-exchange use case, which more strongly values usability. With\nEthereum, the conflict is worse, as there are many people who use\nEthereum for reasons that have nothing to do with ether (see:\ncryptokitties), or even value-bearing digital assets in general (see:\nENS).\n\nAdditionally, even if coin holders are the only relevant class\nof user (one might imagine this to be the case in a cryptocurrency where\nthere is an established social contract that its purpose is to be the\nnext digital gold, and nothing else), there is still the challenge that\na coin holder vote gives a much greater voice to wealthy coin holders\nthan to everyone else, opening the door for centralization of holdings\nto lead to unencumbered centralization of decision making. Or, in other\nwords...\n\nAnd if you want to see a review of a project that seems to combine\nall of these disadvantages at the same time, see this:\nhttps://btcgeek.com/bitshares-trying-memorycoin-year-ago-disastrous-ends/.\n\nThis criticism applies to both tightly coupled and loosely coupled\nvoting equally; however, loosely coupled voting is more amenable to\ncompromises that mitigate its unrepresentativeness, and we will discuss\nthis more later.\n\n## Centralization\n\nLet's look at the existing live experiment that we have in tightly\ncoupled voting on Ethereum, the gas limit. Here's the gas limit\nevolution over the past two years:\n\nYou might notice that the general feel of the curve is a bit like\nanother chart that may be quite familiar to you:\n\nBasically, they both look like magic numbers that are created and\nrepeatedly renegotiated by a fairly centralized group of guys sitting\ntogether in a room. What's happening in the first case? Miners are\ngenerally following the direction favored by the community, which is\nitself gauged via social consensus aids similar to those that drive hard\nforks (core developer support, Reddit upvotes, etc; in Ethereum, the gas\nlimit has never gotten controversial enough to require anything as\nserious as a coin vote).\n\nHence, it is not at all clear that voting will be able to deliver\nresults that are actually decentralized, if voters are not\ntechnically knowledgeable and simply defer to a single dominant tribe of\nexperts. This criticism once again applies to tightly coupled and\nloosely coupled voting equally.\n\nUpdate: since writing this, it seems like Ethereum miners\nmanaged to up the gas limit from 6.7 million to 8 million all without\neven discussing it with the core developers or the Ethereum Foundation.\nSo there is hope; but it takes a lot of hard community building and\nother grueling non-technical work to get to that point.\n\n## Digital Constitutions\n\nOne approach that has been suggested to mitigate the risk of runaway\nbad governance algorithms is \"digital constitutions\" that mathematically\nspecify desired properties that the protocol should have, and require\nany new code changes to come with a computer-verifiable proof that they\nsatisfy these properties. This seems like a good idea at first, but this\ntoo should, in my opinion, be viewed skeptically.\n\nIn general, the idea of having norms about protocol properties, and\nhaving these norms serve the function of one of the coordination flags,\nis a very good one. This allows us to enshrine core properties of a\nprotocol that we consider to be very important and valuable, and make\nthem more difficult to change. However, this is exactly the sort of\nthing that should be enforced in loosely coupled (ie. layer two), rather\nthan tightly coupled (layer one) form.\n\nBasically any meaningful norm is actually quite hard to express in\nits entirety; this is part of the\ncomplexity\nof value problem. This is true even for something as seemingly\nunambiguous as the 21 million coin limit. Sure, one can add a line of\ncode saying assert total_supply <= 21000000, and put a\ncomment around it saying \"do not remove at all costs\", but there are\nplenty of roundabout ways of doing the same thing. For example, one\ncould imagine a soft fork that adds a mandatory transaction fee this is\nproportional to coin value * time since the coins were last sent, and\nthis is equivalent to demurrage, which is equivalent to deflation. One\ncould also implement another currency, called Bjtcoin, with 21 million\nnew units, and add a feature where if a bitcoin transaction is\nsent the miner can intercept it and claim the bitcoin, instead giving\nthe recipient bjtcoin; this would rapidly force bitcoins and bjtcoins to\nbe fungible with each other, increasing the \"total supply\" to 42 million\nwithout ever tripping up that line of code. \"Softer\" norms like not\ninterfering with application state are even harder to enforce.\n\nWe want to be able to say that a protocol change that\nviolates any of these guarantees should be viewed as illegitimate -\nthere should be a coordination institution that waves a red flag - even\nif they get approved by a vote. We also want to be able to say that a\nprotocol change that follows the letter of a norm, but blatantly\nviolates its spirit, the protocol change should still be viewed\nas illegitimate. And having norms exist on layer 2 - in the minds of\nhumans in the community, rather than in the code of the protocol - best\nachieves that goal.\n\n## Toward A Balance\n\nHowever, I am also not willing to go the other way and say that coin\nvoting, or other explicit on-chain voting-like schemes, have no place in\ngovernance whatsoever. The leading alternative seems to be core\ndeveloper consensus, however the failure mode of a system being\ncontrolled by \"ivory tower intellectuals\" who care more about abstract\nphilosophies and solutions that sound technically impressive over and\nabove real day-to-day concerns like user experience and transaction fees\nis, in my view, also a real threat to be taken seriously.\n\nSo how do we solve this conundrum? Well, first, we can heed the\nwords of slatestarcodex in the context of traditional politics:\n\nThe rookie mistake is: you see that some system is partly Moloch [ie.\ncaptured by misaligned special interests], so you say \"Okay, we'll fix\nthat by putting it under the control of this other system. And we'll\ncontrol this other system by writing \u2018DO NOT BECOME MOLOCH' on it in\nbright red marker.\" (\"I see capitalism sometimes gets misaligned. Let's\nfix it by putting it under control of the government. We'll control the\ngovernment by having only virtuous people in high offices.\") I'm not\ngoing to claim there's a great alternative, but the\noccasionally-adequate alternative is the neoliberal one \u2013 find a couple\nof elegant systems that all optimize along different criteria\napproximately aligned with human happiness, pit them off against each\nother in a structure of checks and balances, hope they screw up in\ndifferent places like in that swiss cheese model, keep enough individual\nfree choice around that people can exit any system that gets too\nterrible, and let cultural evolution do the rest.\n\nIn blockchain governance, it seems like this is the only way forward\nas well. The approach for blockchain governance that I advocate is\n\"multifactorial consensus\", where different coordination flags and\ndifferent mechanisms and groups are polled, and the ultimate decision\ndepends on the collective result of all of these mechanisms together.\nThese coordination flags may include:\n\n- The roadmap (ie. the set of ideas broadcasted earlier on in the\nproject's history about the direction the project would be going)\n\n- Consensus among the dominant core development teams\n\n- Coin holder votes\n\n- User votes, through some kind of sybil-resistant polling system\n\n- Established norms (eg. non-interference with applications, the 21\nmillion coin limit)\n\nI would argue that it is very useful for coin voting to be one of\nseveral coordination institutions deciding whether or not a given change\ngets implemented. It is an imperfect and unrepresentative signal, but it\nis a Sybil-resistant one - if you see 10 million ETH voting for\na given proposal, you cannot dismiss that by simply saying \"oh,\nthat's just hired Russian trolls with fake social media accounts\". It is\nalso a signal that is sufficiently disjoint from the core development\nteam that if needed it can serve as a check on it. However, as described\nabove, there are very good reasons why it should not be the\nonly coordination institution.\n\nAnd underpinnning it all is the key difference from traditional\nsystems that makes blockchains interesting: the \"layer 1\" that underpins\nthe whole system is the requirement for individual users to assent to\nany protocol changes, and their freedom, and credible threat, to \"fork\noff\" if someone attempts to force changes on them that they consider\nhostile (see also: http://vitalik.ca/general/2017/05/08/coordination_problems.html).\n\nTightly coupled voting is also okay to have in some limited contexts\n- for example, despite its flaws, miners' ability to vote on the gas\nlimit is a feature that has proven very beneficial on multiple\noccasions. The risk that miners will try to abuse their power may well\nbe lower than the risk that any specific gas limit or block size limit\nhard-coded by the protocol on day 1 will end up leading to serious\nproblems, and in that case letting miners vote on the gas limit is a\ngood thing. However, \"allowing miners or validators to vote on a few\nspecific parameters that need to be rapidly changed from time to time\"\nis a very far cry from giving them arbitrary control over protocol\nrules, or letting voting control validation, and these more expansive\nvisions of on-chain governance have a much murkier potential, both in\ntheory and in practice.",
    "contentLength": 24744,
    "summary": "Tightly coupled on-chain voting is overrated while informal governance used by Bitcoin/Ethereum works better than expected.",
    "detailedSummary": {
      "theme": "Vitalik argues that tightly coupled on-chain voting is overrated and advocates for a multifactorial governance approach that balances various coordination mechanisms in blockchain governance.",
      "summary": "Vitalik presents a comprehensive critique of on-chain governance mechanisms, particularly tightly coupled voting systems where protocol changes happen automatically based on coin holder votes. He distinguishes between two governance models: the 'decision function' view and the 'coordination' view, arguing that governance fundamentally exists to solve coordination problems through institutional 'flags' that signal when collective action should occur. Vitalik identifies several critical flaws with tightly coupled voting including low voter participation, susceptibility to game-theoretic attacks like bribes, non-representativeness since coin holders may not represent all stakeholders, and potential centralization despite appearing decentralized. He demonstrates these issues through real examples from Ethereum's Carbonvote experiments, Lisk's delegate capture, and other blockchain voting mechanisms. Rather than completely rejecting coin voting, Vitalik advocates for 'multifactorial consensus' where coin voting serves as one coordination mechanism among many, including core developer consensus, user feedback, established norms, and project roadmaps, all underpinned by users' ultimate freedom to fork if they disagree with imposed changes.",
      "takeaways": [
        "Tightly coupled on-chain voting creates dangerous defaults favoring majority rule and is vulnerable to bribes, low participation, and centralization despite appearing decentralized",
        "Loosely coupled voting (like Carbonvotes) is superior because communities can ignore compromised votes and apply social consensus to validate results",
        "Blockchain governance is fundamentally about coordination rather than decision-making, requiring institutional 'flags' to help stakeholders coordinate collective action",
        "A multifactorial consensus approach using multiple coordination mechanisms (developer consensus, coin votes, user input, established norms) is more robust than relying on any single governance method",
        "The ultimate safeguard in blockchain governance is individual users' ability to fork and reject hostile protocol changes, which distinguishes blockchain governance from traditional systems"
      ],
      "controversial": [
        "Vitalik's dismissal of digital constitutions as ineffective may be debated by those who believe formal constraints can meaningfully limit governance overreach",
        "The characterization of current informal governance systems as 'much less bad than commonly thought' contradicts many who see Bitcoin and Ethereum governance as too centralized or inefficient",
        "The argument that people behave as 'profit maximizers' in crypto governance may be overly cynical and not account for genuine ideological motivations"
      ]
    }
  },
  {
    "id": "general-2017-12-14-gas_analysis",
    "title": "A Quick Gasprice Market Analysis",
    "date": "2017-12-14",
    "category": "protocol",
    "url": "https://vitalik.eth.limo/general/2017/12/14/gas_analysis.html",
    "path": "general/2017/12/14/gas_analysis.html",
    "content": "- \n\n- \n\n  \n    \n\n  \n  \n  Dark Mode Toggle\n  \n    \n\n  \n\n##  A Quick Gasprice Market Analysis \n\n 2017 Dec 14 \nSee all posts\n\n \n \n\n A Quick Gasprice Market Analysis \n\nHere is a file\nthat contains data, extracted from geth, about transaction fees in every\nblock between 4710000 and 4730000. For each block, it contains an object\nof the form:\n\n{\n    \"block\":4710000,\n    \"coinbase\":\"0x829bd824b016326a401d083b33d092293333a830\",\n    \"deciles\":[40,40.1,44.100030001,44.100030001,44.100030001,44.100030001,44.100030001,44.100030001,50,66.150044,100]\n    ,\"free\":10248,\n    \"timedelta\":8\n}\n\nThe \"deciles\" variable contains 11 values, where the lowest is the\nlowest gasprice in each block, the next is the gasprice that only 10% of\nother transaction gasprices are lower than, and so forth; the last is\nthe highest gasprice in each block. This gives us a convenient summary\nof the distribution of transaction fees that each block contains. We can\nuse this data to perform some interesting analyses.\n\nFirst, a chart of the deciles, taking 50-block moving averages to\nsmooth it out:\n\nWhat we see is a gasprice market that seems to actually stay\nreasonably stable over the course of more than three days. There are a\nfew occasional spikes, most notably the one around block 4720000, but\notherwise the deciles all stay within the same band all the way through.\nThe only exception is the highest gasprice transaction (that red\nsquiggle at the top left), which fluctuates wildly because it can be\npushed upward by a single very-high-gasprice transaction.\n\nWe can try to interpret the data in another way: by calculating, for\neach gasprice level, the average number of blocks that you need to wait\nuntil you see a block where the lowest gasprice included is lower than\nthat gasprice. Assuming that miners are rational and all have the same\nview (implying that if the lowest gasprice in a block is X, then that\nmeans there are no more transactions with gasprices above X waiting to\nbe included), this might be a good proxy for the average amount of time\nthat a transaction sender needs to wait to get included if they use that\ngasprice. The stats are:\n\nThere is clear clustering going on at the 4, 10 and 20 levels; it\nseems to be an underexploited strategy to send transactions with fees\nslightly above these levels, getting in before the crowd of transactions\nright at the level but only paying a little more.\n\nHowever, there is quite a bit of evidence that miners do\nnot have the same view; that is, some miners see a very\ndifferent set of transactions from other miners. First of all, we can\nfilter blocks by miner address, and check what the deciles of each miner\nare. Here is the output of this data, splitting by 2000-block ranges so\nwe can spot behavior that is consistent across the entire period, and\nfiltering out miners that mine less than 10 blocks in any period, as\nwell as filtering out blocks with more 21000 free gas (high levels of\nfree gas may signify an abnormally high minimum gas price policy, like\nfor example 0x6a7a43be33ba930fe58f34e07d0ad6ba7adb9b1f at ~40 gwei and\n0xb75d1e62b10e4ba91315c4aa3facc536f8a922f5 at ~10 gwei). We get:\n\n0x829bd824b016326a401d083b33d092293333a830 [30, 28, 27, 21, 28, 34, 23, 24, 32, 32]\n0xea674fdde714fd979de3edf0f56aa9716b898ec8 [17, 11, 10, 15, 17, 23, 17, 13, 16, 17]\n0x5a0b54d5dc17e0aadc383d2db43b0a0d3e029c4c [31, 17, 20, 18, 16, 27, 21, 15, 21, 21]\n0x52bc44d5378309ee2abf1539bf71de1b7d7be3b5 [20, 16, 19, 14, 17, 18, 17, 14, 15, 15]\n0xb2930b35844a230f00e51431acae96fe543a0347 [21, 17, 19, 17, 17, 25, 17, 16, 19, 19]\n0x180ba8f73897c0cb26d76265fc7868cfd936e617 [13, 13, 15, 18, 12, 26, 16, 13, 20, 20]\n0xf3b9d2c81f2b24b0fa0acaaa865b7d9ced5fc2fb [26, 25, 23, 21, 22, 28, 25, 24, 26, 25]\n0x4bb96091ee9d802ed039c4d1a5f6216f90f81b01 [17, 21, 17, 14, 21, 32, 14, 14, 19, 23]\n0x2a65aca4d5fc5b5c859090a6c34d164135398226 [26, 24, 20, 16, 22, 33, 20, 18, 24, 24]\n\nThe first miner is consistently higher than the others; the last is\nalso higher than average, and the second is consistently among the\nlowest.\n\nAnother thing we can look at is timestamp differences - the\ndifference between a block's timestamp and its parent. There is a clear\ncorrelation between timestamp difference and lowest gasprice:\n\nThis makes a lot of sense, as a block that comes right after another\nblock should be cleaning up only the transactions that are too low in\ngasprice for the parent block to have included, and a block that comes a\nlong time after its predecessor would have many more not-yet-included\ntransactions to choose from. The differences are large, suggesting that\na single block is enough to bite off a very substantial chunk of the\nunconfirmed transaction pool, adding to the evidence that most\ntransactions are included quite quickly.\n\nHowever, if we look at the data in more detail, we see very many\ninstances of blocks with low timestamp differences that contain many\ntransactions with higher gasprices than their parents. Sometimes we do\nsee blocks that actually look like they clean up what their parents\ncould not, like this:\n\n{\"block\":4710093,\"coinbase\":\"0x5a0b54d5dc17e0aadc383d2db43b0a0d3e029c4c\",\"deciles\":[25,40,40,40,40,40,40,43,50,64.100030001,120],\"free\":6030,\"timedelta\":8},\n{\"block\":4710094,\"coinbase\":\"0xea674fdde714fd979de3edf0f56aa9716b898ec8\",\"deciles\":[4,16,20,20,21,21,22,29,30,40,59],\"free\":963366,\"timedelta\":2},\n\nBut sometimes we see this:\n\n{\"block\":4710372,\"coinbase\":\"0x52bc44d5378309ee2abf1539bf71de1b7d7be3b5\",\"deciles\":[1,30,35,40,40,40,40,40,40,55,100],\"free\":13320,\"timedelta\":7},\n{\"block\":4710373,\"coinbase\":\"0x52bc44d5378309ee2abf1539bf71de1b7d7be3b5\",\"deciles\":[1,32,32,40,40,56,56,56,56,70,80],\"free\":1672720,\"timedelta\":2}\n\nAnd sometimes we see miners suddenly including many 1-gwei\ntransactions:\n\n{\"block\":4710379,\"coinbase\":\"0x5a0b54d5dc17e0aadc383d2db43b0a0d3e029c4c\",\"deciles\":[21,25,31,40,40,40,40,40,40,50,80],\"free\":4979,\"timedelta\":13},\n{\"block\":4710380,\"coinbase\":\"0x52bc44d5378309ee2abf1539bf71de1b7d7be3b5\",\"deciles\":[1,1,1,1,1,1,40,45,55,61.10006,2067.909560115],\"free\":16730,\"timedelta\":35}\n\nThis strongly suggests that a miner including transactions with\ngasprice X should NOT be taken as evidence that there are not still many\ntransactions with gasprice higher than X left to process. This is likely\nbecause of imperfections in network propagation.\n\nIn general, however, what we see seems to be a rather\nwell-functioning fee market, though there is still room to improve in\nfee estimation and, most importantly of all, continuing to work hard to\nimprove base-chain scalability so that more transactions can get\nincluded in the first place.",
    "contentLength": 6617,
    "summary": "Analyzing Ethereum blocks 4710000-4730000, gas prices stayed stable (~4-100 gwei) over 3 days despite network propagation causing miner inconsistencies.",
    "detailedSummary": {
      "theme": "Vitalik analyzes Ethereum transaction fee data across 20,000 blocks to examine gas price market dynamics and miner behavior patterns.",
      "summary": "Vitalik presents an empirical analysis of Ethereum's gas price market using data from blocks 4,710,000 to 4,730,000, examining transaction fee distributions through decile analysis. He finds that the gas price market remains relatively stable over three days despite occasional spikes, with clear clustering at certain fee levels (4, 10, and 20 gwei) that creates opportunities for users to pay slightly more to jump ahead in the queue. However, Vitalik discovers significant evidence that miners have different views of the transaction pool, as different miners consistently show varying gas price patterns and there are many instances where blocks with short time intervals contain transactions with higher gas prices than their parents. This suggests network propagation imperfections prevent miners from seeing identical sets of pending transactions, contradicting the assumption of perfect information in a rational fee market.",
      "takeaways": [
        "Gas price markets show remarkable stability over multi-day periods with only occasional spikes affecting the overall fee structure",
        "There is clear clustering at common fee levels (4, 10, 20 gwei), creating underexploited arbitrage opportunities for users willing to pay slightly above these thresholds",
        "Different miners consistently exhibit different gas price inclusion patterns, indicating they have varying views of the transaction pool",
        "Network propagation imperfections mean miners don't see identical sets of pending transactions, challenging assumptions about rational fee market behavior",
        "Despite imperfections, Ethereum's fee market functions reasonably well, though improvements in fee estimation and base-chain scalability remain important priorities"
      ],
      "controversial": [
        "The suggestion that network propagation imperfections significantly impact miner transaction selection challenges the assumption of efficient market behavior",
        "The implication that users could exploit predictable clustering patterns in gas pricing to gain advantages over other transaction senders"
      ]
    }
  },
  {
    "id": "general-2017-11-22-starks_part_2",
    "title": "STARKs, Part II: Thank Goodness It's FRI-day",
    "date": "2017-11-22",
    "category": "math",
    "url": "https://vitalik.eth.limo/general/2017/11/22/starks_part_2.html",
    "path": "general/2017/11/22/starks_part_2.html",
    "content": "- \n\n- \n\n  \n    \n\n  \n  \n  Dark Mode Toggle\n  \n    \n\n  \n\n##  STARKs, Part II: Thank Goodness It's FRI-day \n\n 2017 Nov 22 \nSee all posts\n\n \n \n\n STARKs, Part II: Thank Goodness It's FRI-day \n\nSpecial thanks to Eli Ben-Sasson for ongoing help and\nexplanations, and Justin Drake for reviewing\n\nIn the last part of this series, we talked about how you can make\nsome pretty interesting succinct proofs of computation, such as proving\nthat you have computed the millionth Fibonacci number, using a technique\ninvolving polynomial composition and division. However, it rested on one\ncritical ingredient: the ability to prove that at least the great\nmajority of a given large set of points are on the same low-degree\npolynomial. This problem, called \"low-degree testing\", is perhaps the\nsingle most complex part of the protocol.\n\nWe'll start off by once again re-stating the problem. Suppose that\nyou have a set of points, and you claim that they are all on the same\npolynomial, with degree less than \\(D\\)\n(ie. \\(deg < 2\\) means they're on\nthe same line, \\(deg < 3\\) means\nthey're on the same line or parabola, etc). You want to create a\nsuccinct probabilistic proof that this is actually true.\n\nLeft: points all on the same \\(deg <\n3\\) polynomial. Right: points not on the same \\(deg < 3\\) polynomial\n\n If you want to verify that the points are all on the\nsame degree \\(< D\\) polynomial, you\nwould have to actually check every point, as if you fail to check even\none point there is always some chance that that point will not be on the\npolynomial even if all the others are. But what you can do is\nprobabilistically check that at least some fraction\n(eg. 90%) of all the points are on the same polynomial.\n\n Top left: possibly close enough to a polynomial. Top\nright: not close enough to a polynomial. Bottom left: somewhat close to\ntwo polynomials, but not close enough to either one. Bottom right:\ndefinitely not close enough to a polynomial.\n\nIf you have the ability to look at every point on the\npolynomial, then the problem is easy. But what if you can only look at a\nfew points - that is, you can ask for whatever specific point you want,\nand the prover is obligated to give you the data for that point as part\nof the protocol, but the total number of queries is limited? Then the\nquestion becomes, how many points do you need to check to be able to\ntell with some given degree of certainty?\n\nClearly, \\(D\\) points is\nnot enough. \\(D\\)\npoints are exactly what you need to uniquely define a degree \\(< D\\) polynomial, so any set of\npoints that you receive will correspond to some degree \\(< D\\) polynomial. As we see in the\nfigure above, however, \\(D+1\\) points\nor more do give some indication.\n\nThe algorithm to check if a given set of values is on the same degree\n\\(< D\\) polynomial with \\(D+1\\) queries is not too complex. First,\nselect a random subset of \\(D\\) points,\nand use something like Lagrange interpolation (search for \"Lagrange\ninterpolation\" here\nfor a more detailed description) to recover the unique degree \\(< D\\) polynomial that passes through all\nof them. Then, randomly sample one more point, and check that it is on\nthe same polynomial.\n\nNote that this is only a proximity test, because there's always the\npossibility that most points are on the same low-degree polynomial, but\na few are not, and the \\(D+1\\) sample\nmissed those points entirely. However, we can derive the result that if\nless than 90% of the points are on the same degree \\(< D\\) polynomial, then the test will\nfail with high probability. Specifically, if you make \\(D+k\\) queries, and if at least some portion\n\\(p\\) of the points are not on the same\npolynomial as the rest of the points, then the test will only pass with\nprobability \\((1 - p)^k\\).\n\nBut what if, as in the examples from the previous article, \\(D\\) is very high, and you want to verify a\npolynomial's degree with less than \\(D\\) queries? This is, of course, impossible\nto do directly, because of the simple argument made above (namely, that\nany \\(k \\leq D\\) points are\nall on at least one degree \\(< D\\)\npolynomial). However, it's quite possible to do this indirectly by\nproviding auxiliary data, and achieve massive efficiency gains by\ndoing so. And this is exactly what new protocols like FRI (\"Fast RS\nIOPP\", RS = \"Reed-Solomon\",\nIOPP = \"Interactive Oracle Proofs of Proximity\"), and similar earlier\ndesigns called probabilistically checkable proofs of proximity (PCPPs),\ntry to achieve.\n\n## A First Look at Sublinearity\n\nTo prove that this is at all possible, we'll start off with a\nrelatively simple protocol, with fairly poor tradeoffs, but that still\nachieves the goal of sublinear verification complexity - that is, you\ncan prove proximity to a degree \\(<\nD\\) polynomial with less than \\(D\\) queries (and, for that matter, less\nthan \\(O(D)\\) computation to verify the\nproof).\n\nThe idea is as follows. Suppose there are N points (we'll say \\(N\\) = 1 billion), and they are all on a\ndegree \\(<\\) 1,000,000 polynomial\n\\(f(x)\\). We find a bivariate\npolynomial (ie. an expression like \\(1 + x +\nxy + x^5 \\cdot y^3 + x^{12} + x \\cdot y^{11}\\)), which we will\ndenote \\(g(x, y)\\), such that \\(g(x, x^{1000}) = f(x)\\). This can be done\nas follows: for the \\(k\\)th degree term\nin \\(f(x)\\) (eg. \\(1744 \\cdot x^{185423}\\)), we decompose it\ninto \\(x^{k \\% 1000} \\cdot y^{floor(k /\n1000)}\\) (in this case, \\(1744 \\cdot\nx^{423} \\cdot y^{185}\\)). You can see that if \\(y = x^{1000}\\), then \\(1744 \\cdot x^{423} \\cdot y^{185}\\) equals\n\\(1744 \\cdot x^{185423}\\).\n\nIn the first stage of the proof, the prover commits to (ie. makes a\nMerkle tree of) the evaluation of \\(g(x,\ny)\\) over the entire square \\([1 ... N] x \\{x^{1000}: 1 \\leq x \\leq N\\}\\)\n- that is, all 1 billion \\(x\\)\ncoordinates for the columns, and all 1 billion corresponding\nthousandth powers for the \\(y\\) coordinates of the rows. The diagonal\nof the square represents the values of \\(g(x,\ny)\\) that are of the form \\(g(x,\nx^{1000})\\), and thus correspond to values of \\(f(x)\\).\n\nThe verifier then randomly picks perhaps a few dozen rows and columns\n(possibly using the\nMerkle root of the square as a source of pseudorandomness if we want\na non-interactive proof), and for each row or column that it picks the\nverifier asks for a sample of, say, 1010 points on the row and column,\nmaking sure in each case that one of the points demanded is on the\ndiagonal. The prover must reply back with those points, along with\nMerkle branches proving that they are part of the original data\ncommitted to by the prover. The verifier checks that the Merkle branches\nmatch up, and that the points that the prover provides actually do\ncorrespond to a degree-1000 polynomial.\n\nThis gives the verifier a statistical proof that (i) most rows are\npopulated mostly by points on degree \\(<\n1000\\) polynomials, (ii) most columns are populated mostly by\npoints on degree \\(< 1000\\)\npolynomials, and (iii) the diagonal line is mostly on these polynomials.\nThis thus convinces the verifier that most points on the diagonal\nactually do correspond to a degree \\(<\n1,000,000\\) polynomial.\n\nIf we pick thirty rows and thirty columns, the verifier needs to\naccess a total of 1010 points \\(\\cdot\\)\n60 rows+cols = 60600 points, less than the original 1,000,000, but not\nby that much. As far as computation time goes, interpolating the degree\n\\(< 1000\\) polynomials will have its\nown overhead, though since polynomial interpolation can be made\nsubquadratic the algorithm as a whole is still sublinear to verify. The\nprover complexity is higher: the prover needs to calculate and\ncommit to the entire \\(N \\cdot N\\)\nrectangle, which is a total of \\(10^{18}\\) computational effort (actually a\nbit more because polynomial evaluation is still superlinear). In all of\nthese algorithms, it will be the case that proving a computation is\nsubstantially more complex than just running it; but as we will see the\noverhead does not have to be that high.\n\n## A Modular Math Interlude\n\nBefore we go into our more complex protocols, we will need to take a\nbit of a digression into the world of modular arithmetic. Usually, when\nwe work with algebraic expressions and polynomials, we are working with\nregular numbers, and the arithmetic, using the operators +, -, \\(\\cdot\\), / (and exponentiation, which is\njust repeated multiplication), is done in the usual way that we have all\nbeen taught since school: \\(2 + 2 =\n4\\), \\(72 / 5 = 14.4\\), \\(1001 \\cdot 1001 = 1002001\\), etc. However,\nwhat mathematicians have realized is that these ways of defining\naddition, multiplication, subtraction and division are not the\nonly self-consistent ways of defining those operators.\n\nThe simplest example of an alternate way to define these operators is\nmodular arithmetic, defined as follows. The % operator means \"take the\nremainder of\": \\(15 % 7 = 1\\), \\(53 % 10 = 3\\), etc (note that the answer is\nalways non-negative, so for example \\(-1 % 10\n= 9\\)). For any specific prime number \\(p\\), we can redefine:\n\n\\(x + y \\Rightarrow (x + y)\\) %\n\\(p\\)\n\n\\(x \\cdot y \\Rightarrow (x \\cdot\ny)\\) % \\(p\\)\n\n\\(x^y \\Rightarrow (x^y)\\) % \\(p\\)\n\n\\(x - y \\Rightarrow (x - y)\\) %\n\\(p\\)\n\n\\(x / y \\Rightarrow (x \\cdot y\n^{p-2})\\) % \\(p\\)\n\nThe above rules are all self-consistent. For example, if \\(p = 7\\), then:\n\n- \\(5 + 3 = 1\\) (as \\(8\\) % \\(7 =\n1\\))\n\n- \\(1 - 3 = 5\\) (as \\(-2\\) % \\(7 =\n5\\))\n\n- \\(2 \\cdot 5 = 3\\)\n\n- \\(3 / 5 = 2\\) (as (\\(3 \\cdot 5^5\\)) % \\(7 = 9375\\) % \\(7\n= 2\\))\n\nMore complex identities such as the distributive law also hold: \\((2 + 4) \\cdot 3\\) and \\(2 \\cdot 3 + 4 \\cdot 3\\) both evaluate to\n\\(4\\). Even formulas like \\((a^2 - b^2)\\) = \\((a - b) \\cdot (a + b)\\) are still true in\nthis new kind of arithmetic. Division is the hardest part; we can't use\nregular division because we want the values to always remain integers,\nand regular division often gives non-integer results (as in the case of\n\\(3/5\\)). The funny \\(p-2\\) exponent in the division formula\nabove is a consequence of getting around this problem using Fermat's\nlittle theorem, which states that for any nonzero \\(x < p\\), it holds that \\(x^{p-1}\\) % \\(p =\n1\\). This implies that \\(x^{p-2}\\) gives a number which, if\nmultiplied by \\(x\\) one more time,\ngives \\(1\\), and so we can say that\n\\(x^{p-2}\\) (which is an integer)\nequals \\(\\frac{1}{x}\\). A somewhat more\ncomplicated but faster way to evaluate this modular division operator is\nthe extended\nEuclidean algorithm, implemented in python here.\n\nBecause of how the numbers \"wrap around\", modular arithmetic is\nsometimes called \"clock math\"\n\nWith modular math we've created an entirely new system of arithmetic,\nand because it's self-consistent in all the same ways traditional\narithmetic is self-consistent we can talk about all of the same kinds of\nstructures over this field, including polynomials, that we talk about in\n\"regular math\". Cryptographers love working in modular math (or, more\ngenerally, \"finite fields\") because there is a bound on the size of a\nnumber that can arise as a result of any modular math calculation - no\nmatter what you do, the values will not \"escape\" the set \\(\\{0, 1, 2 ... p-1\\}\\).\n\nFermat's little theorem also has another interesting consequence. If\n\\(p-1\\) is a multiple of some number\n\\(k\\), then the function \\(x \\rightarrow x^k\\) has a small \"image\" -\nthat is, the function can only give \\(\\frac{p-1}{k} + 1\\) possible results. For\nexample, \\(x \\rightarrow x^2\\) with\n\\(p=17\\) has only 9 possible\nresults.\n\nWith higher exponents the results are more striking: for example,\n\\(x \\rightarrow x^8\\) with \\(p=17\\) has only 3 possible results. And of\ncourse, \\(x \\rightarrow x^{16}\\) with\n\\(p=17\\) has only 2 possible results:\nfor \\(0\\) it returns \\(0\\), and for everything else it returns\n\\(1\\).\n\n## Now A Bit More Efficiency\n\nLet us now move on to a slightly more complicated version of the\nprotocol, which has the modest goal of reducing the prover complexity\nfrom \\(10^{18}\\) to \\(10^{15}\\), and then \\(10^{9}\\). First, instead of operating over\nregular numbers, we are going to be checking proximity to polynomials\nas evaluated with modular math. As we saw in the previous\narticle, we need to do this to prevent numbers in our STARKs from\ngrowing to 200,000 digits anyway. Here, however, we are going to use the\n\"small image\" property of certain modular exponentiations as a side\neffect to make our protocols far more efficient.\n\nSpecifically, we will work with \\(p\n=\\) 1,000,005,001. We pick this modulus because (i) it's greater\nthan 1 billion, and we need it to be at least 1 billion so we can check\n1 billion points, (ii) it's prime, and (iii) \\(p-1\\) is an even multiple of 1000. The\nexponentiation \\(x^{1000}\\) will have\nan image of size 1,000,006 - that is, the exponentiation can only give\n1,000,006 possible results.\n\nThis means that the \"diagonal\" (\\(x\\), \\(x^{1000}\\)) now becomes a diagonal with a\nwraparound; as \\(x^{1000}\\) can only\ntake on 1,000,006 possible values, we only need 1,000,006 rows. And so,\nthe full evaluation of \\(g(x,\nx^{1000})\\) now has only ~\\(10^{15}\\) elements.\n\nAs it turns out, we can go further: we can have the prover only\ncommit to the evaluation of \\(g\\) on a\nsingle column. The key trick is that the original data itself already\ncontains 1000 points that are on any given row, so we can simply sample\nthose, derive the degree \\(< 1000\\)\npolynomial that they are on, and then check that the corresponding point\non the column is on the same polynomial. We then check that the column\nitself is \\(a < 1000\\)\npolynomial.\n\nThe verifier complexity is still sublinear, but the prover complexity\nhas now decreased to \\(10^9\\), making\nit linear in the number of queries (though it's still superlinear in\npractice because of polynomial evaluation overhead).\n\n## And Even More Efficiency\n\nThe prover complexity is now basically as low as it can be. But we\ncan still knock the verifier complexity down further, from quadratic to\nlogarithmic. And the way we do that is by making the algorithm\nrecursive. We start off with the last protocol above, but instead of\ntrying to embed a polynomial into a 2D polynomial where the degrees in\n\\(x\\) and \\(y\\) are equal, we embed the polynomial into\na 2D polynomial where the degree bound in \\(x\\) is a small constant value; for\nsimplicity, we can even say this must be 2. That is, we express \\(f(x) = g(x, x^2)\\), so that the row check\nalways requires only checking 3 points on each row that we sample (2\nfrom the diagonal plus one from the column).\n\nIf the original polynomial has degree \\(< n\\), then the rows have degree \\(< 2\\) (ie. the rows are straight lines),\nand the column has degree \\(<\n\\frac{n}{2}\\). Hence, what we now have is a linear-time process\nfor converting a problem of proving proximity to a polynomial of degree\n\\(< n\\) into a problem of proving\nproximity to a polynomial of degree \\(<\n\\frac{n}{2}\\). Furthermore, the number of points that need to be\ncommitted to, and thus the prover's computational complexity, goes down\nby a factor of 2 each time (Eli Ben-Sasson likes to compare this aspect\nof FRI to fast fourier\ntransforms, with the key difference that unlike with FFTs, each step\nof recursion only introduces one new sub-problem instead of branching\nout into two). Hence, we can simply keep using the protocol on the\ncolumn created in the previous round of the protocol, until the column\nbecomes so small that we can simply check it directly; the total\ncomplexity is something like \\(n + \\frac{n}{2}\n+ \\frac{n}{4} + ... \\approx 2n\\).\n\nIn reality, the protocol will need to be repeated several times,\nbecause there is still a significant probability that an attacker will\ncheat one round of the protocol. However, even still the proofs\nare not too large; the verification complexity is logarithmic in the\ndegree, though it goes up to \\(\\log\n^{2}n\\) if you count the size of the Merkle proofs.\n\nThe \"real\" FRI protocol also has some other modifications; for\nexample, it uses a binary Galois\nfield (another weird kind of finite field; basically, the same thing\nas the 12th degree extension fields I talk about here,\nbut with the prime modulus being 2). The exponent used for the row is\nalso typically 4 and not 2. These modifications increase efficiency and\nmake the system friendlier to building STARKs on top of it. However,\nthese modifications are not essential to understanding how the algorithm\nworks, and if you really wanted to, you could definitely make STARKs\nwith the simple modular math-based FRI described here too.\n\n## Soundness\n\nI will warn that calculating soundness - that is,\ndetermining just how low the probability is that an optimally generated\nfake proof will pass the test for a given number of checks - is still\nsomewhat of a \"here be dragons\" area in this space. For the simple test\nwhere you take 1,000,000 \\(+ k\\)\npoints, there is a simple lower bound: if a given dataset has the\nproperty that, for any polynomial, at least portion p of the dataset is\nnot on the polynomial, then a test on that dataset will pass with at\nmost \\((1-p)^k\\) probability. However,\neven that is a very pessimistic lower bound - for example, it's not\npossible to be much more than 50% close to two low-degree\npolynomials at the same time, and the probability that the first points\nyou select will be the one with the most points on it is quite low. For\nfull-blown FRI, there are also complexities involving various specific\nkinds of attacks.\n\nHere is a\nrecent article by Ben-Sasson et al describing soundness properties of\nFRI in the context of the entire STARK scheme. In general, the \"good\nnews\" is that it seems likely that in order to pass the \\(D(x) \\cdot Z(x) = C(P(x))\\) check on the\nSTARK, the \\(D(x)\\) values for an\ninvalid solution would need to be \"worst case\" in a certain sense - they\nwould need to be maximally far from any valid polynomial. This\nimplies that we don't need to check for that much proximity.\nThere are proven lower bounds, but these bounds would imply that an\nactual STARK need to be ~1-3 megabytes in size; conjectured but not\nproven stronger bounds reduce the required number of checks by a factor\nof 4.\n\nThe third part of this series will deal with the last major part of\nthe challenge in building STARKs: how we actually construct constraint\nchecking polynomials so that we can prove statements about arbitrary\ncomputation, and not just a few Fibonacci numbers.",
    "contentLength": 18289,
    "summary": "This blog post explains the FRI protocol for low-degree testing in STARKs, showing how to verify polynomial proximity with fewer than D queries.",
    "detailedSummary": {
      "theme": "Vitalik explains the FRI (Fast Reed-Solomon Interactive Oracle Proofs of Proximity) protocol, which enables efficient verification that a large set of points lies on a low-degree polynomial with sublinear complexity.",
      "summary": "Vitalik tackles the critical problem of low-degree testing in STARKs - proving that most points in a large dataset lie on the same low-degree polynomial without checking every point. He explains that while checking all points would be definitive, probabilistic methods can verify that at least 90% of points are on the same polynomial with far fewer queries. Vitalik walks through increasingly efficient protocols, starting with a simple bivariate polynomial approach that achieves sublinear verification but has poor prover complexity, then introducing modular arithmetic optimizations that dramatically reduce computational overhead. The key insight is using modular math properties where certain exponentiations have small 'images' (limited possible outputs), allowing the embedding of high-degree polynomials into 2D structures that can be verified more efficiently. The final recursive FRI protocol reduces a degree-n polynomial proximity problem to a degree-n/2 problem repeatedly, achieving logarithmic verification complexity while maintaining linear prover complexity.",
      "takeaways": [
        "Low-degree testing is the most complex part of STARKs, requiring proof that most points in a large dataset lie on the same polynomial",
        "Direct verification requires checking every point, but probabilistic methods can verify 90%+ proximity with far fewer queries",
        "Modular arithmetic enables more efficient protocols by using the 'small image' property of certain exponentiations to reduce computational complexity",
        "The recursive FRI protocol achieves logarithmic verification complexity by repeatedly halving the degree of the polynomial problem",
        "Soundness analysis remains challenging, with proven bounds requiring larger proof sizes than conjectured stronger bounds that reduce verification overhead"
      ],
      "controversial": [
        "Soundness analysis is described as a 'here be dragons' area with significant gaps between proven lower bounds (requiring 1-3 megabyte STARKs) and conjectured stronger bounds",
        "The protocol relies on probabilistic verification rather than deterministic proof, introducing inherent uncertainty about whether malicious provers could craft attacks"
      ]
    }
  },
  {
    "id": "general-2017-11-09-starks_part_1",
    "title": "STARKs, Part I: Proofs with Polynomials",
    "date": "2017-11-09",
    "category": "math",
    "url": "https://vitalik.eth.limo/general/2017/11/09/starks_part_1.html",
    "path": "general/2017/11/09/starks_part_1.html",
    "content": "- \n\n- \n\n  \n    \n\n  \n  \n  Dark Mode Toggle\n  \n    \n\n  \n\n##  STARKs, Part I: Proofs with Polynomials \n\n 2017 Nov 09 \nSee all posts\n\n \n \n\n STARKs, Part I: Proofs with Polynomials \n\nSpecial thanks to Eli Ben-Sasson for ongoing help, explanations\nand review, coming up with some of the examples used in this post, and\nmost crucially of all inventing a lot of this stuff; thanks to Hsiao-wei\nWang for reviewing\n\nHopefully many people by now have heard of ZK-SNARKs,\nthe general-purpose succinct zero knowledge proof technology that can be\nused for all sorts of usecases ranging from verifiable computation to\nprivacy-preserving cryptocurrency. What you might not know is that\nZK-SNARKs have a newer, shinier cousin: ZK-STARKs. With the T standing\nfor \"transparent\", ZK-STARKs resolve one of the primary weaknesses of\nZK-SNARKs, its reliance on a \"trusted setup\". They also come with much\nsimpler cryptographic assumptions, avoiding the need for elliptic\ncurves, pairings and the knowledge-of-exponent assumption and instead\nrelying purely on hashes and information theory; this also means that\nthey are secure even against attackers with quantum computers.\n\nHowever, this comes at a cost: the size of a proof goes up from 288\nbytes to a few hundred kilobytes. Sometimes the cost will not be worth\nit, but at other times, particularly in the context of public blockchain\napplications where the need for trust minimization is high, it may well\nbe. And if elliptic curves break or quantum computers do come\naround, it definitely will be.\n\nSo how does this other kind of zero knowledge proof work? First of\nall, let us review what a general-purpose succinct ZKP does. Suppose\nthat you have a (public) function \\(f\\), a (private) input \\(x\\) and a (public) output \\(y\\). You want to prove that you know an\n\\(x\\) such that \\(f(x) = y\\), without revealing what \\(x\\) is. Furthermore, for the proof to be\nsuccinct, you want it to be verifiable much more quickly than\ncomputing \\(f\\) itself.\n\nLet's go through a few examples:\n\n- \\(f\\) is a computation that takes\ntwo weeks to run on a regular computer, but two hours on a data center.\nYou send the data center the computation (ie. the code to run \\(f\\) ), the data center runs it, and gives\nback the answer \\(y\\) with a proof. You\nverify the proof in a few milliseconds, and are convinced that \\(y\\) actually is the answer.\n\n- You have an encrypted transaction, of the form \"\\(X_1\\) was my old balance. \\(X_2\\) was your old balance. \\(X_3\\) is my new balance. \\(X_4\\) is your new balance\". You want to\ncreate a proof that this transaction is valid (specifically, old and new\nbalances are non-negative, and the decrease in my balance cancels out\nthe increase in your balance). \\(x\\)\ncan be the pair of encryption keys, and \\(f\\) can be a function which contains as a\nbuilt-in public input the transaction, takes as input the keys, decrypts\nthe transaction, performs the check, and returns 1 if it passes and 0 if\nit does not. \\(y\\) would of course be\n1.\n\n- You have a blockchain like Ethereum, and you download the most\nrecent block. You want a proof that this block is valid, and that this\nblock is at the tip of a chain where every block in the chain is valid.\nYou ask an existing full node to provide such a proof. \\(x\\) is the entire blockchain (yes, all ??\ngigabytes of it), \\(f\\) is a function\nthat processes it block by block, verifies the validity and outputs the\nhash of the last block, and \\(y\\) is\nthe hash of the block you just downloaded.\n\nSo what's so hard about all this? As it turns out, the zero\nknowledge (ie. privacy) guarantee is (relatively!) easy to provide;\nthere are a bunch of ways to convert any computation into an instance of\nsomething like the three color graph problem, where a three-coloring of\nthe graph corresponds to a solution of the original problem, and then\nuse a traditional zero knowledge proof protocol to prove that you have a\nvalid graph coloring without revealing what it is. This excellent\npost by Matthew Green from 2014 describes this in some detail.\n\nThe much harder thing to provide is succinctness.\nIntuitively speaking, proving things about computation succinctly is\nhard because computation is incredibly fragile. If you have a\nlong and complex computation, and you as an evil genie have the ability\nto flip a 0 to a 1 anywhere in the middle of the computation, then in\nmany cases even one flipped bit will be enough to make the computation\ngive a completely different result. Hence, it's hard to see how you can\ndo something like randomly sampling a computation trace in order to\ngauge its correctness, as it's just too easy to miss that \"one evil\nbit\". However, with some fancy math, it turns out that you can.\n\nThe general very high level intuition is that the protocols that\naccomplish this use similar math to what is used in erasure coding,\nwhich is frequently used to make data fault-tolerant. If you\nhave a piece of data, and you encode the data as a line, then you can\npick out four points on the line. Any two of those four points are\nenough to reconstruct the original line, and therefore also give you the\nother two points. Furthermore, if you make even the slightest change to\nthe data, then it is guaranteed at least three of those four points. You\ncan also encode the data as a degree-1,000,000 polynomial, and pick out\n2,000,000 points on the polynomial; any 1,000,001 of those points will\nrecover the original data and therefore the other points, and any\ndeviation in the original data will change at least 1,000,000 points.\nThe algorithms shown here will make heavy use of polynomials in this way\nfor error amplification.\n\nChanging even one point in the original data will lead to large\nchanges in a polynomial's trajectory\n\n## A Somewhat Simple Example\n\nSuppose that you want to prove that you have a polynomial \\(P\\) such that \\(P(x)\\) is an integer with \\(0 \\leq P(x) \\leq 9\\) for all \\(x\\) from 1 to 1 million. This is a simple\ninstance of the fairly common task of \"range checking\"; you might\nimagine this kind of check being used to verify, for example, that a set\nof account balances is still positive after applying some set of\ntransactions. If it were \\(1 \\leq P(x) \\leq\n9\\), this could be part of checking that the values form a\ncorrect Sudoku solution.\n\nThe \"traditional\" way to prove this would be to just show all\n1,000,000 points, and verify it by checking the values. However, we want\nto see if we can make a proof that can be verified in less than\n1,000,000 steps. Simply randomly checking evaluations of \\(P\\) won't do; there's always the\npossibility that a malicious prover came up with a \\(P\\) which satisfies the constraint in\n999,999 places but does not satisfy it in the last one, and random\nsampling only a few values will almost always miss that value. So what\ncan we do?\n\nLet's mathematically transform the problem somewhat. Let \\(C(x)\\) be a constraint checking\npolynomial; \\(C(x) = 0\\) if \\(0 \\leq x \\leq 9\\) and is nonzero otherwise.\nThere's a simple way to construct \\(C(x)\\): \\(x \\cdot\n(x-1) \\cdot (x-2) \\cdot \\ldots(x-9)\\) (we'll assume all of our\npolynomials and other values use exclusively integers, so we don't need\nto worry about numbers in between).\n\nNow, the problem becomes: prove that you know \\(P\\) such that \\(C(P(x)) = 0\\) for all \\(x\\) from 1 to 1,000,000. Let \\(Z(x) = (x-1) \\cdot (x-2) \\cdot \\ldots\n(x-1000000)\\). It's a known mathematical fact that any\npolynomial which equals zero at all \\(x\\) from 1 to 1,000,000 is a multiple of\n\\(Z(x)\\). Hence, the problem can now be\ntransformed again: prove that you know \\(P\\) and \\(D\\) such that \\(C(P(x)) = Z(x) \\cdot D(x)\\) for all \\(x\\) (note that if you know a suitable \\(C(P(x))\\) then dividing it by \\(Z(x)\\) to compute \\(D(x)\\) is not too difficult; you can use long polynomial\ndivision or more realistically a faster algorithm based on FFTs).\nNow, we've converted our original statement into something that looks\nmathematically clean and possibly quite provable.\n\nSo how does one prove this claim? We can imagine the proof process as\na three-step communication between a prover and a verifier: the prover\nsends some information, then the verifier sends some requests, then the\nprover sends some more information. First, the prover commits to (ie.\nmakes a Merkle tree and sends the verifier the root hash of) the\nevaluations of \\(P(x)\\) and \\(D(x)\\) for all \\(x\\) from 1 to 1 billion (yes, billion).\nThis includes the 1 million points where \\(0\n\\leq P(x) \\leq 9\\) as well as the 999 million points where that\n(probably) is not the case.\n\nWe assume the verifier already knows the evaluation of \\(Z(x)\\) at all of these points; the \\(Z(x)\\) is like a \"public verification key\"\nfor this scheme that everyone must know ahead of time (clients that do\nnot have the space to store \\(Z(x)\\) in\nits entirety can simply store the Merkle root of \\(Z(x)\\) and require the prover to also\nprovide branches for every \\(Z(x)\\)\nvalue that the verifier needs to query; alternatively, there are some\nnumber fields over which \\(Z(x)\\) for\ncertain \\(x\\) is very easy to\ncalculate). After receiving the commitment (ie. Merkle root) the\nverifier then selects a random 16 \\(x\\)\nvalues between 1 and 1 billion, and asks the prover to provide the\nMerkle branches for \\(P(x)\\) and \\(D(x)\\) there. The prover provides these\nvalues, and the verifier checks that (i) the branches match the Merkle\nroot that was provided earlier, and (ii) \\(C(P(x))\\) actually equals \\(Z(x) \\cdot D(x)\\) in all 16 cases.\n\nWe know that this proof perfect completeness - if you\nactually know a suitable \\(P(x)\\), then\nif you calculate \\(D(x)\\) and construct\nthe proof correctly it will always pass all 16 checks. But what about\nsoundness - that is, if a malicious prover provides a bad \\(P(x)\\), what is the minimum probability\nthat they will get caught? We can analyze as follows. Because \\(C(P(x))\\) is a degree-10 polynomial\ncomposed with a degree-1,000,000 polynomial, its degree will be at most\n10,000,000. In general, we know that two different degree-\\(N\\) polynomials agree on at most \\(N\\) points; hence, a degree-10,000,000\npolynomial which is not equal to any polynomial which always equals\n\\(Z(x) \\cdot D(x)\\) for some \\(x\\) will necessarily disagree with them all\nat at least 990,000,000 points. Hence, the probability that a bad \\(P(x)\\) will get caught in even one round is\nalready 99%; with 16 checks, the probability of getting caught goes up\nto \\(1 - 10^{-32}\\); that is to say,\nthe scheme is about as hard to spoof as it is to compute a hash\ncollision.\n\nSo... what did we just do? We used polynomials to \"boost\" the error in\nany bad solution, so that any incorrect solution to the original\nproblem, which would have required a million checks to find directly,\nturns into a solution to the verification protocol that can get flagged\nas erroneous at 99% of the time with even a single check.\n\nWe can convert this three-step mechanism into a non-interactive\nproof, which can be broadcasted by a single prover once and then\nverified by anyone, using the Fiat-Shamir\nheuristic. The prover first builds up a Merkle tree of the \\(P(x)\\) and \\(D(x)\\) values, and computes the root hash\nof the tree. The root itself is then used as the source of entropy that\ndetermines what branches of the tree the prover needs to provide. The\nprover then broadcasts the Merkle root and the branches together as the\nproof. The computation is all done on the prover side; the process of\ncomputing the Merkle root from the data, and then using that to select\nthe branches that get audited, effectively substitutes the need for an\ninteractive verifier.\n\nThe only thing a malicious prover without a valid \\(P(x)\\) can do is try to make a valid proof\nover and over again until eventually they get extremely lucky\nwith the branches that a Merkle root that they compute selects, but with\na soundness of \\(1 - 10^{-32}\\) (ie.\nprobability of at least \\(1 -\n10^{-32}\\) that a given attempted fake proof will fail the check)\nit would take a malicious prover billions of years to make a passable\nproof.\n\n## Going Further\n\nTo illustrate the power of this technique, let's use it to do\nsomething a little less trivial: prove that you know the millionth\nFibonacci number. To accomplish this, we'll prove that you have\nknowledge of a polynomial which represents a computation tape, with\n\\(P(x)\\) representing the \\(x\\)th Fibonacci number. The constraint\nchecking polynomial will now hop across three x-coordinates: \\(C(x_1, x_2, x_3) = x_3-x_2-x_1\\) (notice\nhow if \\(C(P(x), P(x+1), P(x+2)) = 0\\)\nfor all \\(x\\) then \\(P(x)\\) represents a Fibonacci\nsequence).\n\nThe translated problem becomes: prove that you know \\(P\\) and \\(D\\) such that \\(C(P(x), P(x+1), P(x+2)) = Z(x) \\cdot\nD(x)\\). For each of the 16 indices that the proof audits, the\nprover will need to provide Merkle branches for \\(P(x)\\), \\(P(x+1)\\), \\(P(x+2)\\) and \\(D(x)\\). The prover will additionally need\nto provide Merkle branches to show that \\(P(0)\n= P(1) = 1\\). Otherwise, the entire process is the same.\n\nNow, to accomplish this in reality there are two problems that need\nto be resolved. The first problem is that if we actually try to work\nwith regular numbers the solution would not be efficient in\npractice, because the numbers themselves very easily get extremely\nlarge. The millionth Fibonacci number, for example, has 208988 digits.\nIf we actually want to achieve succinctness in practice, instead of\ndoing these polynomials with regular numbers, we need to use finite\nfields - number systems that still follow the same arithmetic laws we\nknow and love, like \\(a \\cdot (b+c) = (a\\cdot\nb) + (a\\cdot c)\\) and \\((a^2 - b^2) =\n(a-b) \\cdot (a+b)\\), but where each number is guaranteed to take\nup a constant amount of space. Proving claims about the millionth\nFibonacci number would then require a more complicated design that\nimplements big-number arithmetic on top of this finite field\nmath.\n\nThe simplest possible finite field is modular arithmetic; that is,\nreplace every instance of \\(a + b\\)\nwith \\(a + b \\mod{N}\\) for some prime\n\\(N\\), do the same for subtraction and\nmultiplication, and for division use modular\ninverses (eg. if \\(N = 7\\), then\n\\(3 + 4 = 0\\), \\(2 + 6 = 1\\), \\(3\n\\cdot 4 = 5\\), \\(4 / 2 = 2\\) and\n\\(5 / 2 = 6\\)). You can learn more\nabout these kinds of number systems from my description on prime fields\nhere\n(search \"prime field\" in the page) or this Wikipedia\narticle on modular arithmetic (the articles that you'll find by\nsearching directly for \"finite fields\" and \"prime fields\" unfortunately\ntend to be very complicated and go straight into abstract algebra, don't\nbother with those).\n\nSecond, you might have noticed that in my above proof sketch for\nsoundness I neglected to cover one kind of attack: what if, instead of a\nplausible degree-1,000,000 \\(P(x)\\) and\ndegree-9,000,000 \\(D(x)\\), the attacker\ncommits to some values that are not on any such\nrelatively-low-degree polynomial? Then, the argument that an invalid\n\\(C(P(x))\\) must differ from any valid\n\\(C(P(x))\\) on at least 990 million\npoints does not apply, and so different and much more effective kinds of\nattacks are possible. For example, an attacker could generate a\nrandom value \\(p\\) for every \\(x\\), then compute \\(d = C(p) / Z(x)\\) and commit to these\nvalues in place of \\(P(x)\\) and \\(D(x)\\). These values would not be on any\nkind of low-degree polynomial, but they would pass the\ntest.\n\nIt turns out that this possibility can be effectively defended\nagainst, though the tools for doing so are fairly complex, and so you\ncan quite legitimately say that they make up the bulk of the\nmathematical innovation in STARKs. Also, the solution has a limitation:\nyou can weed out commitments to data that are very far from any\ndegree-1,000,000 polynomial (eg. you would need to change 20% of all the\nvalues to make it a degree-1,000,000 polynomial), but you cannot weed\nout commitments to data that only differ from a polynomial in only one\nor two coordinates. Hence, what these tools will provide is proof of\nproximity - proof that most of the points on \\(P\\) and \\(D\\) correspond to the right kind of\npolynomial.\n\nAs it turns out, this is sufficient to make a proof, though there are\ntwo \"catches\". First, the verifier needs to check a few more indices to\nmake up for the additional room for error that this limitation\nintroduces. Second, if we are doing \"boundary constraint checking\" (eg.\nverifying \\(P(0) = P(1) = 1\\) in the\nFibonacci example above), then we need to extend the proof of proximity\nto not only prove that most points are on the same polynomial, but also\nprove that those two specific points (or whatever other number\nof specific points you want to check) are on that polynomial.\n\nIn the next part of this series, I will describe the solution to\nproximity checking in much more detail, and in the third part I will\ndescribe how more complex constraint functions can be constructed to\ncheck not just Fibonacci numbers and ranges, but also arbitrary\ncomputation.",
    "contentLength": 16967,
    "summary": "STARKs use polynomial math to create zero-knowledge proofs without trusted setup, trading SNARK's 288-byte proofs for kilobyte-sized quantum-resistant ones.",
    "detailedSummary": {
      "theme": "An introduction to ZK-STARKs explaining how they use polynomial mathematics to create succinct zero-knowledge proofs without trusted setup requirements.",
      "summary": "Vitalik introduces ZK-STARKs as a 'newer, shinier cousin' to ZK-SNARKs that eliminates the need for trusted setup by relying purely on hashes and information theory rather than elliptic curves and pairings. While this makes STARKs quantum-resistant and based on simpler cryptographic assumptions, it comes at the cost of much larger proof sizes (hundreds of kilobytes versus 288 bytes). Vitalik explains that the core challenge in succinct zero-knowledge proofs is not the privacy aspect but achieving succinctness, since computation is 'incredibly fragile' and even flipping one bit can completely change results. The breakthrough comes from using polynomial mathematics similar to erasure coding for error amplification - any deviation in the original data causes large changes across the polynomial, making errors detectable through random sampling. Through detailed examples involving range checking and Fibonacci number verification, Vitalik demonstrates how mathematical constraints can be transformed into polynomial equations that can be verified probabilistically with extremely high confidence (1 - 10^-32 soundness) while requiring far fewer checks than the naive approach.",
      "takeaways": [
        "ZK-STARKs eliminate trusted setup requirements and are quantum-resistant, but produce much larger proofs than ZK-SNARKs (hundreds of KB vs 288 bytes)",
        "The main technical challenge in succinct proofs is achieving succinctness rather than zero-knowledge privacy, due to computation's fragility to single bit changes",
        "Polynomial mathematics enables 'error amplification' where any incorrect solution gets detected with 99%+ probability even with minimal random sampling",
        "Constraint checking problems can be mathematically transformed into polynomial equations that can be verified through probabilistic sampling with extremely high confidence",
        "Practical implementations require finite field arithmetic to keep numbers manageable and additional 'proof of proximity' techniques to prevent attacks using non-polynomial data"
      ],
      "controversial": []
    }
  },
  {
    "id": "general-2017-10-17-moe",
    "title": "On Medium-of-Exchange Token Valuations",
    "date": "2017-10-17",
    "category": "applications",
    "url": "https://vitalik.eth.limo/general/2017/10/17/moe.html",
    "path": "general/2017/10/17/moe.html",
    "content": "- \n\n- \n\n  \n    \n\n  \n  \n  Dark Mode Toggle\n  \n    \n\n  \n\n##  On Medium-of-Exchange Token Valuations \n\n 2017 Oct 17 \nSee all posts\n\n \n \n\n On Medium-of-Exchange Token Valuations \n\nOne kind of token model that has become popular among many recent\ntoken sale projects is the \"network medium of exchange token\". The\ngeneral pitch for this kind of token goes as follows. We, the\ndevelopers, build a network, and this network allows you to do new cool\nstuff. This network is a sharing-economy-style system: it consists\npurely of a set of sellers, that provide resources within some protocol,\nand buyers that purchase the services, where both buyers and sellers\ncome from the community. But the purchase and sale of things within this\nnetwork must be done with the new token that we're selling, and\nthis is why the token will have value.\n\nIf it were the developers themselves that were acting as the seller,\nthen this would be a very reasonable and normal arrangement, very\nsimilar in nature to a Kickstarter-style product sale. The token\nactually would, in a meaningful economic sense, be backed by the\nservices that are provided by the developers.\n\nWe can see this in more detail by describing what is going on in a\nsimple economic model. Suppose that \\(N\\) people value a product that a developer\nwants to release at \\(\\$x\\), and\nbelieve the developer will give them the product. The developer does a\nsale, and raises \\(N\\) units for \\(\\$w < x\\) each, thus raising a total\nrevenue of \\(\\$Nw\\). The developer\nbuilds the product, and gives it to each of the buyers. At the end of\nthe day, the buyers are happy, and the developer is happy. Nobody feels\nlike they made an avoidable mistake in participating, and everyone's\nexpectations have been met. This kind of economic model is clearly\nstable.\n\nNow, let's look at the story with a \"medium of exchange\" token. \\(N\\) people value a product that will exist\nin a decentralized network at \\(\\$x\\);\nthe product will be sold at a price of \\(\\$w\n< x\\). They each buy \\(\\$w\\)\nof tokens in the sale. The developer builds the network. Some sellers\ncome in, and offer the product inside the network for \\(\\$w\\). The buyers use their tokens to\npurchase this product, spending \\(\\$w\\)\nof tokens and getting \\(\\$x\\) of value.\nThe sellers spend \\(\\$v < w\\) of\nresources and effort producing this product, and they now have \\(\\$w\\) worth of tokens.\n\nNotice that here, the cycle is not complete, and in fact it never\nwill be; there needs to be an ongoing stream of buyers and sellers for\nthe token to continue having its value. The stream does not strictly\nspeaking have to be endless; if in every round there is a\nchance of at least \\(\\frac{v}{w}\\) that\nthere will be a next round, then the model still works, as even though\nsomeone will eventually be cheated, the risk of any individual\nparticipant becoming that person is lower than the benefit that they get\nfrom participating. It's also totally possible that the token would\ndepreciate in each round, with its value multiplying by some factor\n\\(f\\) where \\(\\frac{v}{w} < f < 1\\), until it\neventually reaches a price of zero, and it would still be on net in\neveryone's interest to participate. Hence, the model is theoretically\nfeasible, but you can see how this model is more complex and more\ntenuous than the simple \"developers as seller\" model.\n\nTraditional macroeconomics has a simple\nequation to try to value a medium of exchange:\n\n\\(MV = PT\\)\n\nHere:\n\n- \\(M\\) is the total money supply;\nthat is, the total number of coins\n\n- \\(V\\) is the \"velocity of money\";\nthat is, the number of times that an average coin changes hands every\nday\n\n- \\(P\\) is the \"price level\". This is\nthe price of goods and services in terms of the token; so it is\nactually the inverse of the currency's price\n\n- \\(T\\) is the transaction volume:\nthe economic value of transactions per day\n\nThe proof for this is a trivial equality: if there are \\(N\\) coins, and each changes hands \\(M\\) times per day, then this is \\(M \\cdot N\\) coins' worth of economic value\ntransacted per day. If this represents \\(\\$T\\) worth of economic value, then the\nprice of each coin is \\(\\frac{T}{M \\cdot\nN}\\), so the \"price level\" is the inverse of this, \\(\\frac{M \\cdot N}{T}\\).\n\nFor easier analysis, we can recast two variables:\n\n- We refer to \\(\\frac{1}{V}\\) with\n\\(H\\), the time that a user holds a\ncoin before using it to make a transaction\n\n- We refer to \\(\\frac{1}{P}\\) with\n\\(C\\), the price of the currency (think\n\\(C = cost\\))\n\nNow, we have:\n\n\\(\\frac{M}{H} = \\frac{T}{C}\\)\n\n\\(MC = TH\\)\n\nThe left term is quite simply the market cap. The right term is the\neconomic value transacted per day, multiplied by the amount of time that\na user holds a coin before using it to transact.\n\nThis is a steady-state model, assuming that the same quantity of\nusers will also be there. In reality, however, the quantity of users may\nchange, and so the price may change. The time that users hold a coin may\nchange, and this may cause the price to change as well.\n\nLet us now look once again at the economic effect on the users.\nWhat do users lose by using an application with a built-in\nappcoin rather than plain old ether (or bitcoin, or USD)? The simplest\nway to express this is as follows: the \"implicit cost\" imposed by such a\nsystem on users the cost to the user of holding those coins for that\nperiod of time, instead of holding that value in the currency that they\nwould otherwise have preferred to hold.\n\nThere are many factors involved in this cost:\u00a0cognitive costs,\nexchange costs and spreads, transaction fees, and many smaller items.\nOne particular significant factor of this implicit cost is expected\nreturn. If a user expects the appcoin to only grow in value by 1% per\nyear, while their other available alternatives grow 3% per year, and\nthey hold $20 of the currency for five days, then that is an expected\nloss of roughly \\(\\$20 \\cdot 2% \\cdot 5 / 365\n= \\$0.0054\\).\n\nOne immediate conclusion from this particular insight is that\nappcoins are very much a multi-equilibrium game. If the\nappcoin grows at 2% per year, then the fee drops to $0.0027, and this\nessentially makes the \"de-facto fee\" of the application (or at least a\nlarge component of it) 2x cheaper, attracting more users and growing its\nvalue more. If the appcoin starts falling at 10% per year, however, then\nthe \"de-facto fee\" grows to $0.035, driving many users away and\naccelerating its growth.\n\nThis leads to increased opportunities for market manipulation, as a\nmanipulator would not just be wasting their money fighting against a\nsingle equilibrium, but may in fact successfully nudge a given currency\nfrom one equilibrium into another, and profit from successfully\n\"predicting\" (ie. causing) this shift. It also means there is a large\namount of path dependency, and established brands matter a lot; witness\nthe epic battles over which fork of the bitcoin blockchain can be called\nBitcoin for one particular high-profile example.\n\nAnother, and perhaps even more important, conclusion is that the\nmarket cap of an appcoin depends crucially on the holding time\n\\(H\\). If someone creates a\nvery efficient exchange, which allows users to purchase an appcoin in\nreal time and then immediately use it in the application, then allowing\nsellers to immediately cash out, then the market cap would drop\nprecipitously. If a currency is stable or prospects are looking\noptimistic, then this may not matter because users actually see no\ndisadvantage from holding the token instead of holding something else\n(ie. zero \"de-facto fee\"), but if prospects start to turn sour then such\na well-functioning exchange can acelerate its demise.\n\nYou might think that exchanges are inherently inefficient, requiring\nusers to create an account, login, deposit coins, wait for 36\nconfirmations, trade and logout, but in fact hyper-efficient exchanges\nare around the corner. Here\nis a thread discussing designs for fully autonomous synchronous on-chain\ntransactions, which can convert token A into token B, and possibly even\nthen use token B to do something, within a single transaction.\nMany other platforms are being developed as well.\n\nWhat this all serves to show is that relying purely on the\nmedium-of-exchange argument to support a token value, while attractive\nbecause of its seeming ability to print money out of thin air, is\nultimately quite brittle. Protocol tokens using this model may well be\nsustained for some time due to irrationality and temporary equilibria\nwhere the implicit cost of holding the token is zero, but it is a kind\nof model which always has an unavoidable risk of collapsing at any\ntime.\n\nSo what is the alternative? One simple alternative is the etherdelta\napproach, where an application simply collects fees in the interface.\nOne common criticism is: but can't someone fork the interface to take\nout the fees? A counter-retort is: someone can also fork the interface\nto replace your protocol token with ETH, BTC, DOGE or whatever else\nusers would prefer to use. One can make a more sophisticated argument\nthat this is hard because the \"pirate\" version would have to compete\nwith the \"official\" version for network effect, but one can just as\neasily create an official fee-paying client that refuses to interact\nwith non-fee-paying clients as well; this kind of network effect-based\nenforcement is similar to how value-added-taxes are typically enforced\nin Europe and other places. Official-client buyers would not interact\nwith non-official-client sellers, and official-client sellers would not\ninteract with non-official-client buyers, so a large group of users\nwould need to switch to the \"pirate\" client at the same time to\nsuccessfully dodge fees. This is not perfectly robust, but it is\ncertainly as good as the approach of creating a new protocol token.\n\nIf developers want to front-load revenue to fund initial development,\nthen they can sell a token, with the property that all fees paid are\nused to buy back some of the token and burn it; this would make the\ntoken backed by the future expected value of upcoming fees spent inside\nthe system. One can transform this design into a more direct utility\ntoken by requiring users to use the utility token to pay fees, and\nhaving the interface use an exchange to automatically purchase tokens if\nthe user does not have tokens already.\n\nThe important thing is that for the token to have a stable value, it\nis highly beneficial for the token supply to have sinks\n- places where tokens actually disappear and so the total token quantity\ndecreases over time. This way, there is a more transparent and explicit\nfee paid by users, instead of the highly variable and difficult to\ncalculate \"de-facto fee\", and there is also a more transparent and\nexplicit way to figure out what the value of protocol tokens should\nbe.",
    "contentLength": 10780,
    "summary": "Medium-of-exchange tokens are economically unstable because efficient exchanges can reduce holding time and crash their value.",
    "detailedSummary": {
      "theme": "Vitalik analyzes the economic fragility of medium-of-exchange token models and argues for alternative token designs with more stable value propositions.",
      "summary": "Vitalik examines the popular \"network medium of exchange token\" model used in many token sales, where buyers must use a specific token to purchase services within a decentralized network. Using the traditional macroeconomic equation MV = PT, Vitalik demonstrates that these tokens face inherent instability because their value depends on continuous streams of buyers and sellers, creating a multi-equilibrium game susceptible to manipulation and collapse. He identifies the critical vulnerability that hyper-efficient exchanges can drastically reduce holding times, which would cause token values to plummet, and notes that the implicit costs imposed on users (cognitive costs, exchange fees, expected returns) make these systems brittle compared to direct payment methods. Vitalik advocates for alternative approaches such as fee collection models similar to EtherDelta, or tokens backed by fee buyback-and-burn mechanisms that create token supply sinks, arguing these provide more transparent pricing and stable value propositions than relying solely on medium-of-exchange utility.",
      "takeaways": [
        "Medium-of-exchange token models are economically fragile because they require continuous buyer-seller cycles and lack the stability of direct developer-backed products",
        "The value of appcoins is highly dependent on holding time (H) - efficient exchanges that reduce holding time can cause market caps to drop precipitously",
        "These token models create multi-equilibrium games that are susceptible to market manipulation and path dependency, where manipulators can profit by nudging tokens between equilibria",
        "Users face implicit costs when using appcoins instead of preferred currencies, including cognitive costs, exchange fees, and opportunity costs from expected returns",
        "More stable alternatives include fee collection models or tokens with supply sinks (buyback-and-burn mechanisms) that provide transparent pricing and explicit value backing"
      ],
      "controversial": [
        "Vitalik's assertion that medium-of-exchange tokens are 'printing money out of thin air' and fundamentally unsustainable may be disputed by projects successfully using this model",
        "His recommendation for fee-based models over protocol tokens contradicts the decentralization philosophy many crypto projects espouse"
      ]
    }
  },
  {
    "id": "general-2017-09-14-prehistory",
    "title": "A Prehistory of the Ethereum Protocol",
    "date": "2017-09-14",
    "category": "protocol",
    "url": "https://vitalik.eth.limo/general/2017/09/14/prehistory.html",
    "path": "general/2017/09/14/prehistory.html",
    "content": "- \n\n- \n\n  \n    \n\n  \n  \n  Dark Mode Toggle\n  \n    \n\n  \n\n##  A Prehistory of the Ethereum Protocol \n\n 2017 Sep 14 \nSee all posts\n\n \n \n\n A Prehistory of the Ethereum Protocol \n\nAlthough the ideas behind the current Ethereum protocol have largely\nbeen stable for two years, Ethereum did not emerge all at once, in its\ncurrent conception and fully formed. Before the blockchain has launched,\nthe protocol went through a number of significant evolutions and design\ndecisions. The purpose of this article will be to go through the various\nevolutions that the protocol went through from start to launch; the\ncountless work that was done on the implementations of the protocol such\nas Geth, cppethereum, pyethereum, and EthereumJ, as well as the history\nof applications and businesses in the Ethereum ecosystem, is\ndeliberately out of scope.\n\nAlso out of scope is the history of Casper and sharding research.\nWhile we can certainly make more blog posts talking about all of the\nvarious ideas Vlad, Gavin, myself and others came up with, and\ndiscarded, including \"proof of proof of work\", hub-and-spoke chains, \"hypercubes\",\nshadow\nchains (arguably a precursor to Plasma), chain\nfibers, and various\niterations of Casper, as well as Vlad's rapidly evolving thoughts on\nreasoning about incentives of actors in consensus protocols and\nproperties thereof, this would also be far too complex a story to go\nthrough in one post, so we will leave it out for now.\n\nLet us first begin with the very earliest version of what would\neventually become Ethereum, back when it was not even called Ethereum.\nWhen I was visiting Israel in October 2013, I spent quite a bit of time\nwith the Mastercoin team, and even suggested a few features for them.\nAfter spending a couple of times thinking about what they were doing, I\nsent the team a proposal to make their protocol more generalized and\nsupport more types of contracts without adding an equally large and\ncomplex set of features:\n\nhttps://web.archive.org/web/20150627031414/http://vbuterin.com/ultimatescripting.html\n\nNotice that this is very far from the later and more expansive vision\nof Ethereum: it specialized purely in what Mastercoin was trying to\nspecialize in already, namely two-party contracts where parties A and B\nwould both put in money, and then they would later get money out\naccording to some formula specified in the contract (eg. a bet would say\n\"if X happens then give all the money to A, otherwise give all the money\nto B\"). The scripting language was not Turing-complete.\n\nThe Mastercoin team was impressed, but they were not interested in\ndropping everything they were doing to go in this direction, which I was\nincreasingly convinced is the correct choice. So here comes version 2,\ncirca December:\n\nhttps://web.archive.org/web/20131219030753/http://vitalik.ca/ethereum.html\n\nHere you can see the results of a substantial rearchitecting, largely\na result of a long walk through San Francisco I took in November once I\nrealized that smart contracts could potentially be fully generalized.\nInstead of the scripting language being simply a way of describing the\nterms of relations between two parties, contracts were themselves\nfully-fledged accounts, and had the ability to hold, send and receive\nassets, and even maintain a permanent storage (back then, the permanent\nstorage was called \"memory\", and the only temporary \"memory\" was the 256\nregisters). The language switched from being a stack-based machine to\nbeing a register-based one on my own volition; I had little argument for\nthis other than that it seemed more sophisticated.\n\nAdditionally, notice that there is now a built-in fee mechanism:\n\nAt this point, ether literally was gas; after every single\ncomputational step, the balance of the contract that a transaction was\ncalling would drop a little bit, and if the contract ran out of money\nexecution would halt. Note that this \"receiver pays\" mechanism meant\nthat the contract itself had to require the sender to pay the contract a\nfee, and immediately exit if this fee is not present; the protocol\nallocated an allowance of 16 free execution steps to allow contracts to\nreject non-fee-paying transactions.\n\nThis was the time when the Ethereum protocol was entirely my own\ncreation. From here on, however, new participants started to join the\nfold. By far the most prominent on the protocol side was Gavin Wood, who\nreached out to me in an about.me message in December 2013:\n\nJeffrey Wilcke, lead developer of the Go client (back then called\n\"ethereal\") also reached out and started coding around the same time,\nthough his contributions were much more on the side of client\ndevelopment rather than protocol research.\n\n \"Hey Jeremy, glad to see you're interested in Ethereum...\"\n\n Gavin's initial contributions were two-fold. First, you might\nnotice that the contract calling model in the initial design was an\nasynchronous one: although contract A could create an \"internal\ntransaction\" to contract B (\"internal transaction\" is Etherscan's lingo;\ninitially they were just called \"transactions\" and then later \"message\ncalls\" or \"calls\"), the internal transaction's execution would not start\nuntil the execution of the first transaction completely finished. This\nmeant that transactions could not use internal transactions as a way of\ngetting information from other contracts; the only way to do that was\nthe EXTRO opcode (kind of like an SLOAD that you could use to read other\ncontracts' storage), and this too was later removed with the support of\nGavin and others.\n\nWhen implementing my initial spec, Gavin naturally implemented\ninternal transactions synchronously without even realizing that the\nintent was different - that is to say, in Gavin's implementation, when a\ncontract calls another contract, the internal transaction gets executed\nimmediately, and once that execution finishes, the VM returns back to\nthe contract that created the internal transaction and proceeds to the\nnext opcode. This approach seemed to both of us to be superior, so we\ndecided to make it part of the spec.\n\nSecond, a discussion between him and myself (during a walk in San\nFrancisco, so the exact details will be forever lost to the winds of\nhistory and possibly a copy or two in the deep archives of the NSA) led\nto a re-factoring of the transaction fee model, moving away from the\n\"contract pays\" approach to a \"sender pays\" approach, and also switching\nto the \"gas\" architecture. Instead of each individual transaction step\nimmediately taking away a bit of ether, the transaction sender pays for\nand is allocated some \"gas\" (roughly, a counter of computational steps),\nand computational steps drew from this allowance of gas. If a\ntransaction runs out of gas, the gas would still be forfeit, but the\nentire execution would be reverted; this seemed like the safest thing to\ndo, as it removed an entire class of \"partial execution\" attacks that\ncontracts previously had to worry about. When a transaction execution\nfinishes, the fee for any unused gas is refunded.\n\nGavin can also be largely credited for the subtle change in vision\nfrom viewing Ethereum as a platform for building programmable money,\nwith blockchain-based contracts that can hold digital assets and\ntransfer them according to pre-set rules, to a general-purpose computing\nplatform. This started with subtle changes in emphasis and terminology,\nand later this influence became stronger with the increasing emphasis on\nthe \"Web 3\" ensemble, which saw Ethereum as being one piece of a suite\nof decentralized technologies, the other two being Whisper and\nSwarm.\n\nThere were also changes made around the start of 2014 that were\nsuggested by others. We ended up moving back to a stack-based\narchitecture after the idea was suggested by Andrew Miller and\nothers.\n\n \n\nCharles Hoskinson suggested the switch from Bitcoin's SHA256 to the\nnewer SHA3 (or, more accurately, keccak256). Although there was some\ncontroversy for a while, discussions with Gavin, Andrew and others led\nto establishing that the size of values on the stack should be limited\nto 32 bytes; the other alternative being considered, unlimited-size\nintegers, had the problem that it was too difficult to figure out how\nmuch gas to charge for additions, multiplications and other\noperations.\n\nThe initial mining algorithm that we had in mind, back in January\n2014, was a contraption called Dagger:\n\nhttps://github.com/ethereum/wiki/blob/master/Dagger.md\n\nDagger was named after the \"directed acyclic graph\" (DAG), the\nmathematical structure that is used in the algorithm. The idea is that\nevery N blocks, a new DAG would be pseudorandomly generated from a seed,\nand the bottom layer of the DAG would be a collection of nodes that\ntakes several gigabytes to store. However, generating any individual\nvalue in the DAG would require calculating only a few thousand entries.\nA \"Dagger computation\" involved getting some number of values in random\npositions in this bottom-level dataset and hashing them together. This\nmeant that there was a fast way to make a Dagger calculation - already\nhaving the data in memory, and a slow, but not memory intensive way -\nregenerating each value from the DAG that you need to get from\nscratch.\n\nThe intention of this algorithm was to have the same\n\"memory-hardness\" properties as algorithms that were popular at the\ntime, like Scrypt, but still be light-client friendly. Miners would use\nthe fast way, and so their mining would be constrained by memory\nbandwidth (the theory is that consumer-grade RAM is already very heavily\noptimized, and so it would be hard to further optimize it with ASICs),\nbut light clients could use the memory-free but slower version for\nverification. The fast way might take a few microseconds and the slow\nbut memory-free way a few milliseconds, so it would still be very viable\nfor light clients.\n\nFrom here, the algorithm would change several times over the course\nof Ethereum development. The next idea that we went through is \"adaptive\nproof of work\"; here, the proof of work would involve executing randomly\nselected Ethereum contracts, and there is a clever reason why this is\nexpected to be ASIC-resistant: if an ASIC was developed, competing\nminers would have the incentive to create and publish many contracts\nthat that ASIC was not good at executing. There is no such thing as an\nASIC for general computation, the story goes, as that is just a CPU, so\nwe could instead use this kind of adversarial incentive mechanism to\nmake a proof of work that essentially was executing general\ncomputation.\n\nThis fell apart for one simple reason: long-range\nattacks. An attacker could start a chain from block 1, fill it up\nwith only simple contracts that they can create specialized hardware\nfor, and rapidly overtake the main chain. So... back to the drawing\nboard.\n\nThe next algorithm was something called Random Circuit, described in\nthis google doc here,\nproposed by myself and Vlad Zamfir, and analyzed\nby Matthew Wampler-Doty and others. The idea here was also to\nsimulate general-purpose computation inside a mining algorithm, this\ntime by executing randomly generated circuits. There's no hard proof\nthat something based on these principles could not work, but the\ncomputer hardware experts that we reached out to in 2014 tended to be\nfairly pessimistic on it. Matthew Wampler-Doty himself suggested a proof\nof work based on SAT solving, but this too was ultimately rejected.\n\nFinally, we came full circle with an algorithm called \"Dagger\nHashimoto\". \"Dashimoto\", as it was sometimes called in short, borrowed\nmany ideas from Hashimoto,\na proof of work algorithm by Thaddeus Dryja that pioneered the notion of\n\"I/O bound proof of work\", where the dominant limiting factor in mining\nspeed was not hashes per second, but rather megabytes per second of RAM\naccess. However, it combined this with Dagger's notion of\nlight-client-friendly DAG-generated datasets. After many rounds of\ntweaking by myself, Matthew, Tim and others, the ideas finally converged\ninto the algorithm we now call Ethash.\n\nBy the summer of 2014, the protocol had considerably stabilized, with\nthe major exception of the proof of work algorithm which would not reach\nthe Ethash phase until around the beginning of 2015, and a semi-formal\nspecification existed in the form of Gavin's yellow paper.\n\nIn August 2014, I developed and introduced the\nuncle mechanism, which allows Ethereum's blockchain to have a\nshorter block time and higher capacity while mitigating centralization\nrisks. This was introduced as part of PoC6.\n\nDiscussions with the Bitshares team led us to consider adding\nheaps as a first-class data structure, though we ended up not doing\nthis due to lack of time, and later security audits and DoS attacks will\nshow that it is actually much harder than we had thought at the time to\ndo this safely.\n\nIn September, Gavin and I planned out the next two major changes to\nthe protocol design. First, alongside the state tree and transaction\ntree, every block would also contain a \"receipt tree\". The receipt tree\nwould include hashes of the logs created by a transaction, along with\nintermediate state roots. Logs would allow transactions to create\n\"outputs\" that are saved in the blockchain, and are accessible to light\nclients, but that are not accessible to future state calculations. This\ncould be used to allow decentralized applications to easily query for\nevents, such as token transfers, purchases, exchange orders being\ncreated and filled, auctions being started, and so forth.\n\nThere were other ideas that were considered, like making a Merkle\ntree out of the entire execution trace of a transaction to allow\nanything to be proven; logs were chosen because they were a compromise\nbetween simplicity and completeness.\n\nThe second was the idea of \"precompiles\", solving the problem of\nallowing complex cryptographic computations to be usable in the EVM\nwithout having to deal with EVM overhead. We had also gone through many\nmore ambitious ideas about \"native\ncontracts\", where if miners have an optimized implementation of some\ncontracts they could \"vote\" the gasprice of those contracts down, so\ncontracts that most miners could execute much more quickly would\nnaturally have a lower gas price; however, all of these ideas were\nrejected because we could not come up with a cryptoeconomically safe way\nto implement such a thing. An attacker could always create a contract\nwhich executes some trapdoored cryptographic operation, distribute the\ntrapdoor to themselves and their friends to allow them to execute this\ncontract much faster, then vote the gasprice down and use this to DoS\nthe network. Instead we opted for the much less ambitious approach of\nhaving a smaller number of precompiles that are simply specified in the\nprotocol, for common operations such as hashes and signature\nschemes.\n\nGavin was also a key initial voice in developing the idea of \"protocol\nabstraction\" - moving as many parts of the protocol such as ether\nbalances, transaction signing algorithms, nonces, etc into the protocol\nitself as contracts, with a theoretical final goal of reaching a\nsituation where the entire ethereum protocol could be described as\nmaking a function call into a virtual machine that has some\npre-initialized state. There was not enough time for these ideas to get\ninto the initial Frontier release, but the principles are expected to\nstart slowly getting integrated through some of the Constantinople\nchanges, the Casper contract and the sharding specification.\n\nThis was all implemented in PoC7; after PoC7, the protocol did not\nreally change much, with the exception of minor, though in some cases\nimportant, details that would come out through security audits...\n\nIn early 2015, came the pre-launch security audits organized by Jutta\nSteiner and others, which included both software code audits and\nacademic audits. The software audits were primarily on the C++ and Go\nimplementations, which were led by Gavin Wood and Jeffrey Wilcke,\nrespectively, though there was also a smaller audit on my pyethereum\nimplementation. Of the two academic audits, one was performed by Ittay\nEyal (of \"selfish mining\" fame), and the other by Andrew Miller and\nothers from Least Authority. The Eyal audit led to a minor protocol\nchange: the total difficulty of a chain would not include uncles. The Least\nAuthority audit was more focused on smart contract and gas\neconomics, as well as the Patricia tree. This audit led to several\nprotocol changes. One small one is the use of sha3(addr) and sha3(key)\nas trie keys instead of the address and key directly; this would make it\nharder to perform a worst-case attack on the trie.\n\nAnd a warning that was perhaps a bit too far ahead of its\ntime...\n\n Another significant thing that we discussed was the gas limit\nvoting mechanism. At the time, we were already concerned by perceived\nlack of progress in the bitcoin block size debate, and wanted to have a\nmore flexible design in Ethereum that could adjust over time as needed.\nBut the challenge is: what is the optimal limit? My initial thought had\nbeen to make a dynamic limit, targeting \\(1.5\n\\cdot\\) the long-term exponential moving average of the actual\ngas usage, so that in the long run on average blocks would be \\(\\frac{2}{3}\\) full. However, Andrew showed\nthat this was exploitable in some ways - specifically, miners who wanted\nto raise the limit would simply include transactions in their own blocks\nthat consume a very large amount of gas, but take very little time to\nprocess, and thereby always create full blocks at no cost to themselves.\nThe security model was thus, at least in the upward direction,\nequivalent to simply having miners vote on the gas limit.\n\nWe did not manage to come up with a gas limit strategy that was less\nlikely to break, and so Andrew's recommended solution was to simply have\nminers vote on the gas limit explicitly, and have the default strategy\nfor voting be the \\(1.5\\cdot\\) EMA\nrule. The reasoning was that we were still very far from knowing the\nright approach for setting maximum gas limits, and the risk of any\nspecific approach failing seemed greater than the risk of miners abusing\ntheir voting power. Hence, we might as well simply let miners vote on\nthe gas limit, and accept the risk that the limit will go too high or\ntoo low, in exchange for the benefit of flexibility, and the ability for\nminers to work together to very quickly adjust the limit upwards or\ndownwards as needed.\n\nAfter a mini-hackathon between Gavin, Jeff and myself, PoC9 was\nlaunched in March, and was intended to be the final proof of concept\nrelease. A testnet, Olympic, ran for four months, using the protocol\nthat was intended to be used in the livenet, and Ethereum's long-term\nplan was established. Vinay Gupta wrote a blog post, \"The\nEthereum Launch Process\", that described the four expected stages of\nEthereum livenet development, and gave them their current names:\nFrontier, Homestead, Metropolis and Serenity.\n\nOlympic ran for four months. In the first two months, many bugs were\nfound in the various implementations, consensus failures happened, among\nother issues, but around June the network noticeably stabilized. In July\na decision was made to make a code-freeze, followed by a release, and on\nJuly 30 the release took place.",
    "contentLength": 19280,
    "summary": "Ethereum's protocol evolved from a limited two-party contract system for Mastercoin in 2013 to a general computing platform through contributions from Gavin Wood and others.",
    "detailedSummary": {
      "theme": "Vitalik traces the evolution of Ethereum's protocol design from its earliest conception as a specialized scripting system for Mastercoin to its launch as a general-purpose computing platform.",
      "summary": "Vitalik chronicles Ethereum's technical evolution from October 2013 to its July 2015 launch, beginning with his proposal to the Mastercoin team for a more generalized contract system. The protocol underwent significant architectural changes, including the shift from a register-based to stack-based virtual machine, the evolution from asynchronous to synchronous contract calls, and the development of the gas mechanism from a 'receiver pays' to 'sender pays' model. Key collaborators like Gavin Wood played crucial roles in refining the vision from programmable money to a general-purpose computing platform, while the mining algorithm evolved through multiple iterations from Dagger to Ethash. The final protocol incorporated features like the uncle mechanism for faster block times, receipt trees for light client accessibility, and precompiled contracts for cryptographic operations, with security audits in early 2015 leading to final refinements before the Frontier launch.",
      "takeaways": [
        "Ethereum evolved from a specialized two-party contract system for Mastercoin into a general-purpose computing platform through multiple architectural iterations",
        "Key design decisions like synchronous contract execution and the gas fee model emerged from collaboration between Vitalik and Gavin Wood rather than individual planning",
        "The mining algorithm went through several failed attempts (including adaptive proof of work and Random Circuit) before settling on Ethash due to security concerns like long-range attacks",
        "Security audits in early 2015 led to important protocol changes including modifications to the gas limit voting mechanism and trie key structures",
        "The transition from 'receiver pays' to 'sender pays' gas model was crucial for eliminating partial execution attacks and making the system more secure"
      ],
      "controversial": [
        "The decision to let miners vote on gas limits rather than implementing an automated algorithm, accepting the risk of potential abuse in exchange for flexibility",
        "The rejection of more ambitious native contract optimization ideas due to cryptoeconomic security concerns, potentially limiting efficiency gains"
      ]
    }
  },
  {
    "id": "general-2017-07-27-metcalfe",
    "title": "A Note on Metcalfe's Law, Externalities and Ecosystem Splits",
    "date": "2017-07-27",
    "category": "general",
    "url": "https://vitalik.eth.limo/general/2017/07/27/metcalfe.html",
    "path": "general/2017/07/27/metcalfe.html",
    "content": "- \n\n- \n\n  \n    \n\n  \n  \n  Dark Mode Toggle\n  \n    \n\n  \n\n##  A Note on Metcalfe's Law, Externalities and Ecosystem Splits \n\n 2017 Jul 27 \nSee all posts\n\n \n \n\n A Note on Metcalfe's Law, Externalities and Ecosystem Splits \n\nLooks like it's blockchain split season again. For background of various\npeople discussing the topic, and whether such splits are good or\nbad:\n\n- Power laws and network effects (arguing the BTC/BCC split may\ndestroy value due to network effect loss): https://medium.com/crypto-fundamental/power-laws-and-network-effects-why-bitcoincash-is-not-a-free-lunch-5adb579972aa\n\n- Brian Armstrong on the Ethereum Hard Fork (last year): https://blog.coinbase.com/on-the-ethereum-hard-fork-780f1577e986\n\n- Phil Daian on the ETH/ETC split: http://pdaian.com/blog/stop-worrying-love-etc/\n\nGiven that ecosystem splits are not going away, and we may well see more\nof them in the crypto industry over the next decade, it seems useful to\ninform the discussion with some simple economic modeling. With that in\nmind, let's get right to it. \n\nSuppose that there exist two projects A and B, and a set of users of\ntotal size \\(N\\), where A has \\(N_a\\) users and B has \\(N_b\\) users. Both projects benefit from\nnetwork effects, so they have a utility that increases with the number\nof users. However, users also have their own differing taste\npreferences, and this may lead them to choose the smaller platform over\nthe bigger platform if it suits them better.\n\nWe can model each individual's private utility in one of four\nways:\n\n1. \\(U(A) = p + N_a\\)\n\\(U(B) = q + N_b\\)\n\n2. \\(U(A) = p \\cdot N_a\\)\n\\(U(B) = q \\cdot N_b\\)\n\n3. \\(U(A) = p + \\ln{N_a}\\)\n\\(U(B) = q + \\ln{N_b}\\)\n\n4. \\(U(A) = p \\cdot \\ln{N_a}\\)\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\n\\(U(B) = q \\cdot \\ln{N_b}\\)\n\n\\(p\\) and \\(q\\) are private per-user parameters that\nyou can think of as corresponding to users' distinct preferences. The\ndifference between the first two approaches and the last two reflects\ndifferences between interpretations of Metcalfe's law, or more broadly\nthe idea that the per-user value of a system grows with the number of\nusers. The original\nformulation suggested a per-user value of \\(N\\) (that is, a total network value of\n\\(N^{2}\\)), but other analysis (see here)\nsuggests that above very small scales \\(N\\log{N}\\) usually dominates; there is a\ncontroversy over which model is correct. The difference between the\nfirst and second (and between the third and fourth) is the extent to\nwhich utility from a system's intrinsic quality and utility from network\neffects are complementary - that is, are the two things good in\ncompletely separate ways that do not interact with each other, like\nsocial media and coconuts, or are network effects an important part of\nletting the intrinsic quality of a system shine?\n\nWe can now analyze each case in turn by looking at a situation where\n\\(N_a\\) users choose A and \\(N_b\\) users choose B, and analyze each\nindividual's decision to choose one or the other from the perspective of\neconomic externalities - that is, does a user's choice to switch from A\nto B have a positive net effect on others' utility or a negative one? If\nswitching has a positive externality, then it is virtuous and should be\nsocially nudged or encouraged, and if it has a negative externality then\nit should be discouraged. We model an \"ecosystem split\" as a game where\nto start off \\(N_a = N\\) and \\(N_b = 0\\) and users are deciding for\nthemselves whether or not to join the split, that is, to move from A to\nB, possibly causing \\(N_a\\) to fall and\n\\(N_b\\) to rise.\n\nSwitching (or not switching) from A to B has externalties because A\nand B both have network effects; switching from A to B has the negative\nexternality of reducing A's network effect, and so hurting all remaining\nA users, but it also has the positive externality of increasing B's\nnetwork effect, and so benefiting all B users.\n\n## Case 1\n\nSwitching from A to B gives \\(N_a\\)\nusers a negative externality of one, so a total loss of \\(N_a\\), and it gives \\(N_b\\) users a positive externality of one,\nso a total gain of \\(N_b\\). Hence, the\ntotal externality is of size \\(N_b -\nN_a\\); that is, switching from the smaller to the larger platform\nhas positive externalities, and switching from the larger platform to\nthe smaller platform has negative externalities.\n\n## Case 2\n\nSuppose \\(P_a\\) is the sum of \\(p\\) values of \\(N_a\\) users, and \\(Q_b\\) is the sum of \\(q\\) values of \\(N_b\\) users. The total negative externality\nis \\(P_a\\) and the total positive\nexternality is \\(Q_b\\). Hence,\nswitching from the smaller platform to the larger has positive social\nexternalities if the two platforms have equal intrinsic quality to their\nusers (ie. users of A intrinsically enjoy A as much as users of B\nintrinsically enjoy B, so \\(p\\) and\n\\(q\\) values are evenly distributed),\nbut if it is the case that A is bigger but B is better, then there are\npositive externalities in switching to B.\n\nFurthermore, notice that if a user is making a switch from a larger A\nto a smaller B, then this itself is revealed-preference evidence that,\nfor that user, and for all existing users of B, \\(\\frac{q}{p} > \\frac{N_a}{N_b}\\).\nHowever, if the split stays as a split, and does not proceed to become a\nfull-scale migration, then that means that users of A hold different\nviews, though this could be for two reasons: (i) they intrinsically\ndislike A but not by enough to justify the switch, (ii) they\nintrinsically like A more than B. This could arise because (a) A users\nhave a higher opinion of A than B users, or (b) A users have a lower\nopinion of B than B users. In general, we see that moving from a system\nthat makes its average user less happy to a system that makes its\naverage user more happy has positive externalities, and in other\nsituations it's difficult to say.\n\n## Case 3\n\nThe derivative of \\(\\ln{x}\\) is\n\\(\\frac{1}{x}\\). Hence, switching from\nA to B gives \\(N_a\\) users a negative\nexternality of \\(\\frac{1}{N_a}\\), and\nit gives \\(N_b\\) users a positive\nexternality of \\(\\frac{1}{N_b}\\).\nHence, the negative and positive externalities are both of total size\none, and thus cancel out. Hence, switching from one platform to the\nother imposes no social externalities, and it is socially optimal if all\nusers switch from A to B if and only if they think that it is a good\nidea for them personally to do so.\n\n## Case 4\n\nLet \\(P_a\\) and \\(Q_b\\) are before. The negative externality\nis of total size \\(\\frac{P_a}{N_a}\\)\nand the positive externality is of total size \\(\\frac{Q_b}{N_b}\\). Hence, if the two\nsystems have equal intrinsic quality, the externality is of size zero,\nbut if one system has higher intrinsic quality, then it is virtuous to\nswitch to it. Note that as in case 2, if users are switching\nfrom a larger system to a smaller system, then that means that they find\nthe smaller system to have higher intrinsic quality, although, also as\nin case 2, if the split remains a split and does not become a full-scale\nmigration, then that means other users see the intrinsic quality of the\nlarger system as higher, or at least not lower by enough to be worth the\nnetwork effects.\n\nThe existence of users switching to B suggests that for them, \\(\\frac{q}{p} \\geq\n\\frac{log{N_a}}{log{N_b}}\\), so for the \\(\\frac{Q_B}{N_b} > \\frac{P_a}{N_a}\\)\ncondition to not hold (ie. for a move from a larger system to a smaller\nsystem not to have positive externalities) it would need to be the case\nthat users of A have similarly high values of \\(p\\) - an approximate heuristic is, the\nusers of A would need to love A so much that if they were the\nones in the minority that would be willing to split off and move to (or\nstay with) the smaller system. In general, it thus seems that moves from\nlarger systems to smaller systems that actually do happen will have\npositive externalities, but it is far from ironclad that this is the\ncase.\n\nHence, if the first model is true, then to maximize social welfare we\nshould be trying to nudge people to switch to (or stay with) larger\nsystems over smaller systems, and splits should be discouraged. If the\nfourth model is true, then we should be at least slightly trying to\nnudge people to switch to smaller systems over larger systems, and\nsplits should be slightly encouraged. If the third model is true, then\npeople will choose the socially optimal thing all by themselves, and if\nthe second model is true, it's a toss-up.\n\nIt is my personal view that the truth lies somewhere between the\nthird and fourth models, and the first and second greatly overstate\nnetwork effects above small scales. The first and second model (the\n\\(N^{2}\\) form of Metcalfe's law)\nessentially state that a system growing from 990 million to 1 billion\nusers gives the same increase in per-user utility as growing from\n100,000 to 10.1 million users, which seems very unrealistic, whereas the\n\\(N\\log{N}\\) model (growing from 100\nmillion to 1 billion users gives the same increase in per-user utility\nas growing from 100,000 to 10 million users) intuitively seems much more\ncorrect.\n\nAnd the third model says: if you see people splitting off from a\nlarger system to create a smaller system because they want something\nthat more closely matches their personal values, then the fact that\nthese people have already shown that they value this switch enough to\ngive up the comforts of the original system's network effects is by\nitself enough evidence to show that the split is socially beneficial.\nHence, unless I can be convinced that the first model is true, or that\nthe second model is true and the specific distributions of \\(p\\) and \\(q\\) values make splits make negative\nnegative externalities, I maintain my existing view that those splits\nthat actually do happen (though likely not hypothetical splits\nthat end up not happening due to lack of interest) are in the long term\nsocially beneficial, value-generating events.",
    "contentLength": 9851,
    "summary": "The blog argues that blockchain ecosystem splits are socially beneficial because the weaker N*log(N) network effects model is more accurate than N^2.",
    "detailedSummary": {
      "theme": "Vitalik analyzes the economic externalities of blockchain ecosystem splits using mathematical models to determine whether such splits create or destroy social value.",
      "summary": "Vitalik examines the economics of blockchain ecosystem splits (like Bitcoin/Bitcoin Cash and Ethereum/Ethereum Classic) by modeling four different interpretations of network effects and Metcalfe's Law. He analyzes the externalities created when users switch between platforms, considering both the negative effect of reducing the original platform's network and the positive effect of strengthening the new platform. The key distinction lies between models that assume network value grows as N\u00b2 (traditional Metcalfe's Law) versus N log N, and whether intrinsic quality and network effects are additive or multiplicative. Under the N\u00b2 models, splits generally have negative externalities and should be discouraged, while under the N log N models, the externalities are either neutral or slightly positive, making splits socially beneficial. Vitalik argues that the N log N models are more realistic, as the marginal utility of additional users decreases at large scales - growing from 990 million to 1 billion users shouldn't provide the same per-user benefit as growing from 100,000 to 10.1 million users.",
      "takeaways": [
        "Traditional Metcalfe's Law (N\u00b2 network value) likely overstates network effects at large scales, making the N log N model more realistic",
        "Under logarithmic network effect models, ecosystem splits that actually occur tend to be socially beneficial rather than value-destructive",
        "The mere fact that users are willing to abandon a larger platform's network effects for a smaller one provides revealed-preference evidence that the split creates value",
        "Whether splits are beneficial depends critically on how network effects scale and whether intrinsic platform quality and network size are complementary",
        "Splits from larger to smaller platforms generally indicate the smaller platform has higher intrinsic quality for those users, suggesting positive externalities"
      ],
      "controversial": [
        "Vitalik's rejection of traditional Metcalfe's Law challenges widely-accepted economic thinking about network effects",
        "His conclusion that most blockchain splits are socially beneficial contradicts common arguments about the value-destructive nature of ecosystem fragmentation"
      ]
    }
  },
  {
    "id": "general-2017-07-16-triangle_of_harm",
    "title": "The Triangle of Harm",
    "date": "2017-07-16",
    "category": "society",
    "url": "https://vitalik.eth.limo/general/2017/07/16/triangle_of_harm.html",
    "path": "general/2017/07/16/triangle_of_harm.html",
    "content": "- \n\n- \n\n  \n    \n\n  \n  \n  Dark Mode Toggle\n  \n    \n\n  \n\n##  The Triangle of Harm \n\n 2017 Jul 16 \nSee all posts\n\n \n \n\n The Triangle of Harm \n\nThe following is a diagram from a slide that I made in one of my\npresentations at Cornell this week: \n\n If there was one diagram that could capture the core\nprinciple of Casper's incentivization philosophy, this might be it.\nHence, it warrants some further explanation.\n\nThe diagram shows three constituencies - the minority, the majority\nand the protocol (ie. users), and four arrows representing possible\nadversarial actions: the minority attacking the protocol, the minority\nattacking the majority, the majority attacking the protocol, and the\nmajority attacking the minority. Examples of each include:\n\n- Minority attacking the protocol - Finney\nattacks (an attack done by a miner on a proof of work blockchain\nwhere the miner double-spends unconfirmed, or possibly single-confirmed,\ntransactions)\n\n- Minority attacking the majority - feather\nforking (a minority in a proof of work chain attempting to revert\nany block that contains some undesired transactions, though giving up if\nthe block gets two confirmations)\n\n- Majority attacking the protocol - traditional 51%\nattacks\n\n- Majority attacking the minority - a 51% censorship\nattack, where a cartel refuses to accept any blocks from miners (or\nvalidators) outside the cartel\n\nThe essence of Casper's philosophy is this: for all four\ncategories of attack, we want to put an upper bound on the\nratio between the amount of harm suffered by the victims of the attack\nand the cost to the attacker. In some ways, every design decision in\nCasper flows out of this principle.\n\nThis differs greatly from the usual proof of work incentivization\nschool of thought in that in the proof of work view, the last two\nattacks are left undefended against. The first two attacks, Finney\nattacks and feather forking, are costly because the attacker risks their\nblocks not getting included into the chain and so loses revenue. If the\nattacker is a majority, however, the attack is costless, because the\nattacker can always guarantee that their chain will be the main chain.\nIn the long run, difficulty adjustment ensures that the total revenue of\nall miners is exactly the same no matter what, and this further means\nthat if an attack causes some victims to lose money, then the attacker\ngains money.\n\nThis property of proof of work arises because traditional Nakamoto\nproof of work fundamentally punishes dissent - if you as a\nminer make a block that aligns with the consensus, you get rewarded, and\nif you make a block that does not align with the consensus you get\npenalized (the penalty is not in the protocol; rather, it comes from the\nfact that such a miner expends electricity and capital to produce the\nblock and gets no reward).\nCasper, on the other hand, works primarily by punishing\nequivocation - if you send two messages that conflict with each\nother, then you get very heavily penalized, even if one of those\nmessages aligns with the consensus (read more on this in the blog\npost on \"minimal slashing conditions\"). Hence, in the event of a\nfinality reversion attack, those who caused the reversion event are\npenalized, and everyone else is left untouched; the majority can attack\nthe protocol only at heavy cost, and the majority cannot cause the\nminority to lose money. \n\nIt gets more challenging when we move to talking about two other\nkinds of attacks - liveness faults, and censorship. A liveness fault is\none where a large portion of Casper validators go offline, preventing\nthe consensus from reaching finality, and a censorship fault is one\nwhere a majority of Casper validators refuse to accept some\ntransactions, or refuse to accept consensus messages from other Casper\nvalidators (the victims) in order to deprive them of rewards.\n\nThis touches on a fundamental dichotomy: speaker/listener\nfault equivalence.\n\nSuppose that person B says that they did not receive a message from\nperson A. There are two possible explanations: (i) person A did not send\nthe message, (ii) person B pretended not to hear the message. Given just\nthe evidence of B's claim, there is no way to tell which of the two\nexplanations is correct. The relation to blockchain protocol\nincentivization is this: if you see a protocol execution where 70% of\nvalidators' messages are included in the chain and 30% are not, and see\nnothing else (and this is what the blockchain sees), then there is no\nway to tell whether the problem is that 30% are offline or 70% are\ncensoring. If we want to make both kinds of attacks expensive, there is\nonly one thing that we can do: penalize both sides.\n\nPenalizing both sides allows either side to \"grief\" the other, by\ngoing offline if they are a minority and censoring if they are a\nmajority. However, we can establish bounds on how easy this griefing is,\nthrough the technique of griefing factor analysis. The\ngriefing factor of a strategy is essentially the amount of money lost by\nthe victims divided by the amount of money lost by the attackers, and\nthe griefing factor of a protocol is the highest griefing factor that it\nallows. For example, if a protocol allows me to cause you to lose $3 at\na cost of $1 to myself, then the griefing factor is 3. If there are no\nways to cause others to lose money, the griefing factor is zero, and if\nyou can cause others to lose money at no cost to yourself (or at a\nbenefit to yourself), the griefing factor is infinity.\n\nIn general, wherever a speaker/listener dichotomy exists, the\ngriefing factor cannot be globally bounded above by any value below\n1. The reason is simple: either side can grief the other, so if\nside \\(A\\) can grief side \\(B\\) with a factor of \\(x\\) then side \\(B\\) can grief side \\(A\\) with a factor of \\(\\frac{1}{x}\\); \\(x\\) and \\(\\frac{1}{x}\\) cannot both be below 1\nsimultaneously. We can play around with the factors; for example, it may\nbe considered okay to allow griefing factors of 2 for majority attackers\nin exchange for keeping the griefing factor at 0.5 for minorities, with\nthe reasoning that minority attackers are more likely. We can also allow\ngriefing factors of 1 for small-scale attacks, but specifically for\nlarge-scale attacks force a chain split where on one chain one side is\npenalized and on another chain another side is penalized, trusting the\nmarket to pick the chain where attackers are not favored. Hence there is\na lot of room for compromise and making tradeoffs between different\nconcerns within this framework.\n\nPenalizing both sides has another benefit: it ensures that if the\nprotocol is harmed, the attacker is penalized. This ensures that whoever\nthe attacker is, they have an incentive to avoid attacking that is\ncommensurate with the amount of harm done to the protocol. However, if\nwe want to bound the ratio of harm to the protocol over cost to\nattackers, we need a formalized way of measuring how much harm to the\nprotocol was done.\n\nThis introduces the concept of the protocol utility\nfunction - a formula that tells us how well the protocol is\ndoing, that should ideally be calculable from inside the blockchain. In\nthe case of a proof of work chain, this could be the percentage of all\nblocks produced that are in the main chain. In Casper, protocol utility\nis zero for a perfect execution where every epoch is finalized and no\nsafety failures ever take place, with some penalty for every epoch that\nis not finalized, and a very large penalty for every safety failure. If\na protocol utility function can be formalized, then penalties for faults\ncan be set as close to the loss of protocol utility resulting from those\nfaults as possible.",
    "contentLength": 7653,
    "summary": "Casper's blockchain consensus mechanism limits attack damage by penalizing both attackers and victims in ambiguous fault scenarios.",
    "detailedSummary": {
      "theme": "Vitalik explains Casper's incentivization philosophy of bounding the harm-to-cost ratio for all types of attacks on blockchain protocols through his 'Triangle of Harm' framework.",
      "summary": "Vitalik presents the 'Triangle of Harm' diagram showing three constituencies (minority, majority, protocol) and four types of adversarial actions between them. He explains how Casper's design philosophy aims to establish upper bounds on the ratio between harm suffered by victims and cost to attackers across all attack categories, differing from proof of work which leaves majority attacks undefended. While proof of work punishes dissent, Casper punishes equivocation, ensuring that those who cause finality reversions are penalized while others remain untouched. Vitalik addresses the challenge of liveness faults and censorship through the concept of speaker/listener fault equivalence, where the protocol cannot distinguish between a validator being offline versus being censored. He introduces griefing factor analysis to measure attack costs and explains how penalizing both sides in ambiguous situations, while creating griefing opportunities, allows for bounded ratios and ensures attackers face consequences proportional to protocol harm through formalized protocol utility functions.",
      "takeaways": [
        "Casper aims to bound the harm-to-cost ratio for all four types of attacks: minority vs protocol, minority vs majority, majority vs protocol, and majority vs minority",
        "Unlike proof of work which punishes dissent, Casper punishes equivocation, making majority attacks costly rather than free",
        "The speaker/listener fault equivalence problem means protocols cannot distinguish between offline validators and censorship, requiring penalties for both sides",
        "Griefing factor analysis measures the ratio of victim losses to attacker costs, with factors necessarily being at least 1 where speaker/listener dichotomies exist",
        "Protocol utility functions can formalize harm measurement to set penalties proportional to actual damage done to the protocol"
      ],
      "controversial": [
        "The approach of penalizing both sides in ambiguous situations creates griefing opportunities where either party can cause losses to the other",
        "The concept that griefing factors cannot be globally bounded below 1 in speaker/listener scenarios may be seen as a fundamental limitation rather than an acceptable tradeoff"
      ]
    }
  },
  {
    "id": "general-2017-06-22-marketmakers",
    "title": "On Path Independence",
    "date": "2017-06-22",
    "category": "society",
    "url": "https://vitalik.eth.limo/general/2017/06/22/marketmakers.html",
    "path": "general/2017/06/22/marketmakers.html",
    "content": "- \n\n- \n\n  \n    \n\n  \n  \n  Dark Mode Toggle\n  \n    \n\n  \n\n##  On Path Independence \n\n 2017 Jun 22 \nSee all posts\n\n \n \n\n On Path Independence \n\nSuppose that someone walks up to you and starts exclaiming to you\nthat he thinks he has figured out how to create a source of unlimited\nfree energy. His scheme looks as follows. First, you get a spaceship up\nto low Earth orbit. There, Earth's gravity is fairly high, and so the\nspaceship will start to accelerate heavily toward the earth. The\nspaceship puts itself into a trajectory so that it barely brushes past\nthe Earth's atmosphere, and then keeps hurtling far into space. Further\nin space, the gravity is lower, and so the spaceship can go higher\nbefore it starts once again coming down. When it comes down, it takes a\ncurved path toward the Earth, so as to maximize its time in low orbit,\nmaximizing the acceleration it gets from the high gravity there, so that\nafter it passes by the Earth it goes even higher. After it goes high\nenough, it flies through the Earth's atmosphere, slowing itself down but\nusing the waste heat to power a thermal reactor. Then, it would go back\nto step one and keep going.\n\nSomething like this:\n\nNow, if you know anything about Newtonian dynamics, chances are\nyou'll immediately recognize that this scheme is total bollocks. But how\ndo you know? You could make an appeal to symmetry, saying \"look, for\nevery slice of the orbital path where you say gravity gives you high\nacceleration, there's a corresponding slice of the orbital path where\ngravity gives you just as high deceleration, so I don't see where the\nnet gains are coming from\". But then, suppose the man presses you. \"Ah,\"\nhe says, \"but in that slice where there is high acceleration your\ninitial velocity is low, and so you spend a lot of time inside of it,\nwhereas in the corresponding slice, your incoming velocity is high, and\nso you have less time to decelerate\". How do you really, conclusively,\nprove him wrong?\n\nOne approach is to dig deeply into the math, calculate the integrals,\nand show that the supposed net gains are in fact exactly equal to zero.\nBut there is also a simple approach: recognize that energy is\npath-independent. That is, when the spaceship moves from point\n\\(A\\) to point \\(B\\), where point \\(B\\) is closer to the earth, its kinetic\nenergy certainly goes up because its speed increases. But because total\nenergy (kinetic plus potential) is\nconserved, and potential energy is only dependent on the spaceship's\nposition, and not how it got there, we know that regardless of\nwhat path from point \\(A\\) to point\n\\(B\\) the spaceship takes, once it gets\nto point \\(B\\) the total change in\nkinetic energy will be exactly the same.\n\nDifferent paths, same change in energy \n\n Furthermore, we know that the kinetic energy gain from going\nfrom point \\(A\\) to point \\(A\\) is also independent of the path\nyou take along the way: in all cases it's exactly zero. \n One concern\nsometimes cited\nagainst on-chain market makers (that is, fully automated on-chain\nmechanisms that act as always-available counterparties for people who\nwish to trade one type of token for another) is that they are invariably\neasy to exploit.\n\nAs an example, let me quote a recent\npost discussing this issue in the context of Bancor:\n\nThe prices that Bancor offers for tokens have nothing to do with the\nactual market equilibrium. Bancor will always trail the market, and in\ndoing so, will bleed its reserves. A simple thought experiment suffices\nto illustrate the problem. Suppose that market panic sets around X.\nUnfounded news about your system overtake social media. Let's suppose\nthat people got convinced that your CEO has absconded to a remote island\nwith no extradition treaty, that your CFO has been embezzling money, and\nyour CTO was buying drugs from the darknet markets and shipping them to\nhis work address to make a Scarface-like mound of white powder on his\ndesk. Worse, let's suppose that you know these allegations to be false.\nThey were spread by a troll army wielded by a company with no products,\nwhose business plan is to block everyone's coin stream. Bancor would\noffer ever decreasing prices for X coins during a bank run, until it has\nno reserves left. You'd watch the market panic take hold and eat away\nyour reserves. Recall that people are convinced that the true value of X\nis 0 in this scenario, and the Bancor formula is guaranteed to offer a\nprice above that. So your entire reserve would be gone.\n\nThe post discusses many issues around the Bancor protocol, including\ndetails such as code quality, and I will not touch on any of those;\ninstead, I will focus purely on the topic of on-chain market maker\nefficiency and exploitability, using Bancor (along with MKR) purely as\nexamples and not seeing to make any judgements on the quality of either\nproject as a whole.\n\nFor many classes of naively designed on-chain market makers, the\ncomment above about exploitability and trailing markets applies\nverbatim, and quite seriously so. However, there are also classes of\non-chain market makers that are definitely not suspect to their entire\nreserve being drained due to some kind of money-pumping attack. To take\na simple example, consider the market maker selling MKR for ETH whose\ninternal state consists of a current price, \\(p\\), and which is willing to buy or sell an\ninfinitesimal amount of MKR at each price level. For example, suppose\nthat \\(p = 5\\), and you wanted to buy 2\nMKR. The market would sell you:\n\n- 0.00...01 MKR at a price of 5 ETH/MKR\n\n- 0.00...01 MKR at a price of 5.00...01 ETH/MKR\n\n- 0.00...01 MKR at a price of 5.00...02 ETH/MKR\n\n- ....\n\n- 0.00...01 MKR at a price of 6.99...98 ETH/MKR\n\n- 0.00...01 MKR at a price of 6.99...99 ETH/MKR\n\nAltogether, it's selling you 2 MKR at an average price of 6 ETH/MKR\n(ie. total cost 12 ETH), and at the end of the operation \\(p\\) has increased to 7. If someone\nthen wanted to sell 1 MKR, they would be spending 6.5 ETH, and\nat the end of that operation \\(p\\) would drop back down to 6.\n\nNow, suppose that I told you that such a market maker started off at\na price of \\(p = 5\\), and after an\nunspecified series of events \\(p\\) is\nnow 4. Two questions:\n\n- How much MKR did the market maker gain or lose?\n\n- How much ETH did the market maker gain or lose?\n\nThe answers are: it gained 1 MKR, and lost 4.5 ETH. Notice that this\nresult is totally independent of the path that \\(p\\) took. Those answers are correct if\n\\(p\\) went from 5 to 4 directly with\none buyer, they're correct if there was first one buyer that took \\(p\\) from 5 to 4.7 and a second buyer that\ntook \\(p\\) the rest of the way to 4,\nand they're even correct if \\(p\\) first\ndropped to 2, then increased to 9.818, then dropped again to 0.53, then\nfinally rose again to 4.\n\nWhy is this the case? The simplest way to see this is to see that if\n\\(p\\) drops below 4 and then comes back\nup to 4, the sells on the way down are exactly counterbalanced by buys\non the way up; each sell has a corresponding buy of the same magnitude\nat the exact same price. But we can also see this by viewing the market\nmaker's core mechanism differently. Define the market maker as having a\nsingle-dimensional internal state \\(p\\), and having MKR and ETH balances\ndefined by the following formulas:\n\n\\(mkr\\_balance(p) = 10 - p\\)\n\n\\(eth\\_balance(p) = p^2/2\\)\n\nAnyone has the power to \"edit\" \\(p\\)\n(though only to values between 0 and 10), but they can only do so by\nsupplying the right amount of MKR or ETH, and getting the right amount\nof MKR and ETH back, so that the balances still match up; that is, so\nthat the amount of MKR and ETH held by the market maker after the\noperation is the amount that they are supposed to hold according to the\nabove formulas, with the new value for \\(p\\) that was set. Any edit to \\(p\\) that does not come with MKR and ETH\ntransactions that make the balances match up automatically fails.\n\nNow, the fact that any series of events that drops \\(p\\) from 5 to 4 also raises the market\nmaker's MKR balance by 1 and drops its ETH balance by 4.5, regardless of\nwhat series of events it was, should look elementary: \\(mkr\\_balance(4) - mkr\\_balance(5) = 1\\) and\n\\(eth\\_balance(4) - eth\\_balance(5) =\n-4.5\\).\n\nWhat this means is that a \"reserve bleeding\" attack on a market maker\nthat preserves this kind of path independence property is impossible.\nEven if some trolls successfully create a market panic that drops prices\nto near-zero, when the panic subsides, and prices return to their\noriginal levels, the market maker's position will be unchanged - even if\nboth the price, and the market maker's balances, made a bunch of crazy\nmoves in the meantime.\n\nNow, this does not mean that market makers cannot lose money,\ncompared to other holding strategies. If, when you start off, 1 MKR = 5\nETH, and then the MKR price moves, and we compare the performance of\nholding 5 MKR and 12.5 ETH in the market maker versus the performance of\njust holding the assets, the result looks like this:\n\nHolding a balanced portfolio always wins, except in the case where\nprices stay exactly the same, in which case the returns of the market\nmaker and the balanced portfolio are equal. Hence, the purpose of a\nmarket maker of this type is to subsidize guaranteed liquidity as a\npublic good for users, serving as trader of last resort, and not to earn\nrevenue. However, we certainly can modify the market maker to earn\nrevenue, and quite simply: we have it charge a spread. That is, the\nmarket maker might charge \\(1.005\\cdot\np\\) for buys and offer only \\(0.995\\cdot p\\) for sells. Now, being the\nbeneficiary of a market maker becomes a bet: if, in the long run, prices\ntend to move in one direction, then the market maker loses, at least\nrelative to what they could have gained if they had a balanced\nportfolio. If, on the other hand, prices tend to bounce around wildly\nbut ultimately come back to the same point, then the market maker can\nearn a nice profit. This sacrifices the \"path independence\" property,\nbut in such a way that any deviations from path independence are always\nin the market maker's favor.\n\nThere are many designs that path-independent market makers could\ntake; if you are willing to create a token that can issue an unlimited\nquantity of units, then the \"constant reserve ratio\" mechanism (where\nfor some constant ratio \\(0 \\leq r \\leq\n1\\), the token supply is \\(p^{1/r -\n1}\\) and the reserve size is \\(r \\cdot\np^{1/r}\\) also counts as one, provided that it is implemented\ncorrectly and path independence is not compromised by bounds and\nrounding errors.\n\nIf you want to make a market maker for existing tokens without a\nprice cap, my favorite (credit to Martin Koppelmann) mechanism is that\nwhich maintains the invariant \\(tokenA\\_balance(p) \\cdot tokenB\\_balance(p) =\nk\\) for some constant \\(k\\). So\nthe formulas would be:\n\n\\(tokenA\\_balance(p) = \\sqrt{k\\cdot\np}\\)\n\n\\(tokenB\\_balance(p) =\n\\sqrt{k/p}\\)\n\nWhere \\(p\\) is the price of \\(tokenB\\) denominated in \\(tokenA\\). In general, you can make a\npath-independent market maker by defining any (monotonic) relation\nbetween \\(tokenA\\_balance\\) and \\(tokenB\\_balance\\) and calculating its\nderivative at any point to give the price.\n\n The above only discusses the role of path independence in\npreventing one particular type of issue: that where an attacker somehow\nmakes a series of transactions in the context of a series of price\nmovements in order to repeatedly drain the market maker of money. With a\npath independent market maker, such \"money pump\" vulnerabilities are\nimpossible. However, there certainly are other kinds of\ninefficiencies that may exist. If the price of MKR drops from 5 ETH to 1\nETH, then the market maker used in the example above will have lost 28\nETH worth of value, whereas a balanced portfolio would only have lost 20\nETH. Where did that 8 ETH go?\n\nIn the best case, the price (that is to say, the \"real\" price, the\nprice level where supply and demand among all users and traders matches\nup) drops quickly, and some lucky trader snaps up the deal, claiming an\n8 ETH profit minus negligible transaction fees. But what if there are\nmultiple traders? Then, if the price between block \\(n\\) and block \\(n+1\\) differs, the fact that traders can\nbid against each other by setting transaction fees creates an all-pay\nauction, with revenues going to the miner. As a consequence of the revenue\nequivalence theorem, we can deduce that we can expect that the\ntransaction fees that traders send into this mechanism will keep going\nup until they are roughly equal to the size of the profit earned (at\nleast initially; the real equilibrium is for miners to just\nsnap up the money themselves). Hence, either way schemes like this are\nultimately a gift to the miners.\n\nOne way to increase social welfare in such a design is to make it\npossible to create purchase transactions that are only worthwhile for\nminers to include if they actually make the purchase. That is, if the\n\"real\" price of MKR falls from 5 to 4.9, and there are 50 traders racing\nto arbitrage the market maker, and only the first one of those 50 will\nmake the trade, then only that one should pay the miner a transaction\nfee. This way, the other 49 failed trades will not clog up the\nblockchain. EIP\n86, slated for Metropolis, opens up a path toward standardizing such\na conditional transaction fee mechanism (another good side effect is\nthat this can also make token sales more unobtrusive, as similar\nall-pay-auction mechanics apply in many token sales).\n\nAdditionally, there are other inefficiencies if the market maker is\nthe only available trading venue for tokens. For example, if\ntwo traders want to exchange a large amount, then they would need to do\nso via a long series of small buy and sell transactions, needlessly\nclogging up the blockchain. To mitigate such efficiencies, an on-chain\nmarket maker should only be one of the trading venues\navailable, and not the only one. However, this is arguably not a large\nconcern for protocol developers; if there ends up being a demand for a\nvenue for facilitating large-scale trades, then someone else will likely\nprovide it.\n\nFurthermore, the arguments here only talk about path independence of\nthe market maker assuming a given starting price and ending\nprice. However, because of various psychological effects, as well as\nmulti-equilibrium effects, the ending price is plausibly a function not\njust of the starting price and recent events that affect the\n\"fundamental\" value of the asset, but also of the pattern of trades that\nhappens in response to those events. If a price-dropping event takes\nplace, and because of poor liquidity the price of the asset drops\nquickly, it may end up recovering to a lower point than if more\nliquidity had been present in the first place. That said, this may\nactually be an argument in favor of subsidied market makers: if such\nmultiplier effects exist, then they will have a positive impact on price\nstability that goes beyond the first-order effect of the liquidity that\nthe market maker itself provides.\n\nThere is likely a lot of research to be done in determining exactly\nwhich path-independent market maker is optimal. There is also the\npossibility of hybrid semi-automated market makers that have the same\nguaranteed-liquidity properties, but which include some element of\nasynchrony, as well as the ability for the operator to \"cut in line\" and\ncollect the profits in cases where large amounts of capital would\notherwise be lost to miners. There is also not yet a coherent theory of\njust how much (if any) on-chain automated guaranteed liquidity is\noptimal for various objectives, and to what extent, and by whom, these\nmarket makers should be subsidized. All in all, the on-chain mechanism\ndesign space is still in its early days, and it's certainly worth much\nmore broadly researching and exploring various options.",
    "contentLength": 15829,
    "summary": "This blog post explains how path independence in physics prevents perpetual motion and similarly protects well-designed on-chain market makers from being drained.",
    "detailedSummary": {
      "theme": "Path independence in automated on-chain market makers prevents money-pump attacks by ensuring that the final state depends only on starting and ending prices, not the trading path taken.",
      "summary": "Vitalik begins with a physics analogy about a perpetual motion machine to illustrate path independence - just as energy conservation makes such schemes impossible regardless of orbital path complexity, path-independent market makers are immune to certain exploits. He addresses criticisms of protocols like Bancor that claim automated market makers will inevitably be drained by attackers who exploit price differences during market volatility. Vitalik demonstrates that properly designed path-independent market makers cannot be 'money-pumped' because their final token balances depend only on the starting and ending prices, not the sequence of trades that occurred between those points. He provides mathematical examples showing how market makers with formulas like constant product (tokenA_balance \u00d7 tokenB_balance = k) maintain this property, making reserve-draining attacks impossible even during extreme price volatility.",
      "takeaways": [
        "Path-independent market makers are immune to money-pump attacks because their final state depends only on starting and ending prices, not the trading path",
        "Market makers with proper mathematical invariants (like constant product formulas) cannot have their reserves completely drained through arbitrage exploitation",
        "While path independence prevents certain attacks, market makers still underperform compared to holding balanced portfolios when prices move directionally",
        "Transaction fee competition among arbitrageurs effectively transfers market maker inefficiencies to miners rather than eliminating them",
        "On-chain market makers should serve as liquidity providers of last resort alongside other trading venues, not as the sole trading mechanism"
      ],
      "controversial": [
        "The claim that path-independent market makers are completely immune to reserve-draining attacks may oversimplify real-world implementation challenges and edge cases",
        "The assertion that market maker inefficiencies ultimately become gifts to miners through transaction fee auctions could be disputed by those who see more complex equilibrium dynamics"
      ]
    }
  },
  {
    "id": "general-2017-06-09-sales",
    "title": "Analyzing Token Sale Models",
    "date": "2017-06-09",
    "category": "applications",
    "url": "https://vitalik.eth.limo/general/2017/06/09/sales.html",
    "path": "general/2017/06/09/sales.html",
    "content": "- \n\n- \n\n  \n    \n\n  \n  \n  Dark Mode Toggle\n  \n    \n\n  \n\n##  Analyzing Token Sale Models \n\n 2017 Jun 09 \nSee all posts\n\n \n \n\n Analyzing Token Sale Models \n\nNote: I mention the names of various projects below only to\ncompare and contrast their token sale mechanisms; this should NOT be\ntaken as an endorsement or criticism of any specific project as a whole.\nIt's entirely possible for any given project to be total trash as a\nwhole and yet still have an awesome token sale model.\n\nThe last few months have seen an increasing amount of innovation in\ntoken sale models. Two years ago, the space was simple: there were\ncapped sales, which sold a fixed number of tokens at a fixed price and\nhence fixed valuation and would often quickly sell out, and there were\nuncapped sales, which sold as many tokens as people were willing to buy.\nNow, we have been seeing a surge of interest, both in terms of\ntheoretical investigation and in many cases real-world implementation,\nof hybrid capped sales, reverse dutch auctions, Vickrey auctions,\nproportional refunds, and many other mechanisms.\n\nMany of these mechanisms have arisen as responses to perceived\nfailures in previous designs. Nearly every significant sale, including\nBrave's Basic Attention Tokens, Gnosis, upcoming sales such as Bancor,\nand older ones such as Maidsafe and even the Ethereum sale itself, has\nbeen met with a substantial amount of criticism, all of which points to\na simple fact: so far, we have still not yet discovered a mechanism that\nhas all, or even most, of the properties that we would like.\n\nLet us review a few examples.\n\n## Maidsafe\n\nThe decentralized internet\nplatform raised $7m in\nfive hours. However, they made the mistake of accepting payment in\ntwo currencies (BTC and MSC), and giving a favorable rate to MSC buyers.\nThis led\nto a temporary ~2x appreciation in the MSC price, as users rushed in\nto buy MSC to participate in the sale at the more favorable rate, but\nthen the price saw a similarly steep drop after the sale ended. Many\nusers converted their BTC to MSC to participate in the sale, but then\nthe sale closed too quickly for them, leading to them being stuck with a\n~30% loss.\n\nThis sale, and several others after it (cough cough WeTrust,\nTokenCard),\nshowed a lesson that should hopefully by now be uncontroversial: running\na sale that accepts multiple currencies at a fixed exchange rate is\ndangerous and bad. Don't do it.\n\n## Ethereum\n\nThe Ethereum sale was uncapped, and ran for 42 days. The sale price\nwas 2000 ETH for 1 BTC for the first 14 days, and then started\nincreasing linearly, finishing at 1337 ETH for 1 BTC.\n\nNearly every uncapped sale is criticized for being \"greedy\" (a\ncriticism I have significant reservations about, but we'll get back to\nthis later), though there is also another more interesting criticism of\nthese sales: they give participants high uncertainty about the\nvaluation that they are buying at. To use a not-yet-started sale as\na example, there are likely many people who would be willing to pay\n$10,000 for a pile of Bancor tokens if they knew for a fact that this\npile represented 1% of all Bancor tokens in existence, but many of them\nwould become quite apprehensive if they were buying a pile of, say, 5000\nBancor tokens, and they had no idea whether the total supply would be\n50000, 500000 or 500 million.\n\nIn the Ethereum sale, buyers who really cared about predictability of\nvaluation generally bought on the 14th day, reasoning that this was the\nlast day of the full discount period and so on this day they had maximum\npredictability together with the full discount, but the pattern above is\nhardly economically optimal behavior; the equilibrium would be something\nlike everyone buying in on the last hour of the 14th day, making a\nprivate tradeoff between certainty of valuation and taking the 1.5% hit\n(or, if certainty was really important, purchases could spill over into\nthe 15th, 16th and later days). Hence, the model certainly has some\nrather weird economic properties that we would really like to avoid if\nthere is a convenient way to do so.\n\n## BAT\n\nThroughout 2016 and early 2017, the capped sale design was most\npopular. Capped sales have the property that it is very likely that\ninterest is oversubscribed, and so there is a large incentive to getting\nin first. Initially, sales took a few hours to finish. However, soon the\nspeed began to accelerate. First Blood made a lot of news by finishing\ntheir $5.5m sale in two\nminutes - while active\ndenial-of-service attacks on the Ethereum blockchain were taking\nplace. However, the apotheosis of this race-to-the-Nash-equilibrium\ndid not come until the BAT sale last month, when a $35m\nsale was completed within 30 seconds due to the large amount of\ninterest in the project.\n\nNot only did the sale finish within two blocks, but also:\n\n- The total transaction fees paid were 70.15\nETH (>$15,000), with the highest single fee being ~$6,600\n\n- 185 purchases were successful, and over 10,000 failed\n\n- The Ethereum blockchain's capacity was full for 3 hours after the\nsale started\n\nThus, we are starting to see capped sales approach their natural\nequilibrium: people trying to outbid each other's transaction fees, to\nthe point where potentially millions of dollars of surplus would be\nburned into the hands of miners. And that's before the next stage\nstarts: large mining pools butting into the start of the line and just\nbuying up all of the tokens themselves before anyone else can.\n\n## Gnosis\n\nThe Gnosis sale attempted to alleviate these issues with a novel\nmechanism: the reverse dutch auction. The terms, in simplified form, are\nas follows. There was a capped sale, with a cap of $12.5 million USD.\nHowever, the portion of tokens that would actually be given to\npurchasers depended on how long the sale took to finish. If it finished\non the first day, then only ~5% of tokens would be distributed among\npurchasers, and the rest held by the Gnosis team; if it finished on the\nsecond day, it would be ~10%, and so forth.\n\nThe purpose of this is to create a scheme where, if you buy at time\n\\(T\\), then you are guaranteed to buy\nin at a valuation which is at most \\(\\frac{1}{T}\\).\n\nThe goal is to create a mechanism where the optimal strategy is\nsimple. First, you personally decide what is the highest valuation you\nwould be willing to buy at (call it V). Then, when the sale starts, you\ndon't buy in immediately; rather, you wait until the valuation drops to\nbelow that level, and then send your transaction.\n\nThere are two possible outcomes:\n\n- The sale closes before the valuation drops to below V. Then, you are\nhappy because you stayed out of what you thought is a bad deal.\n\n- The sale closes after the valuation drops to below V. Then, you sent\nyour transaction, and you are happy because you got into what you\nthought is a good deal.\n\nHowever, many people predicted that because of \"fear of missing out\"\n(FOMO), many people would just \"irrationally\" buy in at the first day,\nwithout even looking at the valuation. And this is exactly what\nhappened: the sale finished in a few hours, with the result that the\nsale reached its cap of $12.5 million when it was only selling about 5%\nof all tokens that would be in existence - an implied valuation of over\n$300 million.\n\nAll of this would of course be an excellent piece of confirming\nevidence for the narrative that markets are totally irrational, people\ndon't think clearly before throwing in large quantities of money (and\noften, as a subtext, that the entire space needs to be somehow\nsuppressed to prevent further exuberance) if it weren't for one\ninconvenient fact: the traders who bought into the sale were\nright.\n\nEven in ETH terms, despite the massive ETH price rise, the price of 1\nGNO has increased from ~0.6 ETH to ~0.8 ETH.\n\nWhat happened? A couple of weeks before the sale started, facing\npublic criticism that if they end up holding the majority of the coins\nthey would act like a central bank with the ability to heavily\nmanipulate GNO prices, the Gnosis team agreed to hold 90% of the coins\nthat were not sold for a year. From a trader's point of view, coins that\nare locked up for a long time are coins that cannot affect the market,\nand so in a short term analysis, might as well not exist. This is what\ninitially propped up Steem to such a high valuation last year in\nJuly, as well as Zcash in the very early moments when the price of\neach coin was\nover $1,000.\n\nNow, one year is not that long a time, and locking up coins\nfor a year is nowhere close to the same thing as locking them up\nforever. However, the reasoning goes further. Even after the one year\nholding period expires, you can argue that it is in the Gnosis team's\ninterest to only release the locked coins if they believe that doing so\nwill make the price go up, and so if you trust the Gnosis team's\njudgement this means that they are going to do something which is at\nleast as good for the GNO price as simply locking up the coins\nforever. Hence, in reality, the GNO sale was really much more like\na capped sale with a cap of $12.5 million and a valuation of $37.5\nmillion. And the traders who participated in the sale reacted exactly as\nthey should have, leaving scores of internet commentators wondering what\njust happened.\n\nThere is certainly a weird bubbliness about crypto-assets, with various no-name assets attaining\nmarket caps of $1-100 million (including BitBean as of\nthe time of this writing at $12m, PotCoin at $22m,\nPepeCash at\n$13m and SmileyCoin at\n$14.7m) just because. However, there's a strong case to be made that the\nparticipants at the sale stage are in many cases doing nothing\nwrong, at least for themselves; rather, traders who buy in sales are\nsimply (correctly) predicting the existence of an ongoing bubble has\nbeen brewing since the start of 2015 (and arguably, since the start of\n2010).\n\nMore importantly though, bubble behavior aside, there is another\nlegitimate criticism of the Gnosis sale: despite their 1-year no-selling\npromise, eventually they will have access to the entirety of their\ncoins, and they will to a limited extent be able to act\nlike a central bank with the ability to heavily manipulate GNO prices,\nand traders will have to deal with all of the monetary policy\nuncertainty that that entails.\n\n## Specifying the problem\n\nSo what would a good token sale mechanism look like?\nOne way that we can start off is by looking through the criticisms of\nexisting sale models that we have seen and coming up with a list of\ndesired properties.\n\nLet's do that. Some natural properties include:\n\n- Certainty of valuation - if you participate in a\nsale, you should have certainty over at least a ceiling on the valuation\n(or, in other words, a floor on the percentage of all tokens you are\ngetting).\n\n- Certainty of participation - if you try to\nparticipate in a sale, you should be able to generally count on\nsucceeding.\n\n- Capping the amount raised - to avoid being\nperceived as greedy (or possibly to mitigate risk of regulatory\nattention), the sale should have a limit on the amount of money it is\ncollecting.\n\n- No central banking - the token sale issuer should\nnot be able to end up with an unexpectedly very large percentage of the\ntokens that would give them control over the market.\n\n- Efficiency - the sale should not lead to\nsubstantial economic inefficiencies or deadweight losses.\n\nSounds reasonable?\n\nWell, here's the not-so-fun part.\n\n- (1) and (2) cannot be fully satisfied simultaneously.\n\n- At least without resorting to very clever tricks, (3), (4) and (5)\ncannot be satisfied simultaneously.\n\nThese can be cited as \"the first token sale dilemma\" and \"the second\ntoken sale trilemma\".\n\nThe proof for the first dilemma is easy: suppose you have a sale\nwhere you provide users with certainty of a $100 million valuation. Now,\nsuppose that users try to throw $101 million into the sale. At least\nsome will fail. The proof for the second trilemma is a simple\nsupply-and-demand argument. If you satisfy (4), then you are selling\nall, or some fixed large percentage, of the tokens, and so the valuation\nyou are selling at is proportional to the price you are selling at. If\nyou satisfy (3), then you are putting a cap on the price. However, this\nimplies the possibility that the equilibrium price at the quantity you\nare selling exceeds the price cap that you set, and so you get a\nshortage, which inevitably leads to either (i) the digital equivalent of\nstanding in line for 4 hours at a very popular restaurant, or (ii) the\ndigital equivalent of ticket scalping - both large deadwight losses,\ncontradicting (5).\n\nThe first dilemma cannot be overcome; some valuation uncertainty or\nparticipation uncertainty is inescapable, though when the choice exists\nit seems better to try to choose participation uncertainty rather than\nvaluation uncertainty. The closest that we can come is compromising on\nfull participation to guarantee partial participation.\nThis can be done with a proportional refund (eg. if $101 million buy in\nat a $100 million valuation, then everyone gets a 1% refund). We can\nalso think of this mechanism as being an uncapped sale where part of the\npayment comes in the form of locking up capital rather than\nspending it; from this viewpoint, however, it becomes clear that the\nrequirement to lock up capital is an efficiency loss, and so such a\nmechanism fails to satisfy (5). If ether holdings are not\nwell-distributed then it arguably harms fairness by favoring wealthy\nstakeholders.\n\nThe second dilemma is difficult to overcome, and many attempts to\novercome it can easily fail or backfire. For example, the Bancor sale is\nconsidering limiting the transaction gas price for purchases to 50\nshannon (~12x the normal gasprice). However, this now means that the\noptimal strategy for a buyer is to set up a large number of accounts,\nand from each of those accounts send a transaction that triggers a\ncontract, which then attempts to buy in (the indirection is there to\nmake it impossible for the buyer to accidentally buy in more than they\nwanted, and to reduce capital requirements). The more accounts a buyer\nsets up, the more likely they are to get in. Hence, in equilibrium, this\ncould lead to even more clogging of the Ethereum blockchain\nthan a BAT-style sale, where at least the $6600 fees were spent on a\nsingle transaction and not an entire denial-of-service attack on the\nnetwork. Furthermore, any kind of on-chain transaction spam contest\nseverely harms fairness, because the cost of participating in the\ncontest is constant, whereas the reward is proportional to how much\nmoney you have, and so the result disproportionately favors wealthy\nstakeholders.\n\n## Moving forward\n\nThere are three more clever things that you can do. First, you can do\na reverse dutch auction just like Gnosis, but with one change: instead\nof holding the unsold tokens, put them toward some kind of public good.\nSimple examples include: (i) airdrop (ie. redistributing to all ETH\nholders), (ii) donating to the Ethereum Foundation, (iii)\ndonating to Parity, Brainbot, Smartpool or other companies and\nindividuals independently building infrastructure for the Ethereum\nspace, or (iv) some combination of all three, possibly with the ratios\nsomehow being voted on by the token buyers.\n\nSecond, you can keep the unsold tokens, but solve the \"central\nbanking\" problem by committing to a fully automated plan for how they\nwould be spent. The reasoning here is similar to that for why many\neconomists are interested in rules-based\nmonetary policy: even if a centralized entity has a large amount of\ncontrol over a powerful resource, much of the political uncertainty that\nresults can be mitigated if the entity credibly commits to following a\nset of programmatic rules for how they apply it. For example, the unsold\ntokens can be put into a market maker that is tasked with preserving the\ntokens' price stability.\n\nThird, you can do a capped sale, where you limit the amount that can\nbe bought by each person. Doing this effectively requires a KYC process,\nbut the nice thing is a KYC entity can do this once, whitelisting users'\naddresses after they verify that the address represents a unique\nindividual, and this can then be reused for every token sale, alongside\nother applications that can benefit from per-person sybil resistance\nlike Akasha's quadratic voting.\nThere is still deadweight loss (ie. inefficiency) here, because this\nwill lead to individuals with no personal interest in tokens\nparticipating in sales because they know they will be able to quickly\nflip them on the market for a profit. However, this is arguably not that\nbad: it creates a kind of crypto universal basic\nincome, and if behavioral economics assumptions like the endowment\neffect are even slightly true it will also succeed at the goal of\nensuring widely distributed ownership.\nAre single round sales even\ngood?\n\nLet us get back to the topic of \"greed\". I would claim that not many\npeople are, in principle, opposed to the idea of development teams that\nare capable of spending $500 million to create a really great project\ngetting $500 million. Rather, what people are opposed to is (i) the idea\nof completely new and untested development teams getting $50 million all\nat once, and (ii) even more importantly, the time mismatch between\ndevelopers' rewards and token buyers' interests. In a single-round\nsale, the developers have only one chance to get money to build the\nproject, and that is near the start of the development process. There is\nno feedback mechanism where teams are first given a small amount of\nmoney to prove themselves, and then given access to more and more\ncapital over time as they prove themselves to be reliable and\nsuccessful. During the sale, there is comparatively little information\nto filter between good development teams and bad ones, and once the sale\nis completed, the incentive to developers to keep working is relatively\nlow compared to traditional companies. The \"greed\" isn't about getting\nlots of money, it's about getting lots of money without working hard to\nshow you're capable of spending it wisely.\n\nIf we want to strike at the heart of this problem, how would we solve\nit? I would say the answer is simple: start moving to mechanisms other\nthan single round sales.\n\nI can offer several examples as inspiration:\n\n- Angelshares\n- this project ran a sale in 2014 where it sold off a fixed percentage\nof all AGS every day for a period of several months. During each day,\npeople could contribute an unlimited amount to the crowdsale, and the\nAGS allocation for that day would be split among all contributors.\nBasically, this is like having a hundred \"micro-rounds\" of uncapped\nsales over the course of most of a year; I would claim that the duration\nof the sales could be stretched even further.\n\n- Mysterium,\nwhich held a little-noticed micro-sale\nsix months before the big one.\n\n- Bancor, which\nrecently\nagreed to put all funds raised over a cap into a market maker which\nwill maintain price stability along with maintaining a price floor of\n0.01 ETH. These funds cannot be removed from the market maker for two\nyears.\n\nIt seems hard to see the relationship between Bancor's strategy and\nsolving time mismatch incentives, but an element of a solution is there.\nTo see why, consider two scenarios. As a first case, suppose the sale\nraises $30 million, the cap is $10 million, but then after one year\neveryone agrees that the project is a flop. In this case, the price\nwould try to drop below 0.01 ETH, and the market maker would lose all of\nits money trying to maintain the price floor, and so the team would only\nhave $10 million to work with. As a second case, suppose the sale raises\n$30 million, the cap is $10 million, and after two years everyone is\nhappy with the project. In this case, the market maker will not have\nbeen triggered, and the team would have access to the entire $30\nmillion.\n\nA related proposal is Vlad Zamfir's \"safe\ntoken sale mechanism\". The concept is a very broad one that could be\nparametrized in many ways, but one way to parametrize it is to sell\ncoins at a price ceiling and then have a price floor slightly below that\nceiling, and then allow the two to diverge over time, freeing up capital\nfor development over time if the price maintains itself.\n\nArguably, none of the above three are sufficient; we want sales that\nare spread out over an even longer period of time, giving us much more\ntime to see which development teams are the most worthwhile before\ngiving them the bulk of their capital. But nevertheless, this seems like\nthe most productive direction to explore in.\n\n## Coming out of the Dilemmas\n\nFrom the above, it should hopefully be clear that while there is no\nway to counteract the dilemma and trilemma head on, there are ways to\nchip away at the edges by thinking outside the box and compromising on\nvariables that are not apparent from a simplistic view of the problem.\nWe can compromise on guarantee of participation slightly, mitigating the\nimpact by using time as a third dimension: if you don't get in during\nround \\(N\\), you can just wait until\nround \\(N+1\\) which will be in a week\nand where the price probably will not be that different.\n\nWe can have a sale which is uncapped as a whole, but which consists\nof a variable number of periods, where the sale within each period is\ncapped; this way teams would not be asking for very large amounts of\nmoney without proving their ability to handle smaller rounds first. We\ncan sell small portions of the token supply at a time, removing the\npolitical uncertainty that this entails by putting the remaining supply\ninto a contract that continues to sell it automatically according to a\nprespecified formula.\n\nHere are a few possible mechanisms that follow some of the spirit of\nthe above ideas:\n\n- Host a Gnosis-style reverse dutch auction with a low cap (say, $1\nmillion). If the auction sells less than 100% of the token supply,\nautomatically put the remaining funds into another auction two months\nlater with a 30% higher cap. Repeat until the entire token supply is\nsold.\n\n- Sell an unlimited number of tokens at a price of \\(\\$X\\) and put 90% of the proceeds into a\nsmart contract that guarantees a price floor of \\(\\$0.9 \\cdot X\\). Have the price ceiling go\nup hyperbolically toward infinity, and the price floor go down linearly\ntoward zero, over a five-year period.\n\n- Do the exact same thing AngelShares did, though stretch it out over\n5 years instead of a few months.\n\n- Host a Gnosis-style reverse dutch auction. If the auction sells less\nthan 100% of the token supply, put the remaining funds into an automated\nmarket maker that attempts to ensure the token's price stability (note\nthat if the price continues going up anyway, then the market maker would\nbe selling tokens, and some of these earnings could be given to the\ndevelopment team).\n\n- Immediately put all tokens into a market maker with\nparameters+variables \\(X\\) (minimum\nprice), \\(s\\) (fraction of all tokens\nalready sold), \\(t\\) (time since sale\nstarted), \\(T\\) (intended duration of\nsale, say 5 years), that sells tokens at a price of \\(\\dfrac{k}{(\\frac{t}{T - s})}\\) (this one is\nweird and may need to be economically studied more).\n\nNote that there are other mechanisms that should be tried to solve\nother problems with token sales; for example, revenues going into a\nmultisig of curators, which only hand out funds if milestones are being\nmet, is one very interesting idea that should be done more. However, the\ndesign space is highly multidimensional, and there are a lot more things\nthat could be tried.",
    "contentLength": 23585,
    "summary": "The blog analyzes various crypto token sale models (capped, uncapped, Dutch auctions) showing each has flaws like rush behavior, valuation uncertainty, or high transaction fees, with no ideal mechanism yet discovered.",
    "detailedSummary": {
      "theme": "Vitalik analyzes the inherent trade-offs in token sale mechanisms and proposes multi-round, time-distributed alternatives to overcome fundamental dilemmas in current models.",
      "summary": "Vitalik examines various token sale models from 2014-2017, identifying critical flaws in each approach. He analyzes failures like Maidsafe's multi-currency acceptance, BAT's 30-second sell-out causing network congestion, and Ethereum's valuation uncertainty issues. Vitalik identifies two fundamental constraints: the 'first token sale dilemma' where certainty of valuation and participation cannot both be guaranteed, and the 'second token sale trilemma' where capping funds raised, avoiding central banking control, and maintaining efficiency cannot all be achieved simultaneously. He argues the core problem isn't teams raising large amounts, but rather the mismatch between getting money upfront versus proving capability over time. Vitalik proposes moving beyond single-round sales to multi-round mechanisms distributed over years, citing examples like Angelshares' daily auctions and Bancor's market maker approach. He suggests several novel mechanisms including reverse Dutch auctions with automatic follow-up rounds, automated market makers with dynamic pricing, and time-distributed sales that provide capital gradually as teams prove themselves.",
      "takeaways": [
        "Token sales face two fundamental impossibility theorems: you cannot guarantee both valuation certainty and participation certainty, and you cannot simultaneously cap fundraising, avoid central banking, and maintain efficiency",
        "Current models like capped sales lead to gas fee wars and network congestion, while uncapped sales create valuation uncertainty for participants",
        "The real issue with 'greedy' token sales isn't the amount raised, but the timing mismatch between developers getting money upfront and proving their capability over time",
        "Multi-round, time-distributed sales could solve incentive alignment by releasing capital gradually as teams demonstrate progress and competence",
        "Creative solutions like automated market makers, reverse Dutch auctions with follow-up rounds, and predetermined algorithmic token distribution can help navigate the inherent trade-offs in token sale design"
      ],
      "controversial": [
        "Vitalik's assertion that BAT sale participants who paid extremely high gas fees and caused network congestion were acting rationally",
        "The argument that 'greed' criticism of large token raises is misplaced and the real issue is timing rather than amount",
        "The suggestion that regulatory attention might be avoided by capping sale amounts, which could be seen as acknowledging potential regulatory concerns"
      ]
    }
  },
  {
    "id": "general-2017-05-08-coordination_problems",
    "title": "Engineering Security Through Coordination Problems",
    "date": "2017-05-08",
    "category": "governance",
    "url": "https://vitalik.eth.limo/general/2017/05/08/coordination_problems.html",
    "path": "general/2017/05/08/coordination_problems.html",
    "content": "- \n\n- \n\n  \n    \n\n  \n  \n  Dark Mode Toggle\n  \n    \n\n  \n\n##  Engineering Security Through Coordination Problems \n\n 2017 May 08 \nSee all posts\n\n \n \n\n Engineering Security Through Coordination Problems \n\nRecently, there was a small spat between the Core and Unlimited\nfactions of the Bitcoin community, a spat which represents perhaps the\nfiftieth time the same theme was debated, but which is nonetheless\ninteresting because of how it highlights a very subtle philosophical\npoint about how blockchains work.\n\nViaBTC, a mining pool that favors Unlimited, tweeted \"hashpower is\nlaw\", a usual talking point for the Unlimited side, which believes that\nminers have, and should have, a very large role in the governance of\nBitcoin, the usual argument for this being that miners are the one\ncategory of users that has a large and illiquid financial incentive in\nBitcoin's success. Greg Maxwell (from the Core side) replied\nthat \"Bitcoin's security works precisely because hash power is NOT\nlaw\".\nThe Core argument is that miners only have a limited role in the Bitcoin\nsystem, to secure the ordering of transactions, and they should NOT have\nthe power to determine anything else, including block size limits and\nother block validity rules. These constraints are enforced by full nodes\nrun by users - if miners start producing blocks according to a set of\nrules different than the rules that users' nodes enforce, then the\nusers' nodes will simply reject the blocks, regardless of whether 10% or\n60% or 99% of the hashpower is behind them. To this, Unlimited often\nreplies with something like \"if 90% of the hashpower is behind a new\nchain that increases the block limit, and the old chain with 10%\nhashpower is now ten times slower for five months until difficulty\nreadjusts, would you really not update your client to accept\nthe new chain?\" \n\n Many people often argue\nagainst\nthe use of public blockchains for applications that involve real-world\nassets or anything with counterparty risk. The critiques are either\ntotal, saying that there is no point in implementing such use cases on\npublic blockchains, or partial, saying that while there may be\nadvantages to storing the data on a public chain, the\nbusiness logic should be executed off chain.\nThe argument usually used is that in such applications, points of trust\nexist already - there is someone who owns the physical assets that back\nthe on-chain permissioned assets, and that someone could always choose\nto run away with the assets or be compelled to freeze them by a\ngovernment or bank, and so managing the digital representations of these\nassets on a blockchain is like paying for a reinforced steel door for\none's house when the window is open. Instead, such systems should use\nprivate chains, or even traditional server-based solutions, perhaps\nadding bits and pieces of cryptography to improve auditability, and\nthereby save on the inefficiencies and costs of putting everything on a\nblockchain. \n\n The arguments above are both flawed in their pure forms, and\nthey are flawed in a similar way. While it is theoretically\npossible for miners to switch 99% of their hashpower to a chain\nwith new rules (to make an example where this is uncontroversially bad,\nsuppose that they are increasing the block reward), and even spawn-camp\nthe old chain to make it permanently useless, and it is also\ntheoretically possible for a centralized manager of an asset-backed\ncurrency to cease honoring one digital token, make a new digital token\nwith the same balances as the old token except with one particular\naccount's balance reduced to zero, and start honoring the new token, in\npractice those things are both quite hard to do.\n\nIn the first case, users will have to realize that something is wrong\nwith the existing chain, agree that they should go to the new chain that\nthe miners are now mining on, and download the software that accepts the\nnew rules. In the second case, all clients and applications that depend\non the original digital token will break, users will need to update\ntheir clients to switch to the new digital token, and smart contracts\nwith no capacity to look to the outside world and see that they need to\nupdate will break entirely. In the middle of all this, opponents of the\nswitch can create a fear-uncertainty-and-doubt campaign to try to\nconvince people that maybe they shouldn't update their clients after\nall, or update their client to some third set of rules (eg.\nchanging proof of work), and this makes implementing the switch even\nmore difficult.\n\nHence, we can say that in both cases, even though there theoretically\nare centralized or quasi-centralized parties that could force a\ntransition from state A to state B, where state B is disagreeable to\nusers but preferable to the centralized parties, doing so requires\nbreaking through a hard coordination problem.\nCoordination problems are everywhere in society and are often a bad\nthing - while it would be better for most people if the English language\ngot rid of its highly complex and irregular spelling system and made a\nphonetic one, or if the United States switched to metric, or if we could\nimmediately drop all\nprices and wages by ten percent in the event of a recession, in\npractice this requires everyone to agree on the switch at the same time,\nand this is often very very hard.\n\nWith blockchain applications, however, we are doing something\ndifferent: we are using coordination problems to our\nadvantage, using the friction that coordination problems create\nas a bulwark against malfeasance by centralized actors. We can build\nsystems that have property X, and we can guarantee that they will\npreserve property X to a high degree because changing the rules from X\nto not-X would require a whole bunch of people to agree to update their\nsoftware at the same time. Even if there is an actor that could force\nthe change, doing so would be hard. This is the kind of security that\nyou gain from client-side validation of blockchain consensus rules.\nNote that this kind of security relies on the decentralization of users\nspecifically. Even if there is only one miner in the world, there is\nstill a difference between a cryptocurrency mined by that miner and a\nPayPal-like centralized system. In the latter case, the operator can\nchoose to arbitrarily change the rules, freeze people's money, offer bad\nservice, jack up their fees or do a whole host of other things, and the\ncoordination problems are in the operator's favor, as such systems have\nsubstantial network effects and so very many users would have to agree\nat the same time to switch to a better system. In the former case,\nclient-side validation means that many attempts at mischief that the\nminer might want to engage in are by default rejected, and the\ncoordination problem now works in the users' favor. \n\n Note that the arguments above do NOT, by themselves,\nimply that it is a bad idea for miners to be the principal actors\ncoordinating and deciding the block size (or in Ethereum's case, the gas\nlimit). It may well be the case that, in the specific case of the\nblock size/gas limit, \"government by coordinated miners with\naligned incentives\" is the optimal approach for deciding this one\nparticular policy parameter, perhaps because the risk of miners abusing\ntheir power is lower than the risk that any specific chosen hard limit\nwill prove wildly inappropriate for market conditions a decade after the\nlimit is set. However, there is nothing unreasonable about saying that\ngovernment-by-miners is the best way to decide one policy parameter, and\nat the same saying that for other parameters (eg. block reward)\nwe want to rely on client-side validation to ensure that miners are\nconstrained. This is the essence of engineering decentralized\ninstutitions: it is about strategically using coordination problems to\nensure that systems continue to satisfy certain desired properties.\nThe arguments above also do not imply that it is always optimal to try\nto put everything onto a blockchain even for services that are\ntrust-requiring. There generally are at least some gains to be made by\nrunning more business logic on a blockchain, but they are often much\nsmaller than the losses to efficiency or privacy. And this ok; the\nblockchain is not the best tool for every task. What the arguments above\ndo imply, though, is that if you are building a\nblockchain-based application that contains many centralized components\nout of necessity, then you can make substantial further gains in\ntrust-minimization by giving users a way to access your application\nthrough a regular blockchain client (eg. in the case of Ethereum, this\nmight be Mist, Parity, Metamask or Status), instead of getting them to\nuse a web interface that you personally control. \n\n Theoretically, the benefits of user-side validation are\noptimized if literally every user runs an independent \"ideal full node\"\n- a node that accepts all blocks that follow the protocol rules that\neveryone agreed to when creating the system, and rejects all blocks that\ndo not. In practice, however, this involves asking every user to process\nevery transaction run by everyone in the network, which is clearly\nuntenable, especially keeping in mind the rapid growth of smartphone\nusers in the developing world.\n\nThere are two ways out here. The first is that we can realize that\nwhile it is optimal from the point of view of the above\narguments that everyone runs a full node, it is certainly not\nrequired. Arguably, any major blockchain running at full\ncapacity will have already reached the point where it will not make\nsense for \"the common people\" to expend a fifth of their hard drive\nspace to run a full node, and so the remaining users are hobbyists and\nbusinesses. As long as there is a fairly large number of them, and they\ncome from diverse backgrounds, the coordination problem of getting these\nusers to collude will still be very hard.\n\nSecond, we can rely on strong light client\ntechnology.\n\nThere are two levels of \"light clients\" that are generally possible\nin blockchain systems. The first, weaker, kind of light client simply\nconvinces the user, with some degree of economic assurance, that they\nare on the chain that is supported by the majority of the network. This\ncan be done much more cheaply than verifying the entire chain, as all\nclients need to do is in proof of work schemes verify nonces or in proof\nstake schemes verify signed certificates that state \"either the root\nhash of the state is what I say it is, or you can publish this\ncertificate into the main chain to delete a large amount of my money\".\nOnce the light client verifies a root hash, they can use Merkle trees to\nverify any specific piece of data that they might want to verify.\n\nLook, it's a Merkle tree!\n\nThe second level is a \"nearly fully verifying\" light client. This\nkind of client doesn't just try to follow the chain that the majority\nfollows; rather, it also tries to follow only chains that follow all the\nrules. This is done by a combination of strategies; the simplest to\nexplain is that a light client can work together with specialized nodes\n(credit to Gavin Wood for coming up with the name \"fishermen\") whose\npurpose is to look for blocks that are invalid and generate \"fraud\nproofs\", short messages that essentially say \"Look! This block has a\nflaw over here!\". Light clients can then verify that specific part of a\nblock and check if it's actually invalid.\n\nIf a block is found to be invalid, it is discarded; if a light client\ndoes not hear any fraud proofs for a given block for a few minutes, then\nit assumes that the block is probably legitimate. There's a bit\nmore complexity involved in handling the case where the problem is\nnot data that is bad, but rather data that is missing,\nbut in general it is possible to get quite close to catching all\npossible ways that miners or validators can violate the rules of the\nprotocol.\n\nNote that in order for a light client to be able to efficiently\nvalidate a set of application rules, those rules must be executed inside\nof consensus - that is, they must be either part of the protocol or part\nof a mechanism executing inside the protocol (like a smart contract).\nThis is a key argument in favor of using the blockchain for both data\nstorage and business logic execution, as opposed to just data\nstorage.\n\nThese light client techniques are imperfect, in that they do rely on\nassumptions about network connectivity and the number of other light\nclients and fishermen that are in the network. But it is actually not\ncrucial for them to work 100% of the time for 100% of validators.\nRather, all that we want is to create a situation where any attempt by a\nhostile cartel of miners/validators to push invalid blocks without user\nconsent will cause a large amount of headaches for lots of people and\nultimately require everyone to update their software if they want to\ncontinue to synchronize with the invalid chain. As long as this is\nsatisfied, we have achieved the goal of security through coordination\nfrictions.",
    "contentLength": 13009,
    "summary": "Blockchains use coordination problems as security features\u2014making rule changes require mass user agreement protects against centralized attacks.",
    "detailedSummary": {
      "theme": "Blockchain security can be engineered by leveraging coordination problems as a defense mechanism against centralized actors attempting to make unwanted changes to the system.",
      "summary": "Vitalik explores how coordination problems, typically seen as obstacles in society, can be strategically used as security features in blockchain systems. He uses the Bitcoin block size debate between Core and Unlimited factions to illustrate how even powerful actors like miners face significant friction when trying to force unwanted changes, because such changes require widespread user coordination to update software and accept new rules. Vitalik extends this principle beyond mining governance to asset-backed tokens and other blockchain applications, arguing that client-side validation creates coordination problems that work in users' favor rather than centralized operators' favor. He acknowledges that while running full nodes would theoretically maximize this security model, practical scalability concerns mean that light clients with fraud proofs can achieve similar coordination friction benefits. The key insight is that blockchain security doesn't just come from cryptographic or economic mechanisms, but from the deliberate engineering of social coordination challenges that make malicious changes difficult to implement.",
      "takeaways": [
        "Coordination problems can be engineered as security features rather than just obstacles to overcome",
        "Client-side validation creates coordination friction that protects users from centralized actors making unwanted changes",
        "Even systems with some centralized components can gain trust-minimization benefits through blockchain-based user interfaces",
        "Light clients with fraud proof mechanisms can provide similar coordination security benefits as full nodes while remaining scalable",
        "The decentralization of users specifically, not just miners or validators, is crucial for this coordination-based security model"
      ],
      "controversial": [
        "The suggestion that miner governance might be optimal for certain parameters like block size limits while inappropriate for others like block rewards",
        "The argument that blockchain applications with centralized components can still provide meaningful decentralization benefits through user-side validation"
      ]
    }
  },
  {
    "id": "general-2017-03-14-forks_and_markets",
    "title": "Hard Forks, Soft Forks, Defaults and Coercion",
    "date": "2017-03-14",
    "category": "protocol",
    "url": "https://vitalik.eth.limo/general/2017/03/14/forks_and_markets.html",
    "path": "general/2017/03/14/forks_and_markets.html",
    "content": "- \n\n- \n\n  \n    \n\n  \n  \n  Dark Mode Toggle\n  \n    \n\n  \n\n##  Hard Forks, Soft Forks, Defaults and Coercion \n\n 2017 Mar 14 \nSee all posts\n\n \n \n\n Hard Forks, Soft Forks, Defaults and Coercion \n\nOne of the important arguments in the blockchain space is that of\nwhether hard forks or soft forks are the preferred protocol upgrade\nmechanism. The basic difference between the two is that soft forks\nchange the rules of a protocol by strictly reducing the set of\ntransactions that is valid, so nodes following the old rules will still\nget on the new chain (provided that the majority of miners/validators\nimplements the fork), whereas hard forks allow previously invalid\ntransactions and blocks to become valid, so clients must upgrade their\nclients in order to stay on the hard-forked chain. There are also two\nsub-types of hard forks: strictly expanding hard forks, which\nstrictly expand the set of transactions that is valid, and so\neffectively the old rules are a soft fork with respect to the new rules,\nand bilateral hard forks, where the two rulesets are\nincompatible both ways.\n\nHere is a Venn diagram to illustrate the fork types:\n\nThe benefits commonly cited for the two are as follows.\n\n- Hard forks allow the developers much more flexibility in making the\nprotocol upgrade, as they do not have to take care to make sure that the\nnew rules \"fit into\" the old rules\n\n- Soft forks are more convenient for users, as users do not need to\nupgrade to stay on the chain\n\n- Soft forks are less likely to lead to a chain split\n\n- Soft forks only really require consent from miners/validators (as\neven if users still use the old rules, if the nodes making the chain use\nthe new rules then only things valid under the new rules will get into\nthe chain in any case); hard forks require opt-in consent from\nusers\n\nAside from this, one major criticism often given for hard forks is\nthat hard forks are \"coercive\". The kind of coercion implied here is not\nphysical force; rather, it's coercion through network effect.\nThat is, if the network changes rules from A to B, then even if you\npersonally like A, if most other users like B and switch to B then you\nhave to switch to B despite your personal disapproval of the change in\norder to be on the same network as everyone else.\n\nProponents of hard forks are often derided as trying to effect a\n\"hostile take over\" of a network, and \"force\" users to go along with\nthem. Additionally, the risk of chain splits is often used to bill hard\nforks as \"unsafe\". \n\nIt is my personal viewpoint that these criticisms are wrong, and\nfurthermore in many cases completely backwards. This viewpoint is not\nspecific to Ethereum, or Bitcoin, or any other blockchain; it arises out\nof general properties of these systems, and is applicable to any of\nthem. Furthermore, the arguments below only apply to controversial\nchanges, where a large portion of at least one constituency\n(miners/validators and users) disapprove of them; if a change is\nnon-contentious, then it can generally be done safely no matter what the\nformat of the fork is.\n\nFirst of all, let us discuss the question of coercion. Hard forks and\nsoft forks both change the protocol in ways that some users may not\nlike; any protocol change will do this if it has less than\nexactly 100% support. Furthermore, it is almost inevitable that at least\nsome of the dissenters, in any scenario, value the network\neffect of sticking with the larger group more than they value their own\npreferences regarding the protocol rules. Hence, both fork types are\ncoercive, in the network-effect sense of the word.\n\nHowever, there is an essential difference between hard forks and soft\nforks: hard forks are opt-in, whereas soft forks allow users no\n\"opting\" at all. In order for a user to join a hard forked chain,\nthey must personally install the software package that implements the\nfork rules, and the set of users that disagrees with a rule change even\nmore strongly than they value network effects can theoretically simply\nstay on the old chain - and, practically speaking, such an event has already happened.\n\nThis is true in the case of both strictly expanding hard forks and\nbilateral hard forks. In the case of soft forks, however, if the\nfork succeeds the unforked chain does not exist. Hence,\nsoft forks clearly institutionally favor coercion over\nsecession, whereas hard forks have the opposite bias. My own\nmoral views lead me to favor secession over coercion, though others may\ndiffer (the most common argument raised is that network effects are\nreally really important and it is essential that \"one\ncoin rule them all\", though more moderate versions of this also\nexist).\n\nIf I had to guess why, despite these arguments, soft forks are often\nbilled as \"less coercive\" than hard forks, I would say that it is\nbecause it feels like a hard fork \"forces\" the user into installing a\nsoftware update, whereas with a soft fork users do not \"have\" to do\nanything at all. However, this intuition is misguided: what matters is\nnot whether or not individual users have to perform the simple\nbureaucratic step of clicking a \"download\" button, but rather whether or\nnot the user is coerced into accepting a change in protocol\nrules that they would rather not accept. And by this metric, as\nmentioned above, both kinds of forks are ultimately coercive, and it is\nhard forks that come out as being somewhat better at preserving user\nfreedom.\n\n Now, let's look at highly controversial forks, particularly\nforks where miner/validator preferences and user preferences conflict.\nThere are three cases here: (i) bilateral hard forks, (ii) strictly\nexpanding hard forks, and (iii) so-called \"user-activated soft forks\"\n(UASF). A fourth category is where miners activate a soft fork\nwithout user consent; we will get to this later.\n\nFirst, bilateral hard forks. In the best case, the situation is\nsimple. The two coins trade on the market, and traders decide the\nrelative value of the two. From the ETC/ETH case, we have overwhelming\nevidence that miners are overwhelmingly likely to simply assign their\nhashrate to coins based on the ratio of prices in order to maximize\ntheir profit, regardless of their own ideological views.\n\nEven if some miners profess ideological preferences toward one side\nor the other, it is overwhemingly likely that there will be enough\nminers that are willing to arbitrage any mismatch between price ratio\nand hashpower ratio, and bring the two into alignment. If a cartel of\nminers tries to form to not mine on one chain, there are overwheming\nincentives to defect.\n\nThere are two edge cases here. The first is the possibilty that,\nbecause of an inefficient difficulty adjustment algorithm, the value of\nmining the coin goes down becase price drops but difficulty does not go\ndown to compensate, making mining very unprofitable, and there are no\nminers willing to mine at a loss to keep pushing the chain forward until\nits difficulty comes back into balance. This was not the case with\nEthereum, but may well be\nthe case with Bitcoin. Hence, the minority chain may well simply\nnever get off the ground, and so it will die. Note that the normative\nquestion of whether or not this is a good thing depends on your\nviews on coercion versus secession; as you can imagine from what I wrote\nabove I personally believe that such minority-chain-hostile difficulty\nadjustment algorithms are bad.\n\nThe second edge case is that if the disparity is very large, the\nlarge chain can 51% attack the smaller chain. Even in the case of an\nETH/ETC split with a 10:1 ratio, this has not happened; so it is\ncertainly not a given. However, it is always a possibility if miners on\nthe dominant chain prefer coercion to allowing secession and act on\nthese values. \n\n Next, let's look at strictly expanding hard forks. In an SEHF,\nthere is the property that the non-forked chain is valid under the\nforked rules, and so if the fork has a lower price than the non-forked\nchain, it will have less hashpower than the non-forked chain, and so the\nnon-forked chain will end up being accepted as the longest chain by\nboth original-client and forked-client rules - and so the forked\nchain \"will be\nannihilated\".\n\nThere is an argument that there is thus a strong inherent bias\nagainst such a fork succeeding, as the possibility that the forked chain\nwill get annihiliated will be baked into the price, pushing the price\nlower, making it even more likely that the chain will be annihilated...\nThis argument to me seems strong, and so it is a very good reason to\nmake any contentious hard fork bilateral rather than strictly\nexpanding.\n\nBitcoin Unlimited developers suggest dealing with this problem by making\nthe hard fork bilateral manually after it happens, but a better\nchoice would be to make the bilaterality built-in; for example, in the\nbitcoin case, one can add a rule to ban some unused opcode, and then\nmake a transaction containing that opcode on the non-forked chain, so\nthat under the forked rules the non-forked chain will from then on be\nconsidered forever invalid. In the Ethereum case, because of various\ndetails about how state calculation works, nearly all hard forks are\nbilateral almost automatically. Other chains may have different\nproperties depending on their architecture. \n\n The last type of fork that was mentioned above is the\nuser-activated soft fork. In a UASF, users turn on the soft fork rules\nwithout bothering to get consensus from miners; miners are expected to\nsimply fall in line out of economic interest. If many users do not go\nalong with the UASF, then there will be a coin split, and this will lead\nto a scenario identical to the strictly expanding hard fork, except -\nand this is the really clever and devious part of the concept - the\nsame \"risk of annihilation\" pressure that strongly disfavors the forked\nchain in a strictly expanding hard fork instead strongly favors the\nforked chain in a UASF. Even though a UASF is opt-in, it uses\neconomic asymmetry in order to bias itself toward success (though the\nbias is not absolute; if a UASF is decidedly unpopular then it will not\nsucceed and will simply lead to a chain split).\n\nHowever, UASFs are a dangerous game. For example, let us suppose that\nthe developers of a project want to make a UASF patch that converts an\nunused opcode that previously accepted all transactions into an opcode\nthat only accepts transactions that comply with the rules of some cool\nnew feature, though one that is politically or technically controversial\nand miners dislike. Miners have a clever and devious way to fight back:\nthey can unilaterally implement a miner-activated soft fork that\nmakes all transactions using the feature created by the soft fork always\nfail.\n\nNow, we have three rulesets:\n\n- The original rules where opcode X is always valid.\n\n- The rules where opcode X is only valid if the rest of the\ntransaction complies with the new rules\n\n- The rules where opcode X is always invalid.\n\nNote that (2) is a soft-fork with respect to (1), and (3) is a\nsoft-fork with respect to (2). Now, there is strong economic pressure in\nfavor of (3), and so the soft-fork fails to accomplish its\nobjective.\n\nThe conclusion is this. Soft forks are a dangerous game, and they\nbecome even more dangerous if they are contentious and miners start\nfighting back. Strictly expanding hard forks are also a dangerous game.\nMiner-activated soft forks are coercive; user-activated soft forks are\nless coercive, though still quite coercive because of the economic\npressure, and they also have their dangers. If you really want to make a\ncontentious change, and have decided that the high social costs of doing\nso are worth it, just do a clean bilateral hard fork, spend some time to\nadd some proper replay protection, and let the market sort it out.",
    "contentLength": 11765,
    "summary": "Hard forks are less coercive than soft forks because they allow opt-in choice and minority chains to secede, while soft forks force rule changes.",
    "detailedSummary": {
      "theme": "Vitalik argues that hard forks are less coercive than soft forks and advocates for bilateral hard forks as the cleanest approach to contentious protocol changes.",
      "summary": "Vitalik challenges the conventional wisdom that soft forks are less coercive than hard forks in blockchain protocol upgrades. He argues that while both types of forks involve coercion through network effects, hard forks are actually more respectful of user freedom because they are opt-in and allow dissenting users to remain on the original chain, whereas soft forks force all users to accept rule changes without any alternative. Vitalik explains that the perception of soft forks as 'less coercive' is misguided, focusing too much on the bureaucratic act of downloading software rather than on whether users are forced to accept unwanted protocol changes.\n\nVitalik analyzes different types of contentious forks, including bilateral hard forks, strictly expanding hard forks, and user-activated soft forks (UASF). He notes that bilateral hard forks allow the market to determine the value of competing chains, with miners typically following profitability rather than ideology. However, he warns that strictly expanding hard forks face inherent bias toward failure due to the risk of chain annihilation, while UASFs create dangerous economic pressures and can lead to escalating conflicts between users and miners. His ultimate recommendation is that for truly contentious changes, developers should implement clean bilateral hard forks with proper replay protection and let market forces decide the outcome.",
      "takeaways": [
        "Hard forks are opt-in and preserve user choice by allowing dissenting users to stay on the original chain, while soft forks eliminate this option entirely",
        "Both hard forks and soft forks are coercive through network effects, but soft forks institutionally favor coercion over secession",
        "Miners typically follow economic incentives rather than ideological preferences, as demonstrated by the ETC/ETH split",
        "Strictly expanding hard forks face inherent bias toward failure due to the risk of chain annihilation and should be avoided in favor of bilateral forks",
        "For contentious protocol changes, bilateral hard forks with proper replay protection are the cleanest approach, allowing the market to determine outcomes"
      ],
      "controversial": [
        "The claim that hard forks are less coercive than soft forks contradicts widely held beliefs in the blockchain community",
        "The argument that soft forks 'institutionally favor coercion over secession' challenges the conventional preference for soft forks in Bitcoin and other communities",
        "The recommendation to use bilateral hard forks for contentious changes goes against the common view that chain splits should be avoided at all costs"
      ]
    }
  },
  {
    "id": "general-2017-03-11-a_note_on_charity",
    "title": "A Note On Charity Through Marginal Price Discrimination",
    "date": "2017-03-11",
    "category": "general",
    "url": "https://vitalik.eth.limo/general/2017/03/11/a_note_on_charity.html",
    "path": "general/2017/03/11/a_note_on_charity.html",
    "content": "- \n\n- \n\n  \n    \n\n  \n  \n  Dark Mode Toggle\n  \n    \n\n  \n\n##  A Note On Charity Through Marginal Price Discrimination \n\n 2017 Mar 11 \nSee all posts\n\n \n \n\n A Note On Charity Through Marginal Price Discrimination \n\nUpdated 2018-07-28. See end note.\n\nThe following is an interesting idea that I had two years ago that I\npersonally believe has promise and could be easily implemented in the\ncontext of a blockchain ecosystem, though if desired it could certainly\nalso be implemented with more traditional technologies (blockchains\nwould help get the scheme network effects by putting the core logic on a\nmore neutral platform).\n\nSuppose that you are a restaurant selling sandwiches, and you\nordinarily sell sandwiches for $7.50. Why did you choose to sell them\nfor $7.50, and not $7.75 or $7.25? It clearly can't be the case that the\ncost of production is exactly $7.49999, as in that case you would be\nmaking no profit, and would not be able to cover fixed costs; hence, in\nmost normal situations you would still be able to make some\nprofit if you sold at $7.25 or $7.75, though less. Why less at $7.25?\nBecause the price is lower. Why less at $7.75? Because you get fewer\ncustomers. It just so happens that $7.50 is the point at which the\nbalance between those two factors is optimal for you.\n\nNotice one consequence of this: if you make a slight\ndistortion to the optimal price, then even compared to the magnitude of\nthe distortion the losses that you face are minimal. If you raise prices\nby 1%, from $7.50 to $7.575, then your profit declines from $6750 to\n$6733.12\u200a-\u200aa tiny 0.25% reduction. And that's profit\u200a-\u200aif you\nhad instead donated 1% of the price of each sandwich, it would have\nreduced your profit by 5%. The smaller the distortion the more favorable\nthe ratio: raising prices by 0.2% only cuts your profits down by\n0.01%.\n\nNow, you could argue that stores are not perfectly rational, and not\nperfectly informed, and so they may not actually be charging at\noptimal prices, all factors considered. However, if you don't know what\ndirection the deviation is in for any given store, then even still,\nin expectation, the scheme works the same way\u200a-\u200aexcept instead\nof losing $17 it's more like flipping a coin where half the time you\ngain $50 and half the time you lose $84. Furthermore, in the more\ncomplex scheme that we will describe later, we'll be adjusting prices in\nboth directions simultaneously, and so there will not even be any extra\nrisk - no matter how correct or incorrect the original price was, the\nscheme will give you a predictable small net loss.\n\nAlso, the above example was one where marginal costs are high, and\ncustomers are picky about prices\u200a-\u200ain the above model, charging $9 would\nhave netted you no customers at all. In a situation where marginal costs are\nmuch lower, and customers are less price-sensitive, the losses from\nraising or lowering prices would be even lower.\n\nSo what is the point of all this? Well, suppose that our sandwich\nshop changes its policy: it sells sandwiches for $7.55 to the general\npublic, but lowers the prices to $7.35 for people who volunteered in\nsome charity that maintains some local park (say, this is 25% of the\npopulation). The store's new profit is \\(\\$6682.5 \\cdot 0.25+\\$6742.5 \\cdot\n0.75=\\$6727.5\\) (that's a $22.5 loss), but the result is that you\nare now paying all 4500 of your customers 20 cents each to volunteer at\nthat charity\u200a-\u200aan incentive size of $900 (if you just count the\ncustomers who actually do volunteer, $225). So the store loses a bit,\nbut gets a huge amount of leverage, de-facto contributing at least $225\ndepending on how you measure it for a cost of only $22.5.\n\nNow, what we can start to do is build up an ecosystem of \"stickers\",\nwhich are non-transferable digital \"tokens\" that organizations hand out\nto people who they think are contributing to worthy causes. Tokens could\nbe organized by category (eg. poverty relief, science research,\nenvironmental, local community projects, open source software\ndevelopment, writing good blogs), and merchants would be free to charge\nmarginally lower prices to holders of the tokens that represent whatever\ncauses they personally approve of.\n\nThe next stage is to make the scheme recursive - being or working for\na merchant that offers lower prices to holders of green stickers is\nitself enough to merit you a green sticker, albeit one that is of lower\npotency and gives you a lower discount. This way, if an entire community\napproves of a particular cause, it may actually be profit-maximizing to\nstart offering discounts for the associated sticker, and so economic and\nsocial pressure will maintain a certain level of spending and\nparticipation toward the cause in a stable equilibrium.\n\nAs far as implementation goes, this requires:\n\n- A standard for stickers, including wallets where people can hold\nstickers\n\n- Payment systems that have support for charging lower prices to\nsticker holders included\n\n- At least a few sticker-issuing organizations (the lowest overhead is\nlikely to be issuing stickers for charity donations, and for easily\nverifiable online content, eg. open source software and blogs)\n\nSo this is something that can certainly be bootstrapped within a\nsmall community and user base and then let to grow over time.\n\nUpdate 2017.03.14: here\nis an economic model/simulation showing the above implemented as a\nPython script.\n\nUpdate 2018.07.28: after discussions with others (Glen Weyl and\nseveral Reddit commenters), I realized a few extra things about this\nmechanism, some encouraging and some worrying:\n\n- The above mechanism could be used not just by charities, but also by\ncentralized corporate actors. For example, a large corporation could\noffer a bribe of $40 to any store that offers the 20-cent discount to\ncustomers of its products, gaining additional revenue much higher than\n$40. So it's empowering but potentially dangerous in the wrong hands... (I\nhave not researched it but I'm sure this kind of technique is used in\nvarious kinds of loyalty programs already)\n\n- The above mechanism has the property that a merchant can \"donate\"\n\\(\\$x\\) to charity at a cost of \\(\\$x^{2}\\) (note: \\(x^{2}<x\\) at the scales we're talking\nabout here). This gives it a structure that's economically optimal in\ncertain ways (see quadratic\nvoting), as a merchant that feels twice as strongly about some\npublic good will be inclined to offer twice as large a subsidy, whereas\nmost other social choice mechanisms tend to either undervalue (as in\ntraditional voting) or overvalue (as in buying policies via lobbying)\nstronger vs weaker preferences.",
    "contentLength": 6591,
    "summary": "This blog post proposes a charity system where merchants offer small discounts to holders of digital \"stickers\" earned through good deeds.",
    "detailedSummary": {
      "theme": "Vitalik proposes a blockchain-based system where merchants offer small discounts to holders of digital 'stickers' representing charitable contributions, creating economic incentives for charitable behavior through marginal price discrimination.",
      "summary": "Vitalik begins by explaining how businesses can adjust prices slightly from their profit-optimizing point with minimal impact on profits due to the mathematical properties of optimization curves. He demonstrates that a restaurant charging $7.50 for sandwiches could offer a 20-cent discount to charity volunteers while only losing $22.5 in profit, yet effectively incentivizing $225 worth of charitable activity. Vitalik then proposes scaling this concept through a digital ecosystem of non-transferable 'stickers' that organizations award for charitable contributions, with merchants offering discounts to sticker holders based on causes they support. The system becomes recursive, where merchants offering charitable discounts themselves earn stickers, potentially creating stable economic equilibria that maintain charitable participation across communities.",
      "takeaways": [
        "Small price deviations from profit-optimal points cause disproportionately small losses in profit, creating leverage opportunities for charitable incentives",
        "A digital sticker system could create network effects where merchants offer discounts to charity contributors, amplifying charitable impact",
        "The recursive nature of the system means merchants who participate also earn stickers, potentially creating self-sustaining charitable ecosystems",
        "The mechanism has quadratic properties where merchants feeling twice as strongly about a cause will offer twice the subsidy, making it economically optimal for preference aggregation",
        "While promising for charitable causes, the same mechanism could be exploited by corporations for anti-competitive purposes through loyalty programs and bribes"
      ],
      "controversial": [
        "The system could be exploited by large corporations offering bribes to merchants for preferential treatment, potentially creating anti-competitive market distortions",
        "The effectiveness relies on assumptions about business pricing optimization that may not hold true for all merchants in practice"
      ]
    }
  },
  {
    "id": "general-2017-02-01-zk_snarks",
    "title": "[Mirror] Zk-SNARKs: Under the Hood",
    "date": "2017-02-01",
    "category": "math",
    "url": "https://vitalik.eth.limo/general/2017/02/01/zk_snarks.html",
    "path": "general/2017/02/01/zk_snarks.html",
    "content": "- \n\n- \n\n  \n    \n\n  \n  \n  Dark Mode Toggle\n  \n    \n\n  \n\n##  [Mirror] Zk-SNARKs: Under the Hood \n\n 2017 Feb 01 \nSee all posts\n\n \n \n\n [Mirror] Zk-SNARKs: Under the Hood \n\nThis is a mirror of the post at\nhttps://medium.com/@VitalikButerin/zk-snarks-under-the-hood-b33151a013f6\n\nThis is the third part of a series of articles explaining how the\ntechnology behind zk-SNARKs works; the previous articles on quadratic\narithmetic programs and elliptic\ncurve pairings are required reading, and this article will assume\nknowledge of both concepts. Basic knowledge of what zk-SNARKs are and\nwhat they do is also assumed. See also Christian\nReitwiessner's article here for another technical\nintroduction.\n\nIn the previous articles, we introduced the quadratic arithmetic\nprogram, a way of representing any computational problem with a\npolynomial equation that is much more amenable to various forms of\nmathematical trickery. We also introduced elliptic curve pairings, which\nallow a very limited form of one-way homomorphic encryption that lets\nyou do equality checking. Now, we are going to start from where we left\noff, and use elliptic curve pairings, together with a few other\nmathematical tricks, in order to allow a prover to prove that they know\na solution for a particular QAP without revealing anything else about\nthe actual solution.\n\nThis article will focus on the Pinocchio protocol by\nParno, Gentry, Howell and Raykova from 2013 (often called PGHR13); there\nare a few variations on the basic mechanism, so a zk-SNARK scheme\nimplemented in practice may work slightly differently, but the basic\nprinciples will in general remain the same.\n\nTo start off, let us go into the key cryptographic assumption\nunderlying the security of the mechanism that we are going to use: the\n*knowledge-of-exponent*\nassumption.\n\nBasically, if you get a pair of points \\(P\\) and \\(Q\\), where \\(P\n\\cdot k = Q\\), and you get a point \\(C\\), then it is not possible to come up\nwith \\(C \\cdot k\\) unless \\(C\\) is \"derived\" from \\(P\\) in some way that you know. This may\nseem intuitively obvious, but this assumption actually cannot be derived\nfrom any other assumption (eg. discrete log hardness) that we usually\nuse when proving security of elliptic curve-based protocols, and so\nzk-SNARKs do in fact rest on a somewhat shakier foundation than elliptic\ncurve cryptography more generally \u2014 although it's still sturdy enough\nthat most cryptographers are okay with it.\n\nNow, let's go into how this can be used. Supposed that a pair of\npoints \\((P, Q)\\) falls from the sky,\nwhere \\(P \\cdot k = Q\\), but nobody\nknows what the value of \\(k\\) is. Now,\nsuppose that I come up with a pair of points \\((R, S)\\) where \\(R \\cdot k = S\\). Then, the KoE assumption\nimplies that the only way I could have made that pair of points was by\ntaking \\(P\\) and \\(Q\\), and multiplying both by some factor r\nthat I personally know. Note also that thanks to the magic of\nelliptic curve pairings, checking that \\(R = k\n\\cdot S\\) doesn't actually require knowing \\(k\\)  - instead, you can simply check\nwhether or not \\(e(R, Q) = e(P,\nS)\\).\n\nLet's do something more interesting. Suppose that we have ten pairs\nof points fall from the sky: \\((P_1, Q_1),\n(P_2, Q_2)... (P_{10}, Q_{10})\\). In all cases, \\(P_i \\cdot k = Q_i\\). Suppose that I then\nprovide you with a pair of points \\((R,\nS)\\) where \\(R \\cdot k = S\\).\nWhat do you know now? You know that \\(R\\) is some linear combination \\(P_1 \\cdot i_1 + P_2 \\cdot i_2 + ... + P_{10} \\cdot\ni_{10}\\), where I know the coefficients \\(i_1, i_2 ... i_{10}\\). That is, the only\nway to arrive at such a pair of points \\((R,\nS)\\) is to take some multiples of \\(P_1, P_2 ... P_{10}\\) and add them together,\nand make the same calculation with \\(Q_1, Q_2\n... Q_{10}\\).\n\nNote that, given any specific set of \\(P_1...P_{10}\\) points that you might want to\ncheck linear combinations for, you can't actually create the\naccompanying \\(Q_1...Q_{10}\\) points\nwithout knowing what \\(k\\) is, and if\nyou do know what \\(k\\) is then you can\ncreate a pair \\((R, S)\\) where \\(R \\cdot k = S\\) for whatever \\(R\\) you want, without bothering to create a\nlinear combination. Hence, for this to work it's absolutely imperative\nthat whoever creates those points is trustworthy and actually deletes\n\\(k\\) once they created the ten points.\nThis is where the concept of a \"trusted setup\" comes\nfrom.\n\nRemember that the solution to a QAP is a set of polynomials \\((A, B, C)\\) such that \\(A(x) \\cdot B(x) - C(x) = H(x) \\cdot Z(x)\\),\nwhere:\n\n- \n\n\\(A\\) is a linear combination of\na set of polynomials \\(\\{A_1...A_m\\}\\)\n\n- \n\n\\(B\\) is the linear combination\nof \\(\\{B_1...B_m\\}\\) with the same\ncoefficients\n\n- \n\n\\(C\\) is a linear combination of\n\\(\\{C_1...C_m\\}\\) with the same\ncoefficients\n\nThe sets \\(\\{A_1...A_m\\},\n\\{B_1...B_m\\}\\) and \\(\\{C_1...C_m\\}\\) and the polynomial \\(Z\\) are part of the problem statement.\n\nHowever, in most real-world cases, \\(A,\nB\\) and \\(C\\) are extremely\nlarge; for something with many thousands of circuit gates like a hash\nfunction, the polynomials (and the factors for the linear combinations)\nmay have many thousands of terms. Hence, instead of having the prover\nprovide the linear combinations directly, we are going to use the trick\nthat we introduced above to have the prover prove that they are\nproviding something which is a linear combination, but without revealing\nanything else.\n\nYou might have noticed that the trick above works on elliptic curve\npoints, not polynomials. Hence, what actually happens is that we add the\nfollowing values to the trusted setup:\n\n- \n\n\\(G \\cdot A_1(t), G \\cdot A_1(t) \\cdot\nk_a\\)\n\n- \n\n\\(G \\cdot A_2(t), G \\cdot A_2(t) \\cdot\nk_a\\)\n\n- \n\n...\n\n- \n\n\\(G \\cdot B_1(t), G \\cdot B_1(t) \\cdot\nk_b\\)\n\n- \n\n\\(G \\cdot B_2(t), G \\cdot B_2(t) \\cdot\nk_b\\)\n\n- \n\n...\n\n- \n\n\\(G \\cdot C_1(t), G \\cdot C_1(t) \\cdot\nk_c\\)\n\n- \n\n\\(G \\cdot C_2(t), G \\cdot C_2(t) \\cdot\nk_c\\)\n\n- \n\n...\n\nYou can think of t as a \"secret point\" at which the polynomial is\nevaluated. \\(G\\) is a \"generator\" (some\nrandom elliptic curve point that is specified as part of the protocol)\nand \\(t, k_a, k_b\\) and \\(k_c\\) are \"toxic waste\", numbers that\nabsolutely must be deleted at all costs, or else whoever has them will\nbe able to make fake proofs. Now, if someone gives you a pair of points\n\\(P\\), \\(Q\\) such that \\(P\n\\cdot k_a = Q\\) (reminder: we don't need \\(k_a\\) to check this, as we can do a pairing\ncheck), then you know that what they are giving you is a linear\ncombination of \\(A_i\\) polynomials\nevaluated at \\(t\\).\n\nHence, so far the prover must give:\n\n- \n\n\\(\\pi _a = G \\cdot A(t), \\pi '_a =\nG \\cdot A(t) \\cdot k_a\\)\n\n- \n\n\\(\\pi _b = G \\cdot B(t), \\pi '_b =\nG \\cdot B(t) \\cdot k_b\\)\n\n- \n\n\\(\\pi _c = G \\cdot C(t), \\pi '_c =\nG \\cdot C(t) \\cdot k_c\\)\n\nNote that the prover doesn't actually need to know (and shouldn't\nknow!) \\(t, k_a, k_b\\) or \\(k_c\\) to compute these values; rather, the\nprover should be able to compute these values just from the points that\nwe're adding to the trusted setup.\n\nThe next step is to make sure that all three linear combinations have\nthe same coefficients. This we can do by adding another set of values to\nthe trusted setup: \\(G \\cdot (A_i(t) + B_i(t)\n+ C_i(t)) \\cdot b\\), where \\(b\\)\nis another number that should be considered \"toxic waste\" and discarded\nas soon as the trusted setup is completed. We can then have the prover\ncreate a linear combination with these values with the same\ncoefficients, and use the same pairing trick as above to verify that\nthis value matches up with the provided \\(A +\nB + C\\).\n\nFinally, we need to prove that \\(A \\cdot B\n- C = H \\cdot Z\\). We do this once again with a pairing\ncheck:\n\n\\(e(\\pi _a, \\pi _b) / e(\\pi _c, G) ?= e(\\pi\n_h, G \\cdot Z(t))\\)\n\nWhere \\(\\pi _h= G \\cdot H(t)\\). If\nthe connection between this equation and \\(A\n\\cdot B - C = H \\cdot Z\\) does not make sense to you, go back and\nread the article\non pairings.\n\nWe saw above how to convert \\(A, B\\)\nand \\(C\\) into elliptic curve points;\n\\(G\\) is just the generator (ie. the\nelliptic curve point equivalent of the number one). We can add \\(G \\cdot Z(t)\\) to the trusted setup. \\(H\\) is harder; \\(H\\) is just a polynomial, and we predict\nvery little ahead of time about what its coefficients will be for each\nindividual QAP solution. Hence, we need to add yet more data to the\ntrusted setup; specifically the sequence:\n\n\\(G, G \\cdot t, G \\cdot t^2, G \\cdot t^3, G\n\\cdot t^4 ...\\).\n\nIn the Zcash trusted setup, the sequence here goes up to about 2\nmillion; this is how many powers of \\(t\\) you need to make sure that you will\nalways be able to compute \\(H(t)\\), at\nleast for the specific QAP instance that they care about. And with that,\nthe prover can provide all of the information for the verifier to make\nthe final check.\n\nThere is one more detail that we need to discuss. Most of the time we\ndon't just want to prove in the abstract that some solution exists for\nsome specific problem; rather, we want to prove either the correctness\nof some specific solution (eg. proving that if you take the word \"cow\"\nand SHA3 hash it a million times, the final result starts with\n0x73064fe5), or that a solution exists if you restrict some of the\nparameters. For example, in a cryptocurrency instantiation where\ntransaction amounts and account balances are encrypted, you want to\nprove that you know some decryption key k such that:\n\n- \n\ndecrypt(old_balance, k) >= decrypt(tx_value, k)\n\n- \n\ndecrypt(old_balance, k) - decrypt(tx_value, k) = decrypt(new_balance, k)\n\nThe encrypted old_balance, tx_value and\nnew_balance should be specified publicly, as those are the\nspecific values that we are looking to verify at that particular time;\nonly the decryption key should be hidden. Some slight modifications to\nthe protocol are needed to create a \"custom verification key\" that\ncorresponds to some specific restriction on the inputs.\n\nNow, let's step back a bit. First of all, here's the verification\nalgorithm in its entirety, courtesy of ben Sasson, Tromer, Virza and\nChiesa:\n\nThe first line deals with parametrization; essentially, you can think\nof its function as being to create a \"custom verification key\" for\nthe specific instance of the problem where some of the arguments\nare specified. The second line is the linear combination check for \\(A, B\\) and \\(C\\); the third line is the check that the\nlinear combinations have the same coefficients, and the fourth line is\nthe product check \\(A \\cdot B - C = H \\cdot\nZ\\).\n\nAltogether, the verification process is a few elliptic curve\nmultiplications (one for each \"public\" input variable), and five pairing\nchecks, one of which includes an additional pairing multiplication. The\nproof contains eight elliptic curve points: a pair of points each for\n\\(A(t), B(t)\\) and \\(C(t)\\), a point \\(\\pi _k\\) for \\(b\n\\cdot (A(t) + B(t) + C(t))\\), and a point \\(\\pi _h\\) for \\(H(t)\\). Seven of these points are on the\n\\(F_p\\) curve (32 bytes each, as you\ncan compress the \\(y\\) coordinate to a\nsingle bit), and in the Zcash implementation one point (\\(\\pi _b\\)) is on the twisted curve in \\(F_{p^2}\\) (64 bytes), so the total size of\nthe proof is ~288 bytes.\n\nThe two computationally hardest parts of creating a proof are:\n\n- \n\nDividing \\((A \\cdot B - C) / Z\\)\nto get \\(H\\) (algorithms based on the\nFast\nFourier transform can do this in sub-quadratic time, but it's still\nquite computationally intensive)\n\n- \n\nMaking the elliptic curve multiplications and additions to create\nthe \\(A(t), B(t), C(t)\\) and \\(H(t)\\) values and their corresponding\npairs\n\nThe basic reason why creating a proof is so hard is the fact that\nwhat was a single binary logic gate in the original computation turns\ninto an operation that must be cryptographically processed through\nelliptic curve operations if we are making a zero-knowledge proof out of\nit. This fact, together with the superlinearity of fast Fourier\ntransforms, means that proof creation takes ~20\u201340 seconds for a Zcash\ntransaction.\n\nAnother very important question is: can we try to make the trusted\nsetup a little... less trust-demanding? Unfortunately we can't make it\ncompletely trustless; the KoE assumption itself precludes making\nindependent pairs \\((P_i, P_i \\cdot\nk)\\) without knowing what \\(k\\)\nis. However, we can increase security greatly by using \\(N\\)-of-\\(N\\) multiparty computation - that is,\nconstructing the trusted setup between \\(N\\) parties in such a way that as long as\nat least one of the participants deleted their toxic waste then you're\nokay.\n\nTo get a bit of a feel for how you would do this, here's a simple\nalgorithm for taking an existing set (\\(G, G\n\\cdot t, G \\cdot t^2, G \\cdot t^3...\\)), and \"adding in\" your own\nsecret so that you need both your secret and the previous secret (or\nprevious set of secrets) to cheat.\n\nThe output set is simply:\n\n\\(G, (G \\cdot t) \\cdot s, (G \\cdot t^2)\n\\cdot s^2, (G \\cdot t^3) \\cdot s^3...\\)\n\nNote that you can produce this set knowing only the original set and\ns, and the new set functions in the same way as the old set, except now\nusing \\(t \\cdot s\\) as the \"toxic\nwaste\" instead of \\(t\\). As long as you\nand the person (or people) who created the previous set do not both fail\nto delete your toxic waste and later collude, the set is \"safe\".\n\nDoing this for the complete trusted setup is quite a bit harder, as\nthere are several values involved, and the algorithm has to be done\nbetween the parties in several rounds. It's an area of active research\nto see if the multi-party computation algorithm can be simplified\nfurther and made to require fewer rounds or made more parallelizable, as\nthe more you can do that the more parties it becomes feasible to include\ninto the trusted setup procedure. It's reasonable to see why a trusted\nsetup between six participants who all know and work with each other\nmight make some people uncomfortable, but a trusted setup with thousands\nof participants would be nearly indistinguishable from no trust at all -\nand if you're really paranoid, you can get in and participate in the\nsetup procedure yourself, and be sure that you personally deleted your\nvalue.\n\nAnother area of active research is the use of other approaches that\ndo not use pairings and the same trusted setup paradigm to achieve the\nsame goal; see Eli\nben Sasson's recent presentation for one alternative (though be\nwarned, it's at least as mathematically complicated as SNARKs are!)\n\nSpecial thanks to Ariel Gabizon and Christian Reitwiessner for\nreviewing.",
    "contentLength": 14412,
    "summary": "Vitalik explains the Pinocchio protocol (PGHR13) showing how zk-SNARKs use elliptic curve pairings & trusted setups to prove QAP solutions.",
    "detailedSummary": {
      "theme": "A technical deep-dive into the cryptographic mechanisms underlying zk-SNARKs, specifically the Pinocchio protocol, explaining how mathematical concepts like quadratic arithmetic programs and elliptic curve pairings enable zero-knowledge proofs.",
      "summary": "Vitalik provides a comprehensive technical explanation of how zk-SNARKs work under the hood, focusing on the Pinocchio protocol from 2013. He explains how the knowledge-of-exponent assumption enables provers to demonstrate they know a solution to a quadratic arithmetic program without revealing the solution itself. The process involves a trusted setup that creates pairs of elliptic curve points with secret relationships, allowing verifiers to check that provers have constructed valid linear combinations of polynomials. Vitalik details how the verification process works through pairing checks and explains why proof creation is computationally intensive, taking 20-40 seconds for a Zcash transaction. He also discusses the trust requirements of the setup phase and how multi-party computation can reduce but not eliminate the need for trusted participants.",
      "takeaways": [
        "zk-SNARKs rely on the knowledge-of-exponent assumption, which cannot be derived from standard cryptographic assumptions like discrete log hardness, making them rest on somewhat shakier foundations than typical elliptic curve cryptography",
        "The trusted setup is critical and requires participants to permanently delete 'toxic waste' values - if these secrets are retained and participants collude, fake proofs can be created",
        "A complete zk-SNARK proof is remarkably compact at only ~288 bytes, containing eight elliptic curve points that encode all necessary verification information",
        "Proof creation is computationally expensive because each binary logic gate in the original computation must be processed through elliptic curve operations, plus polynomial division using Fast Fourier transforms",
        "Multi-party computation can significantly improve the security of trusted setups by requiring collusion between all participants rather than just one trusted party, with the ideal being thousands of participants making the setup nearly trustless"
      ],
      "controversial": [
        "The reliance on the knowledge-of-exponent assumption represents a potentially weaker cryptographic foundation compared to more established assumptions used in elliptic curve cryptography",
        "The requirement for a trusted setup where participants must be trusted to delete secret values introduces a significant trust assumption that some may find unacceptable for certain applications"
      ]
    }
  },
  {
    "id": "general-2017-01-14-exploring_ecp",
    "title": "[Mirror] Exploring Elliptic Curve Pairings",
    "date": "2017-01-14",
    "category": "math",
    "url": "https://vitalik.eth.limo/general/2017/01/14/exploring_ecp.html",
    "path": "general/2017/01/14/exploring_ecp.html",
    "content": "- \n\n- \n\n  \n    \n\n  \n  \n  Dark Mode Toggle\n  \n    \n\n  \n\n##  [Mirror] Exploring Elliptic Curve Pairings \n\n 2017 Jan 14 \nSee all posts\n\n \n \n\n [Mirror] Exploring Elliptic Curve Pairings \n\nThis is a mirror of the post at\nhttps://medium.com/@VitalikButerin/exploring-elliptic-curve-pairings-c73c1864e627\n\nTrigger warning: math.\n\nOne of the key cryptographic primitives behind various constructions,\nincluding deterministic threshold signatures, zk-SNARKs and other\nsimpler forms of zero-knowledge proofs is the elliptic curve pairing.\nElliptic curve pairings (or \"bilinear maps\") are a recent addition to a\n30-year-long history of using elliptic curves for cryptographic\napplications including encryption and digital signatures; pairings\nintroduce a form of \"encrypted multiplication\", greatly expanding what\nelliptic curve-based protocols can do. The purpose of this article will\nbe to go into elliptic curve pairings in detail, and explain a general\noutline of how they work.\n\nYou're not expected to understand everything here the first time you\nread it, or even the tenth time; this stuff is genuinely hard. But\nhopefully this article will give you at least a bit of an idea as to\nwhat is going on under the hood.\n\nElliptic curves themselves are very much a nontrivial topic to\nunderstand, and this article will generally assume that you know how\nthey work; if you do not, I recommend this article here as a primer: https://blog.cloudflare.com/a-relatively-easy-to-understand-primer-on-elliptic-curve-cryptography/.\nAs a quick summary, elliptic curve cryptography involves mathematical\nobjects called \"points\" (these are literal two-dimensional points with\n\\((x, y)\\) coordinates), with special\nformulas for adding and subtracting them (ie. for calculating the\ncoordinates of \\(R = P + Q\\)), and you\ncan also multiply a point by an integer (ie. \\(P \\cdot n = P + P + ... + P\\), though there's\na much faster way to compute it if \\(n\\) is big).\n\nHere's how point addition looks like graphically.\n\nThere exists a special point called the \"point at infinity\" (\\(O\\)), the equivalent of zero in point\narithmetic; it's always the case that \\(P + O\n= P\\). Also, a curve has an \"order\"; there\nexists a number \\(n\\) such that \\(P \\cdot n = O\\) for any \\(P\\) (and of course, \\(P \\cdot (n+1) = P, P \\cdot (7 \\cdot n + 5) = P\n\\cdot 5\\), and so on). There is also some commonly agreed upon\n\"generator point\" \\(G\\), which is\nunderstood to in some sense represent the number \\(1\\). Theoretically, any point on a curve\n(except \\(O\\)) can be \\(G\\); all that matters is that \\(G\\) is standardized.\n\nPairings go a step further in that they allow you to check certain\nkinds of more complicated equations on elliptic curve points \u2014 for\nexample, if \\(P = G \\cdot p, Q = G \\cdot\nq\\) and \\(R = G \\cdot r\\), you\ncan check whether or not \\(p \\cdot q =\nr\\), having just \\(P, Q\\) and\n\\(R\\) as inputs. This might seem like\nthe fundamental security guarantees of elliptic curves are being broken,\nas information about \\(p\\) is leaking\nfrom just knowing P, but it turns out that the leakage is highly\ncontained \u2014 specifically, the decisional\nDiffie Hellman problem is easy, but the computational Diffie Hellman\nproblem (knowing \\(P\\) and \\(Q\\) in the above example,\ncomputing \\(R = G \\cdot p \\cdot\nq\\)) and the discrete\nlogarithm problem (recovering \\(p\\)\nfrom \\(P\\)) remain computationally\ninfeasible (at least, if they were before).\n\nA third way to look at what pairings do, and one that is perhaps most\nilluminating for most of the use cases that we are about, is that if you\nview elliptic curve points as one-way encrypted numbers (that is, \\(encrypt(p) = p \\cdot G = P\\)), then whereas\ntraditional elliptic curve math lets you check linear\nconstraints on the numbers (eg. if \\(P = G\n\\cdot p, Q = G \\cdot q\\) and \\(R = G\n\\cdot r\\), checking \\(5 \\cdot P + 7\n\\cdot Q = 11 \\cdot R\\) is really checking that \\(5 \\cdot p + 7 \\cdot q = 11 \\cdot r\\)),\npairings let you check quadratic constraints (eg. checking\n\\(e(P, Q) \\cdot e(G, G \\cdot 5) = 1\\)\nis really checking that \\(p \\cdot q +\n5 = 0\\)). And going up to quadratic is enough to let us work with\ndeterministic threshold signatures, quadratic arithmetic programs and\nall that other good stuff.\n\nNow, what is this funny \\(e(P, Q)\\)\noperator that we introduced above? This is the pairing. Mathematicians\nalso sometimes call it a bilinear map; the word \"bilinear\" here\nbasically means that it satisfies the constraints:\n\n\\(e(P, Q + R) = e(P, Q) \\cdot e(P,\nR)\\)\n\n\\(e(P + S, Q) = e(P, Q) \\cdot e(S,\nQ)\\)\n\nNote that \\(+\\) and \\(\\cdot\\) can be arbitrary operators; when\nyou're creating fancy new kinds of mathematical objects, abstract\nalgebra doesn't care how \\(+\\) and\n\\(\\cdot\\) are defined, as long\nas they are consistent in the usual ways, eg. \\(a + b = b + a, (a \\cdot b) \\cdot c = a \\cdot (b\n\\cdot c)\\) and \\((a \\cdot c) + (b \\cdot\nc) = (a + b) \\cdot c\\).\n\nIf \\(P\\), \\(Q\\), \\(R\\)\nand \\(S\\) were simple numbers,\nthen making a simple pairing is easy: we can do \\(e(x, y) = 2^{xy}\\). Then, we can see:\n\n\\(e(3, 4+ 5) = 2^{3 \\cdot 9} =\n2^{27}\\)\n\n\\(e(3, 4) \\cdot e(3, 5) = 2^{3 \\cdot 4}\n\\cdot 2^{3 \\cdot 5} = 2^{12} \\cdot 2^{15} = 2^{27}\\)\n\nIt's bilinear!\n\nHowever, such simple pairings are not suitable for cryptography\nbecause the objects that they work on are simple integers and are too\neasy to analyze; integers make it easy to divide, compute logarithms,\nand make various other computations; simple integers have no concept of\na \"public key\" or a \"one-way function\". Additionally, with the pairing\ndescribed above you can go backwards - knowing \\(x\\), and knowing \\(e(x, y)\\), you can simply compute a\ndivision and a logarithm to determine \\(y\\). We want mathematical objects that are\nas close as possible to \"black boxes\", where you can add, subtract,\nmultiply and divide, but do nothing else. This is where\nelliptic curves and elliptic curve pairings come in.\n\nIt turns out that it is possible to make a bilinear map over elliptic\ncurve points \u2014 that is, come up with a function \\(e(P, Q)\\) where the inputs \\(P\\) and \\(Q\\) are elliptic curve points, and where\nthe output is what's called an \\((F_p)^{12}\\) element (at least in the\nspecific case we will cover here; the specifics differ depending on the\ndetails of the curve, more on this later), but the math behind doing so\nis quite complex.\n\nFirst, let's cover prime fields and extension fields. The pretty\nelliptic curve in the picture earlier in this post only looks that way\nif you assume that the curve equation is defined using regular real\nnumbers. However, if we actually use regular real numbers in\ncryptography, then you can use logarithms to \"go backwards\", and\neverything breaks; additionally, the amount of space needed to actually\nstore and represent the numbers may grow arbitrarily. Hence, we instead\nuse numbers in a prime field.\n\nA prime field consists of the set of numbers \\(0, 1, 2... p-1\\), where \\(p\\) is prime, and the various operations\nare defined as follows:\n\n\\(a + b: (a + b)\\) % \\(p\\)\n\n\\(a \\cdot b: (a \\cdot b)\\) % \\(p\\)\n\n\\(a - b: (a - b)\\) % \\(p\\)\n\n\\(a / b: (a \\cdot b^{p-2})\\) % \\(p\\)\n\nBasically, all math is done modulo \\(p\\) (see here\nfor an introduction to modular math). Division is a special case;\nnormally, \\(\\frac{3}{2}\\) is not an\ninteger, and here we want to deal only with integers, so we instead try\nto find the number \\(x\\) such that\n\\(x \\cdot 2 = 3\\), where \\(\\cdot\\) of course refers to modular\nmultiplication as defined above. Thanks to Fermat's\nlittle theorem, the exponentiation trick shown above does the job,\nbut there is also a faster way to do it, using the Extended\nEuclidean Algorithm. Suppose \\(p =\n7\\); here are a few examples:\n\n\\(2 + 3 = 5\\) % \\(7 = 5\\)\n\n\\(4 + 6 = 10\\) % \\(7 = 3\\)\n\n\\(2 - 5 = -3\\) % \\(7 = 4\\)\n\n\\(6 \\cdot 3 = 18\\) % \\(7 = 4\\)\n\n\\(3 / 2 = (3 \\cdot 2^5)\\) % \\(7 = 5\\)\n\n\\(5 \\cdot 2 = 10\\) % \\(7 = 3\\)\n\nIf you play around with this kind of math, you'll notice that it's\nperfectly consistent and satisfies all of the usual rules. The last two\nexamples above show how \\((a / b) \\cdot b =\na\\); you can also see that \\((a + b) +\nc = a + (b + c), (a + b) \\cdot c = a \\cdot c + b \\cdot c\\), and\nall the other high school algebraic identities you know and love\ncontinue to hold true as well. In elliptic curves in reality, the points\nand equations are usually computed in prime fields.\n\nNow, let's talk about extension fields. You have\nprobably already seen an extension field before; the most common example\nthat you encounter in math textbooks is the field of complex numbers,\nwhere the field of real numbers is \"extended\" with the additional\nelement \\(\\sqrt{-1} = i\\). Basically,\nextension fields work by taking an existing field, then \"inventing\" a\nnew element and defining the relationship between that element and\nexisting elements (in this case, \\(i^2 + 1 =\n0\\)), making sure that this equation does not hold true for any\nnumber that is in the original field, and looking at the set of all\nlinear combinations of elements of the original field and the new\nelement that you have just created.\n\nWe can do extensions of prime fields too; for example, we can extend\nthe prime field \\(\\bmod 7\\) that we\ndescribed above with \\(i\\), and then we\ncan do:\n\n\\((2 + 3i) + (4 + 2i) = 6 + 5i\\)\n\n\\((5 + 2i) + 3 = 1 + 2i\\)\n\n\\((6 + 2i) \\cdot 2 = 5 + 4i\\)\n\n\\(4i \\cdot (2 + i) = 3 + i\\)\n\nThat last result may be a bit hard to figure out; what happened there\nwas that we first decompose the product into \\(4i \\cdot 2 + 4i \\cdot i\\), which gives\n\\(8i - 4\\), and then because we are\nworking in \\(\\bmod 7\\) math that\nbecomes \\(i + 3\\). To divide, we\ndo:\n\n\\(a / b: (a \\cdot b^{(p^2-2)})\\) %\n\\(p\\)\n\nNote that the exponent for Fermat's little theorem is now \\(p^2\\) instead of \\(p\\), though once again if we want to be\nmore efficient we can also instead extend the Extended Euclidean\nAlgorithm to do the job. Note that \\(x^{p^2 -\n1} = 1\\) for any \\(x\\) in this\nfield, so we call \\(p^2 - 1\\) the\n\"order of the multiplicative group in the field\".\n\nWith real numbers, the Fundamental\nTheorem of Algebra ensures that the quadratic extension that we call\nthe complex numbers is \"complete\" \u2014 you cannot extend it further,\nbecause for any mathematical relationship (at least, any mathematical\nrelationship defined by an algebraic formula) that you can come up with\nbetween some new element \\(j\\) and the\nexisting complex numbers, it's possible to come up with at least one\ncomplex number that already satisfies that relationship. With prime\nfields, however, we do not have this issue, and so we can go further and\nmake cubic extensions (where the mathematical relationship between some\nnew element \\(w\\) and existing field\nelements is a cubic equation, so \\(1,\nw\\) and \\(w^2\\) are all linearly\nindependent of each other), higher-order extensions, extensions of\nextensions, etc. And it is these kinds of supercharged modular complex\nnumbers that elliptic curve pairings are built on.\n\nFor those interested in seeing the exact math involved in making all\nof these operations written out in code, prime fields and field\nextensions are implemented here: https://github.com/ethereum/py_pairing/blob/master/py_ecc/bn128/bn128_field_elements.py\n\nNow, on to elliptic curve pairings. An elliptic curve pairing (or\nrather, the specific form of pairing we'll explore here; there are also\nother types of pairings, though their logic is fairly similar) is a map\n\\(G_2 \\times G_1 \\rightarrow G_t\\),\nwhere:\n\n- \n\n\\(\\bf G_1\\) is an elliptic\ncurve, where points satisfy an equation of the form \\(y^2 = x^3 + b\\), and where both coordinates\nare elements of \\(F_p\\) (ie. they are\nsimple numbers, except arithmetic is all done modulo some prime\nnumber)\n\n- \n\n\\(\\bf G_2\\) is an elliptic\ncurve, where points satisfy the same equation as \\(G_1\\), except where the coordinates are\nelements of \\((F_p)^{12}\\) (ie. they\nare the supercharged complex numbers we talked about above; we define a\nnew \"magic number\" \\(w\\), which is\ndefined by a \\(12\\)th degree polynomial\nlike \\(w^{12} - 18 \\cdot w^6 + 82 =\n0\\))\n\n- \n\n\\(\\bf G_t\\) is the type of\nobject that the result of the elliptic curve goes into. In the curves\nthat we look at, \\(G_t\\) is \\(\\bf (F_p)^{12}\\) (the same supercharged\ncomplex number as used in \\(G_2\\))\n\nThe main property that it must satisfy is bilinearity, which in this\ncontext means that:\n\n- \n\n\\(e(P, Q + R) = e(P, Q) \\cdot e(P,\nR)\\)\n\n- \n\n\\(e(P + Q, R) = e(P, R) \\cdot e(Q,\nR)\\)\n\nThere are two other important criteria:\n\n- \n\nEfficient computability (eg. we can make an easy\npairing by simply taking the discrete logarithms of all points and\nmultiplying them together, but this is as computationally hard as\nbreaking elliptic curve cryptography in the first place, so it doesn't\ncount)\n\n- \n\nNon-degeneracy (sure, you could just define\n\\(e(P, Q) = 1\\), but that's not a\nparticularly useful pairing)\n\nSo how do we do this?\n\nThe math behind why pairing functions work is quite tricky and\ninvolves quite a bit of advanced algebra going even beyond what we've\nseen so far, but I'll provide an outline. First of all, we need to\ndefine the concept of a divisor, basically an\nalternative way of representing functions on elliptic curve points. A\ndivisor of a function basically counts the zeroes and the infinities of\nthe function. To see what this means, let's go through a few examples.\nLet us fix some point \\(P = (P_x,\nP_y)\\), and consider the following function:\n\n\\(f(x, y) = x - P_x\\)\n\nThe divisor is \\([P] + [-P] - 2 \\cdot\n[O]\\) (the square brackets are used to represent the fact that we\nare referring to the presence of the point \\(P\\) in the set of zeroes and infinities of\nthe function, not the point P itself; \\([P] + [Q]\\) is not the\nsame thing as \\([P + Q]\\)). The\nreasoning is as follows:\n\n- \n\nThe function is equal to zero at \\(P\\), since \\(x\\) is \\(P_x\\), so \\(x -\nP_x = 0\\)\n\n- \n\nThe function is equal to zero at \\(-P\\), since \\(-P\\) and \\(P\\) share the same \\(x\\) coordinate\n\n- \n\nThe function goes to infinity as \\(x\\) goes to infinity, so we say the\nfunction is equal to infinity at \\(O\\).\nThere's a technical reason why this infinity needs to be counted twice,\nso \\(O\\) gets added with a\n\"multiplicity\" of \\(-2\\) (negative\nbecause it's an infinity and not a zero, two because of this double\ncounting).\n\nThe technical reason is roughly this: because the equation of the\ncurve is \\(x^3 = y^2 + b, y\\) goes to\ninfinity \"\\(1.5\\) times faster\" than\n\\(x\\) does in order for \\(y^2\\) to keep up with \\(x^3\\); hence, if a linear function includes\nonly \\(x\\) then it is represented as an\ninfinity of multiplicity \\(2\\), but if\nit includes \\(y\\) then it is\nrepresented as an infinity of multiplicity \\(3\\).\n\nNow, consider a \"line function\":\n\n\\(ax + by + c = 0\\)\n\nWhere \\(a\\), \\(b\\) and \\(c\\) are carefully chosen so that the line\npasses through points \\(P\\) and \\(Q\\). Because of how elliptic curve addition\nworks (see the diagram at the top), this also means that it passes\nthrough \\(-P-Q\\). And it goes up to\ninfinity dependent on both \\(x\\) and\n\\(y\\), so the divisor becomes \\([P]+ [Q] + [-P-Q] - 3 \\cdot [O]\\).\n\nWe know that every \"rational function\" (ie. a function defined only\nusing a finite number of \\(+, -,\n\\cdot\\) and \\(/\\) operations on\nthe coordinates of the point) uniquely corresponds to some divisor, up\nto multiplication by a constant (ie. if two functions \\(F\\) and \\(G\\) have the same divisor, then \\(F = G \\cdot k\\) for some constant \\(k\\)).\n\nFor any two functions \\(F\\) and\n\\(G\\), the divisor of \\(F \\cdot G\\) is equal to the divisor of\n\\(F\\) plus the divisor of \\(G\\) (in math textbooks, you'll see \\((F \\cdot G) = (F) + (G)\\)), so for example\nif \\(f(x, y) = P_x - x\\), then \\((f^3) = 3 \\cdot [P] + 3 \\cdot [-P] - 6 \\cdot\n[O]\\); \\(P\\) and \\(-P\\) are \"triple-counted\" to account for\nthe fact that \\(f^3\\) approaches \\(0\\) at those points \"three times as\nquickly\" in a certain mathematical sense.\n\nNote that there is a theorem that states that if you \"remove the\nsquare brackets\" from a divisor of a function, the points must add up to\n\\(O ([P] + [Q] + [-P-Q] - 3 \\cdot [O]\\)\nclearly fits, as \\(P + Q - P - Q - 3 \\cdot O =\nO)\\), and any divisor that has this property is the divisor of a\nfunction.\n\nNow, we're ready to look at Tate pairings. Consider the following\nfunctions, defined via their divisors:\n\n- \n\n\\((F_P) = n \\cdot [P] - n \\cdot\n[O]\\), where \\(n\\) is the order\nof \\(G_1\\), ie. \\(n \\cdot P = O\\) for any \\(P\\)\n\n- \n\n\\((F_Q) = n \\cdot [Q] - n \\cdot\n[O]\\)\n\n- \n\n\\((g) = [P + Q] - [P] - [Q] +\n[O]\\)\n\nNow, let's look at the product \\(F_P \\cdot\nF_Q \\cdot g^n\\). The divisor is:\n\n\\(n \\cdot [P] - n \\cdot [O] + n \\cdot [Q] -\nn \\cdot [O] + n \\cdot [P + Q] - n \\cdot [P] - n \\cdot [Q] + n \\cdot\n[O]\\)\n\nWhich simplifies neatly to:\n\n\\(n \\cdot [P + Q] - n \\cdot\n[O]\\)\n\nNotice that this divisor is of exactly the same format as the divisor\nfor \\(F_P\\) and \\(F_Q\\) above. Hence, \\(F_P \\cdot F_Q \\cdot g^n = F_{P + Q}\\).\n\nNow, we introduce a procedure called the \"final exponentiation\" step,\nwhere we take the result of our functions above (\\(F_P, F_Q\\), etc.) and raise it to the power\n\\(z = (p^{12} - 1) / n\\), where \\(p^{12} - 1\\) is the order of the\nmultiplicative group in \\((F_p)^{12}\\)\n(ie. for any \\(x \\in (F_p)^{12},\nx^{(p^{12} - 1)} = 1\\)). Notice that if you apply this\nexponentiation to any result that has already been raised to\nthe power of \\(n\\), you get an\nexponentiation to the power of \\(p^{12} -\n1\\), so the result turns into \\(1\\). Hence, after final exponentiation,\n\\(g^n\\) cancels out and we get \\(F_P^z \\cdot F_Q^z = (F_{P + Q})^z\\).\nThere's some bilinearity for you.\n\nNow, if you want to make a function that's bilinear in both\narguments, you need to go into spookier math, where instead of taking\n\\(F_P\\) of a value directly, you take\n\\(F_P\\) of a divisor, and\nthat's where the full \"Tate pairing\" comes from. To prove some more\nresults you have to deal with notions like \"linear equivalence\" and\n\"Weil reciprocity\", and the rabbit hole goes on from there. You can find\nmore reading material on all of this here\nand here.\n\nFor an implementation of a modified version of the Tate pairing,\ncalled the optimal Ate paring, see\nhere. The code implements Miller's\nalgorithm, which is needed to actually compute \\(F_P\\).\n\nNote that the fact pairings like this are possible is somewhat of a\nmixed blessing: on the one hand, it means that all the protocols we can\ndo with pairings become possible, but is also means that we have to be\nmore careful about what elliptic curves we use.\n\nEvery elliptic curve has a value called an embedding degree;\nessentially, the smallest \\(k\\) such\nthat \\(p^k - 1\\) is a multiple of \\(n\\) (where \\(p\\) is the prime used for the field and\n\\(n\\) is the curve order). In the\nfields above, \\(k = 12\\), and in the\nfields used for traditional ECC (ie. where we don't care about\npairings), the embedding degree is often extremely large, to the point\nthat pairings are computationally infeasible to compute; however, if we\nare not careful then we can generate fields where \\(k = 4\\) or even \\(1\\).\n\nIf \\(k = 1\\), then the \"discrete\nlogarithm\" problem for elliptic curves (essentially, recovering \\(p\\) knowing only the point \\(P = G \\cdot p\\), the problem that you have\nto solve to \"crack\" an elliptic curve private key) can be reduced into a\nsimilar math problem over \\(F_p\\),\nwhere the problem becomes much easier (this is called the MOV\nattack); using curves with an embedding degree of \\(12\\) or higher ensures that this reduction\nis either unavailable, or that solving the discrete log problem over\npairing results is at least as hard as recovering a private key from a\npublic key \"the normal way\" (ie. computationally infeasible). Do not\nworry; all standard curve parameters have been thoroughly checked for\nthis issue.\n\nStay tuned for a mathematical explanation of how zk-SNARKs work,\ncoming soon.\n\nSpecial thanks to Christian Reitwiessner, Ariel Gabizon (from\nZcash) and Alfred Menezes for reviewing and making corrections.",
    "contentLength": 19881,
    "summary": "Elliptic curve pairings enable \"encrypted multiplication\" to check quadratic constraints on encrypted numbers, powering zk-SNARKs and threshold signatures.",
    "detailedSummary": {
      "theme": "Vitalik provides a detailed mathematical explanation of elliptic curve pairings, a cryptographic primitive that enables 'encrypted multiplication' and forms the foundation for advanced protocols like zk-SNARKs and threshold signatures.",
      "summary": "Vitalik explores elliptic curve pairings (bilinear maps) as a revolutionary extension to traditional elliptic curve cryptography that introduces the concept of 'encrypted multiplication.' While standard elliptic curve operations only allow checking linear constraints on encrypted numbers, pairings enable verification of quadratic constraints, which is sufficient for implementing sophisticated cryptographic protocols like deterministic threshold signatures and zk-SNARKs. Vitalik explains that pairings work by taking two elliptic curve points as input and producing an element in an extension field, maintaining bilinearity while preserving the computational hardness of the discrete logarithm problem. The mathematical foundation involves complex concepts including prime fields, extension fields, divisors, and the Tate pairing, with the actual implementation requiring careful selection of curves with appropriate embedding degrees to maintain security. Vitalik emphasizes that while the decisional Diffie-Hellman problem becomes easy with pairings, the computational Diffie-Hellman and discrete logarithm problems remain computationally infeasible, preserving the overall security model.",
      "takeaways": [
        "Elliptic curve pairings introduce 'encrypted multiplication' capabilities that greatly expand what elliptic curve-based protocols can accomplish beyond traditional linear operations",
        "Pairings are bilinear maps that take two elliptic curve points as input and output an element in an extension field like (Fp)^12, while maintaining mathematical properties crucial for cryptographic applications",
        "While pairings make the decisional Diffie-Hellman problem easy, they preserve the computational hardness of both the computational Diffie-Hellman problem and the discrete logarithm problem",
        "The mathematical foundation involves sophisticated concepts including prime fields, extension fields, divisors, and the Tate pairing, implemented through algorithms like Miller's algorithm for practical computation",
        "Curve selection requires careful attention to embedding degree - curves must have sufficiently high embedding degrees (12 or higher) to prevent MOV attacks that could reduce elliptic curve security to easier problems over finite fields"
      ],
      "controversial": []
    }
  },
  {
    "id": "general-2016-12-29-pos_design",
    "title": "[Mirror] A Proof of Stake Design Philosophy",
    "date": "2016-12-29",
    "category": "protocol",
    "url": "https://vitalik.eth.limo/general/2016/12/29/pos_design.html",
    "path": "general/2016/12/29/pos_design.html",
    "content": "- \n\n- \n\n  \n    \n\n  \n  \n  Dark Mode Toggle\n  \n    \n\n  \n\n##  [Mirror] A Proof of Stake Design Philosophy \n\n 2016 Dec 29 \nSee all posts\n\n \n \n\n [Mirror] A Proof of Stake Design Philosophy \n\nThis is a mirror of the post at\nhttps://medium.com/@VitalikButerin/a-proof-of-stake-design-philosophy-506585978d51\n\nSystems like Ethereum (and Bitcoin, and NXT, and Bitshares, etc) are\na fundamentally new class of cryptoeconomic organisms \u2014 decentralized,\njurisdictionless entities that exist entirely in cyberspace, maintained\nby a combination of cryptography, economics and social consensus. They\nare kind of like BitTorrent, but they are also not like BitTorrent, as\nBitTorrent has no concept of state \u2014 a distinction that turns out to be\ncrucially important. They are sometimes described as decentralized\nautonomous corporations, but they are also not quite corporations \u2014\nyou can't hard fork Microsoft. They are kind of like open source\nsoftware projects, but they are not quite that either \u2014 you can fork a\nblockchain, but not quite as easily as you can fork OpenOffice.\n\nThese cryptoeconomic networks come in many flavors \u2014 ASIC-based PoW,\nGPU-based PoW, naive PoS, delegated PoS, hopefully soon Casper PoS \u2014 and\neach of these flavors inevitably comes with its own underlying\nphilosophy. One well-known example is the maximalist vision of proof of\nwork, where \"the\" correct blockchain, singular, is defined as the chain\nthat miners have burned the largest amount of economic capital to\ncreate. Originally a mere in-protocol fork choice rule, this mechanism\nhas in many cases been elevated to a sacred tenet \u2014 see this\nTwitter discussion between myself and Chris DeRose for an example of\nsomeone seriously trying to defend the idea in a pure form, even in the\nface of hash-algorithm-changing protocol hard forks. Bitshares' delegated\nproof of stake presents another coherent philosophy, where\neverything once again flows from a single tenet, but one that can be\ndescribed even more simply: shareholders\nvote.\n\nEach of these philosophies; Nakamoto consensus, social consensus,\nshareholder voting consensus, leads to its own set of conclusions and\nleads to a system of values that makes quite a bit of sense when viewed\non its own terms \u2014 though they can certainly be criticized when compared\nagainst each other. Casper consensus has a philosophical underpinning\ntoo, though one that has so far not been as succinctly articulated.\n\nMyself, Vlad, Dominic, Jae and others all have their own views on why\nproof of stake protocols exist and how to design them, but here I intend\nto explain where I personally am coming from.\n\nI'll proceed to listing observations and then conclusions\ndirectly.\n\n- \n\nCryptography is truly special in the 21st century because\ncryptography is one of the very few fields where adversarial\nconflict continues to heavily favor the defender. Castles are\nfar easier to destroy than build, islands are defendable but can still\nbe attacked, but an average person's ECC keys are secure enough to\nresist even state-level actors. Cypherpunk philosophy is fundamentally\nabout leveraging this precious asymmetry to create a world that better\npreserves the autonomy of the individual, and cryptoeconomics is to some\nextent an extension of that, except this time protecting the safety and\nliveness of complex systems of coordination and collaboration, rather\nthan simply the integrity and confidentiality of private messages.\nSystems that consider themselves ideological heirs to the\ncypherpunk spirit should maintain this basic property, and be much more\nexpensive to destroy or disrupt than they are to use and\nmaintain.\n\n- \n\nThe \"cypherpunk spirit\" isn't just about idealism; making systems\nthat are easier to defend than they are to attack is also simply sound\nengineering.\n\n- \n\nOn medium to long time scales, humans are quite good at\nconsensus. Even if an adversary had access to unlimited hashing\npower, and came out with a 51% attack of any major blockchain that\nreverted even the last month of history, convincing the community that\nthis chain is legitimate is much harder than just outrunning the main\nchain's hashpower. They would need to subvert block explorers, every\ntrusted member in the community, the New York Times, archive.org, and\nmany other sources on the internet; all in all, convincing the world\nthat the new attack chain is the one that came first in the information\ntechnology-dense 21st century is about as hard as convincing the world\nthat the US moon landings never happened. These social\nconsiderations are what ultimately protect any blockchain in the long\nterm, regardless of whether or not the blockchain's community\nadmits it (note\nthat Bitcoin Core does\nadmit this primacy of the social layer).\n\n- \n\nHowever, a blockchain protected by social consensus alone would\nbe far too inefficient and slow, and too easy for disagreements to\ncontinue without end (though despite all difficulties, it\nhas happened); hence, economic consensus serves an extremely\nimportant role in protecting liveness and safety properties in the short\nterm.\n\n- \n\nBecause proof of work security can only come from block rewards\n(in Dominic Williams' terms, it lacks two\nof the three Es), and incentives to miners can only come from the\nrisk of them losing their future block rewards, proof of work\nnecessarily operates on a logic of massive power incentivized into\nexistence by massive rewards. Recovery from attacks in PoW is\nvery hard: the first time it happens, you can hard fork to change the\nPoW and thereby render the attacker's ASICs useless, but the second time\nyou no longer have that option, and so the attacker can attack again and\nagain. Hence, the size of the mining network has to be so large that\nattacks are inconceivable. Attackers of size less than X are discouraged\nfrom appearing by having the network constantly spend X every single\nday. I reject this logic because (i) it kills trees, and (ii) it fails\nto realize the cypherpunk spirit \u2014 cost of attack and cost of defense\nare at a 1:1 ratio, so there is no defender's\nadvantage.\n\n- \n\nProof of stake breaks this symmetry by relying not on\nrewards for security, but rather penalties. Validators put\nmoney (\"deposits\") at stake, are rewarded slightly to compensate them\nfor locking up their capital and maintaining nodes and taking extra\nprecaution to ensure their private key safety, but the bulk of the cost\nof reverting transactions comes from penalties that are hundreds or\nthousands of times larger than the rewards that they got in the\nmeantime. The \"one-sentence philosophy\" of proof of stake is\nthus not \"security comes from burning energy\", but rather \"security\ncomes from putting up economic value-at-loss\". A given block or\nstate has $X security if you can prove that achieving an equal level of\nfinalization for any conflicting block or state cannot be accomplished\nunless malicious nodes complicit in an attempt to make the switch pay $X\nworth of in-protocol penalties.\n\n- \n\nTheoretically, a majority collusion of validators may take over a\nproof of stake chain, and start acting maliciously. However, (i) through\nclever protocol design, their ability to earn extra profits through such\nmanipulation can be limited as much as possible, and more importantly\n(ii) if they try to prevent new validators from joining, or execute 51%\nattacks, then the community can simply coordinate a hard fork and delete\nthe offending validators' deposits. A successful attack may cost\n$50 million, but the process of cleaning up the consequences will not be\nthat much more onerous than the geth/parity\nconsensus failure of 2016.11.25. Two days later, the\nblockchain and community are back on track, attackers are $50 million\npoorer, and the rest of the community is likely richer since the attack\nwill have caused the value of the token to go up due to the\nensuing supply crunch. That's attack/defense asymmetry for\nyou.\n\n- \n\nThe above should not be taken to mean that unscheduled hard forks\nwill become a regular occurrence; if desired, the cost of a\nsingle 51% attack on proof of stake can certainly be set to be\nas high as the cost of a permanent 51% attack on proof of work,\nand the sheer cost and ineffectiveness of an attack should ensure that\nit is almost never attempted in practice.\n\n- \n\nEconomics is not everything. Individual actors\nmay be motivated by extra-protocol motives, they may get hacked, they\nmay get kidnapped, or they may simply get drunk and decide to wreck the\nblockchain one day and to hell with the cost. Furthermore, on the bright\nside, individuals' moral forbearances and communication\ninefficiencies will often raise the cost of an attack to levels much\nhigher than the nominal protocol-defined value-at-loss. This is\nan advantage that we cannot rely on, but at the same time it is an\nadvantage that we should not needlessly throw away.\n\n- \n\nHence, the best protocols are protocols that work well\nunder a variety of models and assumptions \u2014 economic\nrationality with coordinated choice, economic rationality with\nindividual choice, simple fault tolerance, Byzantine fault tolerance\n(ideally both the adaptive and non-adaptive adversary variants), Ariely/Kahneman-inspired\nbehavioral economic models (\"we all cheat just a little\") and\nideally any other model that's realistic and practical to reason about.\nIt is important to have both layers of defense: economic\nincentives to discourage centralized cartels from acting anti-socially,\nand anti-centralization incentives to discourage cartels from forming in\nthe first place.\n\n- \n\nConsensus protocols that work as-fast-as-possible have\nrisks and should be approached very carefully if at all,\nbecause if the possibility to be very fast is tied to\nincentives to do so, the combination will reward very high and\nsystemic-risk-inducing levels of network-level\ncentralization (eg. all validators running from the same\nhosting provider). Consensus protocols that don't care too much how fast\na validator sends a message, as long as they do so within some\nacceptably long time interval (eg. 4\u20138 seconds, as we empirically know\nthat latency in ethereum is usually ~500ms-1s) do not have these\nconcerns. A possible middle ground is creating protocols that can work\nvery quickly, but where mechanics similar to Ethereum's uncle mechanism\nensure that the marginal reward for a node increasing its degree of\nnetwork connectivity beyond some easily attainable point is fairly\nlow.\n\nFrom here, there are of course many details and many ways to diverge\non the details, but the above are the core principles that at least my\nversion of Casper is based on. From here, we can certainly debate\ntradeoffs between competing values . Do we give ETH a 1% annual issuance\nrate and get an $50 million cost of forcing a remedial hard fork, or a\nzero annual issuance rate and get a $5 million cost of forcing a\nremedial hard fork? When do we increase a protocol's security under the\neconomic model in exchange for decreasing its security under a fault\ntolerance model? Do we care more about having a predictable level of\nsecurity or a predictable level of issuance? These are all questions for\nanother post, and the various ways of implementing the\ndifferent tradeoffs between these values are questions for yet more\nposts. But we'll get to it :)",
    "contentLength": 11270,
    "summary": "Vitalik argues that Proof of Stake achieves defender's advantage by using economic penalties rather than energy rewards for security.",
    "detailedSummary": {
      "theme": "Vitalik outlines his philosophical framework for proof of stake consensus, emphasizing economic penalties over energy consumption to create defender-advantaged cryptoeconomic systems.",
      "summary": "Vitalik presents his design philosophy for Casper proof of stake, positioning it within the broader context of cryptoeconomic systems that exist as decentralized entities maintained by cryptography, economics, and social consensus. He argues that proof of work fundamentally fails to embody the cypherpunk spirit because it creates a 1:1 ratio between attack and defense costs, requiring massive energy expenditure to maintain security. In contrast, Vitalik's proof of stake philosophy centers on economic penalties rather than rewards - validators stake deposits and face penalties hundreds or thousands of times larger than their rewards if they act maliciously. This creates an asymmetric advantage for defenders, as successful attacks can be remedied through hard forks that delete malicious validators' deposits, making attackers significantly poorer while potentially enriching the remaining community through supply reduction. Vitalik emphasizes that the best protocols work under multiple models and assumptions, balancing economic incentives with anti-centralization mechanisms while avoiding designs that reward excessive speed at the cost of dangerous centralization.",
      "takeaways": [
        "Proof of stake achieves security through economic penalties rather than energy expenditure, creating a defender's advantage where attack costs far exceed defense costs",
        "Social consensus ultimately protects blockchains in the long term, but economic consensus is crucial for short-term liveness and safety",
        "Successful attacks on proof of stake can be remedied through hard forks that delete attackers' deposits, making attacks costly and ineffective",
        "The best consensus protocols should work under multiple models including economic rationality, fault tolerance, and behavioral economics",
        "Speed-optimized consensus protocols risk dangerous centralization and should be approached carefully or avoided entirely"
      ],
      "controversial": [
        "The assertion that proof of work 'kills trees' and lacks defender's advantage may be disputed by Bitcoin maximalists who view energy consumption as a feature, not a bug",
        "The reliance on hard forks to remedy attacks could be seen as undermining immutability, a core principle many consider essential to blockchain systems",
        "The claim that social consensus ultimately protects blockchains may be controversial among those who prefer purely algorithmic/economic security models"
      ]
    }
  },
  {
    "id": "general-2016-12-10-qap",
    "title": "[Mirror] Quadratic Arithmetic Programs: from Zero to Hero",
    "date": "2016-12-10",
    "category": "math",
    "url": "https://vitalik.eth.limo/general/2016/12/10/qap.html",
    "path": "general/2016/12/10/qap.html",
    "content": "- \n\n- \n\n  \n    \n\n  \n  \n  Dark Mode Toggle\n  \n    \n\n  \n\n##  [Mirror] Quadratic Arithmetic Programs: from Zero to Hero \n\n 2016 Dec 10 \nSee all posts\n\n \n \n\n [Mirror] Quadratic Arithmetic Programs: from Zero to Hero \n\nThis is a mirror of the post at\nhttps://medium.com/@VitalikButerin/quadratic-arithmetic-programs-from-zero-to-hero-f6d558cea649\n\nThere has been a lot of interest lately in the technology behind\nzk-SNARKs, and people are increasingly trying\nto demystify something that many have come to call \"moon math\" due\nto its perceived sheer indecipherable complexity. zk-SNARKs are indeed\nquite challenging to grasp, especially due to the sheer number of moving\nparts that need to come together for the whole thing to work, but if we\nbreak the technology down piece by piece then comprehending it becomes\nsimpler.\n\nThe purpose of this post is not to serve as a full introduction to\nzk-SNARKs; it assumes as background knowledge that (i) you know what\nzk-SNARKs are and what they do, and (ii) know enough math to be able to\nreason about things like polynomials (if the statement \\(P(x) + Q(x) = (P + Q)(x)\\) , where \\(P\\) and \\(Q\\) are polynomials, seems natural and\nobvious to you, then you're at the right level). Rather, the post digs\ndeeper into the machinery behind the technology, and tries to explain as\nwell as possible the first half of the pipeline, as drawn by zk-SNARK\nresearcher Eran Tromer here:\n\nThe steps here can be broken up into two halves. First, zk-SNARKs\ncannot be applied to any computational problem directly; rather, you\nhave to convert the problem into the right \"form\" for the problem to\noperate on. The form is called a \"quadratic arithmetic program\" (QAP),\nand transforming the code of a function into one of these is itself\nhighly nontrivial. Along with the process for converting the code of a\nfunction into a QAP is another process that can be run alongside so that\nif you have an input to the code you can create a corresponding solution\n(sometimes called \"witness\" to the QAP). After this, there is another\nfairly intricate process for creating the actual \"zero knowledge proof\"\nfor this witness, and a separate process for verifying a proof that\nsomeone else passes along to you, but these are details that are out of\nscope for this post.\n\nThe example that we will choose is a simple one: proving that you\nknow the solution to a cubic equation: \\(x^3 +\nx + 5 = 35\\) (hint: the answer is \\(3\\)). This problem is simple enough that\nthe resulting QAP will not be so large as to be intimidating, but\nnontrivial enough that you can see all of the machinery come into\nplay.\n\nLet us write out our function as follows:\n\ndef qeval(x):\n    y = x**3\n    return x + y + 5\n\nThe simple special-purpose programming language that we are using\nhere supports basic arithmetic (\\(+\\),\n\\(-\\), \\(\\cdot\\), \\(/\\)), constant-power exponentiation (\\(x^7\\) but not \\(x^y\\)) and variable assignment, which is\npowerful enough that you can theoretically do any computation inside of\nit (as long as the number of computational steps is bounded; no loops\nallowed). Note that modulo (%) and comparison operators (\\(<\\), \\(>\\), \\(\\leq\\), \\(\\geq\\)) are NOT supported, as there is no\nefficient way to do modulo or comparison directly in finite cyclic group\narithmetic (be thankful for this; if there was a way to do either one,\nthen elliptic curve cryptography would be broken faster than you can say\n\"binary search\" and \"Chinese remainder theorem\").\n\nYou can extend the language to modulo and comparisons by providing\nbit decompositions (eg. \\(13 = 2^3 + 2^2 +\n1\\)) as auxiliary inputs, proving correctness of those\ndecompositions and doing the math in binary circuits; in finite field\narithmetic, doing equality (==) checks is also doable and in fact a bit\neasier, but these are both details we won't get into right now. We can\nextend the language to support conditionals (eg. if \\(x < 5: y = 7;\\) else: \\(y = 9\\)) by converting them to an\narithmetic form: \\(y = 7 \\cdot (x < 5) + 9\n\\cdot (x \\geq 5)\\) though note that both \"paths\" of the\nconditional would need to be executed, and if you have many nested\nconditionals then this can lead to a large amount of overhead.\n\nLet us now go through this process step by step. If you want to do\nthis yourself for any piece of code, I implemented\na compiler here (for educational purposes only; not ready for making\nQAPs for real-world zk-SNARKs quite yet!).\n\n## Flattening\n\nThe first step is a \"flattening\" procedure, where we convert the\noriginal code, which may contain arbitrarily complex statements and\nexpressions, into a sequence of statements that are of two forms: \\(x = y\\) (where \\(y\\) can be a variable or a number) and\n\\(x = y\\) \\((op)\\) \\(z\\) (where \\(op\\) can be \\(+\\), \\(-\\), \\(\\cdot\\), \\(/\\) and \\(y\\) and \\(z\\) can be variables, numbers or themselves\nsub-expressions). You can think of each of these statements as being\nkind of like logic gates in a circuit. The result of the flattening\nprocess for the above code is as follows:\n\nsym_1 = x * x\ny = sym_1 * x\nsym_2 = y + x\n~out = sym_2 + 5\n\nIf you read the original code and the code here, you can fairly\neasily see that the two are equivalent.\n\n## Gates to R1CS\n\nNow, we convert this into something called a rank-1 constraint system\n(R1CS). An R1CS is a sequence of groups of three vectors (\\(a\\), \\(b\\), \\(c\\)), and the solution to an R1CS is a\nvector \\(s\\), where \\(s\\) must satisfy the equation \\(s . a \\cdot s . b - s . c = 0\\), where\n\\(.\\) represents the dot product - in\nsimpler terms, if we \"zip together\" \\(a\\) and \\(s\\), multiplying the two values in the same\npositions, and then take the sum of these products, then do the same to\n\\(b\\) and \\(s\\) and then \\(c\\) and \\(s\\), then the third result equals the\nproduct of the first two results. For example, this is a satisfied\nR1CS:\n\nBut instead of having just one constraint, we are going to have many\nconstraints: one for each logic gate. There is a standard way of\nconverting a logic gate into a \\((a, b,\nc)\\) triple depending on what the operation is (\\(+\\), \\(-\\), \\(\\cdot\\) or \\(/\\)) and whether the arguments are\nvariables or numbers. The length of each vector is equal to the total\nnumber of variables in the system, including a dummy variable ~one at\nthe first index representing the number \\(1\\), the input variables, a dummy variable\n~out representing the output, and then all of the intermediate variables\n(\\(sym_1\\) and \\(sym_2\\) above); the vectors are generally\ngoing to be very sparse, only filling in the slots corresponding to the\nvariables that are affected by some particular logic gate.\n\nFirst, we'll provide the variable mapping that we'll use:\n\n'~one', 'x', '~out', 'sym_1', 'y', 'sym_2'\n\nThe solution vector will consist of assignments for all of these\nvariables, in that order.\n\nNow, we'll give the \\((a, b, c)\\)\ntriple for the first gate:\n\na = [0, 1, 0, 0, 0, 0]\nb = [0, 1, 0, 0, 0, 0]\nc = [0, 0, 0, 1, 0, 0]\n\nYou can see that if the solution vector contains \\(3\\) in the second position, and \\(9\\) in the fourth position, then regardless\nof the rest of the contents of the solution vector, the dot product\ncheck will boil down to \\(3 \\cdot 3 =\n9\\), and so it will pass. If the solution vector has \\(-3\\) in the second position and \\(9\\) in the fourth position, the check will\nalso pass; in fact, if the solution vector has \\(7\\) in the second position and \\(49\\) in the fourth position then that check\nwill still pass \u2014 the purpose of this first check is to verify the\nconsistency of the inputs and outputs of the first gate only.\n\nNow, let's go on to the second gate:\n\na = [0, 0, 0, 1, 0, 0]\nb = [0, 1, 0, 0, 0, 0]\nc = [0, 0, 0, 0, 1, 0]\n\nIn a similar style to the first dot product check, here we're\nchecking that \\(sym_1 \\cdot x =\ny\\).\n\nNow, the third gate:\n\na = [0, 1, 0, 0, 1, 0]\nb = [1, 0, 0, 0, 0, 0]\nc = [0, 0, 0, 0, 0, 1]\n\nHere, the pattern is somewhat different: it's multiplying the first\nelement in the solution vector by the second element, then by the fifth\nelement, adding the two results, and checking if the sum equals the\nsixth element. Because the first element in the solution vector is\nalways one, this is just an addition check, checking that the output\nequals the sum of the two inputs.\n\nFinally, the fourth gate:\n\na = [5, 0, 0, 0, 0, 1]\nb = [1, 0, 0, 0, 0, 0]\nc = [0, 0, 1, 0, 0, 0]\n\nHere, we're evaluating the last check, ~out \\(= sym_2 + 5\\). The dot product check works\nby taking the sixth element in the solution vector, adding five times\nthe first element (reminder: the first element is \\(1\\), so this effectively means adding \\(5\\)), and checking it against the third\nelement, which is where we store the output variable.\n\nAnd there we have our R1CS with four constraints. The witness is\nsimply the assignment to all the variables, including input, output and\ninternal variables:\n\n[1, 3, 35, 9, 27, 30]\n\nYou can compute this for yourself by simply \"executing\" the flattened\ncode above, starting off with the input variable assignment \\(x=3\\), and putting in the values of all the\nintermediate variables and the output as you compute them.\n\nThe complete R1CS put together is:\n\nA\n[0, 1, 0, 0, 0, 0]\n[0, 0, 0, 1, 0, 0]\n[0, 1, 0, 0, 1, 0]\n[5, 0, 0, 0, 0, 1]\n\nB\n[0, 1, 0, 0, 0, 0]\n[0, 1, 0, 0, 0, 0]\n[1, 0, 0, 0, 0, 0]\n[1, 0, 0, 0, 0, 0]\n\nC\n[0, 0, 0, 1, 0, 0]\n[0, 0, 0, 0, 1, 0]\n[0, 0, 0, 0, 0, 1]\n[0, 0, 1, 0, 0, 0]\n\n## R1CS to QAP\n\nThe next step is taking this R1CS and converting it into QAP form,\nwhich implements the exact same logic except using polynomials instead\nof dot products. We do this as follows. We go 3from four groups of three\nvectors of length six to six groups of three degree-3 polynomials, where\nevaluating the polynomials at each x coordinate represents one\nof the constraints. That is, if we evaluate the polynomials at \\(x=1\\), then we get our first set of\nvectors, if we evaluate the polynomials at \\(x=2\\), then we get our second set of\nvectors, and so on.\n\nWe can make this transformation using something called a Lagrange\ninterpolation. The problem that a Lagrange interpolation solves is\nthis: if you have a set of points (ie. \\((x,\ny)\\) coordinate pairs), then doing a Lagrange interpolation on\nthose points gives you a polynomial that passes through all of those\npoints. We do this by decomposing the problem: for each \\(x\\) coordinate, we create a polynomial that\nhas the desired \\(y\\) coordinate at\nthat \\(x\\) coordinate and a \\(y\\) coordinate of \\(0\\) at all the other \\(x\\) coordinates we are interested in, and\nthen to get the final result we add all of the polynomials together.\n\nLet's do an example. Suppose that we want a polynomial that passes\nthrough \\((1, 3), (2, 2)\\) and \\((3, 4)\\). We start off by making a\npolynomial that passes through \\((1, 3), (2,\n0)\\) and \\((3, 0)\\). As it turns\nout, making a polynomial that \"sticks out\" at \\(x=1\\) and is zero at the other points of\ninterest is easy; we simply do:\n\n(x - 2) * (x - 3)\n\nWhich looks like this:\n\nNow, we just need to \"rescale\" it so that the height at x=1 is\nright:\n\n(x - 2) * (x - 3) * 3 / ((1 - 2) * (1 - 3))\n\nThis gives us:\n\n1.5 * x**2 - 7.5 * x + 9\n\nWe then do the same with the other two points, and get two other\nsimilar-looking polynomials, except that they \"stick out\" at \\(x=2\\) and \\(x=3\\) instead of \\(x=1\\). We add all three together and\nget:\n\n1.5 * x**2 - 5.5 * x + 7\n\nWith exactly the coordinates that we want. The algorithm as described\nabove takes \\(O(n^3)\\) time, as there\nare \\(n\\) points and each point\nrequires \\(O(n^2)\\) time to multiply\nthe polynomials together; with a little thinking, this can be reduced to\n\\(O(n^2)\\) time, and with a lot more\nthinking, using fast Fourier transform algorithms and the like, it can\nbe reduced even further \u2014 a crucial optimization when functions that get\nused in zk-SNARKs in practice often have many thousands of gates.\n\nNow, let's use Lagrange interpolation to transform our R1CS. What we\nare going to do is take the first value out of every \\(a\\) vector, use Lagrange interpolation to\nmake a polynomial out of that (where evaluating the polynomial at \\(i\\) gets you the first value of the \\(i\\)th \\(a\\) vector), repeat the process for the\nfirst value of every \\(b\\) and \\(c\\) vector, and then repeat that process\nfor the second values, the third, values, and so on. For convenience\nI'll provide the answers right now:\n\nA polynomials\n[-5.0, 9.166, -5.0, 0.833]\n[8.0, -11.333, 5.0, -0.666]\n[0.0, 0.0, 0.0, 0.0]\n[-6.0, 9.5, -4.0, 0.5]\n[4.0, -7.0, 3.5, -0.5]\n[-1.0, 1.833, -1.0, 0.166]\n\nB polynomials\n[3.0, -5.166, 2.5, -0.333]\n[-2.0, 5.166, -2.5, 0.333]\n[0.0, 0.0, 0.0, 0.0]\n[0.0, 0.0, 0.0, 0.0]\n[0.0, 0.0, 0.0, 0.0]\n[0.0, 0.0, 0.0, 0.0]\n\nC polynomials\n[0.0, 0.0, 0.0, 0.0]\n[0.0, 0.0, 0.0, 0.0]\n[-1.0, 1.833, -1.0, 0.166]\n[4.0, -4.333, 1.5, -0.166]\n[-6.0, 9.5, -4.0, 0.5]\n[4.0, -7.0, 3.5, -0.5]\n\nCoefficients are in ascending order, so the first polynomial above is\nactually \\(0.833 \\cdot x^3 \u2014 5 \\cdot x^2 +\n9.166 \\cdot x - 5\\). This set of polynomials (plus a Z polynomial\nthat I will explain later) makes up the parameters for this particular\nQAP instance. Note that all of the work up until this point needs to be\ndone only once for every function that you are trying to use zk-SNARKs\nto verify; once the QAP parameters are generated, they can be\nreused.\n\nLet's try evaluating all of these polynomials at \\(x=1\\). Evaluating a polynomial at \\(x=1\\) simply means adding up all the\ncoefficients (as \\(1^k = 1\\) for all\n\\(k\\)), so it's not difficult. We\nget:\n\nA results at x=1\n0\n1\n0\n0\n0\n0\n\nB results at x=1\n0\n1\n0\n0\n0\n0\n\nC results at x=1\n0\n0\n0\n1\n0\n0\n\nAnd lo and behold, what we have here is exactly the same as the set\nof three vectors for the first logic gate that we created above.\n\n## Checking the QAP\n\nNow what's the point of this crazy transformation? The answer is that\ninstead of checking the constraints in the R1CS individually, we can now\ncheck all of the constraints at the same time by doing the dot\nproduct check on the polynomials.\n\nBecause in this case the dot product check is a series of additions\nand multiplications of polynomials, the result is itself going to be a\npolynomial. If the resulting polynomial, evaluated at every \\(x\\) coordinate that we used above to\nrepresent a logic gate, is equal to zero, then that means that all of\nthe checks pass; if the resulting polynomial evaluated at at least one\nof the \\(x\\) coordinate representing a\nlogic gate gives a nonzero value, then that means that the values going\ninto and out of that logic gate are inconsistent (ie. the gate is \\(y = x \\cdot sym_1\\) but the provided values\nmight be \\(x = 2,sym_1 = 2\\) and \\(y = 5\\)).\n\nNote that the resulting polynomial does not itself have to be zero,\nand in fact in most cases won't be; it could have any behavior at the\npoints that don't correspond to any logic gates, as long as the result\nis zero at all the points that do correspond to some gate. To\ncheck correctness, we don't actually evaluate the polynomial \\(t = A . s \\cdot B . s - C . s\\) at every\npoint corresponding to a gate; instead, we divide \\(t\\) by another polynomial, \\(Z\\), and check that \\(Z\\) evenly divides \\(t\\) - that is, the division \\(t / Z\\) leaves no remainder.\n\n\\(Z\\) is defined as \\((x - 1) \\cdot (x - 2) \\cdot (x - 3) ...\\) -\nthe simplest polynomial that is equal to zero at all points that\ncorrespond to logic gates. It is an elementary fact of algebra that\nany polynomial that is equal to zero at all of these points has\nto be a multiple of this minimal polynomial, and if a polynomial is a\nmultiple of \\(Z\\) then its evaluation\nat any of those points will be zero; this equivalence makes our job much\neasier.\n\nNow, let's actually do the dot product check with the polynomials\nabove. First, the intermediate polynomials:\n\nA . s = [43.0, -73.333, 38.5, -5.166]\nB . s = [-3.0, 10.333, -5.0, 0.666]\nC . s = [-41.0, 71.666, -24.5, 2.833]\n\nNow, \\(A . s \\cdot B . s \u2014 C .\ns\\):\n\nt = [-88.0, 592.666, -1063.777, 805.833, -294.777, 51.5, -3.444]\n\nNow, the minimal polynomial \\(Z = (x - 1)\n\\cdot (x - 2) \\cdot (x - 3) \\cdot (x - 4)\\):\n\nZ = [24, -50, 35, -10, 1]\n\nAnd if we divide the result above by \\(Z\\), we get:\n\nh = t / Z = [-3.666, 17.055, -3.444]\n\nWith no remainder.\n\nAnd so we have the solution for the QAP. If we try to falsify any of\nthe variables in the R1CS solution that we are deriving this QAP\nsolution from \u2014 say, set the last one to \\(31\\) instead of \\(30\\), then we get a \\(t\\) polynomial that fails one of the checks\n(in that particular case, the result at \\(x=3\\) would equal \\(-1\\) instead of \\(0\\)), and furthermore \\(t\\) would not be a multiple of \\(Z\\); rather, dividing \\(t / Z\\) would give a remainder of \\([-5.0, 8.833, -4.5, 0.666]\\).\n\nNote that the above is a simplification; \"in the real world\", the\naddition, multiplication, subtraction and division will happen not with\nregular numbers, but rather with finite field elements \u2014 a spooky kind\nof arithmetic which is self-consistent, so all the algebraic laws we\nknow and love still hold true, but where all answers are elements of\nsome finite-sized set, usually integers within the range from \\(0\\) to \\(n-1\\) for some \\(n\\). For example, if \\(n = 13\\), then \\(1 / 2 = 7\\) (and \\(7 \\cdot 2 = 1), 3 \\cdot 5 = 2\\), and so\nforth. Using finite field arithmetic removes the need to worry about\nrounding errors and allows the system to work nicely with elliptic\ncurves, which end up being necessary for the rest of the zk-SNARK\nmachinery that makes the zk-SNARK protocol actually secure.\n\nSpecial thanks to Eran Tromer for helping to explain many details\nabout the inner workings of zk-SNARKs to me.",
    "contentLength": 17539,
    "summary": "This technical blog post explains how to convert code into Quadratic Arithmetic Programs (QAPs), the first step in zk-SNARK proofs.",
    "detailedSummary": {
      "theme": "Vitalik provides a detailed technical explanation of how to convert computational problems into Quadratic Arithmetic Programs (QAPs), which is the first crucial step in creating zk-SNARKs.",
      "summary": "Vitalik tackles the complex mathematics behind zk-SNARKs by focusing specifically on the first half of the zk-SNARK pipeline: converting computational problems into Quadratic Arithmetic Programs (QAPs). Using a simple cubic equation example (x\u00b3 + x + 5 = 35), Vitalik walks through the multi-step process of flattening code into basic operations, converting these operations into a Rank-1 Constraint System (R1CS), and finally transforming the R1CS into polynomial form using Lagrange interpolation. The post demonstrates how this transformation allows verification of all computational constraints simultaneously through polynomial arithmetic rather than checking each constraint individually. Vitalik emphasizes that while zk-SNARKs appear intimidatingly complex ('moon math'), breaking down the process step-by-step makes the underlying machinery comprehensible, though he acknowledges this covers only the first half of the complete zk-SNARK pipeline.",
      "takeaways": [
        "zk-SNARKs require converting computational problems into a specific mathematical form called Quadratic Arithmetic Programs (QAPs) before cryptographic proofs can be generated",
        "The conversion process involves three main steps: flattening code into basic operations, creating a Rank-1 Constraint System (R1CS), and transforming constraints into polynomial form using Lagrange interpolation",
        "The programming language for zk-SNARKs is limited to basic arithmetic operations and cannot efficiently handle modulo, comparison operators, or loops due to finite field arithmetic constraints",
        "Converting from R1CS to QAP allows checking all computational constraints simultaneously through polynomial division rather than verifying each constraint individually",
        "The mathematical operations in real-world implementations use finite field arithmetic rather than regular numbers, which eliminates rounding errors and enables compatibility with elliptic curve cryptography"
      ],
      "controversial": []
    }
  }
]